{"post_id": "15433426", "sentence": "We can use pd.melt to make the hour columns into one column with that value :", "API": [["pd.melt", [11, 18]]], "LVCDE": [], "RCDE": [["that value", [65, 75], "hourly values from the dataframe columns"]]}
{"post_id": "16102866", "sentence": "So the _slice() slice it by default axis 0 .", "API": [["_slice()", [7, 15]]], "LVCDE": [], "RCDE": [["it", [22, 24], "DataFrame"]]}
{"post_id": "22118042", "sentence": "StandardScaler transforms all of your features into Mean-0-Std-1 features .", "API": [["StandardScaler", [0, 14]]], "LVCDE": [], "RCDE": [["your features", [33, 46], "TF-IDF Features from WebsiteText, AlexisRank, GooglePageRank."]]}
{"post_id": "2535654", "sentence": "The numbers you extract from d are all turned into float and applied using numpy.vectorize .", "API": [["numpy.vectorize", [75, 90]]], "LVCDE": [["d", [29, 30], "an array of numbers ranging from 0 to 9/d = np.arange(10.0)"]], "RCDE": []}
{"post_id": "4002456", "sentence": "The basic random numbers in scipy / numpy are created by Mersenne-Twister PRNG in numpy.random .\n", "API": [["numpy.random", [82, 94]]], "LVCDE": [], "RCDE": []}
{"post_id": "17699049", "sentence": "If you want zeros instead of NaN's as in your example , you can use fillna :", "API": [["fillna", [68, 74]]], "LVCDE": [], "RCDE": [["your example", [41, 53], "Convert DataFrame column values Into new Dataframe indices and columns with zeros replacing NaNs"]]}
{"post_id": "4625132", "sentence": "You could also smooth your array before this step using numpy.convolve() .", "API": [["numpy.convolve()", [56, 72]]], "LVCDE": [], "RCDE": [["your array", [22, 32], "the 1D numpy array"], ["this step", [40, 49], "looking for all entries in the 1d array smaller than their neighbors"]]}
{"post_id": "14137002", "sentence": "You can construct such an estimator using the OneVsRestClassifier wrapper :", "API": [["OneVsRestClassifier", [46, 65]]], "LVCDE": [], "RCDE": [["such an estimator", [18, 35], "An estimator that handles multiple classifications"]]}
{"post_id": "13086305", "sentence": "df.head(n) returns a DataFrame holding the first n rows of df .", "API": [["df.head(n)", [0, 10]], ["DataFrame", [21, 30]]], "LVCDE": [["df", [59, 61], "the DataFrame"]], "RCDE": []}
{"post_id": "9994484", "sentence": "Typically , you'd use numpy.polyfit to fit a line to your data , but in this case you'll need to do use numpy.linalg.lstsq directly , as you want to set the intercept to zero .", "API": [["numpy.polyfit", [22, 35]], ["numpy.linalg.lstsq", [104, 122]]], "LVCDE": [], "RCDE": [["your data", [53, 62], "some more or less linear data"], ["this case", [72, 81], "forcing the y-intercept to be zero"]]}
{"post_id": "5484165", "sentence": "To fit a parabola to those points , use numpy.polyfit() :", "API": [["numpy.polyfit()", [40, 55]]], "LVCDE": [], "RCDE": [["those points", [21, 33], "data points defined by two numpy arrays: x = numpy.array([0.0, 1.0, 2.0, 3.0]), y = numpy.array([3.6, 1.3, 0.2, 0.9])"]]}
{"post_id": "15829494", "sentence": "Here I use pandas.factorize() to convert the page_id to an array in range 0 and N . where N is the unique count of elements in page_id .", "API": [["pandas.factorize()", [11, 29]]], "LVCDE": [["page_id", [45, 52], "a Pandas Series containing page identifiers"], ["page_id", [127, 134], "a Pandas Series containing page identifiers"]], "RCDE": []}
{"post_id": "6032083", "sentence": "Use p.set_clim([5,50]) to set the color scaling minimums and maximums in the case of your example .", "API": [["p.set_clim([5,50])", [4, 22]]], "LVCDE": [], "RCDE": [["your example", [85, 97], "setting manual color range for PatchCollection plotting"]]}
{"post_id": "4523180", "sentence": "a.tofile() and numpy.fromfile() would work as well , but don't save any metadata .\n", "API": [["a.tofile()", [0, 10]], ["numpy.fromfile()", [15, 31]]], "LVCDE": [], "RCDE": []}
{"post_id": "15144847", "sentence": "We can modify those rows of df where idx is True by assigning to df.loc[idx,...] .", "API": [["df.loc[idx,...]", [65, 80]]], "LVCDE": [["idx", [37, 40], "a conditional index that identifies rows in the DataFrame."]], "RCDE": [["those rows of df", [14, 30], "rows of pandas.DataFrame"]]}
{"post_id": "1614065", "sentence": "Note that np.intersect1d gives the wrong answer if a or b have nonunique elements .", "API": [["np.intersect1d", [10, 24]]], "LVCDE": [["a", [51, 52], "numpy array"], ["b", [56, 57], "numpy array"]], "RCDE": []}
{"post_id": "8055823", "sentence": "You could smooth your data with a gaussian_filter :", "API": [["gaussian_filter", [34, 49]]], "LVCDE": [], "RCDE": [["your data", [17, 26], "a multidimensional array that is 12 long about 2000 wide"]]}
{"post_id": "17109187", "sentence": "I don't think pandas offers a way to do this in read_csv .", "API": [["read_csv", [48, 56]]], "LVCDE": [], "RCDE": [["this", [40, 44], "efficiently reading the last few lines of a large CSV file without loading the entire file into memory"]]}
{"post_id": "1969296", "sentence": "As noted by ~unutbu , numpy.interp is also an option ( with less dependencies ):\n", "API": [["numpy.interp", [22, 34]]], "LVCDE": [], "RCDE": []}
{"post_id": "4266645", "sentence": "scipy.stats.rv_discrete might be what you want .", "API": [["scipy.stats.rv_discrete", [0, 23]]], "LVCDE": [], "RCDE": [["what you want", [33, 46], "Generate random numbers with a given (numerical) distribution"]]}
{"post_id": "6243689", "sentence": "So a solution is to use x.max() or print numpy.max(x) in the second line .\n", "API": [["x.max()", [24, 31]], ["numpy.max(x)", [41, 53]]], "LVCDE": [], "RCDE": []}
{"post_id": "3718712", "sentence": "You'll probably want to use the various functions in matplotlib.dates , plot_date to plot your values , imshow ( and / or pcolor in some cases ) to plot your specgrams of various sorts , and matplotlib.mlab.specgram to compute them .", "API": [["matplotlib.dates", [53, 69]], ["plot_date", [72, 81]], ["imshow", [104, 110]], ["pcolor", [122, 128]], ["matplotlib.mlab.specgram", [191, 215]]], "LVCDE": [], "RCDE": [["your values", [90, 101], "the time-series data points you want to plot"], ["your specgrams", [153, 167], "the spectrograms you generate from your data."], ["them", [227, 231], "the spectrograms you generate from your data."]]}
{"post_id": "4046233", "sentence": "One way to automatically do this is the bbox_inches='tight ' kwarg to plt.savefig .", "API": [["plt.savefig", [70, 81]]], "LVCDE": [], "RCDE": [["this", [28, 32], "Reduce left and right margins in matplotlib plot"]]}
{"post_id": "13839029", "sentence": "As commented , in newer pandas , Series has a replace method to do this more elegantly :", "API": [["Series", [33, 39]]], "LVCDE": [], "RCDE": [["this", [67, 71], "custom sort using a dictionary in pandas dataframe"]]}
{"post_id": "13216688", "sentence": "This can be accomplished quite simply with the DataFrame method apply .", "API": [["apply", [64, 69]]], "LVCDE": [], "RCDE": [["This", [0, 4], "multiply all the columns of a Pandas DataFrame by a column vector stored in a Series"]]}
{"post_id": "8218887", "sentence": "To get a feel of how to play with this sort of things , read through matplotlib's documentation , particularly on the subject of Axes , Axis and Artist .", "API": [["Axes", [129, 133]], ["Axis", [136, 140]], ["Artist", [145, 151]]], "LVCDE": [], "RCDE": [["this sort of things", [34, 53], "Various manipulations that can be performed on Matplotlib figures to adjust their size."]]}
{"post_id": "17070356", "sentence": "It has a DataFrame object which can hold your data and allow you manipulate in an intuitive way .", "API": [["DataFrame", [9, 18]]], "LVCDE": [], "RCDE": [["It", [0, 2], "pandas"], ["your data", [41, 50], "the values in the CSV file"]]}
{"post_id": "5162153", "sentence": "numpy.savez or numpy.savez_compressed is the way to go .\n", "API": [["numpy.savez", [0, 11]], ["numpy.savez_compressed", [15, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "17115229", "sentence": "As @Jeff pointed out in the comments , in pandas versions < 0.11 . 1 , manually tack .convert_objects() onto the end to properly convert tesst and set to int64 columns , in case that matters in subsequent operations .", "API": [[".convert_objects()", [85, 103]]], "LVCDE": [["tesst", [137, 142], "The column named \u201ctesst\u201d in the dataframe."]], "RCDE": []}
{"post_id": "5344981", "sentence": "I've come to the conclusion that numpy.tensordot() is not the best solution here .\n", "API": [["numpy.tensordot()", [33, 50]]], "LVCDE": [], "RCDE": []}
{"post_id": "4523180", "sentence": "a.tofile() and numpy.fromfile() would work as well , but don't save any metadata .\n", "API": [["a.tofile()", [0, 10]], ["numpy.fromfile()", [15, 31]]], "LVCDE": [], "RCDE": []}
{"post_id": "21423342", "sentence": "The fit function will interpret it as containing 150 training examples , each of which consists of four values .", "API": [["fit", [4, 7]]], "LVCDE": [], "RCDE": [["it", [32, 34], "data matrix with shape (150,4)"]]}
{"post_id": "3532408", "sentence": "To demonstrate this , look at the output of ax.get_xticks() .", "API": [["ax.get_xticks()", [44, 59]]], "LVCDE": [], "RCDE": [["this", [15, 19], "xticks extend outside of the displayed figure when using matshow"]]}
{"post_id": "4366379", "sentence": "The reshape() actually copies the data in this case ( see the comments ) .", "API": [["reshape()", [4, 13]]], "LVCDE": [], "RCDE": [["this case", [42, 51], "transforming a one-dimensional array into a two-dimensional Fortran-ordered matrix"]]}
{"post_id": "3443640", "sentence": "numpy.corrcoef should do just what you want , and it's a lot faster .", "API": [["numpy.corrcoef", [0, 14]]], "LVCDE": [], "RCDE": [["what you want", [30, 43], "print the correlation coefficient between each row in the matrix"]]}
{"post_id": "13446268", "sentence": "But you can use .shift to shift it by any number of days ( or any frequency for that matter ):", "API": [[".shift", [16, 22]]], "LVCDE": [], "RCDE": [["it", [32, 34], "the time series or date range generated using freq='M'"]]}
{"post_id": "13216292", "sentence": "This can be done using the new append option in the set_index method .", "API": [["set_index", [52, 61]]], "LVCDE": [], "RCDE": [["This", [0, 4], "append a new column into the DataFrame"]]}
{"post_id": "15709354", "sentence": "As @DSM points out , you can do [13] - [15] much nicer using pivot_table :", "API": [["pivot_table", [61, 72]]], "LVCDE": [["[13]", [32, 36], "The step where you set a multi-level index on the DataFrame/df[['symbol', 'signed_shares']].set_index('symbol', append=True)"], ["[15]", [39, 43], "The step where you view the reshaped DataFrame after unstacking it./a"]], "RCDE": []}
{"post_id": "13839029", "sentence": "As commented , in newer pandas , Series has a replace method to do this more elegantly :", "API": [["Series", [33, 39]], ["replace", [46, 53]]], "LVCDE": [], "RCDE": [["this", [67, 71], "custom sort using a dictionary in pandas dataframe"]]}
{"post_id": "5267083", "sentence": "there is a specific numpy function to do this , np.searchsorted , which is much faster than bisect .", "API": [["np.searchsorted", [48, 63]], ["bisect", [92, 98]]], "LVCDE": [], "RCDE": [["this", [41, 45], "the process of locating values within a sorted numpy array up to a given threshold via the numpy function np.searchsorted."]]}
{"post_id": "15582359", "sentence": "to calculate the left outer join , use numpy.setdiff1d() to find all the rows of df_a that not in the inner join :", "API": [["numpy.setdiff1d()", [39, 56]]], "LVCDE": [["df_a", [81, 85], "DataFrame/df_a = pd.DataFrame([{\"a\": 1, \"b\": 4}, {\"a\": 2, \"b\": 5}, {\"a\": 3, \"b\": 6}])"]], "RCDE": []}
{"post_id": "15829494", "sentence": "You can also use a dict to cache the last_score of every page_id without using pandas.factorize() .", "API": [["pandas.factorize()", [79, 97]]], "LVCDE": [["last_score", [37, 47], "the most recent score of a page_id before the current date/last_score = np.zeros(np.max(page_id)+1, dtype=np.int64)"], ["page_id", [57, 64], "an identifier for a specific page within the document"]], "RCDE": []}
{"post_id": "17134750", "sentence": "The easiest way is to use to_datetime :\n", "API": [["to_datetime", [26, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "10730365", "sentence": "Check out p.read_csv() for reading in your data .", "API": [["p.read_csv()", [10, 22]]], "LVCDE": [], "RCDE": [["your data", [38, 47], "electricity consumption data for a full year whose measurements in between the 15-minute steps"]]}
{"post_id": "5941450", "sentence": "The general function is numpy.apply_along_axis() :\n", "API": [["numpy.apply_along_axis()", [24, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "9016570", "sentence": "Alternatively , the sparse matrix classes own dot method will also work :\n", "API": [["dot", [46, 49]]], "LVCDE": [], "RCDE": []}
{"post_id": "17481681", "sentence": "Can do that when you use the read_csv method .", "API": [["read_csv", [29, 37]]], "LVCDE": [], "RCDE": [["that", [7, 11], "change all occurrences of -99.9 to \"NaN\"."]]}
{"post_id": "4046233", "sentence": "Another way is to use fig.tight_layout()\n", "API": [["fig.tight_layout()", [22, 40]]], "LVCDE": [], "RCDE": []}
{"post_id": "12417434", "sentence": "read_table correctly identifies them as float64 :", "API": [["read_table", [0, 10]], ["float64", [40, 47]]], "LVCDE": [], "RCDE": [["them", [32, 36], "strings representing a value in scientific notation"]]}
{"post_id": "20319282", "sentence": "Following up on them suggests that scipy.spatial.distance.pdist is the way to go .", "API": [["scipy.spatial.distance.pdist", [35, 63]]], "LVCDE": [], "RCDE": [["them", [16, 20], "Hints provided by previous answers"]]}
{"post_id": "21423342", "sentence": "The fit function itself doesn't support anything like that .", "API": [["fit", [4, 7]]], "LVCDE": [], "RCDE": [["that", [54, 58], "directly taking feature names as part of inputs for the fit function"]]}
{"post_id": "8482798", "sentence": "One solution would be to use the plt.legend function , even if you don't want an actual legend .\n", "API": [["plt.legend", [33, 43]]], "LVCDE": [], "RCDE": []}
{"post_id": "17699049", "sentence": "You can use the pivot method for this .", "API": [["pivot", [16, 21]]], "LVCDE": [], "RCDE": [["this", [33, 37], "Convert DataFrame Column Values Into New Dataframe Indices and Columns"]]}
{"post_id": "8218887", "sentence": "To get a feel of how to play with this sort of things , read through matplotlib's documentation , particularly on the subject of Axes , Axis and Artist .", "API": [["Axes", [129, 133]], ["Axis", [136, 140]], ["Artist", [145, 151]]], "LVCDE": [], "RCDE": [["this sort of things", [34, 53], "Various manipulations that can be performed on Matplotlib figures to adjust their size."]]}
{"post_id": "14137002", "sentence": "You should convert the Y I gave above to a matrix with a MultiLabelBinarizer :", "API": [["MultiLabelBinarizer", [57, 76]]], "LVCDE": [["Y", [23, 24], "Y = [[\"foo\", \"bar\"], [\"foo\"], [\"bar\", \"baz\"]]"]], "RCDE": []}
{"post_id": "13216688", "sentence": "This could be done more concisely by defining the anonymous function inside apply", "API": [["apply", [76, 81]]], "LVCDE": [], "RCDE": [["This", [0, 4], "multiply all the columns of a Pandas DataFrame by a column vector stored in a Series"]]}
{"post_id": "5344981", "sentence": "numpy.tensordot() is the right way to do it :", "API": [["numpy.tensordot()", [0, 17]]], "LVCDE": [], "RCDE": [["it", [41, 43], "A loopless 3D matrix multiplication in python"]]}
{"post_id": "14375841", "sentence": "and if you need you can convert it to a dict using to_dict() method :", "API": [["to_dict()", [51, 60]]], "LVCDE": [], "RCDE": [["it", [32, 34], "pandas.DataFrame"]]}
{"post_id": "22264337", "sentence": "To get it's underlying numpy array , simply use AllAlexaAndGoogleInfo.values .", "API": [["AllAlexaAndGoogleInfo.values", [48, 76]]], "LVCDE": [], "RCDE": [["it", [7, 9], "DataFrame named AllAlexaAndGoogleInfo"]]}
{"post_id": "2054655", "sentence": "Python 3.1 has a collections.OrderedDict class that can be used for this purpose .", "API": [["collections.OrderedDict", [17, 40]]], "LVCDE": [], "RCDE": [["this purpose", [68, 80], "ensuring the iteration order of the dictionaries to be the same if no elements are added or removed from the dictionaries between the first and the last iteration"]]}
{"post_id": "13652027", "sentence": "I didn't find a straight-forward way to do it within context of read_csv .", "API": [["read_csv", [64, 72]]], "LVCDE": [], "RCDE": [["it", [43, 45], "filter which lines of a CSV to be loaded into memory using pandas"]]}
{"post_id": "5927270", "sentence": "The hist() function returns the information you are looking for :", "API": [["hist()", [4, 10]]], "LVCDE": [], "RCDE": [["the information you are looking for", [28, 63], "the bar height corresponding to a given value on the x-axis of the histogram."]]}
{"post_id": "6163403", "sentence": "An alternative to this is to use numpy.histogram() :", "API": [["numpy.histogram()", [33, 50]]], "LVCDE": [], "RCDE": [["this", [18, 22], "take an average of an array in prespecified bins"]]}
{"post_id": "14283678", "sentence": "A solution with pandas built-in df.combine_first :\n", "API": [["df.combine_first", [32, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "7821917", "sentence": "One way is to use fig.canvas.tostring_rgb and then numpy.fromstring with the approriate dtype .\n", "API": [["fig.canvas.tostring_rgb", [18, 41]], ["numpy.fromstring", [51, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "14822400", "sentence": "To do this , read_csv ( and most similar functions ) have an na_values argument :", "API": [["read_csv", [13, 21]]], "LVCDE": [], "RCDE": [["this", [6, 10], "removing special values when reading data"]]}
{"post_id": "5902371", "sentence": "The general method would be to use set_xticks and set_xticklabels , but I'd like something that can handle time scales from a few hours out to a few years ( this means involving the major and minor ticks to make things readable I think ) .\n", "API": [["set_xticks", [35, 45]], ["set_xticklabels", [50, 65]]], "LVCDE": [], "RCDE": []}
{"post_id": "1969296", "sentence": "You can also use scipy.interpolate package to do such conversions ( if you don't mind dependency on SciPy ):", "API": [["scipy.interpolate", [17, 34]]], "LVCDE": [], "RCDE": [["such conversions", [49, 65], "translate one range values to another in Python"]]}
{"post_id": "3584260", "sentence": "The argmax() method should help .\n", "API": [["argmax()", [4, 12]]], "LVCDE": [], "RCDE": []}
{"post_id": "13216688", "sentence": "This can be accomplished quite simply with the DataFrame method apply .", "API": [["DataFrame", [47, 56]], ["apply", [64, 69]]], "LVCDE": [], "RCDE": [["This", [0, 4], "multiply all the columns of a Pandas DataFrame by a column vector stored in a Series"]]}
{"post_id": "8218887", "sentence": "To get a feel of how to play with this sort of things , read through matplotlib's documentation , particularly on the subject of Axes , Axis and Artist .", "API": [["Axes", [129, 133]], ["Axis", [136, 140]], ["Artist", [145, 151]]], "LVCDE": [], "RCDE": [["this sort of things", [34, 53], "Various manipulations that can be performed on Matplotlib figures to adjust their size."]]}
{"post_id": "5267083", "sentence": "there is a specific numpy function to do this , np.searchsorted , which is much faster than bisect .", "API": [["np.searchsorted", [48, 63]], ["bisect", [92, 98]]], "LVCDE": [], "RCDE": [["this", [41, 45], "the process of locating values in a sorted array."]]}
{"post_id": "8729186", "sentence": "The link posted by Jose has been updated and pylab now has a tight_layout() function that does this automatically ( in matplotlib version 1.1 . 0 ) .", "API": [["tight_layout()", [61, 75]]], "LVCDE": [], "RCDE": [["this", [95, 99], "adjust subplot hspace that prevents overlaps"]]}
{"post_id": "22118042", "sentence": "Finally , you can stack this new matrix with X using numpy.hstack .", "API": [["numpy.hstack", [53, 65]]], "LVCDE": [["X", [45, 46], "TF-IDF features matrix"]], "RCDE": [["this new matrix", [24, 39], "the combined DataFrame containing 'AlexaRank' and 'GooglePageRank' features"]]}
{"post_id": "9149619", "sentence": "In the above code this is done by yscale('log') , but it can also be done with plt.minorticks_on() .", "API": [["yscale('log')", [34, 47]], ["plt.minorticks_on()", [79, 98]]], "LVCDE": [], "RCDE": [["above code", [7, 17], "<pre><code>In [9]: plot([23, 456, 676, 89, 906, 34, 2345])\nOut[9]: [&#38;lt;matplotlib.lines.Line2D at 0x6112f90&#38;gt;]\nIn [10]: yscale('log')\nIn [11]: grid(b=True, which='major', color='b', linestyle='-')\nIn [12]: grid(b=True, which='minor', color='r', linestyle='--')</code>"], ["this", [18, 22], "have minor tick marks tur"], ["it", [54, 56], "have minor tick marks tur"]]}
{"post_id": "5902371", "sentence": "The general method would be to use set_xticks and set_xticklabels , but I'd like something that can handle time scales from a few hours out to a few years ( this means involving the major and minor ticks to make things readable I think ) .\n", "API": [["set_xticks", [35, 45]], ["set_xticklabels", [50, 65]]], "LVCDE": [], "RCDE": []}
{"post_id": "6243689", "sentence": "So a solution is to use x.max() or print numpy.max(x) in the second line .\n", "API": [["x.max()", [24, 31]], ["numpy.max(x)", [41, 53]]], "LVCDE": [], "RCDE": []}
{"post_id": "17109187", "sentence": "Perhaps the neatest ( in one pass ) is to use collections.deque :\n", "API": [["collections.deque", [46, 63]]], "LVCDE": [], "RCDE": []}
{"post_id": "13516794", "sentence": "A sklearn.pipeline.Pipeline makes this easier by tying a vectorizer and a classifier together in a single object .", "API": [["sklearn.pipeline.Pipeline", [2, 27]]], "LVCDE": [], "RCDE": [["this", [34, 38], "The whole process of converting the test sample into a matrix and making predictions"]]}
{"post_id": "17181334", "sentence": "You can do this in one go using to_datetime :", "API": [["to_datetime", [32, 43]]], "LVCDE": [], "RCDE": [["this", [11, 15], "ensure the date column is of dates (rather of strings) and to set the index to these dates."]]}
{"post_id": "17326457", "sentence": "numpy.unique does what you need .", "API": [["numpy.unique", [0, 12]]], "LVCDE": [], "RCDE": [["what you need", [18, 31], "You have a pandas DataFrame with various strings that you want to convert to indexed values such that each unique string has a unique integer value."]]}
{"post_id": "14309050", "sentence": "Currently , you can do this in a few steps with the built-in pandas.merge() and boolean indexing .", "API": [["pandas.merge()", [61, 75]]], "LVCDE": [], "RCDE": [["this", [23, 27], "join two DataFrames where the keys match and the date is between the specified range."]]}
{"post_id": "1903579", "sentence": "First you need to find a permutation that sorts a, .argsort is a method that computes this :", "API": [[".argsort", [51, 59]]], "LVCDE": [["a", [48, 49], "a = numpy.array([2, 3, 1])"]], "RCDE": []}
{"post_id": "3721940", "sentence": "You can access this with gca() , if you're using the pylab interface , or matplotlib.pyplot.gca if you're accessing things through pyplot .", "API": [["gca()", [25, 30]], ["pylab", [53, 58]], ["matplotlib.pyplot.gca", [74, 95]], ["pyplot", [131, 137]]], "LVCDE": [], "RCDE": [["this", [15, 19], "current axis instance"]]}
{"post_id": "3860687", "sentence": "You should be able to do this with the second argument to xticks function :", "API": [["xticks", [58, 64]]], "LVCDE": [], "RCDE": [["this", [25, 29], "put a list of data on the X axis."]]}
{"post_id": "13324123", "sentence": "You have that functionality in sklearn.preprocessing :", "API": [["sklearn.preprocessing", [31, 52]]], "LVCDE": [], "RCDE": [["that functionality", [9, 27], "replicate the scaling process of SVM"]]}
{"post_id": "9149619", "sentence": "In the above code this is done by yscale('log') , but it can also be done with plt.minorticks_on() .", "API": [["yscale('log')", [34, 47]], ["plt.minorticks_on()", [79, 98]]], "LVCDE": [], "RCDE": [["above code", [7, 17], "<pre><code>In [9]: plot([23, 456, 676, 89, 906, 34, 2345])\nOut[9]: [&#38;lt;matplotlib.lines.Line2D at 0x6112f90&#38;gt;]\nIn [10]: yscale('log')\nIn [11]: grid(b=True, which='major', color='b', linestyle='-')\nIn [12]: grid(b=True, which='minor', color='r', linestyle='--')</code>"], ["this", [18, 22], "have minor tick marks tur"], ["it", [54, 56], "have minor tick marks tur"]]}
{"post_id": "7769497", "sentence": "Edit : With newer ( > = 1.2 , I think ? ) versions of matplotlib , you can accomplish the same thing as the example below by using the label kwarg to fig.add_subplot .", "API": [["matplotlib", [54, 64]], ["fig.add_subplot", [150, 165]]], "LVCDE": [], "RCDE": [["example below", [108, 121], "overlay multiple plots in Matplotlib by adding new Axes objects at the same position, allowing multiple datasets to be plotted within the same figure area./<pre><code>import matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\n\n# If you just call `plt.axes()` or equivalently `fig.add_subplot()` matplotlib  \n# will just return `ax` again. It _won't_ create a new axis unless we\n# call fig.add_axes() or reset fig._seen\nnewax = fig.add_axes(ax.get_position(), frameon=False)\n\nax.plot(range(10), 'r-')\nnewax.plot(range(50), 'g-')\nnewax.axis('equal')\n\nplt.show()\n</code></pre>"]]}
{"post_id": "3662537", "sentence": "You could use np.roll to make shifted copies of a , then use boolean logic on the masks to identify the spots to be filled in :", "API": [["np.roll", [14, 21]]], "LVCDE": [["a", [48, 49], "2D Numpy masked_array/a = ma.masked_array(a,a==fill_value)"]], "RCDE": []}
{"post_id": "5162153", "sentence": "numpy.savez or numpy.savez_compressed is the way to go .\n", "API": [["numpy.savez", [0, 11]], ["numpy.savez_compressed", [15, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "5899095", "sentence": "Your best choice is numpy.searchsorted() :\n", "API": [["numpy.searchsorted()", [20, 40]]], "LVCDE": [], "RCDE": []}
{"post_id": "5323902", "sentence": "You may find triu_indices useful .\n", "API": [["triu_indices", [13, 25]]], "LVCDE": [], "RCDE": []}
{"post_id": "1829848", "sentence": "Perhaps the function you are seeking is matplotlib.mlab.rec_groupby :\n", "API": [["matplotlib.mlab.rec_groupby", [40, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "2131021", "sentence": "A much more convenient solution is to do pylab.ion() ( interactive mode on ) when you start :\n", "API": [["pylab.ion()", [41, 52]]], "LVCDE": [], "RCDE": []}
{"post_id": "1903579", "sentence": "b[ a.argsort() ] should do the trick .\n", "API": [["b[ a.argsort() ]", [0, 16]]], "LVCDE": [], "RCDE": []}
{"post_id": "2332520", "sentence": "The key for solution is cm.set_bad function .\n", "API": [["cm.set_bad function", [24, 43]]], "LVCDE": [], "RCDE": []}
{"post_id": "7821917", "sentence": "One way is to use fig.canvas.tostring_rgb and then numpy.fromstring with the approriate dtype .\n", "API": [["fig.canvas.tostring_rgb", [18, 41]], ["numpy.fromstring", [51, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "14513503", "sentence": "Which is to say , s remained unchanged by you .set_index('Date ')", "API": [[".set_index('Date ')", [46, 65]]], "LVCDE": [["s", [18, 19], "the dataframe variable/s = pd.read_csv('spy.csv', na_values=[\" \"])"]], "RCDE": []}
{"post_id": "17291339", "sentence": "df.head() will print first 5 rows of your Excel file.", "API": [["df.head()", [0, 9]]], "LVCDE": [], "RCDE": [["your Excel file", [37, 52], "the dataframe loaded from the Excel file"]]}
{"post_id": "6353051", "sentence": "To set the ticks , just , well ... Set the ticks ( see matplotlib.pyplot.xticks or ax.set_xticks ) .\n", "API": [["matplotlib.pyplot.xticks", [55, 79]], ["ax.set_xticks", [83, 96]]], "LVCDE": [], "RCDE": []}
{"post_id": "5391258", "sentence": "Within an axes, the order that the various lines, markers, text, collections, etc appear is determined by the matplotlib.artist.Artist.set_zorder() property.\n", "API": [["matplotlib.artist.Artist.set_zorder()", [110, 147]]], "LVCDE": [], "RCDE": []}
{"post_id": "47201495", "sentence": "* In newer versions of pandas prefer loc or iloc to remove the ambiguity of ix as position or label :\n", "API": [["loc", [37, 40]], ["iloc", [44, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "17813222", "sentence": "You can specify the style of the plotted line when calling df.plot :\n", "API": [["style", [20, 25]], ["df.plot", [59, 66]]], "LVCDE": [], "RCDE": []}
{"post_id": "8964779", "sentence": "( loadtxt and genfromtxt do a lot of guessing and error-checking .\n", "API": [["loadtxt", [2, 9]], ["genfromtxt", [14, 24]]], "LVCDE": [], "RCDE": []}
{"post_id": "6004738", "sentence": "What's happening is that the axis isn't set to \" automatically dimension \" after you call ax.axis() .\n", "API": [["ax.axis()", [90, 99]]], "LVCDE": [], "RCDE": []}
{"post_id": "3718712", "sentence": "To adjust the spacing between subplots , see subplots_adjust .\n", "API": [["subplots_adjust", [45, 60]]], "LVCDE": [], "RCDE": []}
{"post_id": "8028510", "sentence": "If , however , you do pylab.savefig('patch.eps') , you will see that the objects are all opaque .\n", "API": [["pylab.savefig('patch.eps')", [22, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "14130395", "sentence": "Then we make a copy , and use tril_indices_from to get at the lower indices to mask them :\n", "API": [["tril_indices_from", [30, 47]]], "LVCDE": [], "RCDE": []}
{"post_id": "14137002", "sentence": "clf.predict will now produce sequences of sequences as well .\n", "API": [["clf.predict", [0, 11]]], "LVCDE": [], "RCDE": []}
{"post_id": "6205872", "sentence": "You can use scipy.ndimage.correlate to correlate your template against the image .", "API": [["scipy.ndimage.correlate", [12, 35]]], "LVCDE": [], "RCDE": [["your template", [49, 62], "template image corresponding to a background"]]}
{"post_id": "7534860", "sentence": "Since fmin uses a downhill gradient algorithm , it searches in a neighborhood of the initial guess for the direction of steepest descent .\n", "API": [["fmin", [6, 10]]], "LVCDE": [], "RCDE": []}
{"post_id": "3677283", "sentence": "If you want to split the data set once in two parts , you can use numpy.random.shuffle , or numpy.random.permutation if you need to keep track of the indices ( remember to fix the random seed to make everything reproducible ):\n", "API": [["numpy.random.shuffle", [66, 86]], ["numpy.random.permutation", [92, 116]]], "LVCDE": [], "RCDE": []}
{"post_id": "20491503", "sentence": "CountVectorizer / TfidfVectorizer should not produce negative values .\n", "API": [["CountVectorizer", [0, 15]], ["TfidfVectorizer", [18, 33]]], "LVCDE": [], "RCDE": []}
{"post_id": "6353051", "sentence": "Set the ticks ( see matplotlib.pyplot.xticks or ax.set_xticks ) .\n", "API": [["matplotlib.pyplot.xticks", [20, 44]], ["ax.set_xticks", [48, 61]]], "LVCDE": [], "RCDE": []}
{"post_id": "3718712", "sentence": "To disable the x-axis labels on some axes when using sharing an x-axis between the plots , you'll need to use something like matplotlib.pyplot.setp ( ax1.get_xticklabels() , visible=False ) .\n", "API": [["matplotlib.pyplot.setp ( ax1.get_xticklabels() , visible=False )", [125, 189]]], "LVCDE": [], "RCDE": []}
{"post_id": "13925150", "sentence": "Note that DataFrame.update matches rows based on indices ( which is why set_index was called above ) .\n", "API": [["DataFrame.update", [10, 26]], ["set_index", [72, 81]]], "LVCDE": [], "RCDE": []}
{"post_id": "8219171", "sentence": "You can find the bbox of the image inside the axis ( using get_window_extent ) , and use the bbox_inches parameter to save only that portion of the image :\n", "API": [["get_window_extent", [59, 76]], ["bbox_inches", [93, 104]]], "LVCDE": [], "RCDE": []}
{"post_id": "16967324", "sentence": "If you have the file as a JSON you can open it using json.load :\n", "API": [["json.load", [53, 62]]], "LVCDE": [], "RCDE": []}
{"post_id": "6568248", "sentence": "Colorbar cbar will have an . ax attribute that will provide access to the usual axis methods including tick formatting .\n", "API": [["Colorbar cbar", [0, 13]]], "LVCDE": [], "RCDE": []}
{"post_id": "13840061", "sentence": "You can get the averages and totals by using groupby :\n", "API": [["groupby", [45, 52]]], "LVCDE": [], "RCDE": []}
{"post_id": "1902378", "sentence": "Note that when interactive mode is off , you'll need to use the command show() to display the plots .\n", "API": [["show()", [72, 78]]], "LVCDE": [], "RCDE": []}
{"post_id": "9398214", "sentence": "Either specify axes.color_cycle in your .matplotlibrc file or set it at runtime using matplotlib.rcParams or matplotlib.rc .\n", "API": [["axes.color_cycle", [15, 31]], ["matplotlib.rcParams", [86, 105]], ["matplotlib.rc", [109, 122]]], "LVCDE": [], "RCDE": []}
{"post_id": "8920505", "sentence": "Marker dots can be added to the plot using the marker = ' o ' parameter setting in the call to plt.plot :\n", "API": [["marker = ' o '", [47, 61]], ["plt.plot", [95, 103]]], "LVCDE": [], "RCDE": []}
{"post_id": "4821690", "sentence": "I've used this approach with matplotlib , since it's easiest to set graph properties by changing global configuration with matplotlib.rcParams.update() .\n", "API": [["matplotlib.rcParams.update()", [123, 151]]], "LVCDE": [], "RCDE": []}
{"post_id": "20491503", "sentence": "TruncatedSVD should have no problem processing negative values .\n", "API": [["TruncatedSVD", [0, 12]]], "LVCDE": [], "RCDE": []}
{"post_id": "10114652", "sentence": "DataFrame.join is a bit of legacy method and apparently doesn't do column-on-column joins ( originally it did index on column using the on parameter , hence the \" legacy \" designation ) .\n", "API": [["DataFrame.join", [0, 14]]], "LVCDE": [], "RCDE": []}
{"post_id": "2590381", "sentence": "How you'd do that depends on the format of the file - if it's just a file with a number on each line , you can just go through each line , strip() spaces and newlines , and use float() to convert it to a number .", "API": [["strip()", [139, 146]], ["float()", [177, 184]]], "LVCDE": [], "RCDE": [["that", [13, 17], "Open the file and get the data from it"]]}
{"post_id": "18041006", "sentence": "predict gives class labels .\n", "API": [["predict", [0, 7]]], "LVCDE": [], "RCDE": []}
{"post_id": "8167527", "sentence": "The problem can be avoided by using the same axes for each plot , with ax.cla() called to clear the plot after each iteration .\n", "API": [["ax.cla()", [71, 79]]], "LVCDE": [], "RCDE": []}
{"post_id": "7787535", "sentence": "Use pyplot to set up figure objects and for show , and use the axes methods otherwise .\n", "API": [["pyplot", [4, 10]], ["axes", [63, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "17826063", "sentence": "The initial way I suggested ( which is clearly not optimal ) , once you've read it in as a DataFrame you can remove these rows using notnull ( you want to keep only those rows which are all not null ):\n", "API": [["notnull", [133, 140]]], "LVCDE": [], "RCDE": []}
{"post_id": "7379830", "sentence": "It won't add the legend to the plot without explicitly calling ax.legend(...) .\n", "API": [["ax.legend(...)", [63, 77]]], "LVCDE": [], "RCDE": []}
{"post_id": "9649982", "sentence": "The refit option to GridSearchCV will retrain the estimator on the full training set after finding the optimal settings with cross validation .\n", "API": [["GridSearchCV", [20, 32]]], "LVCDE": [], "RCDE": []}
{"post_id": "2448086", "sentence": "To plot interval data , you may use the error bar provided by the errorbar() function and the use axis.xaxis_date() to make matplotlib format the axis like plot_date() function does .\n", "API": [["errorbar()", [66, 76]], ["axis.xaxis_date()", [98, 115]], ["plot_date()", [156, 167]]], "LVCDE": [], "RCDE": []}
{"post_id": "15242142", "sentence": "This is useful to make the model grid search-able with GridSearchCV for automated parameters tuning and behave well with others when combined in a Pipeline .", "API": [["GridSearchCV", [55, 67]]], "LVCDE": [], "RCDE": [["This", [0, 4], "the default implementation of the get_params and set_params methods provided by BaseEstimator"]]}
{"post_id": "13842286", "sentence": "Alternatively, df.xs('C',copy = False)['x']=10 does modify .\n", "API": [["df.xs('C',copy = False)['x']=10", [15, 46]]], "LVCDE": [], "RCDE": []}
{"post_id": "3272066", "sentence": "To find the elements in a numpy array that are None , you can use numpy.equal .\n", "API": [["numpy.equal", [66, 77]]], "LVCDE": [], "RCDE": []}
{"post_id": "12228008", "sentence": "The RandomForestClassifier is copying the dataset several times in memory , especially when n_jobs is large .\n", "API": [["RandomForestClassifier", [4, 26]], ["n_jobs", [92, 98]]], "LVCDE": [], "RCDE": []}
{"post_id": "4406803", "sentence": "NOTE : hypot calculates the square root , thus moving the square root inside the sum .\n", "API": [["hypot", [7, 12]]], "LVCDE": [], "RCDE": []}
{"post_id": "21911813", "sentence": ".set_xticks() on the axes will set the locations and set_xticklabels() will set the displayed text .\n", "API": [[".set_xticks()", [0, 13]], ["set_xticklabels()", [53, 70]]], "LVCDE": [], "RCDE": []}
{"post_id": "2332520", "sentence": "You mask the unneeded parts of the matrix with None or with NumPy masked arrays and set_bad to white , instead of the default black .\n", "API": [["set_bad", [84, 91]]], "LVCDE": [], "RCDE": []}
{"post_id": "8998541", "sentence": "TO convert between pixels and points ( a point is 1 / 72 inches ) , you may be able to play around with matplotlib.transforms.ScaledTransform and fig.dpi_scale_trans ( the tutorial has something on this , I think ) .\n", "API": [["matplotlib.transforms.ScaledTransform", [104, 141]], ["fig.dpi_scale_trans", [146, 165]]], "LVCDE": [], "RCDE": []}
{"post_id": "14603893", "sentence": "You can use join to do the combining :\n", "API": [["join", [12, 16]]], "LVCDE": [], "RCDE": []}
{"post_id": "20491503", "sentence": "CountVectorizer / TfidfVectorizer should not produce negative values .\n", "API": [["CountVectorizer", [0, 15]], ["TfidfVectorizer", [18, 33]]], "LVCDE": [], "RCDE": []}
{"post_id": "8765592", "sentence": "If you want to use an algorithm that exploits the structure of a real symmetric or Hermitian matrix , use scipy.linalg.eigh .\n", "API": [["scipy.linalg.eigh", [106, 123]]], "LVCDE": [], "RCDE": []}
{"post_id": "7014366", "sentence": "Then you can find which age range each age falls into using np.searchsorted :\n", "API": [["np.searchsorted", [60, 75]]], "LVCDE": [], "RCDE": []}
{"post_id": "8998541", "sentence": "In particular , axes.transData.transform(points) returns pixel coordinates where ( 0 , 0 ) is the bottom-left of the viewport .\n", "API": [["axes.transData.transform(points)", [16, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "1643457", "sentence": "The np.ma.masked_invalid method returns a masked array with nans and infs masked out :\n", "API": [["np.ma.masked_invalid", [4, 24]]], "LVCDE": [], "RCDE": []}
{"post_id": "15375176", "sentence": "apply takes a function to apply to each value , not the series , and accepts kwargs .\n", "API": [["apply", [0, 5]]], "LVCDE": [], "RCDE": []}
{"post_id": "4452499", "sentence": "Note that SciPy's fft doesn't divide by N after accumulating .", "API": [["fft", [18, 21]]], "LVCDE": [["N", [40, 41], "total sample length"]], "RCDE": []}
{"post_id": "6568248", "sentence": "Use FixedLocator to statically define explicit tick locations .\n", "API": [["FixedLocator", [4, 16]]], "LVCDE": [], "RCDE": []}
{"post_id": "5852522", "sentence": "If you actually want a numpy matrix vs an array , you can do a = np.asmatrix ( np.eye ( N )) instead .\n", "API": [["a = np.asmatrix ( np.eye ( N ))", [61, 92]]], "LVCDE": [], "RCDE": []}
{"post_id": "14997054", "sentence": "You could calculate the euclidean distances between the feature vectors of the images using the scipy.spatial.distance.pdist method .\n", "API": [["scipy.spatial.distance.pdist", [96, 124]]], "LVCDE": [], "RCDE": []}
{"post_id": "10338816", "sentence": "Any time you call something like Axes3D.clear() , then to enable mouse rotation again you have to call Axes3D.mouse_init() .\n", "API": [["Axes3D.clear()", [33, 47]], ["Axes3D.mouse_init()", [103, 122]]], "LVCDE": [], "RCDE": []}
{"post_id": "1829848", "sentence": "matplotlib.mlab.rec_groupby returns a recarray :\n", "API": [["matplotlib.mlab.rec_groupby", [0, 27]]], "LVCDE": [], "RCDE": []}
{"post_id": "5852522", "sentence": "If you actually want a numpy matrix vs an array , you can do a = np.asmatrix ( np.eye ( N )) instead .\n", "API": [["a = np.asmatrix ( np.eye ( N ))", [61, 92]]], "LVCDE": [], "RCDE": []}
{"post_id": "2979118", "sentence": "plt.imshow calls gca which calls gcf which checks to see if there is a figure ; if not , it creates one .\n", "API": [["plt.imshow", [0, 10]], ["gca", [17, 20]], ["gcf", [33, 36]]], "LVCDE": [], "RCDE": []}
{"post_id": "7982117", "sentence": "Note that astype(below) returns a copy , so calling copy explicitly is no longer needed .\n", "API": [["astype(below)", [10, 23]], ["copy", [52, 56]]], "LVCDE": [], "RCDE": []}
{"post_id": "8739526", "sentence": "for numerical solution , you can use fsolve :\n", "API": [["fsolve", [37, 43]]], "LVCDE": [], "RCDE": []}
{"post_id": "5927270", "sentence": "Given some x value , you can use numpy.searchsorted() to find the index of the bin this value belongs to , and then use n[index] to extract the corresponding bar height .", "API": [["numpy.searchsorted()", [33, 53]]], "LVCDE": [["n", [120, 121], "the corresponding bar height"]], "RCDE": []}
{"post_id": "19034976", "sentence": "So , use a StandardScaler object , instead using a function to scale .\n", "API": [["StandardScaler", [11, 25]]], "LVCDE": [], "RCDE": []}
{"post_id": "13082746", "sentence": "If you have nearly as many classes as cores , it might be better and much easier to just do one class per core , by specifying n_jobs in SGDClassifier .\n", "API": [["SGDClassifier", [137, 150]]], "LVCDE": [], "RCDE": []}
{"post_id": "4816815", "sentence": "Regarding the issue of noise , the mathematical problem is to locate maxima / minima if we want to look at noise we can use something like convolve which was mentioned earlier .\n", "API": [["convolve", [139, 147]]], "LVCDE": [], "RCDE": []}
{"post_id": "6004738", "sentence": "Calling axis turns off autoscaling ( it assumes that if you're manually getting the axis limits , you probably don't want them to change ) .\n", "API": [["axis", [8, 12]]], "LVCDE": [], "RCDE": []}
{"post_id": "10272967", "sentence": "Either 1 ) clear the plot between each image ( In your case , pylab.cla() ) ,", "API": [["pylab.cla()", [62, 73]]], "LVCDE": [], "RCDE": [["your case", [50, 59], "display a simple animation by calling pylab.imshow in a for loop."]]}
{"post_id": "13822227", "sentence": "Edit : If you really need to read / write numerical CSV to / from numpy arrays , you can use numpy.loadtxt / numpy.savetxt\n", "API": [["numpy.loadtxt", [93, 106]], ["numpy.savetxt", [109, 122]]], "LVCDE": [], "RCDE": []}
{"post_id": "7982117", "sentence": "Then call isoformat() to get the date as a string in ISO-8601 format .\n", "API": [["isoformat()", [10, 21]]], "LVCDE": [], "RCDE": []}
{"post_id": "12228008", "sentence": "I am currently working on a subclass of the multiprocessing.Pool class of the standard library that will do no memory copy when numpy.memmap instances are passed to the subprocess workers .\n", "API": [["multiprocessing.Pool", [44, 64]], ["numpy.memmap", [128, 140]]], "LVCDE": [], "RCDE": []}
{"post_id": "9547928", "sentence": "np.delete will take an array of indicies of any size .\n", "API": [["np.delete", [0, 9]]], "LVCDE": [], "RCDE": []}
{"post_id": "9020157", "sentence": "apply_along_axis applies the supplied function along 1D slices of the input array , with the slices taken along the axis you specify .\n", "API": [["apply_along_axis", [0, 16]]], "LVCDE": [], "RCDE": []}
{"post_id": "17333819", "sentence": "CountVectorizer will extract trigrams for you ( using ngram_range =( 3 , 3 )) .\n", "API": [["CountVectorizer", [0, 15]]], "LVCDE": [], "RCDE": []}
{"post_id": "13261966", "sentence": "pandas.append() ( or concat() method ) can only append correctly if you have unique column names .\n", "API": [["pandas.append()", [0, 15]], ["concat()", [21, 29]]], "LVCDE": [], "RCDE": []}
{"post_id": "3584260", "sentence": "You can use unravel_index ( a.argmax() , a.shape ) to get the index as a tuple :\n", "API": [["unravel_index ( a.argmax() , a.shape )", [12, 50]]], "LVCDE": [], "RCDE": []}
{"post_id": "10453861", "sentence": "Try using ax.grid(True,which='both') to position your grid lines on both major and minor ticks , as suggested here .", "API": [["ax.grid(True,which='both')", [10, 36]]], "LVCDE": [], "RCDE": [["your grid lines", [49, 64], "lines generated in creating plot with grid using Matplotlib."]]}
{"post_id": "9649982", "sentence": "Cross validation , as done by StratifiedKFold , is intended for situations where you don't have enough data to hold out a validation set while optimizing the hyperparameters ( the algorithm settings ) .\n", "API": [["StratifiedKFold", [30, 45]]], "LVCDE": [], "RCDE": []}
{"post_id": "3651058", "sentence": "numpy.abs() is slower than abs() because it also handles Numpy arrays : it contains additional code that provides this flexibility .\n", "API": [["numpy.abs()", [0, 11]], ["abs()", [27, 32]]], "LVCDE": [], "RCDE": []}
{"post_id": "3718712", "sentence": "To disable the x-axis labels on some axes when using sharing an x-axis between the plots , you'll need to use something like matplotlib.pyplot.setp ( ax1.get_xticklabels() , visible=False ) .\n", "API": [["matplotlib.pyplot.setp ( ax1.get_xticklabels() , visible=False )", [125, 189]]], "LVCDE": [], "RCDE": []}
{"post_id": "17778560", "sentence": "assigning via df.iloc[:,4:] with a series on the right-hand side copies the data changing type as needed\n", "API": [["df.iloc[:,4:]", [14, 27]]], "LVCDE": [], "RCDE": []}
{"post_id": "3443301", "sentence": "You can use numpy.frompyfunc to convert your existing function kl into a ufunc .", "API": [["numpy.frompyfunc", [12, 28]]], "LVCDE": [["kl", [63, 65], "a scalar function"]], "RCDE": [["your existing function", [40, 62], "the scalar function kl"]]}
{"post_id": "11698391", "sentence": "If you have pandas installed , checkout the read_fwf function that imports a fixed-width file and creates a DataFrame ( 2-d tabular data structure ) .\n", "API": [["read_fwf", [44, 52]]], "LVCDE": [], "RCDE": []}
{"post_id": "2289076", "sentence": "This is because numpy.multiply.reduce() converts the range list to an array of type numpy.int32 , and the reduce operation overflows what can be stored in 32 bits at some point :", "API": [["numpy.multiply.reduce()", [16, 39]]], "LVCDE": [], "RCDE": [["This", [0, 4], "the overflow results with Python multiply() and prod()"]]}
{"post_id": "7205671", "sentence": "This sets the figure to figure 1 , and then gets the current axes ( gca() ) and then clears it with cla() .", "API": [["gca()", [68, 73]], ["cla()", [100, 105]]], "LVCDE": [], "RCDE": [["This", [0, 4], "figure(1)\nfigure(1).gca().cla()"]]}
{"post_id": "20299709", "sentence": "( For the same reason , SGDClassifier will only output probabilities when loss= \" log \" , not using its default loss function which causes it to learn a linear SVM . )\n", "API": [["SGDClassifier", [24, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "5487005", "sentence": "You can easily add a second legend by adding the line : ax2.legend(loc=0) .\n", "API": [["ax2.legend(loc=0)", [56, 73]]], "LVCDE": [], "RCDE": []}
{"post_id": "3274222", "sentence": "I think you can use the xticks function to set string labels :\n", "API": [["xticks", [24, 30]]], "LVCDE": [], "RCDE": []}
{"post_id": "22118042", "sentence": "Therefore , I guess normalizing all variables using StandardScaler to adjust their range will improve the fit .\n", "API": [["StandardScaler", [52, 66]]], "LVCDE": [], "RCDE": []}
{"post_id": "8765592", "sentence": "The eigenvalues returned by scipy.linalg.eig are not real .\n", "API": [["scipy.linalg.eig", [28, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "16958649", "sentence": "To take the first 10 values use .head(10) .\n", "API": [[".head(10)", [32, 41]]], "LVCDE": [], "RCDE": []}
{"post_id": "4523180", "sentence": "The easiest way to save your array including metadata ( dtype , dimensions ) is to use numpy.save() and numpy.load() :", "API": [["numpy.save()", [87, 99]], ["numpy.load()", [104, 116]]], "LVCDE": [], "RCDE": [["your array", [24, 34], "numpy boolean array representing a graph"]]}
{"post_id": "7982117", "sentence": "You could use matplotlib.dates.num2date to convert the nums back into datetime objects .\n", "API": [["matplotlib.dates.num2date", [14, 39]]], "LVCDE": [], "RCDE": []}
{"post_id": "3677283", "sentence": "If you want to split the data set once in two parts , you can use numpy.random.shuffle , or numpy.random.permutation if you need to keep track of the indices ( remember to fix the random seed to make everything reproducible ):\n", "API": [["numpy.random.shuffle", [66, 86]], ["numpy.random.permutation", [92, 116]]], "LVCDE": [], "RCDE": []}
{"post_id": "6925816", "sentence": "comments='[' tells np.genfromtxt to ignore lines that begin with [ .\n", "API": [["comments='['", [0, 12]], ["np.genfromtxt", [19, 32]]], "LVCDE": [], "RCDE": []}
{"post_id": "12830184", "sentence": "If that's not what you want , then pass zero_based=True to load_svmlight_file to pretend that it's actually zero-based and insert an extra column ; see its documentation for details .", "API": [["zero_based=True", [40, 55]], ["load_svmlight_file", [59, 77]]], "LVCDE": [], "RCDE": [["that", [3, 7], "The SVMlight format loader will detect that your input file has one-based indices and will subtract one from every index so as not to waste a column."], ["what you want", [14, 27], "keep the original one-based indexing without modification."]]}
{"post_id": "13842286", "sentence": "df.xs('C') by default , returns a new dataframe with a copy of the data .\n", "API": [["df.xs('C')", [0, 10]]], "LVCDE": [], "RCDE": []}
{"post_id": "8727021", "sentence": "Use plt.xlim and plt.ylim to set the domain and range .\n", "API": [["plt.xlim", [4, 12]], ["plt.ylim", [17, 25]]], "LVCDE": [], "RCDE": []}
{"post_id": "15949294", "sentence": "I think the easiest would be to create a new sparse matrix with your custom features and then use scipy.sparse.hstack to stack the features .", "API": [["scipy.sparse.hstack", [98, 117]]], "LVCDE": [], "RCDE": [["your custom features", [64, 84], "domain specific features extracted from each document"]]}
{"post_id": "4082392", "sentence": "Or , if you need to pay more attention to outliers , then perhaps you could bin your data using np.histogram , and then compose a delta_sample which has representatives from each bin .", "API": [["np.histogram", [96, 108]]], "LVCDE": [["delta_sample", [130, 142], "a sample of points for generate a scatter plot/delta_sample=random.sample(delta,1000)"]], "RCDE": [["your data", [80, 89], "the 3 million points for generate a scatter plot"]]}
{"post_id": "13788201", "sentence": "I assume what you are trying to do is change the frequency of a Time Series that contains data , in which case you can use resample ( documentation ) .\n", "API": [["Time Series", [64, 75]], ["resample", [123, 131]]], "LVCDE": [], "RCDE": []}
{"post_id": "12769082", "sentence": "If you read data from a file with read_csv the default column names of the resulting data frame are set to X.1 to X.N ( and to X1 to XN for versions > = 0.9 ) , which are strings .\n", "API": [["read_csv", [34, 42]]], "LVCDE": [], "RCDE": []}
{"post_id": "17142391", "sentence": "if you use svm.LinearSVC() as estimator , and .decision_function() ( which is like svm.SVC's .predict_proba() ) for sorting the results from most probable class to the least probable one .\n", "API": [["svm.LinearSVC()", [11, 26]], [".decision_function()", [46, 66]], ["svm.SVC's .predict_proba()", [83, 109]]], "LVCDE": [], "RCDE": []}
{"post_id": "7089483", "sentence": "To check multiple values , you can use numpy.in1d() , which is an element-wise function version of the python keyword in .\n", "API": [["numpy.in1d()", [39, 51]], ["in", [118, 120]]], "LVCDE": [], "RCDE": []}
{"post_id": "22118042", "sentence": "Note that we are passing header=0 argument to read_table to maintain original header names from tsv file .\n", "API": [["read_table", [46, 56]]], "LVCDE": [], "RCDE": []}
{"post_id": "13925150", "sentence": "Then you can update NaN values in trades with values from config using the DataFrame.update method .", "API": [["DataFrame.update", [75, 91]]], "LVCDE": [["trades", [34, 40], "The DataFrame called trades/trades = pd.DataFrame({'ticker' : ['IBM', 'MSFT', 'GOOG', 'AAPL'],'date' : pd.date_range('1/1/2000', periods = 4), 'cusip' : [nan, nan, 100, nan]})"], ["config", [58, 64], "The DataFrame called config/config = pd.DataFrame({'ticker' : ['IBM', 'MSFT', 'GOOG', 'AAPL'],'date' : pd.date_range('1/1/2000', periods = 4),'cusip' : [1,2,3,nan]})"]], "RCDE": []}
{"post_id": "13652027", "sentence": "However , read_csv returns a DataFrame , which can be filtered by selecting rows by boolean vector df[bool_vec ]:\n", "API": [["read_csv", [10, 18]]], "LVCDE": [], "RCDE": []}
{"post_id": "1969296", "sentence": "interp1d creates piecewise linear interpolation objects ( which are callable just like functions ) .\n", "API": [["interp1d", [0, 8]]], "LVCDE": [], "RCDE": []}
{"post_id": "3584260", "sentence": "You can use unravel_index ( a.argmax() , a.shape ) to get the index as a tuple :\n", "API": [["unravel_index ( a.argmax() , a.shape )", [12, 50]]], "LVCDE": [], "RCDE": []}
{"post_id": "9226239", "sentence": "Using ginput() you can avoid more complicated event handling .\n", "API": [["ginput()", [6, 14]]], "LVCDE": [], "RCDE": []}
{"post_id": "21331792", "sentence": "When you are processing the test data , you used fit_transform(X_test) which actually recomputes another PCA transformation on the test data .\n", "API": [["fit_transform(X_test)", [49, 70]]], "LVCDE": [], "RCDE": []}
{"post_id": "11054126", "sentence": "Colormap class in matplotlib has nice methods set_over and set_under , which allow to set color to be used for out-of-range values on contour plot .\n", "API": [["Colormap", [0, 8]], ["set_over", [46, 54]], ["set_under", [59, 68]]], "LVCDE": [], "RCDE": []}
{"post_id": "1488889", "sentence": "If you want to minimize a scalar function , fmin is the way to go ,\n", "API": [["fmin", [44, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "6715540", "sentence": "In your specific case , you might find it convenient to use figure.autofmt_xdate() ( Which will rotate the x-axis labels among other things ) .", "API": [["figure.autofmt_xdate()", [60, 82]]], "LVCDE": [], "RCDE": [["your specific case", [3, 21], "Rotate the xticks by 90 to avoid overlapping"]]}
{"post_id": "2799655", "sentence": "The webpage that you linked to mentions numpy.linalg.lstsq to find the vector x which minimizes |b - Ax| .\n", "API": [["numpy.linalg.lstsq", [40, 58]]], "LVCDE": [], "RCDE": []}
{"post_id": "13788201", "sentence": "Then you can change the frequency to seconds using resample , specifying how you want to aggregate the values ( mean , sum etc . ):\n", "API": [["resample", [51, 59]]], "LVCDE": [], "RCDE": []}
{"post_id": "20743758", "sentence": "This is caused by the default token_pattern for CountVectorizer , which removes tokens of a single character :", "API": [["token_pattern", [30, 43]], ["CountVectorizer", [48, 63]]], "LVCDE": [], "RCDE": [["This", [0, 4], "the word \"I\" does not appear in the feature names when vectorizing text using CountVectorizer."]]}
{"post_id": "3453527", "sentence": "The list of supported back-ends can be obtained by giving use() an incorrect back-end name :\n", "API": [["use()", [58, 63]]], "LVCDE": [], "RCDE": []}
{"post_id": "3721940", "sentence": "An AxesImage object is responsible for the image displayed ( e.g. colormaps , data , etc ) , but not the axis that the image resides in .\n", "API": [["AxesImage", [3, 12]]], "LVCDE": [], "RCDE": []}
{"post_id": "5546990", "sentence": "Here , btw , if I instead call plt.ioff() , I don't see the figure or any updates .\n", "API": [["plt.ioff()", [31, 41]]], "LVCDE": [], "RCDE": []}
{"post_id": "11066320", "sentence": "Try doing pylab.get_backend() to see what backend you have set .\n", "API": [["pylab.get_backend()", [10, 29]]], "LVCDE": [], "RCDE": []}
{"post_id": "16102866", "sentence": "DataFrame has a private function _slice() to slice the DataFrame , and it allows the parameter axis to determine which axis to slice .\n", "API": [["DataFrame", [0, 9]], ["_slice()", [33, 41]], ["DataFrame", [55, 64]]], "LVCDE": [], "RCDE": []}
{"post_id": "2296426", "sentence": "If you look at help ( numpy.savetxt ) you can see that there is a ' fmt ' option that allows you to specify the output format .\n", "API": [["help ( numpy.savetxt )", [15, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "26716774", "sentence": "The to_dict() method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly .\n", "API": [["to_dict()", [4, 13]], ["your DataFrame", [88, 102]]], "LVCDE": [], "RCDE": []}
{"post_id": "4523180", "sentence": "The easiest way to save your array including metadata ( dtype , dimensions ) is to use numpy.save() and numpy.load() :", "API": [["numpy.save()", [87, 99]], ["numpy.load()", [104, 116]]], "LVCDE": [], "RCDE": [["your array", [24, 34], "numpy boolean array representing a graph"]]}
{"post_id": "20299709", "sentence": "Use sklearn.linear_model.LogisticRegression , which uses the same algorithm as LinearSVC but with the log loss .\n", "API": [["sklearn.linear_model.LogisticRegression", [4, 43]], ["LinearSVC", [79, 88]]], "LVCDE": [], "RCDE": []}
{"post_id": "6408525", "sentence": "You can use np.nonzero ( or ndarray.nonzero ) on your boolean array to get corresponding numerical indices , then use these to access the sparse matrix .", "API": [["np.nonzero", [12, 22]], ["ndarray.nonzero", [28, 43]]], "LVCDE": [], "RCDE": [["your boolean array", [49, 67], "an array of booleans to select the rows corresponding to True entries"]]}
{"post_id": "1828405", "sentence": "You just add np.newaxis where needed to effect broadcasting as necessary .\n", "API": [["np.newaxis", [13, 23]]], "LVCDE": [], "RCDE": []}
{"post_id": "21911813", "sentence": ".set_xticks() on the axes will set the locations and set_xticklabels() will set the displayed text .\n", "API": [[".set_xticks()", [0, 13]], ["set_xticklabels()", [53, 70]]], "LVCDE": [], "RCDE": []}
{"post_id": "3433503", "sentence": "polyfit supports weighted-least-squares via the w keyword argument .\n", "API": [["polyfit", [0, 7]]], "LVCDE": [], "RCDE": []}
{"post_id": "6254788", "sentence": "To further explain , numpy.indices((5,5)) generates two arrays containing the x and y indices of a 5x5 array like so :\n", "API": [["numpy.indices((5,5))", [21, 41]]], "LVCDE": [], "RCDE": []}
{"post_id": "7379830", "sentence": "Also , you want ax.set_xlim , not ax.xlim to adjust the xaxis limits .\n", "API": [["ax.set_xlim", [16, 27]], ["ax.xlim", [34, 41]]], "LVCDE": [], "RCDE": []}
{"post_id": "10819434", "sentence": "You can run plt.close() to free up the allocation , however there has been some discussion that this method has lead to memory leaks in the past .\n", "API": [["plt.close()", [12, 23]]], "LVCDE": [], "RCDE": []}
{"post_id": "17018759", "sentence": "You can then pass the batch of transformed documents to a linear classifier that supports the partial_fit method ( e.g. SGDClassifier or PassiveAggressiveClassifier ) and then iterate on new batches .\n", "API": [["SGDClassifier", [120, 133]], ["PassiveAggressiveClassifier", [137, 164]]], "LVCDE": [], "RCDE": []}
{"post_id": "16409460", "sentence": "So it is imperative that pd.read_csv skip the second line .\n", "API": [["pd.read_csv", [25, 36]]], "LVCDE": [], "RCDE": []}
{"post_id": "20652740", "sentence": "The current scikit-learn Pipeline API is not well suited for supervised learning with unsupervised pre-training .\n", "API": [["Pipeline", [25, 33]]], "LVCDE": [], "RCDE": []}
{"post_id": "11203825", "sentence": "SGDClassifier will not copy the input data if it's already using the scipy.sparse.csr_matrix memory layout .\n", "API": [["SGDClassifier", [0, 13]], ["scipy.sparse.csr_matrix", [69, 92]]], "LVCDE": [], "RCDE": []}
{"post_id": "16102866", "sentence": "DataFrame has a private function _slice() to slice the DataFrame , and it allows the parameter axis to determine which axis to slice .\n", "API": [["DataFrame", [0, 9]], ["_slice()", [33, 41]], ["DataFrame", [55, 64]]], "LVCDE": [], "RCDE": []}
{"post_id": "2979118", "sentence": "plt.imshow calls gca which calls gcf which checks to see if there is a figure ; if not , it creates one .\n", "API": [["plt.imshow", [0, 10]], ["gca", [17, 20]], ["gcf", [33, 36]]], "LVCDE": [], "RCDE": []}
{"post_id": "5502162", "sentence": "The helper functions num2date and date2num along with python builtin datetime can be used to convert to / from .\n", "API": [["num2date", [21, 29]], ["date2num", [34, 42]]], "LVCDE": [], "RCDE": []}
{"post_id": "9020157", "sentence": "Here , numpy.diff ( i.e. the arithmetic difference of adjacent array elements ) is applied along each slice of either the first or second axis ( dimension ) of the input array .\n", "API": [["numpy.diff", [7, 17]]], "LVCDE": [], "RCDE": []}
{"post_id": "3718712", "sentence": "You'll probably want to use the various functions in matplotlib.dates , plot_date to plot your values , imshow ( and / or pcolor in some cases ) to plot your specgrams of various sorts , and matplotlib.mlab.specgram to compute them .", "API": [["matplotlib.dates", [53, 69]], ["plot_date", [72, 81]], ["imshow", [104, 110]], ["pcolor", [122, 128]], ["matplotlib.mlab.specgram", [191, 215]]], "LVCDE": [], "RCDE": [["your values", [90, 101], "the time-series data points you want to plot"], ["your specgrams", [153, 167], "the spectrograms you generate from your data."], ["them", [227, 231], "the spectrograms you generate from your data."]]}
{"post_id": "15242142", "sentence": "This is useful to make the model grid search-able with GridSearchCV for automated parameters tuning and behave well with others when combined in a Pipeline .", "API": [["GridSearchCV", [55, 67]]], "LVCDE": [], "RCDE": [["This", [0, 4], "the default implementation of the get_params and set_params methods provided by BaseEstimator"]]}
{"post_id": "1643457", "sentence": "The np.ma.compress_cols method returns a 2-D array with any column containing a masked value suppressed :\n", "API": [["np.ma.compress_cols", [4, 23]]], "LVCDE": [], "RCDE": []}
{"post_id": "2054156", "sentence": "If the length of x is not evenly divisible by the group size , then could create a masked array and use np.ma.average to compute the appropriate average .", "API": [["np.ma.average", [104, 117]]], "LVCDE": [["x", [17, 18], "the array generated using numpy, which is initialized as np.arange(12)"]], "RCDE": []}
{"post_id": "3433503", "sentence": "But we need to provide an initialize guess so curve_fit can reach the desired local minimum .\n", "API": [["curve_fit", [46, 55]]], "LVCDE": [], "RCDE": []}
{"post_id": "3662537", "sentence": "Note that np.roll happily rolls the lower edge to the top , so a missing value at the top may be filled in by a value from the very bottom .\n", "API": [["np.roll", [10, 17]]], "LVCDE": [], "RCDE": []}
{"post_id": "15709354", "sentence": "Finally fill in the NaNs with 0 using fillna :\n", "API": [["fillna", [38, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "5964666", "sentence": "pyplot.savefig saves the active figure , but using a particular figure instance's fig.savefig method saves that particular figure , regardless of which one is active .\n", "API": [["pyplot.savefig", [0, 14]], ["fig.savefig", [82, 93]]], "LVCDE": [], "RCDE": []}
{"post_id": "8920505", "sentence": "You can \" lift \" the graph by setting a lower ylim with ax.set_ylim .\n", "API": [["ax.set_ylim", [56, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "14212533", "sentence": "RandomForestClassifier is a classifier to predict class assignment for a discrete number of classes without ordering between the class labels .\n", "API": [["RandomForestClassifier", [0, 22]]], "LVCDE": [], "RCDE": []}
{"post_id": "15723905", "sentence": "You can use the astype method to cast a Series ( one column ):\n", "API": [["astype", [16, 22]]], "LVCDE": [], "RCDE": []}
{"post_id": "1670595", "sentence": "You have to call the show function to actually display anything , like matplotlib.pyplot.show() .\n", "API": [["show", [21, 25]], ["matplotlib.pyplot.show()", [71, 95]]], "LVCDE": [], "RCDE": []}
{"post_id": "5328669", "sentence": "If you are using custom ( non-constant ) bins , you can pass compute the widths using np.diff , pass the widths to ax.bar and use ax.set_xticks to label the bin edges :\n", "API": [["np.diff", [86, 93]], ["ax.bar", [115, 121]], ["ax.set_xticks", [130, 143]]], "LVCDE": [], "RCDE": []}
{"post_id": "17109187", "sentence": "Another option worth trying is to get the number of lines in a first pass and then read the file again , skip that number of rows ( minus n ) using read_csv ...\n", "API": [["read_csv", [148, 156]]], "LVCDE": [], "RCDE": []}
{"post_id": "17070356", "sentence": "you can use df or df.head() to see your dataframe and how your data is managed .", "API": [["df", [12, 14]], ["df.head()", [18, 27]]], "LVCDE": [], "RCDE": [["your dataframe", [35, 49], "DataFrame object containing the CSV file data"], ["your data", [58, 67], "the DataFrame object"]]}
{"post_id": "9692312", "sentence": "You use scipy.ndimage.label to differentiate separate objects in a boolean array and scipy.ndimage.find_objects to find the bounding box of each object .\n", "API": [["scipy.ndimage.label", [8, 27]], ["scipy.ndimage.find_objects", [85, 111]]], "LVCDE": [], "RCDE": []}
{"post_id": "5328669", "sentence": "If you are using custom ( non-constant ) bins , you can pass compute the widths using np.diff , pass the widths to ax.bar and use ax.set_xticks to label the bin edges :\n", "API": [["np.diff", [86, 93]], ["ax.bar", [115, 121]], ["ax.set_xticks", [130, 143]]], "LVCDE": [], "RCDE": []}
{"post_id": "3580047", "sentence": "Use the get_backend() function to obtain a string denoting which backend is in use :\n", "API": [["get_backend()", [8, 21]]], "LVCDE": [], "RCDE": []}
{"post_id": "1488889", "sentence": "On the other hand fmin finds the minimum value of a scalar function .\n", "API": [["fmin", [18, 22]]], "LVCDE": [], "RCDE": []}
{"post_id": "8482798", "sentence": "And if the text overlaps with the plot , you can make it smaller by using legend.fontsize , which will then make the legend smaller .\n", "API": [["legend.fontsize", [74, 89]]], "LVCDE": [], "RCDE": []}
{"post_id": "17217924", "sentence": "Furthermore as you should see in the full traceback of your error message , GridSearchCV is using cross validation internally so you cannot use it a on dataset that does not have at least 2 positive samples .\n", "API": [["GridSearchCV", [76, 88]]], "LVCDE": [], "RCDE": []}
{"post_id": "13516794", "sentence": "You can then use X_test = v.transform(X_test_raw) to transform test samples to matrices .\n", "API": [["X_test = v.transform(X_test_raw)", [17, 49]]], "LVCDE": [], "RCDE": []}
{"post_id": "3718712", "sentence": "You'll probably want to use the various functions in matplotlib.dates , plot_date to plot your values , imshow ( and / or pcolor in some cases ) to plot your specgrams of various sorts , and matplotlib.mlab.specgram to compute them .", "API": [["matplotlib.dates", [53, 69]], ["plot_date", [72, 81]], ["imshow", [104, 110]], ["pcolor", [122, 128]], ["matplotlib.mlab.specgram", [191, 215]]], "LVCDE": [], "RCDE": [["your values", [90, 101], "the time-series data points you want to plot"], ["your specgrams", [153, 167], "the spectrograms you generate from your data."], ["them", [227, 231], "the spectrograms you generate from your data."]]}
{"post_id": "15375176", "sentence": "The .agg() method here takes a function that is applied to all values of the groupby object .\n", "API": [[".agg()", [4, 10]]], "LVCDE": [], "RCDE": []}
{"post_id": "4159629", "sentence": "It can read the three files into three arrays ( using fromfile ) , calculate the average and export it to a text file ( using tofile ) .\n", "API": [["fromfile", [54, 62]], ["tofile", [126, 132]]], "LVCDE": [], "RCDE": []}
{"post_id": "16982901", "sentence": "To make the index consecutive integers you can use reset_index :\n", "API": [["reset_index", [51, 62]]], "LVCDE": [], "RCDE": []}
{"post_id": "3803810", "sentence": "A K-means algorithm is already implemented in scipy.cluster.vq .\n", "API": [["scipy.cluster.vq", [46, 62]]], "LVCDE": [], "RCDE": []}
{"post_id": "10465162", "sentence": "You can use DataFrame.interpolate to get a linear interpolation .\n", "API": [["DataFrame.interpolate", [12, 33]]], "LVCDE": [], "RCDE": []}
{"post_id": "4073024", "sentence": "Ubuntu beat me to it while I was typing this example , but his example just uses linear interpolation , which can be more easily done with numpy.interpolate ...", "API": [["numpy.interpolate", [139, 156]]], "LVCDE": [], "RCDE": [["it", [18, 20], "the task of resampling or interpolating the data from 20x45 to 20x100"], ["this example", [40, 52], "the responder's method for data interpolation using scipy.interpolate.interp1d with a cubic spline"], ["his example", [59, 70], "User ubuntu's method for data interpolation using scipy.interpolate.interp1d"]]}
{"post_id": "2148172", "sentence": "Basically , numpy.where(a,b,c) , for a condition a returns an array of shape a , and with values from b or c , depending upon whether the corresponding element of a is true or not .\n", "API": [["numpy.where(a,b,c)", [12, 30]]], "LVCDE": [], "RCDE": []}
{"post_id": "5502162", "sentence": "You can change an axis on any plot to a date axis using set_major_formatter .\n", "API": [["set_major_formatter", [56, 75]]], "LVCDE": [], "RCDE": []}
{"post_id": "5426627", "sentence": "matplotlib.pyplot.contour() allows complex-valued input arrays .\n", "API": [["matplotlib.pyplot.contour()", [0, 27]]], "LVCDE": [], "RCDE": []}
{"post_id": "4452499", "sentence": "When you called fft(wolfer) , you told the transform to assume a fundamental period equal to the length of the data .\n", "API": [["fft(wolfer)", [16, 27]]], "LVCDE": [], "RCDE": []}
{"post_id": "9890599", "sentence": "When using matplotlib.pyplot.savefig , the file format can be specified by the extension :\n", "API": [["matplotlib.pyplot.savefig", [11, 36]]], "LVCDE": [], "RCDE": []}
{"post_id": "4328608", "sentence": "savefig takes a bbox_inches argument that can be used to selectively save only a portion of a figure to an image .\n", "API": [["savefig", [0, 7]]], "LVCDE": [], "RCDE": []}
{"post_id": "3499042", "sentence": "darr.argmin() will give you the index corresponding to the minimum .\n", "API": [["darr.argmin()", [0, 13]]], "LVCDE": [], "RCDE": []}
{"post_id": "15109783", "sentence": "You can use numpy's genfromtxt function to retrieve data from the file\n", "API": [["genfromtxt", [20, 30]]], "LVCDE": [], "RCDE": []}
{"post_id": "3721940", "sentence": "However , if you're using either one , there is an xticks() function to get / set the xtick labels and locations .\n", "API": [["xticks()", [51, 59]]], "LVCDE": [], "RCDE": []}
{"post_id": "8727021", "sentence": "Use plt.xlim and plt.ylim to set the domain and range .\n", "API": [["plt.xlim", [4, 12]], ["plt.ylim", [17, 25]]], "LVCDE": [], "RCDE": []}
{"post_id": "10939576", "sentence": "you can use the numpy.isnan function to mask your list :", "API": [["numpy.isnan", [16, 27]]], "LVCDE": [], "RCDE": [["your list", [45, 54], "the list of numbers you're trying to plot that contains the nan values"]]}
{"post_id": "20299709", "sentence": "No , LinearSVC will not compute probabilities because it's not trained to do so .\n", "API": [["LinearSVC", [5, 14]]], "LVCDE": [], "RCDE": []}
{"post_id": "5502162", "sentence": "The helper functions num2date and date2num along with python builtin datetime can be used to convert to / from .\n", "API": [["num2date", [21, 29]], ["date2num", [34, 42]]], "LVCDE": [], "RCDE": []}
{"post_id": "21777511", "sentence": "Leaving it at the default value of None means that the fit method will use numpy.random's singleton random state , which is not predictable and not the same across runs .", "API": [["fit", [55, 58]]], "LVCDE": [], "RCDE": [["it", [8, 10], "the 'random_state' parameter"]]}
{"post_id": "4002456", "sentence": "the only generic generation random number generation is by using the ppf ( inverse cdf ) to transform uniform random numbers .\n", "API": [["ppf", [69, 72]], ["cdf", [83, 86]]], "LVCDE": [], "RCDE": []}
{"post_id": "14283678", "sentence": "On the other hand , you may want to look into an operation like pd.concat ( [df1 , df2 , df3] , keys=['d1 ' , ' d2 ' , ' d3 ' ] , axis=1 ) , which produces a dataframe with MultiIndex columns .\n", "API": [["pd.concat ( [df1 , df2 , df3] , keys=['d1 ' , ' d2 ' , ' d3 ' ] , axis=1 )", [64, 138]]], "LVCDE": [], "RCDE": []}
{"post_id": "6408525", "sentence": "You can use np.nonzero ( or ndarray.nonzero ) on your boolean array to get corresponding numerical indices , then use these to access the sparse matrix .", "API": [["np.nonzero", [12, 22]], ["ndarray.nonzero", [28, 43]]], "LVCDE": [], "RCDE": [["your boolean array", [49, 67], "an array of booleans to select the rows corresponding to True entries"]]}
{"post_id": "6024648", "sentence": "To figure out which backend is in use , use the get_backend() function .\n", "API": [["get_backend()", [48, 61]]], "LVCDE": [], "RCDE": []}
{"post_id": "7205671", "sentence": "I think you would benefit from using the cla() axes method , which clears the axes .\n", "API": [["cla()", [41, 46]]], "LVCDE": [], "RCDE": []}
{"post_id": "13822227", "sentence": "Edit : If you really need to read / write numerical CSV to / from numpy arrays , you can use numpy.loadtxt / numpy.savetxt\n", "API": [["numpy.loadtxt", [93, 106]], ["numpy.savetxt", [109, 122]]], "LVCDE": [], "RCDE": []}
{"post_id": "12219964", "sentence": "However , to convert a pandas Series to a numpy array , use the pandas.Series.values method .\n", "API": [["pandas.Series.values", [64, 84]]], "LVCDE": [], "RCDE": []}
{"post_id": "17070356", "sentence": "It also has a read_csv function which takes out a lot of the hassle when dealing with csv files .", "API": [["read_csv", [14, 22]]], "LVCDE": [], "RCDE": [["It", [0, 2], "pandas"]]}
{"post_id": "8462660", "sentence": "You can use matplotlib.pyplot.xticks to set the locations of the x-axis tick marks .\n", "API": [["matplotlib.pyplot.xticks", [12, 36]]], "LVCDE": [], "RCDE": []}
{"post_id": "11708664", "sentence": "You can adjust Pandas print options with set_printoptions .\n", "API": [["set_printoptions", [41, 57]]], "LVCDE": [], "RCDE": []}
{"post_id": "7379830", "sentence": "Also , you want ax.set_xlim , not ax.xlim to adjust the xaxis limits .\n", "API": [["ax.set_xlim", [16, 27]], ["ax.xlim", [34, 41]]], "LVCDE": [], "RCDE": []}
{"post_id": "47201495", "sentence": "* In newer versions of pandas prefer loc or iloc to remove the ambiguity of ix as position or label :\n", "API": [["loc", [37, 40]], ["iloc", [44, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "3410940", "sentence": "If you type help ( np.dtype ) you'll see many examples of how np.dtypes can be specified .\n", "API": [["help ( np.dtype )", [12, 29]], ["np.dtypes", [62, 71]]], "LVCDE": [], "RCDE": []}
{"post_id": "4266645", "sentence": "You can then use the rvs() method of the distribution object to generate random numbers .\n", "API": [["rvs()", [21, 26]]], "LVCDE": [], "RCDE": []}
{"post_id": "2618478", "sentence": "Then I tried my own version , using numpy.random.randint to generate a numpy array of random points on the string :\n", "API": [["numpy.random.randint", [36, 56]]], "LVCDE": [], "RCDE": []}
{"post_id": "8028510", "sentence": "The matplotlib savefig command allows you to specify saving in landscape orientation , but currently only for postscript .\n", "API": [["savefig", [15, 22]]], "LVCDE": [], "RCDE": []}
{"post_id": "3718712", "sentence": "You'll probably want to use the various functions in matplotlib.dates , plot_date to plot your values , imshow ( and / or pcolor in some cases ) to plot your specgrams of various sorts , and matplotlib.mlab.specgram to compute them .", "API": [["matplotlib.dates", [53, 69]], ["plot_date", [72, 81]], ["imshow", [104, 110]], ["pcolor", [122, 128]], ["matplotlib.mlab.specgram", [191, 215]]], "LVCDE": [], "RCDE": [["your values", [90, 101], "the time-series data points you want to plot"], ["your specgrams", [153, 167], "the spectrograms you generate from your data."], ["them", [227, 231], "the spectrograms you generate from your data."]]}
{"post_id": "17709453", "sentence": "Note that transform('count') ignores NaNs .\n", "API": [["transform('count')", [10, 28]]], "LVCDE": [], "RCDE": []}
{"post_id": "10257695", "sentence": "If your data is in data.txt , you can read it with pandas.read_csv() and than sort the resulting DataFrame .\n", "API": [["pandas.read_csv()", [51, 68]], ["DataFrame", [97, 106]]], "LVCDE": [], "RCDE": []}
{"post_id": "6925816", "sentence": "The reshape method places each 16x16 block in its own layer .\n", "API": [["reshape", [4, 11]]], "LVCDE": [], "RCDE": []}
{"post_id": "20983299", "sentence": "I think you can't get away from coercing values to strings , as DictVectorizer uses isinstance(value,six.string_types) as a condition to filter out categorical values in provided data .\n", "API": [["DictVectorizer", [64, 78]], ["isinstance(value,six.string_types)", [84, 118]]], "LVCDE": [], "RCDE": []}
{"post_id": "17070356", "sentence": "Also if you show me how the comments are in the csv file , I can show you how to ignore them with read_csv .\n", "API": [["read_csv", [98, 106]]], "LVCDE": [], "RCDE": []}
{"post_id": "2979118", "sentence": "plt.imshow calls gca which calls gcf which checks to see if there is a figure ; if not , it creates one .\n", "API": [["plt.imshow", [0, 10]], ["gca", [17, 20]], ["gcf", [33, 36]]], "LVCDE": [], "RCDE": []}
{"post_id": "22118042", "sentence": "hstack horizontally combined two multi-dimensional array-like structures provided their lengths are same .\n", "API": [["hstack", [0, 6]]], "LVCDE": [], "RCDE": []}
{"post_id": "2333251", "sentence": "That said , eig(cov(data)) is a really bad way to calculate it , since it does a lot of needless computation and copying and is potentially less accurate than using svd .", "API": [["eig(cov(data))", [12, 26]], ["svd", [165, 168]]], "LVCDE": [], "RCDE": [["it", [60, 62], "the first principal component"]]}
{"post_id": "5136064", "sentence": "It would help to see how you are setting up your plot , but at least for me messing around in pylab , ax.grid(on=False) did the trick .", "API": [["ax.grid(on=False)", [102, 119]]], "LVCDE": [], "RCDE": [["It", [0, 2], "fig=figure()\nax = fig.add_subplot(111,projection=\"3d\")\nax.plot(X,Y,Z)\nax.grid(on=False)\nshow()"]]}
{"post_id": "8462660", "sentence": "The objective is to have tick marks at 1 , 2 , 3 , 4 , and 5 , and the next example does this by using xticks .\n", "API": [["xticks", [103, 109]]], "LVCDE": [], "RCDE": []}
{"post_id": "8998541", "sentence": "TO convert between pixels and points ( a point is 1 / 72 inches ) , you may be able to play around with matplotlib.transforms.ScaledTransform and fig.dpi_scale_trans ( the tutorial has something on this , I think ) .\n", "API": [["matplotlib.transforms.ScaledTransform", [104, 141]], ["fig.dpi_scale_trans", [146, 165]]], "LVCDE": [], "RCDE": []}
{"post_id": "9692312", "sentence": "You use scipy.ndimage.label to differentiate separate objects in a boolean array and scipy.ndimage.find_objects to find the bounding box of each object .\n", "API": [["scipy.ndimage.label", [8, 27]], ["scipy.ndimage.find_objects", [85, 111]]], "LVCDE": [], "RCDE": []}
{"post_id": "12769082", "sentence": "If you have read a file with read_csv than\n", "API": [["read_csv", [29, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "3410940", "sentence": "Note that np.array expects a list of tuples .\n", "API": [["np.array", [10, 18]]], "LVCDE": [], "RCDE": []}
{"post_id": "10466763", "sentence": "To create a line plot for each week , transpose the dataframe , so the columns are week numbers and rows are weekdays ( note this step can be avoided by unstacking week number , in place of weekday , in the previous step ) , and call plot .\n", "API": [["plot", [234, 238]]], "LVCDE": [], "RCDE": []}
{"post_id": "11146434", "sentence": "With pandas.DataFrame.to_csv you can write the columns and the index to a file :\n", "API": [["pandas.DataFrame.to_csv", [5, 28]]], "LVCDE": [], "RCDE": []}
{"post_id": "20352873", "sentence": "No , SGDClassifier does not do multilabel classification -- it does multiclass classification , which is a different problem , although both are solved using a one-vs-all problem reduction .\n", "API": [["SGDClassifier", [5, 18]]], "LVCDE": [], "RCDE": []}
{"post_id": "1902378", "sentence": "It sounds like you have the interactive mode on , so you should just set it to off using the command ioff() .\n", "API": [["ioff()", [101, 107]]], "LVCDE": [], "RCDE": []}
{"post_id": "22264337", "sentence": "As X is a sparse array , instead of numpy.hstack , use scipy.sparse.hstack to join the arrays .\n", "API": [["numpy.hstack", [36, 48]], ["scipy.sparse.hstack", [55, 74]]], "LVCDE": [], "RCDE": []}
{"post_id": "6991597", "sentence": "To cross-correlate 1d arrays use numpy.correlate .\n", "API": [["numpy.correlate", [33, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "7592455", "sentence": "As @rocksportrocker implies , you need to take into account that histogram2d returns the edges in addition to the histogram .\n", "API": [["histogram2d", [65, 76]]], "LVCDE": [], "RCDE": []}
{"post_id": "10466763", "sentence": "Assuming a pandas.TimeSeries object as the starting point , you can group elements by ISO week number and ISO weekday with datetime.date.isocalendar() .\n", "API": [["pandas.TimeSeries", [11, 28]], ["datetime.date.isocalendar()", [123, 150]]], "LVCDE": [], "RCDE": []}
{"post_id": "5059279", "sentence": "You can control the output image's pixel dimensions with fig.set_size_inches and plt.savefig's dpi parameter :\n", "API": [["fig.set_size_inches", [57, 76]], ["plt.savefig's", [81, 94]]], "LVCDE": [], "RCDE": []}
{"post_id": "6024648", "sentence": "By invoking the TkAgg backend , with the call matplotlib.use('TkAgg') the figures were not destroyed before the save .\n", "API": [["matplotlib.use('TkAgg')", [46, 69]]], "LVCDE": [], "RCDE": []}
{"post_id": "5964666", "sentence": "pyplot.savefig saves the active figure , but using a particular figure instance's fig.savefig method saves that particular figure , regardless of which one is active .\n", "API": [["pyplot.savefig", [0, 14]], ["fig.savefig", [82, 93]]], "LVCDE": [], "RCDE": []}
{"post_id": "14508180", "sentence": "Both variants should work and yield the similar predictions as RandomForestRegressor has been made to support multi output regression .\n", "API": [["RandomForestRegressor", [63, 84]]], "LVCDE": [], "RCDE": []}
{"post_id": "9398214", "sentence": "Either specify axes.color_cycle in your .matplotlibrc file or set it at runtime using matplotlib.rcParams or matplotlib.rc .\n", "API": [["axes.color_cycle", [15, 31]], ["matplotlib.rcParams", [86, 105]], ["matplotlib.rc", [109, 122]]], "LVCDE": [], "RCDE": []}
{"post_id": "4159629", "sentence": "It can read the three files into three arrays ( using fromfile ) , calculate the average and export it to a text file ( using tofile ) .\n", "API": [["fromfile", [54, 62]], ["tofile", [126, 132]]], "LVCDE": [], "RCDE": []}
{"post_id": "16949183", "sentence": "if you can't do that for some odd reason , you need to merge single dataframes using pandas concat :", "API": [["concat", [92, 98]]], "LVCDE": [], "RCDE": [["that", [16, 20], "using the pandas read_csv function"]]}
{"post_id": "3669930", "sentence": "So , to generate combinations of names , and combine the columns with np.hstack , you could do this :", "API": [["np.hstack", [70, 79]]], "LVCDE": [], "RCDE": [["this", [95, 99], "<pre><code>for comb in it.combinations(names,2):\n    print(np.hstack(arr[c] for c in comb))\n# [1 2 3 4 5 6]\n# [1 2 3 7 8 9]\n# [4 5 6 7 8 9]\n</code></pre>"]]}
{"post_id": "3410940", "sentence": "If you type help ( np.dtype ) you'll see many examples of how np.dtypes can be specified .\n", "API": [["help ( np.dtype )", [12, 29]], ["np.dtypes", [62, 71]]], "LVCDE": [], "RCDE": []}
{"post_id": "16982901", "sentence": "You can combine the columns of two DataFrames using concat :\n", "API": [["concat", [52, 58]]], "LVCDE": [], "RCDE": []}
{"post_id": "17291339", "sentence": "Or, when you are working with an excel file with multiple sheets , you can use pandas.ExcelFile :\n", "API": [["pandas.ExcelFile", [79, 95]]], "LVCDE": [], "RCDE": []}
{"post_id": "17142391", "sentence": "if you use svm.LinearSVC() as estimator , and .decision_function() ( which is like svm.SVC's .predict_proba() ) for sorting the results from most probable class to the least probable one .\n", "API": [["svm.LinearSVC()", [11, 26]], [".decision_function()", [46, 66]], ["svm.SVC's .predict_proba()", [83, 109]]], "LVCDE": [], "RCDE": []}
{"post_id": "21423342", "sentence": "However , you can draw the decision tree , including feature labels , with the export_graphviz member function .\n", "API": [["export_graphviz", [79, 94]]], "LVCDE": [], "RCDE": []}
{"post_id": "14212533", "sentence": "If you want to output continuous , floating point rating , you should try to use a regression model such as RandomForestRegressor instead .\n", "API": [["RandomForestRegressor", [108, 129]]], "LVCDE": [], "RCDE": []}
{"post_id": "5902579", "sentence": "All plot_date does is plot the function and the call ax.xaxis_date() .\n", "API": [["plot_date", [4, 13]], ["ax.xaxis_date()", [53, 68]]], "LVCDE": [], "RCDE": []}
{"post_id": "3433503", "sentence": "Now , if you can use scipy , you could use scipy.optimize.curve_fit to fit any model without transformations .\n", "API": [["scipy.optimize.curve_fit", [43, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "8727021", "sentence": "You can also control the dpi in the call to plt.savefig .\n", "API": [["plt.savefig", [44, 55]]], "LVCDE": [], "RCDE": []}
{"post_id": "6715540", "sentence": "Also , as a several year later edit , with recent versions of matplotlib , you can call fig.tight_layout() to resize things to fit the labels inside the figure , as @elgehelge notes below .\n", "API": [["matplotlib", [62, 72]], ["fig.tight_layout()", [88, 106]]], "LVCDE": [], "RCDE": []}
{"post_id": "6568248", "sentence": "You might enjoy finer control using a FuncFormatter where you can use the value or position of the tick to decide whether it gets shown :\n", "API": [["FuncFormatter", [38, 51]]], "LVCDE": [], "RCDE": []}
{"post_id": "14513503", "sentence": "The set_index method returns a new DataFrame by default , rather than applying this in place ( in fact , most pandas functions are similar ) .\n", "API": [["set_index", [4, 13]], ["DataFrame", [35, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "3584260", "sentence": "I believe the argmax() method would work for multi dimensional arrays as well .\n", "API": [["argmax()", [14, 22]]], "LVCDE": [], "RCDE": []}
{"post_id": "5059279", "sentence": "You can control the output image's pixel dimensions with fig.set_size_inches and plt.savefig's dpi parameter :\n", "API": [["fig.set_size_inches", [57, 76]], ["plt.savefig's", [81, 94]]], "LVCDE": [], "RCDE": []}
{"post_id": "1591185", "sentence": "accumulate is designed to do what you seem to want ; that is , to proprigate an operation along an array .\n", "API": [["accumulate", [0, 10]]], "LVCDE": [], "RCDE": []}
{"post_id": "14513503", "sentence": "Note : to convert the Index to a DatetimeIndex you can use to_datetime :\n", "API": [["Index", [22, 27]], ["DatetimeIndex", [33, 46]], ["to_datetime", [59, 70]]], "LVCDE": [], "RCDE": []}
{"post_id": "20299709", "sentence": "Use sklearn.linear_model.LogisticRegression , which uses the same algorithm as LinearSVC but with the log loss .\n", "API": [["sklearn.linear_model.LogisticRegression", [4, 43]], ["LinearSVC", [79, 88]]], "LVCDE": [], "RCDE": []}
{"post_id": "741884", "sentence": "You can use figure to create a new plot , for example , or use close after the first plot .\n", "API": [["figure", [12, 18]], ["close", [63, 68]]], "LVCDE": [], "RCDE": []}
{"post_id": "5044364", "sentence": "You have to call numpy.random.shuffle() several times because you are shuffling several sequences independently .\n", "API": [["numpy.random.shuffle()", [17, 39]]], "LVCDE": [], "RCDE": []}
{"post_id": "17709453", "sentence": "If you want to count NaNs , use transform(len) .\n", "API": [["transform(len)", [32, 46]]], "LVCDE": [], "RCDE": []}
{"post_id": "7205671", "sentence": "This sets the figure to figure 1 , and then gets the current axes ( gca() ) and then clears it with cla() .", "API": [["gca()", [68, 73]], ["cla()", [100, 105]]], "LVCDE": [], "RCDE": [["This", [0, 4], "figure(1)\nfigure(1).gca().cla()"]]}
{"post_id": "2314719", "sentence": "Old Answer np.mat ('1 . 23 2.34 3.45 6\\ n1.32 2.43 7 3.54 ') converts the string to a numpy matrix of floating point values .\n", "API": [["np.mat ('1 . 23 2.34 3.45 6\\ n1.32 2.43 7 3.54 ')", [11, 60]]], "LVCDE": [], "RCDE": []}
{"post_id": "6307292", "sentence": "numpy.interp only provides linear interpolation which , evidently , is not what the OP is looking for .", "API": [["numpy.interp", [0, 12]]], "LVCDE": [], "RCDE": [["what the OP is looking for", [75, 101], "a way to perform nearest neighbor matching and regridding of data, rather than simply doing linear interpolation."]]}
{"post_id": "18041006", "sentence": "Use the predict_proba method to get probabilities .\n", "API": [["predict_proba", [8, 21]]], "LVCDE": [], "RCDE": []}
{"post_id": "5779874", "sentence": "You can use numpy.lexsort() to get the indices that sort your arrays using prices as primary key and names as secondary key .", "API": [["numpy.lexsort()", [12, 27]]], "LVCDE": [], "RCDE": [["your arrays", [57, 68], "a 2 dimensional numpy array that contains stock returns"]]}
{"post_id": "12950584", "sentence": "Now for your specific problem , POS tags of a window of words around a word of interest in a sentence ( e.g. for sequence tagging such as named entity detection ) can be encoded appropriately by using the DictVectorizer feature extraction helper class of scikit-learn .", "API": [["DictVectorizer", [205, 219]]], "LVCDE": [], "RCDE": [["your specific problem", [8, 29], "use non-integer string labels with SVM from scikit-learn"]]}
{"post_id": "1488889", "sentence": "leastsq finds the least squared error , generally from a set of idealized curves , and is just one way of doing a \" best fit \" .\n", "API": [["leastsq", [0, 7]]], "LVCDE": [], "RCDE": []}
{"post_id": "11054126", "sentence": "Colormap class in matplotlib has nice methods set_over and set_under , which allow to set color to be used for out-of-range values on contour plot .\n", "API": [["Colormap", [0, 8]], ["set_over", [46, 54]], ["set_under", [59, 68]]], "LVCDE": [], "RCDE": []}
{"post_id": "5643618", "sentence": "You can use numpy structured arrays along with the csv module and use numpy.sort() to sort the data .\n", "API": [["numpy.sort()", [70, 82]]], "LVCDE": [], "RCDE": []}
{"post_id": "10949895", "sentence": "You need to transform string-valued features to numeric ones in a NumPy array ; DictVectorizer does that for you .\n", "API": [["DictVectorizer", [80, 94]]], "LVCDE": [], "RCDE": []}
{"post_id": "2448086", "sentence": "To plot interval data , you may use the error bar provided by the errorbar() function and the use axis.xaxis_date() to make matplotlib format the axis like plot_date() function does .\n", "API": [["errorbar()", [66, 76]], ["axis.xaxis_date()", [98, 115]], ["plot_date()", [156, 167]]], "LVCDE": [], "RCDE": []}
{"post_id": "8964779", "sentence": "If your data is very regular ( e.g. just simple delimited rows of all the same type ) , you can also improve on either by using numpy.fromiter .\n", "API": [["numpy.fromiter", [128, 142]]], "LVCDE": [], "RCDE": []}
{"post_id": "16923367", "sentence": "To delimit by a tab you can use the sep argument of to_csv :\n", "API": [["sep argument", [36, 48]], ["to_csv", [52, 58]]], "LVCDE": [], "RCDE": []}
{"post_id": "13984485", "sentence": "pd.concat() performs an ' outer ' join on the indexes by default and holes can be filled by padding forwards and / or backwards in time .\n", "API": [["pd.concat()", [0, 11]]], "LVCDE": [], "RCDE": []}
{"post_id": "14232097", "sentence": "The return value from a RandomForestRegressor is an array of floats :\n", "API": [["RandomForestRegressor", [24, 45]]], "LVCDE": [], "RCDE": []}
{"post_id": "2448086", "sentence": "To plot interval data , you may use the error bar provided by the errorbar() function and the use axis.xaxis_date() to make matplotlib format the axis like plot_date() function does .\n", "API": [["errorbar()", [66, 76]], ["axis.xaxis_date()", [98, 115]], ["plot_date()", [156, 167]]], "LVCDE": [], "RCDE": []}
{"post_id": "1613597", "sentence": "Numpy has a set function numpy.setmember1d() that works on sorted and uniqued arrays and returns exactly the boolean array that you want .\n", "API": [["numpy.setmember1d()", [25, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "3584260", "sentence": "You can use unravel_index ( a.argmax() , a.shape ) to get the index as a tuple :\n", "API": [["unravel_index ( a.argmax() , a.shape )", [12, 50]]], "LVCDE": [], "RCDE": []}
{"post_id": "15144847", "sentence": "Even though df.loc[idx] may be a copy of a portion of df , assignment to df.loc[idx] modifies df itself .\n", "API": [["df.loc[idx]", [12, 23]], ["df", [54, 56]], ["df.loc[idx]", [73, 84]], ["df", [94, 96]]], "LVCDE": [], "RCDE": []}
{"post_id": "13216688", "sentence": "df.apply acts column-wise by default , but it can can also act row-wise by passing axis=1 as an argument to apply .\n", "API": [["df.apply", [0, 8]], ["axis=1", [83, 89]], ["apply", [108, 113]]], "LVCDE": [], "RCDE": []}
{"post_id": "13261966", "sentence": "pandas.append() ( or concat() method ) can only append correctly if you have unique column names .\n", "API": [["pandas.append()", [0, 15]], ["concat()", [21, 29]]], "LVCDE": [], "RCDE": []}
{"post_id": "4858463", "sentence": "Gnuplot is calling numpy.asarray to convert your Python list into a numpy array .", "API": [["Gnuplot", [0, 7]], ["numpy.asarray", [19, 32]]], "LVCDE": [], "RCDE": [["your Python list", [44, 60], "the list of data for plot, which includes numerical data and the \"?\" symbols representing missing pieces."]]}
{"post_id": "4366379", "sentence": "NumPy provides fromfile() to read binary data .\n", "API": [["fromfile()", [15, 25]]], "LVCDE": [], "RCDE": []}
{"post_id": "22264337", "sentence": "So , use scipy.sparse.hstack when you have a sparse array to stack .\n", "API": [["scipy.sparse.hstack", [9, 28]]], "LVCDE": [], "RCDE": []}
{"post_id": "15709354", "sentence": "First using apply you could add a column with the signed shares ( positive for Buy negative for Sell ):", "API": [["apply", [12, 17]]], "LVCDE": [["Buy", [79, 82], "The value of a column named transaction in the dataframe"], ["Sell", [96, 100], "The value of a column named transaction in the dataframe"]], "RCDE": []}
{"post_id": "69043677", "sentence": "You can use numpy.argsort to get the indices that would put fimportance into sorted order and then index both flist and fimportance with these indices:", "API": [["numpy.argsort", [12, 25]]], "LVCDE": [["fimportance", [60, 71], "a numpy array defined in 'fimportance = np.array([250.14120228,23.95686725,10.71979245,13.38566487,219.41737141, 8.19261323,27.69341779,64.96469182,218.77495366,22.7037686 ])'"], ["flist", [110, 115], "a list of feature names corresponding to the variances in the 'fimportance' array, defined in 'flist = ['int_rate', 'installment', 'log_annual_inc','dti', 'fico', 'days_with_cr_line', 'revol_bal', 'revol_util', 'inq_last_6mths','pub_rec']'"], ["fimportance", [120, 131], "a numpy array defined in 'fimportance = np.array([250.14120228,23.95686725,10.71979245,13.38566487,219.41737141, 8.19261323,27.69341779,64.96469182,218.77495366,22.7037686 ])'"]], "RCDE": []}
{"post_id": "34562807", "sentence": "You can put a, b and c into one 3-d array, and then use numpy.nanmean:", "API": [["numpy.nanmean", [56, 69]]], "LVCDE": [["a", [12, 13], "a numpy array"], ["b", [15, 16], "a numpy array"], ["c", [21, 22], "a numpy array"]], "RCDE": []}
{"post_id": "4603609", "sentence": "The data of a2 and b2 is shared with c.  To shuffle both arrays simultaneously, use numpy.random.shuffle(c).", "API": [["numpy.random.shuffle(c)", [84, 107]]], "LVCDE": [["a2", [12, 14], "a view into a single array 'c', defined in 'a2 = c[:, :a.size//len(a)].reshape(a.shape)'"], ["b2", [19, 21], "a view into a single array 'c', defined in 'b2 = c[:, a.size//len(a):].reshape(b.shape)'"], ["c", [37, 38], "a single array containing all the data from the original arrays 'a' and 'b', defined in 'c = numpy.c_[a.reshape(len(a), -1), b.reshape(len(b), -1)]'"]], "RCDE": []}
{"post_id": "41428689", "sentence": "Or if you want to keep d1 and d2 as regular lists you can use numpy.asarray:", "API": [["numpy.asarray", [62, 75]]], "LVCDE": [["d1", [23, 25], "a list of numpy arrays, defined in 'd1 = [numpy.random.rand(n**2).reshape(n, n) for n in range(1, 5)]'"], ["d2", [30, 32], "a list of numpy arrays, defined in 'd2 = [numpy.random.rand(n**2).reshape(n, n) for n in range(1, 5)]'"]], "RCDE": []}
{"post_id": "71360755", "sentence": "You can use pandas.merge() to combine df1 and df2 with outer ways.", "API": [["pandas.merge()", [12, 26]]], "LVCDE": [["df1", [38, 41], "a dataframe, which contains columns 'Year', 'ID', and 'Flag'"], ["df2", [46, 49], "a dataframe, which contains columns 'Year' and 'ID'"]], "RCDE": []}
{"post_id": "65978232", "sentence": "Using numpy.sort() instead of numpy.argsort() we can return the exact values of in_arr sorted along an axis.", "API": [["numpy.sort()", [6, 18]], ["numpy.argsort()", [30, 45]]], "LVCDE": [["in_arr", [80, 86], "a 2-D numpy array defined in the code, with the values [[20,0,10,40,30], [50,40,60,90,80]]"]], "RCDE": []}
{"post_id": "71408514", "sentence": "Even better, you can use matplotlib.axes.Axes.annotate to get rid of x_space and y_space:", "API": [["matplotlib.axes.Axes.annotate", [25, 54]]], "LVCDE": [["x_space", [69, 76], "a variable used to define the horizontal spacing, defined in 'x_space = 0.4'"], ["y_space", [81, 88], "a variable used to define the vertical spacing, defined in 'y_space = 0.05'"]], "RCDE": []}
{"post_id": "67322889", "sentence": "Use pandas.DataFrame.drop to remove any other unwanted columns from df.", "API": [["pandas.DataFrame.drop", [4, 25]]], "LVCDE": [["df", [68, 70], "a pandas DataFrame"]], "RCDE": []}
{"post_id": "64052349", "sentence": "Use pandas.DataFrame to load the data from d, after the rows have been cleaned.", "API": [["pandas.DataFrame", [4, 20]]], "LVCDE": [["d", [43, 44], "a list of data read from a csv file"]], "RCDE": []}
{"post_id": "57285759", "sentence": "using pandas.DataFrame.sample to get a random sample of items from EateryItem", "API": [["pandas.DataFrame.sample", [6, 29]]], "LVCDE": [["EateryItem", [67, 77], "A column in a pandas DataFrame"]], "RCDE": []}
{"post_id": "8905368", "sentence": "To test whether it is zero everywhere you can do numpy.all(mymatrix.diagonal() == 0).", "API": [["numpy.all(mymatrix.diagonal() == 0)", [49, 84]]], "LVCDE": [], "RCDE": [["it", [16, 18], "the diagonal of a 2-dimensional matrix"]]}
{"post_id": "72862082", "sentence": "Use numpy.concatenate() to glue a and aggs together horizontally.", "API": [["numpy.concatenate()", [4, 23]]], "LVCDE": [["a", [32, 33], "a numpy 2d array from the dataframe"], ["aggs", [38, 42], "a numpy array"]], "RCDE": []}
{"post_id": "28961631", "sentence": "numpy.argmax(numpy.bincount(dlabel)) returns the most common value found in dlabel.", "API": [["numpy.argmax(numpy.bincount(dlabel))", [0, 36]]], "LVCDE": [["dlabel", [76, 82], "a matrix"]], "RCDE": []}
{"post_id": "23626238", "sentence": "The numpy.percentile function maps 0-100 to the range of S.", "API": [["numpy.percentile", [4, 20]]], "LVCDE": [["S", [57, 58], "a numpy array, defined in 'S = pow(np.random.random(1000),3)'"]], "RCDE": []}
{"post_id": "73912640", "sentence": "Use numpy.r_ for concatenation of your var1:", "API": [["numpy.r_", [4, 12]]], "LVCDE": [["var1", [39, 43], "a variable defined in 'var1 = 10'"]], "RCDE": []}
{"post_id": "58859848", "sentence": "Add the filtered out rows from df1, with pandas.concat.", "API": [["pandas.concat", [41, 54]]], "LVCDE": [["df1", [31, 34], "a pandas DataFrame, defined in 'df1 = pd.DataFrame({'name':['tom','keith','sam','joe'],'assets':[{'laptop':1,'scanner':2},{'laptop':1,'printer':3}, {'car':12,'keys':34},{'power-cables':24}]}).'"]], "RCDE": []}
{"post_id": "73559802", "sentence": "You can perfom an inner join by using pandas.merge to grab the Tier column from df2.", "API": [["pandas.merge", [38, 50]]], "LVCDE": [["Tier", [63, 67], "a column in DataFrame"], ["df2", [80, 83], "a DataFrame"]], "RCDE": []}
{"post_id": "55221445", "sentence": "I find out that numpy.einsum() would perfectly meet my need.\n", "API": [["numpy.einsum()", [16, 30]]], "LVCDE": [], "RCDE": []}
{"post_id": "31864426", "sentence": "Use numpy.in1d to figure out if the corresponding value in the second column of b can be found in a.", "API": [["numpy.in1d", [4, 14]]], "LVCDE": [["b", [80, 81], "a 2D numpy array defined as:\nb = \n[[ 463.  0.  ]\n [ 462.  8.  ]\n [ 466.  15. ]\n [ 469.  22. ]\n [ 470.  28. ]\n [ 473.  34. ]]"], ["a", [98, 99], "a 2D numpy array defined as:\na = \n[[ 461.  0.  ]\n [ 480.  15. ]\n [ 463.  28. ]]"]], "RCDE": []}
{"post_id": "42496377", "sentence": "You can use numpy.argpartition to find out the index of the two smallest elements from s and use it as row index to subset x:", "API": [["numpy.argpartition", [12, 30]]], "LVCDE": [["s", [87, 88], "a numpy array containing random integers"], ["x", [123, 124], "a 2D numpy array containing random integers"]], "RCDE": []}
{"post_id": "58807624", "sentence": "If you want to copy the data in y, you could explicitly copy using numpy.array:", "API": [["numpy.array", [67, 78]]], "LVCDE": [["y", [32, 33], "an array"]], "RCDE": []}
{"post_id": "58277945", "sentence": "numpy.where allows you to grab the indices where the Regionname matches.", "API": [["numpy.where", [0, 11]]], "LVCDE": [["Regionname", [53, 63], "a column in the dataset 'melb_data.csv'"]], "RCDE": []}
{"post_id": "51460878", "sentence": "Instead of physically repeating Y, create a broadcasted view of Y with the shape of X, using numpy.broadcast_to:", "API": [["numpy.broadcast_to", [93, 111]]], "LVCDE": [["Y", [32, 33], "a 1 x N matrix defined as `Y = np.array([10, 20, 30])`"], ["Y", [64, 65], "a 1 x N matrix defined as `Y = np.array([10, 20, 30])`"], ["X", [84, 85], "a M x N matrix defined as `X = np.array([[0, 1, 2], [3, 0, 5]])`"]], "RCDE": []}
{"post_id": "31864426", "sentence": "Use numpy.in1d to figure out if the corresponding value in the second column of b can be found in a.", "API": [["numpy.in1d", [4, 14]]], "LVCDE": [["b", [80, 81], "a 2D numpy array with values [[463, 0], [462, 8], [466, 15], [469, 22], [470, 28], [473, 34]]"], ["a", [98, 99], "a 2D numpy array with values [[461, 0], [480, 15], [463, 28]]"]], "RCDE": []}
{"post_id": "72817389", "sentence": "Use pandas.DataFrame.iterrows to iterate through each row of df2.", "API": [["pandas.DataFrame.iterrows", [4, 29]]], "LVCDE": [["df2", [61, 64], "a DataFrame"]], "RCDE": []}
{"post_id": "72817389", "sentence": "Use matplotlib.axes.Axes.hlines and matplotlib.axes.Axes.fill_between to add horizontal lines and fill between the lines for each row in df2.", "API": [["matplotlib.axes.Axes.hlines", [4, 31]], ["matplotlib.axes.Axes.fill_between", [36, 69]]], "LVCDE": [["df2", [137, 140], "a DataFrame"]], "RCDE": []}
{"post_id": "42897430", "sentence": "We can construct an array of indexes that matches the shape of f using numpy.meshgrid:", "API": [["numpy.meshgrid", [71, 85]]], "LVCDE": [["f", [63, 64], "a numpy array defined in 'f= np.array([[[2,1],[-1,0],[1,-1]],[[0,0],[1,1],[-1,-1]],[[0,-1],[-1,-1],[-1,-2]]])'"]], "RCDE": []}
{"post_id": "24396734", "sentence": "It first calculates the full convolution with numpy.correlate between x and y as shown above.", "API": [["numpy.correlate", [46, 61]]], "LVCDE": [], "RCDE": [["It", [0, 2], "the function 'xcorr' from the 'matplotlib' library"]]}
{"post_id": "38088350", "sentence": "Use numpy.where to select conditionally elements based on mat:", "API": [["numpy.where", [4, 15]]], "LVCDE": [["mat", [58, 61], "a binary matrix of size 10-by-10, defined in 'mat = np.random.randint(2, size=(10, 10))'"]], "RCDE": []}
{"post_id": "72817389", "sentence": "Plot df1 with pandas.DataFrame.plot.", "API": [["pandas.DataFrame.plot", [14, 35]]], "LVCDE": [["df1", [5, 8], "a DataFrame object"]], "RCDE": []}
{"post_id": "44037218", "sentence": "Now we use numpy.unique() to find the unique elements of b:", "API": [["numpy.unique()", [11, 25]]], "LVCDE": [["b", [57, 58], "a structured array; it is the one-dimensional view of 'a', which is a sorted version of the original array 'A'. It was defined in the line 'b = a.view(dt)'"]], "RCDE": []}
{"post_id": "74667650", "sentence": "To address the issue of the different column sizes, this solution manipulates the indexes of the two data frames before performing an update of df1 using the pandas.DataFrame.update() method.", "API": [["pandas.DataFrame.update()", [158, 183]]], "LVCDE": [["df1", [144, 147], "a DataFrame"]], "RCDE": []}
{"post_id": "60890159", "sentence": "Use numpy.tile to create an array by repeating elements of a", "API": [["numpy.tile", [4, 14]]], "LVCDE": [["a", [59, 60], "a numpy array defined as 'a = np.array([1.,2.])' in the code"]], "RCDE": []}
{"post_id": "55217056", "sentence": "Alternatively, use pandas.Categorical to set the order of the columns in df, prior to pivoting.", "API": [["pandas.Categorical", [19, 37]]], "LVCDE": [["df", [73, 75], "a DataFrame created from a dictionary"]], "RCDE": []}
{"post_id": "55360533", "sentence": "If you then want to subset df based on the hosts in common with df2, you could use pandas.DataFrame.isin", "API": [["pandas.DataFrame.isin", [83, 104]]], "LVCDE": [["df", [27, 29], "a DataFrame created from a CSV file"], ["df2", [64, 67], "a DataFrame that is created by reading in the 'host_list.txt' file"]], "RCDE": []}
{"post_id": "71911298", "sentence": "Note that you could do the whole thing with just 2 commands (using pandas.merge_asof):\n", "API": [["pandas.merge_asof", [67, 84]]], "LVCDE": [], "RCDE": []}
{"post_id": "49583002", "sentence": "The good news is that numpy.linalg.eig and numpy.linalg.inv both work with stacked matrices just fine:\n", "API": [["numpy.linalg.inv", [43, 59]]], "LVCDE": [], "RCDE": []}
{"post_id": "56154980", "sentence": "You might find that numpy.bincount is all you need.\n", "API": [["numpy.bincount", [20, 34]]], "LVCDE": [], "RCDE": []}
{"post_id": "34956943", "sentence": "For that you would use numpy.apply_along_axis().", "API": [["numpy.apply_along_axis()", [23, 47]]], "LVCDE": [], "RCDE": [["that", [4, 8], "the need to create custom functions to apply over multidimensional datasets"]]}
{"post_id": "20689082", "sentence": "You could try numpy.ndarray.flat, which represents an iterator that you can use for reading and writing into the array.\n", "API": [["numpy.ndarray.flat", [14, 32]]], "LVCDE": [], "RCDE": []}
{"post_id": "38598378", "sentence": "Notice that the equality test has been performed using numpy.allclose instead of fvstack() == fshape() to avoid the round off errors associated to floating point artihmetic.\n", "API": [["numpy.allclose", [55, 69]]], "LVCDE": [], "RCDE": []}
{"post_id": "67298939", "sentence": "Note that if number of lines before line with labels is known you might just exploit pandas.read_csv's skiprows.\n", "API": [["pandas.read_csv", [85, 100]]], "LVCDE": [], "RCDE": []}
{"post_id": "37770230", "sentence": ":)\nscipy.interpolate.Rbf is an interpolator for n-dimensional data that includes a smoothing parameter.\n", "API": [["scipy.interpolate.Rbf", [3, 24]]], "LVCDE": [], "RCDE": []}
{"post_id": "24500274", "sentence": "Note that you need to pass k=-1 to numpy.tril_indices to not include the diagonal.\n", "API": [["numpy.tril_indices", [35, 53]]], "LVCDE": [], "RCDE": []}
{"post_id": "15375714", "sentence": "Here is another method that use numpy.split or numpy.array_split:\n", "API": [["numpy.split ", [32, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "53485863", "sentence": "numpy.histogram2d will do just that for us.", "API": [["numpy.histogram2d", [0, 17]]], "LVCDE": [], "RCDE": [["that", [31, 35], "the action of creating a 2D histogram or a height map suitable for contourf by dividing the space into regular bins and counting the number of data points that fall into each bin"]]}
{"post_id": "44436947", "sentence": "Compare that to the result of scipy.stats.binom_test (which returns just the p-value):", "API": [["scipy.stats.binom_test", [30, 52]]], "LVCDE": [], "RCDE": [["that", [8, 12], "the p-value obtained from the binomial test in R, which is 0.0005951"]]}
{"post_id": "50924523", "sentence": "sklearn.feature_selection.RFE simply trains an estimator that assigns weights to features.\n", "API": [["sklearn.feature_selection.RFE", [0, 29]]], "LVCDE": [], "RCDE": []}
{"post_id": "28398669", "sentence": "plt.hist accepts additional keyword arguments that are passed to the constructor for matplotlib.patches.Patch.\n", "API": [["matplotlib.patches.Patch", [85, 109]]], "LVCDE": [], "RCDE": []}
{"post_id": "10767644", "sentence": "I have found, for example, that numpy.linalg.eig returned incorrect eigenvalues for a complex matrix, whereas scipy.linalg.eig returned correct ones.\n", "API": [["scipy.linalg.eig", [110, 126]]], "LVCDE": [], "RCDE": []}
{"post_id": "6664085", "sentence": "It shows how to use scipy.optimize.leastsq with a function that includes error weighting.", "API": [["scipy.optimize.leastsq", [20, 42]]], "LVCDE": [], "RCDE": [["It", [0, 2], "the section 'Fitting a power-law to data with errors' on the page http://scipy-cookbook.readthedocs.io/items/FittingData.html"]]}
{"post_id": "74108674", "sentence": "Option 1\nConsindering that the goal is to apply the function to the dataframe features, one can use pandas.Series.apply as follows\n", "API": [["pandas.Series.apply", [100, 119]]], "LVCDE": [], "RCDE": []}
{"post_id": "65654725", "sentence": "pandas.Series.dt is an interface on a pandas series that gives you convenient access to operations on data stored as a pandas datetime.\n", "API": [["pandas.Series.dt", [0, 16]]], "LVCDE": [], "RCDE": []}
{"post_id": "74953938", "sentence": "You can use pandas.DataFrame.sort_values and then find the mask that are True and select rows from df_sorted with pandas.DataFrame.loc.\n", "API": [["pandas.DataFrame.loc", [114, 134]]], "LVCDE": [], "RCDE": []}
{"post_id": "70994708", "sentence": "numpy.lib.stride_tricks.as_strided is a function that creates a view of a given array with custom-made strides.\n", "API": [["numpy.lib.stride_tricks.as_strided", [0, 34]]], "LVCDE": [], "RCDE": []}
{"post_id": "72873568", "sentence": "You can use numpy.rot90 by setting axes that you want to rotate.\n", "API": [["numpy.rot90", [12, 23]]], "LVCDE": [], "RCDE": []}
{"post_id": "18654087", "sentence": "I found that scipy.io.wavfile.write() writes in 16-bit integer, which explains the larger file sizes when trying to use a 32-bit integer (the default) instead.\n", "API": [["scipy.io.wavfile.write()", [13, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "63283591", "sentence": "Assuming that the last dimension is the same, numpy.einsum should do the trick:\n", "API": [["numpy.einsum", [46, 58]]], "LVCDE": [], "RCDE": []}
{"post_id": "37191173", "sentence": "If you know for certain that the output type should be real, you can also drop the imaginary components by giving a call to numpy.real on the ifft result if you so wish.\n", "API": [["numpy.real", [124, 134]]], "LVCDE": [], "RCDE": []}
{"post_id": "57649464", "sentence": "I use numpy.polyval, docs are at URL_BLOCK: (output omitted for annotation) - here is a graphical polynomial fitter as an example that uses polyval.\n", "API": [["numpy.polyval", [6, 19]]], "LVCDE": [], "RCDE": []}
{"post_id": "45093891", "sentence": "There is, however, a better solution that cleverly makes use of numpy.random.choice and numpy's fancy indexing:\nStarting with\n", "API": [["numpy.random.choice", [64, 83]]], "LVCDE": [], "RCDE": []}
{"post_id": "71446163", "sentence": "From the Polygon docs, you can see that the hatch keyword is itself interpreted by matplotlib.patches.Patch.set_hatch.\n", "API": [["matplotlib.patches.Patch.set_hatch", [83, 117]]], "LVCDE": [], "RCDE": []}
{"post_id": "51678381", "sentence": "You say you will use scipy for clustering, so I assume that means you will use the function scipy.cluster.hierarchy.linkage.\n", "API": [["scipy.cluster.hierarchy.linkage", [92, 123]]], "LVCDE": [], "RCDE": []}
{"post_id": "53579049", "sentence": "Add the dtype option to pandas.read_csv() to ensure that everything in the column text is imported as a string:\n", "API": [["pandas.read_csv", [24, 39]]], "LVCDE": [], "RCDE": []}
{"post_id": "75215015", "sentence": "A quick search shows that scipy.interpolate.PchipInterpolator does just that.\n", "API": [["scipy.interpolate.PchipInterpolator", [26, 61]]], "LVCDE": [], "RCDE": []}
{"post_id": "56479297", "sentence": "Regardless, there is a built in function that can help you--pandas.Grouper.\n", "API": [["pandas.Grouper", [60, 74]]], "LVCDE": [], "RCDE": []}
{"post_id": "58914113", "sentence": "Once the array is filled, you can remove the added space that was unused with numpy.isnan().\n", "API": [["numpy.isnan", [78, 89]]], "LVCDE": [], "RCDE": []}
{"post_id": "42661988", "sentence": "This function does that and can be applied to a pandas.Series or to a column of a pandas.DataFrame.", "API": [["pandas.DataFrame", [82, 98]]], "LVCDE": [], "RCDE": [["This function", [0, 13], "The function 'convert_naive_dt_to_utc_epoch' defined in the code snippet, which converts naive datetime values to UTC epoch values"], ["that", [19, 23], "The process of converting datetime values to epoch values by creating a datetime.timedelta, subtracting the epoch time from the date to be converted"]]}
{"post_id": "11633977", "sentence": "Alternatively, you could use a combination of numpy.searchsorted() and numpy.bincount(), though I don't see much advantage to that approach.", "API": [["numpy.searchsorted()", [46, 66]], ["numpy.bincount()", [71, 87]]], "LVCDE": [], "RCDE": []}
{"post_id": "59763663", "sentence": "The function pandas.DataFrame.plot.pie wraps matplotlib.pyplot.pie() so we can use some of the techniques from that link.", "API": [["pandas.DataFrame.plot.pie", [13, 38]]], "LVCDE": [], "RCDE": [["that link", [111, 120], "https://matplotlib.org/gallery/pie_and_polar_charts/pie_and_donut_labels.html"]]}
{"post_id": "73851912", "sentence": "The pandas.DataFrame.drop_duplicates() function has a parameter called subset that you can use to determine which columns to include in the duplicates search.\n", "API": [["pandas.DataFrame.drop_duplicates()", [4, 38]]], "LVCDE": [], "RCDE": []}
{"post_id": "73118135", "sentence": "If you go through the documentation for pandas.read_xml, you will find that xpath needs to be specified with a // prefix.", "API": [["pandas.read_xml", [40, 55]]], "LVCDE": [], "RCDE": [["that xpath", [71, 81], "'xpath' refers to a parameter used in pandas.read_xml"]]}
{"post_id": "70693734", "sentence": "One option is to use numpy.isclose or numpy.allclose, that are specifically designed to test close numbers.\n", "API": [["numpy.isclose", [21, 34]]], "LVCDE": [], "RCDE": []}
{"post_id": "56717936", "sentence": "Notice that the column with dates is parsed with pandas.to_datetime in order to have datetime objects instead of simple strings (very handful to work with dates).\n", "API": [["pandas.to_datetime", [49, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "59979450", "sentence": "The numpy.fft.fft() method is a way to get the right frequency that allows you to separate the fft properly.\n", "API": [["numpy.fft.fft()", [4, 19]]], "LVCDE": [], "RCDE": []}
{"post_id": "74575952", "sentence": "You can use pandas.melt function for that :", "API": [["pandas.melt", [12, 23]]], "LVCDE": [], "RCDE": [["that", [37, 41], "the process of transforming a data frame with several columns"]]}
{"post_id": "52027010", "sentence": "textprops accepts a dict, which apparently accepts all options that are accepted by matplotlib.text.Text.\n", "API": [["matplotlib.text.Text", [84, 104]]], "LVCDE": [], "RCDE": []}
{"post_id": "35007804", "sentence": "Results for both opened or periodic curves:\nADDENDUM\nAs of scipy-0.19.0 there is a new scipy.interpolate.BSpline function that can be used.\n", "API": [["scipy.interpolate.BSpline", [87, 112]]], "LVCDE": [], "RCDE": []}
{"post_id": "65838742", "sentence": "Note that when you use pandas.DataFrame.set_index on df2 you get:\n", "API": [["DataFrame.set_index", [30, 49]]], "LVCDE": [], "RCDE": []}
{"post_id": "45258104", "sentence": "Example: \ntrain_data = numpy.reshape(train_dataset,(4800000, 28))\nI guess that will do the job.\n", "API": [["numpy.reshape", [23, 36]]], "LVCDE": [], "RCDE": []}
{"post_id": "67172080", "sentence": "matplotlib.rc is a function that updates matplotlib.rcParams.\n", "API": [["matplotlib.rc", [0, 13]]], "LVCDE": [], "RCDE": []}
{"post_id": "70027314", "sentence": "Finally, pass only that one to pandas.read_excel.", "API": [["pandas.read_excel", [31, 48]]], "LVCDE": [], "RCDE": [["that", [19, 23], "the sheet named 'virtual' or 'Virtual' in the Excel file"]]}
{"post_id": "38948404", "sentence": "You can define a new variable that identifies survey period and use pandas.DataFrame.groupby to avoid for loop.\n", "API": [["pandas.DataFrame.groupby", [68, 92]]], "LVCDE": [], "RCDE": []}
{"post_id": "62000086", "sentence": "A quick look in the source code shows that scipy.stats.norm.pdf simply returns the value for x of the pdf using NumPy:\n", "API": [["scipy.stats.norm", [43, 59]]], "LVCDE": [], "RCDE": []}
{"post_id": "39150393", "sentence": "To check for NaNs, you could use numpy.isnan or the fact that a NaN doesn't compare equal to itself:\n", "API": [["numpy.isnan", [33, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "48694941", "sentence": "I recommend making use of some of the parameters for pandas.DataFrame.to_excel that are outlined in docs (Link Here).\n", "API": [["pandas.DataFrame.to_excel", [53, 78]]], "LVCDE": [], "RCDE": []}
{"post_id": "48096176", "sentence": "There are certain arguments that can be passed to numpy.loadtxt namely, usecols and skiprows which can be found in the documentation.\n", "API": [["numpy.loadtxt", [50, 63]]], "LVCDE": [], "RCDE": []}
{"post_id": "46427487", "sentence": "From that same documentation:\nOP_BLOCK: (output omitted for annotation)\nUse sklearn.decomposition.TruncatedSVD instead.\n", "API": [["sklearn.decomposition.TruncatedSVD", [76, 110]]], "LVCDE": [], "RCDE": []}
{"post_id": "33412761", "sentence": "If you refer to the documentation for numpy.arange, you will see that unlike Matlab, numpy.arange will not include the upper range value in the output.\n", "API": [["numpy.arange", [85, 97]]], "LVCDE": [], "RCDE": []}
{"post_id": "62224238", "sentence": "Note that sklearn.metrics.jaccard_similarity_score is deprecated, and you should probably be looking at sklearn.metrics.jaccard_score.\n", "API": [["sklearn.metrics.jaccard_similarity_score", [10, 50]]], "LVCDE": [], "RCDE": []}
{"post_id": "71569042", "sentence": "You could generate a random array of 0's and 1's with numpy.random.choice and then make sure that the rows are different through numpy.unique:\n", "API": [["numpy.random.choice", [54, 73]]], "LVCDE": [], "RCDE": []}
{"post_id": "33402675", "sentence": "I would like to add numpy.dot based approach into the mix that also uses some reshaping.\n", "API": [["numpy.dot", [20, 29]]], "LVCDE": [], "RCDE": []}
{"post_id": "72039918", "sentence": "Now, considering that you want to order by year, month and day (as you mentioned), one can generate a new column in the dataframe using pandas.to_datetime as\n", "API": [["pandas.to_datetime", [136, 154]]], "LVCDE": [], "RCDE": []}
{"post_id": "30695531", "sentence": "Look into numpy.bincount and numpy.where, that should get you started.\n", "API": [["numpy.where", [29, 40]]], "LVCDE": [], "RCDE": []}
{"post_id": "53103161", "sentence": "numpy.transpose(..) is a function that can permutate the axes in any order.\n", "API": [["numpy.transpose(..)", [0, 19]]], "LVCDE": [], "RCDE": []}
{"post_id": "43322098", "sentence": "You can use numpy.vectorize for that", "API": [["numpy.vectorize", [12, 27]]], "LVCDE": [], "RCDE": [["that", [32, 36], "The process of making a numpy array"]]}
{"post_id": "60881151", "sentence": "The table schema that applies for orient='table' in both pandas.read_json() and Dataframe.to_json() is documented here.\n", "API": [["pandas.read_json()", [57, 75]]], "LVCDE": [], "RCDE": []}
{"post_id": "68399421", "sentence": "@Henry Ecker raised the point that the same is possible in native pandas using pandas.Timestamp.today\n", "API": [["pandas.Timestamp.today", [79, 101]]], "LVCDE": [], "RCDE": []}
{"post_id": "71569042", "sentence": "You could generate a random array of 0's and 1's with numpy.random.choice and then make sure that the rows are different through numpy.unique:\n", "API": [["numpy.unique", [129, 141]]], "LVCDE": [], "RCDE": []}
{"post_id": "71228371", "sentence": "The updated question indicates that the &quot;REV&quot; column has an implicit order, so we can create a pandas.Categorical column with an explicit order:\n", "API": [["pandas.Categorical", [105, 123]]], "LVCDE": [], "RCDE": []}
{"post_id": "60799720", "sentence": "we see that the result is basically the same, so i think it's safe to use scipy.linalg.expm for matrix exponentiation.\n", "API": [["scipy.linalg.expm", [74, 91]]], "LVCDE": [], "RCDE": []}
{"post_id": "20627638", "sentence": "If you look at the source code for scipy.stats.norm, you'll find that the ppf method ultimately calls scipy.special.ndtri.\n", "API": [["scipy.special.ndtri", [102, 121]]], "LVCDE": [], "RCDE": []}
{"post_id": "36109527", "sentence": "You can use numpy.column_stack to concatenate all of the rects that are in the list into a 2D array.\n", "API": [["numpy.column_stack", [12, 30]]], "LVCDE": [], "RCDE": []}
{"post_id": "59913358", "sentence": "scipy.optimize.fmin needs the initial guess for the function parameters to be a 1D array with a number of elements that suits the function to optimize.\n", "API": [["scipy.optimize.fmin", [0, 19]]], "LVCDE": [], "RCDE": []}
{"post_id": "55526181", "sentence": "You are looking for numpy.nonzero together with np.all (to ensure that each of RGB matches):\n", "API": [["numpy.nonzero", [20, 33]]], "LVCDE": [], "RCDE": []}
{"post_id": "15863503", "sentence": "I would suggest that you use numpy.loadtxt().\n", "API": [["numpy.loadtxt()", [29, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "33072516", "sentence": "You can replace that code with a call to numpy.loadtxt:", "API": [["numpy.loadtxt", [41, 54]]], "LVCDE": [], "RCDE": [["that code", [16, 25], "the following code snippet:\nf=open('main_ax.csv')\nb=f.read()\nr=np.array(b)"]]}
{"post_id": "41307453", "sentence": "Note: axis=1 denotes that we are referring to the column and inplace=True can also be used as per pandas.DataFrame.drop docs.\n", "API": [["pandas.DataFrame.drop", [98, 119]]], "LVCDE": [], "RCDE": []}
{"post_id": "73512650", "sentence": "In the case, of pandas.Series, there is already a convenience function that does the comparison, pandas.testing.assert_series_equal.\n", "API": [["pandas.testing.assert_series_equal", [97, 131]]], "LVCDE": [], "RCDE": []}
{"post_id": "32217754", "sentence": "Here's one way to do that with numpy.genfromtxt:", "API": [["numpy.genfromtxt", [31, 47]]], "LVCDE": [], "RCDE": [["that", [21, 25], "convert the list of strings into a numpy record array"]]}
{"post_id": "19165440", "sentence": "I suggest you to start with simple polynomial fit, scipy.optimize.curve_fit tries to fit a function f that you must know to a set of points.\n", "API": [["scipy.optimize.curve_fit", [51, 75]]], "LVCDE": [], "RCDE": []}
{"post_id": "54886321", "sentence": "There are many ways to encode categorical data, but I suggest that you start with\nsklearn.preprocessing.LabelEncoder if cardinality is high and sklearn.preprocessing.OneHotEncoder if cardinality is low.\n", "API": [["sklearn.preprocessing.LabelEncoder", [82, 116]]], "LVCDE": [], "RCDE": []}
{"post_id": "13654389", "sentence": "It might be worth mentioning that there's also numpy.corrcoef(), which computes the correlation matrix (without p-values).\n", "API": [["numpy.corrcoef()", [47, 63]]], "LVCDE": [], "RCDE": []}
{"post_id": "70822997", "sentence": "After that you can use scipy.stats.chisquare", "API": [["scipy.stats.chisquare", [23, 44]]], "LVCDE": [], "RCDE": [["that", [6, 10], "the process of using `pd.factorize` to encode categorical variables in a dataset"]]}
{"post_id": "21059308", "sentence": "windowed_view is a wrapper of a one-line function that uses numpy.lib.stride_tricks.as_strided to make a memory efficient 2d windowed view of the 1d array (full code below).\n", "API": [["numpy.lib.stride_tricks.as_strided", [60, 94]]], "LVCDE": [], "RCDE": []}
{"post_id": "34201584", "sentence": "I guess you want to use numpy.ndarray.flatten\nThere is also a parameter that determines whether to flatten in row-major or column-major order or preserve the ordering.\n", "API": [["numpy.ndarray.flatten", [24, 45]]], "LVCDE": [], "RCDE": []}
{"post_id": "73917772", "sentence": "Make sure that the columns merged have the same dtype and then use pandas.merge :\n", "API": [["pandas.merge", [67, 79]]], "LVCDE": [], "RCDE": []}
{"post_id": "48844157", "sentence": "You can do that with a custom file reader for pandas.read_csv() like:\nCode:", "API": [["pandas.read_csv()", [46, 63]]], "LVCDE": [], "RCDE": [["that", [11, 15], "The task of scraping .csv files from a website, unzipping the files and loading the .csv files into pandas.read_csv() without saving everything to the user's computer"]]}
{"post_id": "60788911", "sentence": "I assume that with Normalization you mean sklearn.preprocessing.Normalizer.\n", "API": [["sklearn.preprocessing.Normalizer", [42, 74]]], "LVCDE": [], "RCDE": []}
{"post_id": "41546342", "sentence": "Use that same expression as the argument to numpy.array to create a complex numpy array:\n", "API": [["numpy.array", [44, 55]]], "LVCDE": [], "RCDE": []}
{"post_id": "39232797", "sentence": "The cosine distance is not defined if one of the input vectors is all 0.  scipy.spatial.distance.cosine returns nan in that case:", "API": [["scipy.spatial.distance.cosine", [74, 103]]], "LVCDE": [], "RCDE": [["that case", [119, 128], "the situation when one of the input vectors is all 0"]]}
{"post_id": "50256962", "sentence": "Note that using pandas.Series.plot gives a very similar plot: counts.plot('bar') or counts.plot.bar()\n", "API": [["pandas.Series.plot", [16, 34]]], "LVCDE": [], "RCDE": []}
{"post_id": "26540643", "sentence": "You should call matplotlib.pyplot.show(), which is a method that displays all the figures.\n", "API": [["matplotlib.pyplot.show()", [16, 40]]], "LVCDE": [], "RCDE": []}
{"post_id": "43747865", "sentence": "We could do the same task with numpy.random.random_integers that follows \u201cdiscrete uniform\u201d distribution.\n", "API": [["numpy.random.random_integers", [31, 59]]], "LVCDE": [], "RCDE": []}
{"post_id": "65614967", "sentence": "Use numpy.all to test that all fields in a numpy array have the same value:\n", "API": [["numpy.all", [4, 13]]], "LVCDE": [], "RCDE": []}
{"post_id": "74553708", "sentence": "You can use pandas.Series.replace that accept dictionnaries as an argument of to_replace:\n", "API": [["pandas.Series.replace", [12, 33]]], "LVCDE": [], "RCDE": []}
{"post_id": "11633977", "sentence": "Alternatively, you could use a combination of numpy.searchsorted() and numpy.bincount(), though I don't see much advantage to that approach.", "API": [["numpy.searchsorted()", [46, 66]], ["numpy.bincount()", [71, 87]]], "LVCDE": [], "RCDE": [["that approach", [126, 139], "the method of using a combination of numpy.searchsorted() and numpy.bincount() to add two more bins for values that are less than 0.0 and greater than 1.0 in a histogram"]]}
{"post_id": "33112003", "sentence": "As such, you're better off using numpy.linspace for that which will include both the starting and ending point.\n", "API": [["numpy.linspace", [33, 47]]], "LVCDE": [], "RCDE": []}
{"post_id": "34280460", "sentence": "For binary classification, sklearn.metrics.f1_score will by default make the assumption that 1 is the positive class, and 0 is the negative class.\n", "API": [["sklearn.metrics.f1_score", [27, 51]]], "LVCDE": [], "RCDE": []}
{"post_id": "40707534", "sentence": "You can get that using scipy.spatial.distance.squareform:\n", "API": [["scipy.spatial.distance.squareform", [23, 56]]], "LVCDE": [], "RCDE": []}
{"post_id": "65599244", "sentence": "For all the information you see in describe output, you have functions in pandas.DataFrame that you can work with, e.g.:\n", "API": [["pandas.DataFrame", [74, 90]]], "LVCDE": [], "RCDE": []}
{"post_id": "62973950", "sentence": "With the expectation that there will only be a single match, or no match, pandas.Series.explode is used to return the value at index 0.\n", "API": [["pandas.Series.explode", [74, 95]]], "LVCDE": [], "RCDE": []}
{"post_id": "28972433", "sentence": "You can get the indices of the rows that has all Trues by using numpy.all on 1st axis:\n", "API": [["numpy.all", [64, 73]]], "LVCDE": [], "RCDE": []}
{"post_id": "74791483", "sentence": "pandas.DataFrame.sort_values\nIn the first argument you pass rows that you need sorting on, and than axis to sort through columns.\n", "API": [["pandas.DataFrame.sort_values", [0, 28]]], "LVCDE": [], "RCDE": []}
{"post_id": "28578391", "sentence": "Remember that * in Numpy is elementwise multiplication, and matrix multiplication is available with numpy.dot() (or with the @ operator, in Python 3.5)\n", "API": [["numpy.dot()", [100, 111]]], "LVCDE": [], "RCDE": []}
{"post_id": "61134485", "sentence": "The difference that you see is because scipy.stats.chi2_contingency applies a \"continuity correction\" when the input array is 2x2.\n", "API": [["scipy.stats.chi2_contingency", [39, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "21819324", "sentence": "You can create a dtype object contains only the fields that you want, and use numpy.ndarray() to create a view of original array:\n", "API": [["numpy.ndarray()", [78, 93]]], "LVCDE": [], "RCDE": []}
{"post_id": "52978820", "sentence": "The thing about an sklearn.pipeline.Pipeline is that every step needs to implement fit and transform.\n", "API": [["sklearn.pipeline.Pipeline", [19, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "72572312", "sentence": "I think that you're looking for numpy.insert\n", "API": [["numpy.insert", [32, 44]]], "LVCDE": [], "RCDE": []}
{"post_id": "72030489", "sentence": "Main issue is as mentioned that pandas.read_html() returns a list of dataframes and you have to specify by index wich you like to choose.\n", "API": [["pandas.read_html()", [32, 50]]], "LVCDE": [], "RCDE": []}
{"post_id": "21766579", "sentence": "We use numpy.choose to generate an array of payoff selections and multiply that with an array of absolute differences between 0.5 and the prediction values, then sum.\n", "API": [["numpy.choose", [7, 19]]], "LVCDE": [], "RCDE": []}
{"post_id": "70740604", "sentence": "You need to define a 2D mathematical domain with numpy.meshgrid, then you can compute the surface on that domain:\n", "API": [["numpy.meshgrid", [49, 63]]], "LVCDE": [], "RCDE": []}
{"post_id": "45176538", "sentence": "Approach #1\nSeems like a perfect setup to use the new functionality of numpy.unique  (v1.13 and newer) that lets us work along an axis of a NumPy array -\n", "API": [["numpy.unique", [71, 83]]], "LVCDE": [], "RCDE": []}
{"post_id": "64585504", "sentence": "From there, you can apply that FIR approximation with any FIR filtering function, like np.convolve, scipy.signal.convolve, or scipy.signal.fftconvolve.\n", "API": [["scipy.signal.convolve", [100, 121]]], "LVCDE": [], "RCDE": []}
{"post_id": "42492759", "sentence": "Now add 1 to each coordinate pair, using numpy.add.at to ensure that duplicates are counted properly:\n", "API": [["numpy.add", [41, 50]]], "LVCDE": [], "RCDE": []}
{"post_id": "53467904", "sentence": "OP_BLOCK: (output omitted for annotation)\nIn that case, I'd use numpy.where.", "API": [["numpy.where", [64, 75]]], "LVCDE": [], "RCDE": [["that case", [45, 54], "the situation where the numbers in the numpy array do not have to be between 0 and 1"]]}
{"post_id": "50051329", "sentence": "It let's you perform most common pandas.DataFrame operations in parallel and/or distributed with data that is too large to fit in memory.\n", "API": [["pandas.DataFrame", [33, 49]]], "LVCDE": [], "RCDE": []}
{"post_id": "71463617", "sentence": "You could use numpy.isin to see if the elements in B exist in A; then use numpy.where to find the indexes of the elements that exist.\n", "API": [["numpy.isin", [14, 24]]], "LVCDE": [], "RCDE": []}
{"post_id": "63319731", "sentence": "From the docs scipy.optimize.minimize you can see that COBYLA does not support eq as a constraint.\n", "API": [["scipy.optimize.minimize", [14, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "9072193", "sentence": "Here is a function that will find the majority much more quickly, it's based on the implementation of numpy.unique.\n", "API": [["numpy.unique", [102, 114]]], "LVCDE": [], "RCDE": []}
{"post_id": "66507961", "sentence": "Looks like numpy.vectorize is an option that numpy provides for doing so:\n", "API": [["numpy.vectorize", [11, 26]]], "LVCDE": [], "RCDE": []}
{"post_id": "58069483", "sentence": "Use the outer ufunc here that numpy provides, combined with numpy.minimum\n", "API": [["numpy.minimum", [60, 73]]], "LVCDE": [], "RCDE": []}
{"post_id": "67405766", "sentence": "Assming that fips_name is a dictionary of fips -&gt; state names, you can use the .map method of a pandas.Series (column):\n", "API": [["pandas.Series", [99, 112]]], "LVCDE": [], "RCDE": []}
{"post_id": "37120345", "sentence": "To ensure that the data is copied to an array in memory, numpy.copy can be used:\n", "API": [["numpy.copy", [57, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "74504096", "sentence": "For accurate streamline density, consider using matplotlib.pyplot.contour, but be aware that contour does not show arrows.\n", "API": [["matplotlib.pyplot.contour", [48, 73]]], "LVCDE": [], "RCDE": []}
{"post_id": "5956788", "sentence": "You can then reference anything from that moudule as numpy.dot or numpy.linalg.eig.", "API": [["numpy.dot", [53, 62]]], "LVCDE": [], "RCDE": [["that moudule", [37, 49], "the numpy module"]]}
{"post_id": "61048431", "sentence": "In scipy.interpolate.interp1d you can define the interpolation mode as \"zero-hold\" or order 0 spline, that is, piecewise constant, with `kind=\"zero\".\n", "API": [["scipy.interpolate.interp1d", [3, 29]]], "LVCDE": [], "RCDE": []}
{"post_id": "30695531", "sentence": "Look into numpy.bincount and numpy.where, that should get you started.\n", "API": [["numpy.bincount", [10, 24]]], "LVCDE": [], "RCDE": []}
{"post_id": "49777903", "sentence": "The matplotlib.patches.Rectangle allows the keyword argument zorder that is by default 1.0.\n", "API": [["matplotlib.patches.Rectangle", [4, 32]]], "LVCDE": [], "RCDE": []}
{"post_id": "64753075", "sentence": "You can do that with pandas.DataFrame.set_index\nSomething like:", "API": [["pandas.DataFrame.set_index", [21, 47]]], "LVCDE": [], "RCDE": [["that", [11, 15], "setting the column 'Text' as index before transposing the dataframe"]]}
{"post_id": "67619603", "sentence": "Or you can just use the function 'numpy.unravel_index' that does the exact same thing.\n", "API": [["numpy.unravel_index", [34, 53]]], "LVCDE": [], "RCDE": []}
{"post_id": "37980933", "sentence": "Note, I make use of the groupby function so that I only have to create a scipy.interpolate.interp1d call once per date\nThe data munging:\n", "API": [["scipy.interpolate.interp1d", [73, 99]]], "LVCDE": [], "RCDE": []}
{"post_id": "62388190", "sentence": "Check out the behavior of pandas.DataFrame.where:\nURL_BLOCK: (output omitted for annotation)\nIt substitutes the values that don't meet the condition with something.\n", "API": [["pandas.DataFrame.where", [26, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "20627638", "sentence": "If you look at the source code for scipy.stats.norm, you'll find that the ppf method ultimately calls scipy.special.ndtri.\n", "API": [["scipy.stats.norm", [35, 51]]], "LVCDE": [], "RCDE": []}
{"post_id": "65232360", "sentence": "If you just want to ensure that the precision of a is 2, you could call pandas.io.formats.style.Styler.set_precision when defining a.\nExample\n", "API": [["pandas.io.formats.style.Styler", [72, 102]]], "LVCDE": [], "RCDE": []}
{"post_id": "10752605", "sentence": "numpy.may_share_memory() is the best heuristic that we have at the moment.\n", "API": [["numpy.may_share_memory()", [0, 24]]], "LVCDE": [], "RCDE": []}
{"post_id": "60629148", "sentence": "You can simply use pandas.DataFrame.shift function to achieve that", "API": [["pandas.DataFrame.shift", [19, 41]]], "LVCDE": [], "RCDE": [["that", [62, 66], "the process of removing each row for columns 'a' and 'b' in a DataFrame and replacing them with the corresponding values from the next row"]]}
{"post_id": "72551593", "sentence": "Then, as the documentation suggests, we have to pass x and y that are convertible to float like values to scipy.interpolate.interp1d to get a interpolation function.\n", "API": [["scipy.interpolate.interp1d", [106, 132]]], "LVCDE": [], "RCDE": []}
{"post_id": "55832745", "sentence": "between\npandas.Series.between will return a boolean mask that can be used to filter the dataframe.\n", "API": [["pandas.Series.between", [8, 29]]], "LVCDE": [], "RCDE": []}
{"post_id": "25472082", "sentence": "You could use that feature, or subclass numpy.ndarray, but also consider using the array module offered by python itself to create a typed array:", "API": [["numpy.ndarray", [40, 53]]], "LVCDE": [], "RCDE": [["that feature", [14, 26], "the feature of numpy.ndarray.astype which allows setting if casting is allowed only when converting an array"]]}
{"post_id": "49086079", "sentence": "t.ppf() ultimately calls scipy.special.stdtrit, so you could also use that function and avoid the slight overhead of going through t.ppf():", "API": [["scipy.special.stdtrit", [25, 46]]], "LVCDE": [], "RCDE": []}
{"post_id": "64265223", "sentence": "The numpy.genfromtxt function has an optional argument called comments that by default is set to &quot;#&quot;.\n", "API": [["numpy.genfromtxt", [4, 20]]], "LVCDE": [], "RCDE": []}
{"post_id": "33144798", "sentence": "If you want to do that kind of indexing, you would need to make time_list a numpy.array.\n", "API": [["numpy.array", [76, 87]]], "LVCDE": [], "RCDE": []}
{"post_id": "47047633", "sentence": "You can use numpy.searchsorted with side='right' to find out the index of the first value in y that is larger than x and then extract the elements with the index;\n", "API": [["numpy.searchsorted", [12, 30]]], "LVCDE": [], "RCDE": []}
{"post_id": "53697404", "sentence": "Fast solution\nHere's how you can convert the output of the fast solution based on scipy.spatial.Voronoi that you linked to into a Numpy array of arbitrary width and height.\n", "API": [["scipy.spatial.Voronoi", [82, 103]]], "LVCDE": [], "RCDE": []}
{"post_id": "50668548", "sentence": "You can use numpy.where and numpy.isnan to locate the indices where values are not nan, then use np.random.choice to pick one index that is not nan:\n", "API": [["numpy.where", [12, 23]]], "LVCDE": [], "RCDE": []}
{"post_id": "45902569", "sentence": "You can use scipy.stats.poisson.cdf to see that", "API": [["scipy.stats.poisson", [12, 31]]], "LVCDE": [], "RCDE": [["that", [43, 47], "the probability that a sample from a Poisson distribution with mean equal to 2 lies above 140."]]}
{"post_id": "43686898", "sentence": "As stated numpy.random.zipf(a, size=None) will produce  plot of Samples that are drawn from a zipf distribution with specified parameter of a > 1.\n", "API": [["numpy.random.zipf", [10, 27]]], "LVCDE": [], "RCDE": []}
{"post_id": "55178824", "sentence": "Use pandas.to_numeric with argument errors='coerce' and create a list comprehension of any column that contains any valid number.\n", "API": [["pandas.to_numeric", [4, 21]]], "LVCDE": [], "RCDE": []}
{"post_id": "67667280", "sentence": "In newer applications you should make use of the new system introduced in version 1.17, including numpy.random.Generator, if you have that version or later.\n", "API": [["numpy.random.Generator", [98, 120]]], "LVCDE": [], "RCDE": []}
{"post_id": "25110947", "sentence": "You might find that scipy.optimize.curve_fit works fine for this.", "API": [["scipy.optimize.curve_fit", [20, 44]]], "LVCDE": [], "RCDE": [["this", [60, 64], "the problem of synchronizing data such that the temperature ramps begin at the same time, while filtering out local fluctuations in the measurements due to instrumentation"]]}
{"post_id": "55399334", "sentence": "You can use standard function of numpy that is numpy.array_equal(a1, a2)\n", "API": [["numpy.array_equal(a1, a2)", [47, 72]]], "LVCDE": [], "RCDE": []}
{"post_id": "30415215", "sentence": "Note that we use numpy.vectorize on the curve function to produce a vectorized version compatible with scipy.optimize.curve_fit.\n", "API": [["scipy.optimize.curve_fit", [103, 127]]], "LVCDE": [], "RCDE": []}
{"post_id": "69377555", "sentence": "matplotlib.image.imsave('file.tiff',data) saves the data contained in 'data' into a tiff (essentially an array that can be viewed as an image.\n", "API": [["matplotlib.image.imsave('file.tiff',data)", [0, 41]]], "LVCDE": [], "RCDE": []}
{"post_id": "74790813", "sentence": "This should create a Boolean pandas.Series that you can use as an index.\n", "API": [["pandas.Series", [29, 42]]], "LVCDE": [], "RCDE": []}
{"post_id": "51045263", "sentence": "At that point you can perform pandas.Series.str.contains and unstack the results back into a dataframe.", "API": [["pandas.Series.str.contains", [30, 56]]], "LVCDE": [], "RCDE": [["that point", [3, 13], "the moment after stacking the columns of a dataframe that might contain the string 'ball' into a series object"]]}
{"post_id": "74953938", "sentence": "You can use pandas.DataFrame.sort_values and then find the mask that are True and select rows from df_sorted with pandas.DataFrame.loc.\n", "API": [["pandas.DataFrame.sort_values", [12, 40]]], "LVCDE": [], "RCDE": []}
{"post_id": "27769636", "sentence": "There's a function that does exactly that: numpy.dstack (\"d\" for \"depth\").", "API": [["numpy.dstack (\"d\" for \"depth\")", [43, 73]]], "LVCDE": [], "RCDE": [["that", [37, 41], "creating a 3-dimensional numpy array of size (7,9,3)"]]}
{"post_id": "44436947", "sentence": "So that agrees with R.\nTo use the survival function of scipy.stats.binom, you have to adjust the first argument (as noted in a comment by Marius):\n", "API": [["scipy.stats.binom", [55, 72]]], "LVCDE": [], "RCDE": []}
{"post_id": "62579855", "sentence": "In each iteration build a list of dictionaries that you can then pass into pandas.DataFrame constructor once outside loop.\n", "API": [["pandas.DataFrame", [75, 91]]], "LVCDE": [], "RCDE": []}
{"post_id": "71217251", "sentence": "You could use the pandas.DataFrame.quantile method: to retrieve the value that separates the first 20% of the data we use df[&quot;runs&quot;].quantile(0.2).\n", "API": [["pandas.DataFrame.quantile", [18, 43]]], "LVCDE": [], "RCDE": []}
{"post_id": "17096108", "sentence": "But by explicitly computing numpy.logical_not() we make sure that bad_mask is always the correct logical inverse of good_mask.\n", "API": [["numpy.logical_not()", [28, 47]]], "LVCDE": [], "RCDE": []}
{"post_id": "50802773", "sentence": "You'd have to drive a matplotlib.pyplot.bar() call directly if you wanted to use a simple sequence of colours (but note that there are better options, listed below).\n", "API": [["matplotlib.pyplot.bar()", [22, 45]]], "LVCDE": [], "RCDE": []}
{"post_id": "63015132", "sentence": "After that you can enrich sa dataframe by the computed column using pandas.DataFrame.transform() method.", "API": [["pandas.DataFrame.transform()", [68, 96]]], "LVCDE": [], "RCDE": [["that", [6, 10], "the action of using pandas.DataFrame.groupby() method to compute sum of lives for every time period"]]}
{"post_id": "51254790", "sentence": "It looks like one easy way is to make the object define an __array__(self) method that returns the array you want numpy.array to return.\n", "API": [["numpy.array", [114, 125]]], "LVCDE": [], "RCDE": []}
{"post_id": "70153168", "sentence": "note that the use of matrix is no longer recommended (see docs, you should use numpy.array instead\n", "API": [["numpy.array", [79, 90]]], "LVCDE": [], "RCDE": []}
{"post_id": "36367699", "sentence": "The normal equation and lstsq give the same result (according to numpy.allclose when using that function's default arguments):\n", "API": [["numpy.allclose", [65, 79]]], "LVCDE": [], "RCDE": []}
{"post_id": "43672098", "sentence": "After that we get frequency of values , convert to numpy array and use numpy.random.zipf function to draw samples from a zipf distribution.", "API": [["numpy.random.zipf", [71, 88]]], "LVCDE": [], "RCDE": [["that", [6, 10], "the process of getting the frequency of each unique word in a corpus, limiting the data to 1000 words, and then converting these frequencies to a numpy array"]]}
{"post_id": "71463617", "sentence": "You could use numpy.isin to see if the elements in B exist in A; then use numpy.where to find the indexes of the elements that exist.\n", "API": [["numpy.where", [74, 85]]], "LVCDE": [], "RCDE": []}
{"post_id": "37349998", "sentence": "I would use the usecols parameter that pandas.read_csv comes with.\n", "API": [["pandas.read_csv", [39, 54]]], "LVCDE": [], "RCDE": []}
{"post_id": "52199330", "sentence": "Note that cov_bias corresponds to the result of numpy.cov(bias=True).\n", "API": [["numpy.cov(bias=True)", [48, 68]]], "LVCDE": [], "RCDE": []}
{"post_id": "67667280", "sentence": "Generator also has a uniform method that works in much the same way as the legacy function numpy.random.uniform.\n", "API": [["numpy.random.uniform", [91, 111]]], "LVCDE": [], "RCDE": []}
{"post_id": "56875147", "sentence": "Then load that with scipy.io.loadmat.", "API": [["scipy.io.loadmat", [20, 36]]], "LVCDE": [], "RCDE": [["that", [10, 14], "the .mat file created by saving a sparse matrix in MATLAB"]]}
{"post_id": "57134503", "sentence": "You can do that with numpy.squeeze.", "API": [["numpy.squeeze", [21, 34]]], "LVCDE": [], "RCDE": [["that", [11, 15], "the process of removing singular dimensions from a numpy array, specifically converting a 4D array of shape (1, 256, 256, 3) to a 3D array of shape (256, 256, 3)"]]}
{"post_id": "49583002", "sentence": "The good news is that numpy.linalg.eig and numpy.linalg.inv both work with stacked matrices just fine:\n", "API": [["numpy.linalg.eig", [22, 38]]], "LVCDE": [], "RCDE": []}
{"post_id": "71762291", "sentence": "However, the strings will be formatted to type numpy.bytes_, so you need to convert that to string using decode('UTF-8') function.", "API": [["numpy.bytes_", [47, 59]]], "LVCDE": [], "RCDE": [["that", [84, 88], "the strings formatted to type numpy.bytes_ from numpy.genfromtxt function"]]}
{"post_id": "58655181", "sentence": "You can do that by using pandas.DataFrame.select_dtypes, exclude all the numeric columns, so you get the string or object type columns:", "API": [["pandas.DataFrame.select_dtypes", [25, 55]]], "LVCDE": [], "RCDE": [["that", [11, 15], "The action of grouping a dataframe by all non-numeric columns and calculating the averages for each group."]]}
{"post_id": "62294972", "sentence": "Also note that you should use matplotlib.figure.Figure instead of pyplot when working with tkinter.\n", "API": [["matplotlib.figure.Figure", [30, 54]]], "LVCDE": [], "RCDE": []}
{"post_id": "75003370", "sentence": "The plural roots refers to the fact that both scipy.optimize.root and scipy.optimize.fsolve try to find one N-dimensional point x (root) of a multivariate function F: R^N -&gt; R^N with F(x) = 0.\n", "API": [["scipy.optimize.fsolve", [70, 91]]], "LVCDE": [], "RCDE": []}
{"post_id": "58409215", "sentence": "Specifically for NumPy, though, you can hook into the mechanisms that numpy.ndarray.__sub__ delegates to.\n", "API": [["numpy.ndarray.__sub__", [70, 91]]], "LVCDE": [], "RCDE": []}
{"post_id": "62504621", "sentence": "If you use scipy.signal.filtfilt: that function operates on time-domain data, not on frequency-domain data.\n", "API": [["scipy.signal.filtfilt", [11, 32]]], "LVCDE": [], "RCDE": []}
{"post_id": "74333099", "sentence": "pandas.DataFrame() includes a copy parameter, and you can use that to force a copy when the dataframe is created.\n", "API": [["pandas.DataFrame()", [0, 18]]], "LVCDE": [], "RCDE": []}
{"post_id": "67549312", "sentence": "The first two arguments to scipy.stats.ttest_ind must be the data sets that are to be compared, not the means of the data sets.\n", "API": [["scipy.stats.ttest_ind", [27, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "3722674", "sentence": "Since white is not a color in cm.jet, note that scipy.cluster.vq.vq maps white to to closest point in the gradient code book, which happens to be a pale green color.)\n", "API": [["scipy.cluster.vq.vq", [48, 67]]], "LVCDE": [], "RCDE": []}
{"post_id": "21610355", "sentence": "numpy.log10(prob) calculates the base 10 logarithm for all elements of prob, even the ones that aren't selected by the where.\n", "API": [["numpy.log10(prob)", [0, 17]]], "LVCDE": [], "RCDE": []}
{"post_id": "36796249", "sentence": "You can use the scipy.stats.gumbel_l.fit function similarily for that respective distribution, which will return the location and scale.\n", "API": [["scipy.stats.gumbel_l", [16, 36]]], "LVCDE": [], "RCDE": []}
{"post_id": "40555989", "sentence": "You can pass a header argument into pandas.read_excel() that indicates how many rows are to be used as headers.\n", "API": [["pandas.read_excel()", [36, 55]]], "LVCDE": [], "RCDE": []}
{"post_id": "44244123", "sentence": "Unless you are certain that the data isn't shared then you need to call numpy.resize rather than the .resize method of the arrays.\n", "API": [["numpy.resize", [72, 84]]], "LVCDE": [], "RCDE": []}
{"post_id": "69984468", "sentence": "Addition operations on arrays will usually delegate to numpy.add, but operations that don't involve arrays will usually not involve numpy.add.\n", "API": [["numpy.add", [55, 64]]], "LVCDE": [], "RCDE": []}
{"post_id": "17291741", "sentence": "Note that numpy.repeat returns a flattened array, hence the use of numpy.reshape.\n", "API": [["numpy.repeat", [10, 22]]], "LVCDE": [], "RCDE": []}
{"post_id": "47169332", "sentence": "You can load the input list as an array (called a) and use numpy.roll to shift the array so that you now have another array (called b) which has stores the shifted array.\n", "API": [["numpy.roll", [59, 69]]], "LVCDE": [], "RCDE": []}
{"post_id": "72993712", "sentence": "By referring to the official document of matplotlib.pyplot.plot(), we can find that linestyle is a keyword argument.", "API": [["matplotlib.pyplot.plot()", [41, 65]]], "LVCDE": [], "RCDE": []}
{"post_id": "16506778", "sentence": "Given that h is a numpy.recarray you can achieve the basic plotting by using\n", "API": [["numpy.recarray", [18, 32]]], "LVCDE": [], "RCDE": []}
{"post_id": "26101660", "sentence": "Original answer: Use the facilities provided in the pandas.DataFrame pivot function, to pivot on the column that you want to serve as the categories.\n", "API": [["pandas.DataFrame", [52, 68]]], "LVCDE": [], "RCDE": []}
{"post_id": "47065794", "sentence": "In that case use numpy.isclose().", "API": [["numpy.isclose()", [17, 32]]], "LVCDE": [], "RCDE": [["that case", [3, 12], "a situation where there is a type mismatch between 'ID_potential' and 'ID_target' in the user's actual code/data or some of the data are floating point numbers and therefore strict equality may not hold"]]}
{"post_id": "58579240", "sentence": "If you're comfortable with importing just the matplotlib.pyplot.ylabel method instead of re-importing matplotlib, that can work too.\n", "API": [["matplotlib.pyplot.ylabel", [46, 70]]], "LVCDE": [], "RCDE": []}
{"post_id": "57135346", "sentence": "This is the calculation that you want, using a Python loop and scipy.stats.entropy.", "API": [["scipy.stats.entropy", [63, 82]]], "LVCDE": [], "RCDE": [["the calculation that you want", [8, 37], "the process of computing the Kullback-Leibler (KL) divergence between each row of a given matrix and a vector."]]}
{"post_id": "42749878", "sentence": "Note that numpy.unique is faster than set() for large arrays, and by a large margin if there are only a few unique entries.\n", "API": [["numpy.unique", [10, 22]]], "LVCDE": [], "RCDE": []}
{"post_id": "10337643", "sentence": "Note that bottleneck.partition() returns the actual values sorted, if you want the indexes of the sorted values (what numpy.argsort() returns) you should use bottleneck.argpartition().\n", "API": [["numpy.argsort()", [118, 133]]], "LVCDE": [], "RCDE": []}
{"post_id": "57064829", "sentence": "If you look at the documentation for pandas.Series.str.replace you will see that the repl argument can be a string or callable, but a dict is not supported.\n", "API": [["pandas.Series.str.replace", [37, 62]]], "LVCDE": [], "RCDE": []}
{"post_id": "56850469", "sentence": "My suggestion is that you use the pandas data loading functionality directly on the pickled file using pandas.read_pickle:\n", "API": [["pandas.read_pickle", [103, 121]]], "LVCDE": [], "RCDE": []}
{"post_id": "72039918", "sentence": "If by &quot;excel&quot; you mean a .xlsx file, considering that the file is named data.xlsx, read the file into a dataframe with pandas.read_excel as\n", "API": [["pandas.read_excel", [129, 146]]], "LVCDE": [], "RCDE": []}
{"post_id": "73783359", "sentence": "Then, considering that OP only wants to retrieve the first line, one can use pandas.DataFrame.iloc\n", "API": [["pandas.DataFrame.iloc", [77, 98]]], "LVCDE": [], "RCDE": []}
{"post_id": "56382824", "sentence": "No need to create a loop to check each value, you can use isin method that pandas.DataFrame has as following:\n", "API": [["pandas.DataFrame", [75, 91]]], "LVCDE": [], "RCDE": []}
{"post_id": "1336822", "sentence": "scipy.optimize.leastsq() takes a function that should return an array of residuals for a given parameter vector.\n", "API": [["scipy.optimize.leastsq()", [0, 24]]], "LVCDE": [], "RCDE": []}
{"post_id": "70867832", "sentence": "pandas.to_datetime() has a dayfirst argument that defaults to false.\n", "API": [["pandas.to_datetime()", [0, 20]]], "LVCDE": [], "RCDE": []}
{"post_id": "15375714", "sentence": "Here is another method that use numpy.split or numpy.array_split:", "API": [["numpy.array_split", [47, 64]]], "LVCDE": [], "RCDE": [["another method that", [8, 27], "A different approach or technique for splitting a pandas DataFrame into groups of a specific size, using the numpy.split or numpy.array_split functions."]]}
{"post_id": "72655207", "sentence": "Note that the final numpy.ndarray.tolist() could be avoided if whatever will consume the output is capable of dealing with the NumPy array itself, thus saving some relatively small but definitely appreciable time.\n", "API": [["numpy.ndarray.tolist()", [20, 42]]], "LVCDE": [], "RCDE": []}
{"post_id": "63034841", "sentence": "There are a set of parameters that the scipy.optimize.differential_evolution(...) function can accept, one is the init parameter which allows you to upload an array of solutions.", "API": [["scipy.optimize.differential_evolution(...)", [39, 81]]], "LVCDE": [], "RCDE": []}
{"post_id": "49331558", "sentence": "My favorite method is using numpy.memmap to create arrays that are also synced to disk.\n", "API": [["numpy.memmap", [28, 40]]], "LVCDE": [], "RCDE": []}
{"post_id": "59063977", "sentence": "After that you can simply pandas.concat(data1, data).", "API": [["pandas.concat(data1, data)", [26, 52]]], "LVCDE": [], "RCDE": [["that", [6, 10], "the action of dropping the duplicate rows from the 'data' DataFrame"]]}
{"post_id": "42897430", "sentence": "We can construct an array of indexes that matches the shape of f using numpy.meshgrid:", "API": [["numpy.meshgrid", [71, 85]]], "LVCDE": [["f", [63, 64], "a numpy array defined in 'f= np.array([[[2,1],[-1,0],[1,-1]],[[0,0],[1,1],[-1,-1]],[[0,-1],[-1,-1],[-1,-2]]])'"]], "RCDE": []}
{"post_id": "58493057", "sentence": "Bonus Material\nThe pandas.DataFrame.to_dict method will return a dictionary that looks similar to this.", "API": [["pandas.DataFrame.to_dict", [19, 43]]], "LVCDE": [], "RCDE": [["this", [98, 102], "a dictionary which has 'index', 'columns', and 'data' as keys, representing respectively the labels of rows, columns, and the data in the dataframe"]]}
{"post_id": "48039909", "sentence": "This can be done with numpy.roll which \"rolls\" the array along given axes, with exactly the kind of wrap-around that you want.", "API": [["numpy.roll", [22, 32]]], "LVCDE": [], "RCDE": [["This", [0, 4], "The process of finding the neighbors for each cell in a 2D/3D numpy array that models a box consisting of cells, while considering periodic boundary conditions."]]}
{"post_id": "55237808", "sentence": "pandas.Series implements the combine method which you could use in the following way to find the elements in b that also appear in the a lists:\n", "API": [["pandas.Series", [0, 13]]], "LVCDE": [], "RCDE": []}
{"post_id": "57042121", "sentence": "You can try sklearn.cluster.MiniBatchKMeans that does incremental updates of the centers positions using mini-batches.\n", "API": [["sklearn.cluster.MiniBatchKMeans", [12, 43]]], "LVCDE": [], "RCDE": []}
{"post_id": "39940289", "sentence": "To avoid the error that might occur because of the text at the end, you can use numpy.genfromtxt with the max_rows argument.", "API": [["numpy.genfromtxt", [80, 96]]], "LVCDE": [], "RCDE": [["the error that", [9, 23], "the incorrect format of the resulting numpy ndarray when trying to read a .txt file with a non-standard format (a header and a footer), using the pandas.read_table method"]]}
{"post_id": "60593641", "sentence": "Instead, you can create an int/float array to represent that data, by converting the dates to numbers using matplotlib.dates.date2num().\n", "API": [["matplotlib.dates.date2num()", [108, 135]]], "LVCDE": [], "RCDE": []}
{"post_id": "47711052", "sentence": "The documentation of scipy.stats.rv_continuous.fit specifies the fact, that floc and fscale keep said parameters fixed.\n", "API": [["scipy.stats.rv_continuous.fit", [21, 50]]], "LVCDE": [], "RCDE": []}
{"post_id": "32636273", "sentence": "Changing how numpy arrays are printed\nThe way that numpy arrays are displayed interactively is controlled by numpy.set_printoptions.\n", "API": [["numpy.set_printoptions", [109, 131]]], "LVCDE": [], "RCDE": []}
{"post_id": "67528707", "sentence": "Referring to the documentation of numpy.linalg.norm, you can see that the axis argument specifies the axis for computing vector norms.\n", "API": [["numpy.linalg.norm", [34, 51]]], "LVCDE": [], "RCDE": []}
{"post_id": "16699067", "sentence": "It appears that there exists a numpy.corrcoef which computes the correlation coefficients, as desired.", "API": [["numpy.corrcoef", [31, 45]]], "LVCDE": [], "RCDE": [["It", [0, 2], "The 'numpy.corrcoef' function in Python's NumPy library"]]}
{"post_id": "63362735", "sentence": "This uses numpy.s_, which creates an index expression that can be used to get slices of an array.\n", "API": [["numpy.s_", [10, 18]]], "LVCDE": [], "RCDE": []}
{"post_id": "44467420", "sentence": "We can do that in a vectorized way using numpy.argmax.", "API": [["numpy.argmax", [41, 53]]], "LVCDE": [], "RCDE": [["that", [10, 14], "The process of extracting the (x,y,z) coordinates of cloud base and cloud top from a three-dimensional array representing the values of cloud water concentration."]]}
{"post_id": "3493488", "sentence": "will show you the valid string arguments that can be sent to matplotlib.use.\n", "API": [["matplotlib.use", [61, 75]]], "LVCDE": [], "RCDE": []}
{"post_id": "65048864", "sentence": "Using numpy.random.normal with the size argument will give you an array with values that are drawn from a distribution with a mean of 0.\n", "API": [["numpy.random.normal", [6, 25]]], "LVCDE": [], "RCDE": []}
{"post_id": "13299279", "sentence": "If you're just looking for a library function that does this, just use numpy.linalg.eig and look for the eigenvector with eigenvalue equal to 1.", "API": [["numpy.linalg.eig", [71, 87]]], "LVCDE": [], "RCDE": [["this", [56, 60], "getting a non-trivial solution for the equation system defined by the code snippet"]]}
{"post_id": "55641152", "sentence": "After that we convert Month to datetime format with pandas.to_datetime, so we can sort on month.", "API": [["pandas.to_datetime", [52, 70]]], "LVCDE": [], "RCDE": [["that", [6, 10], "the process of extending the index with 4 weeks for each month, filling NaN values with forward fill method, and creating a new dataframe"]]}
{"post_id": "21812414", "sentence": "the *args in the signature numpy.apply_along_axis(func1d, axis, arr, *args) means that there are some other positional arguments could be passed.\n", "API": [["numpy.apply_along_axis(func1d, axis, arr, *args)", [27, 75]]], "LVCDE": [], "RCDE": []}
{"post_id": "70693734", "sentence": "One option is to use numpy.isclose or numpy.allclose, that are specifically designed to test close numbers.\n", "API": [["numpy.allclose", [38, 52]]], "LVCDE": [], "RCDE": []}
{"post_id": "20064224", "sentence": "And after that you can use pandas.DataFrame.apply function, with axis=1 (means apply function to each row):", "API": [["pandas.DataFrame.apply", [27, 49]]], "LVCDE": [], "RCDE": [["that", [10, 14], "loading data into a pandas DataFrame using the read_csv method"]]}
{"post_id": "17665309", "sentence": "pandas.DataFrame.from_dict is a specific DataFrame constructor that gives us the orient keyword, not available if you just say DataFrame(...).\n", "API": [["pandas.DataFrame.from_dict", [0, 26]]], "LVCDE": [], "RCDE": []}
{"post_id": "5268894", "sentence": "Update: Given that the files are originally generated by numpy, the fastest way may be numpy.memmap.\n", "API": [["numpy.memmap", [87, 99]]], "LVCDE": [], "RCDE": []}
{"post_id": "62336968", "sentence": "If you pass a negative axis value to numpy.expand_dims, the new axis will be placed in that position in the new array's shape.\n", "API": [["numpy.expand_dims", [37, 54]]], "LVCDE": [], "RCDE": []}
{"post_id": "64669771", "sentence": "SciPy has the function scipy.stats.chi2_contingency that applies the chi-square test to a contingency table.\n", "API": [["scipy.stats.chi2_contingency", [23, 51]]], "LVCDE": [], "RCDE": []}
{"post_id": "45489570", "sentence": "but if you have an existing array and you really need to populate that array with the results, you can use numpy.take:\n", "API": [["numpy.take", [107, 117]]], "LVCDE": [], "RCDE": []}
{"post_id": "25749403", "sentence": "I guess that you're looking for the numpy.savetxt which saves in a human readable format instead of the numpy.save which saves as a binary format.\n", "API": [["numpy.savetxt", [36, 49]]], "LVCDE": [], "RCDE": []}
{"post_id": "50473175", "sentence": "pandas.DataFrame.rename\npass a callable that gets applied to each index value\n", "API": [["pandas.DataFrame.rename", [0, 23]]], "LVCDE": [], "RCDE": []}
{"post_id": "46206376", "sentence": "pandas.HDFStore.put() has parameter append (which defaults to False) - that instructs Pandas to overwrite instead of appending.\n", "API": [["pandas.HDFStore.put()", [0, 21]]], "LVCDE": [], "RCDE": []}
{"post_id": "54090811", "sentence": "Yes, there is a function: numpy.extract(condition, array) returns all values from array that satifsy the condition.\n", "API": [["numpy.extract(condition, array)", [26, 57]]], "LVCDE": [], "RCDE": []}
{"post_id": "57174351", "sentence": "Your issue is that sklearn.cluster.KMeans expects a 2D matrix with [N_samples,N_features].\n", "API": [["sklearn.cluster.KMeans", [19, 41]]], "LVCDE": [], "RCDE": []}
{"post_id": "58477178", "sentence": "Then loop over that result and pandas.DataFrame.query the DataFrame for each unique value to get sub-dataframes containing only entries for each unique CallId.", "API": [["pandas.DataFrame.query", [31, 53]]], "LVCDE": [], "RCDE": [["that result", [15, 26], "the set of all unique values in the 'CallId' column of the DataFrame, obtained by using the 'pandas.Series.unique' method on the 'CallId' column"]]}
{"post_id": "44166253", "sentence": "Verify that the mpmath function gives the same results as scipy.special.lambertw:\n", "API": [["scipy.special.lambertw", [58, 80]]], "LVCDE": [], "RCDE": []}
{"post_id": "71678436", "sentence": "Now there a function like numpy.linalg.inv that let you compute the inverse if one exists (check the conditions for that) and numpy.dot which lets you multiply a matrix and a vector.\n", "API": [["numpy.dot", [126, 135]]], "LVCDE": [], "RCDE": []}
{"post_id": "23771211", "sentence": "With that said, all you need to do is call numpy.copy() after you call pygame.surfarray.array3d().", "API": [["numpy.copy()", [43, 55]]], "LVCDE": [], "RCDE": [["that said", [5, 14], "the previously mentioned issue that numpy doesn't actually update the ndarray when you work on them, it just holds temporary changes in mind, and this causes a problem when you go from pygame to opencv"]]}
{"post_id": "53800727", "sentence": "When you specified chunksize that pandas.read_sql_table will return a generator.\n", "API": [["pandas.read_sql_table", [34, 55]]], "LVCDE": [], "RCDE": []}
{"post_id": "57563383", "sentence": "Similarly, it's instructive to see that the result agrees with scipy.stats.wasserstein_distance for 1-dimensional inputs:\n", "API": [["scipy.stats.wasserstein_distance", [63, 95]]], "LVCDE": [], "RCDE": []}
{"post_id": "19269304", "sentence": "Well, that would be because scipy.signal.deconvolve() only supports 1D deconvolution!", "API": [["scipy.signal.deconvolve()", [28, 53]]], "LVCDE": [], "RCDE": [["that", [6, 10], "the issue of 'scipy.signal.deconvolve' function returning an error 'objects too deep for desired array' when trying to deconvolve a 2D array"]]}
{"post_id": "53467904", "sentence": "numpy.rint is a ufunc that will round the elements of an array to the nearest integer.\n", "API": [["numpy.rint", [0, 10]]], "LVCDE": [], "RCDE": []}
{"post_id": "60376852", "sentence": "I think you could use either pandas.merge or pandas.groupby\nBy doing a 'inner' merge you can obtain updated rows that are in website :\n", "API": [["pandas.merge", [29, 41]]], "LVCDE": [], "RCDE": []}
{"post_id": "73532834", "sentence": "After that, drop the rows that not equal to &quot;TCP&quot; (or any condition you want) by using pandas.DataFrame.drop.", "API": [["pandas.DataFrame.drop", [97, 118]]], "LVCDE": [], "RCDE": [["that", [6, 10], "the process of adding a new column 'Process_name' to the dataframe and applying the 'pandas.DataFrame.shift' function to get the value of the next row/cell of 'Proto'"]]}
{"post_id": "35683768", "sentence": "As a result, the only constraint that the scipy.optimize.minimize function sees is np.sum(np.abs(ff[-1, -1, :])**2) = 1 where -1 means the last element L-1.\n", "API": [["scipy.optimize.minimize", [42, 65]]], "LVCDE": [], "RCDE": []}
{"post_id": "65866025", "sentence": "Or if you don't require filtering to be causal, try scipy.ndimage.convolve1d, which shifts the computation so that the filter is centered:\n", "API": [["scipy.ndimage.convolve1d", [52, 76]]], "LVCDE": [], "RCDE": []}
{"post_id": "32992982", "sentence": "Eventually, note that u0, t and args remain unchanged and you can again call scipy.integrate.odeint(fun, u0, t, args).\n", "API": [["scipy.integrate.odeint(fun, u0, t, args)", [77, 117]]], "LVCDE": [], "RCDE": []}
{"post_id": "43612755", "sentence": "The changes to the names that you see are defined by the suffixes= kwarg in pandas.DataFrame.merge.\n", "API": [["pandas.DataFrame.merge", [76, 98]]], "LVCDE": [], "RCDE": []}
{"post_id": "50843253", "sentence": "You are looking for pandas.merge_asof\nIt allows you to join two DataFrames on keys that are not exact.\n", "API": [["pandas.merge_asof", [20, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "57495900", "sentence": "While not being strictly equivalent to the inf value that you requested, pandas.Timestamp.max could be a handy alternative?\n", "API": [["pandas.Timestamp.max", [73, 93]]], "LVCDE": [], "RCDE": []}
{"post_id": "28357448", "sentence": "To read data at an offset with numpy.fromfile, it's easiest to seek to that offset before calling the function.\n", "API": [["numpy.fromfile", [31, 45]]], "LVCDE": [], "RCDE": []}
{"post_id": "56975221", "sentence": "The main issue with the resampling is that you need to select which value you want to keep (using pandas.DataFrame.last or pandas.DataFrame.first).\n", "API": [["pandas.DataFrame.last", [98, 119]]], "LVCDE": [], "RCDE": []}
{"post_id": "69873223", "sentence": "I used pandas.tseries.offsets.QuarterEnd, .MonthEnd, and .YearEnd, multiplied by specific factors that change based on the input, to achieve the four values you're looking for.\n", "API": [["pandas.tseries.offsets.QuarterEnd", [7, 40]]], "LVCDE": [], "RCDE": []}
{"post_id": "42661988", "sentence": "This function does that and can be applied to a pandas.Series or to a column of a pandas.DataFrame.", "API": [["pandas.Series", [48, 61]]], "LVCDE": [], "RCDE": [["that", [19, 23], "a process of converting datetime values to epoch values by creating a datetime.timedelta, which is done by subtracting the epoch time from the date to be converted"]]}
{"post_id": "48542245", "sentence": "If you really want to use real_if_close, then you should use numpy.asscalar to convert the type numpy.array to a scalar value that format \":g\" understand.\n", "API": [["numpy.array", [96, 107]]], "LVCDE": [], "RCDE": []}
{"post_id": "50668548", "sentence": "You can use numpy.where and numpy.isnan to locate the indices where values are not nan, then use np.random.choice to pick one index that is not nan:\n", "API": [["numpy.isnan", [28, 39]]], "LVCDE": [], "RCDE": []}
{"post_id": "6390653", "sentence": "Since you suspect that numpy might be the culprit, I suggest having a look at the numpy.seterr() function.", "API": [["numpy.seterr()", [82, 96]]], "LVCDE": [], "RCDE": []}
{"post_id": "57457600", "sentence": "If that still is too slow, you could consider enhancing performance with Cython, Numba and pandas.eval(), as outlined in the pandas user guide.", "API": [["pandas.eval()", [91, 104]]], "LVCDE": [], "RCDE": [["that", [3, 7], "the solution of using 'apply' function to verify phone numbers in the dataframe"]]}
{"post_id": "55620490", "sentence": "There are two similar functions that can help you: scipy.signal.argrelmin and scipy.signal.argrelmax.\n", "API": [["scipy.signal.argrelmin", [51, 73]]], "LVCDE": [], "RCDE": []}
{"post_id": "56962526", "sentence": "You can create a view of the array with data type numpy.uint64, and then manipulate the bits in that view as needed.\n", "API": [["numpy.uint64", [50, 62]]], "LVCDE": [], "RCDE": []}
{"post_id": "12717061", "sentence": "Also, sklearn.preprocessing.Normalizer (and the TfidfVectorizer that is used for text classification) normalizes values per sample into the range [0, 1].\n", "API": [["sklearn.preprocessing.Normalizer", [6, 38]]], "LVCDE": [], "RCDE": []}
{"post_id": "34094026", "sentence": "numpy.column_stack takes care of that for you.", "API": [["numpy.column_stack", [0, 18]]], "LVCDE": [], "RCDE": [["that", [33, 37], "the issue of converting 1D arrays into 2D arrays with a single column before appending them to another array"]]}
{"post_id": "57167722", "sentence": "This looks like task for numpy.flip, that is:", "API": [["numpy.flip", [25, 35]]], "LVCDE": [], "RCDE": [["This", [0, 4], "the task of changing a 2d array 'a' into 'b', where 'a' is [[1,2,3], [3,4,5], [6,7,8], [9,10,11]] and 'b' is [[9,10,11], [6,7,8], [3,4,5], [1,2,3]]"]]}
{"post_id": "46559552", "sentence": "Here's one way to do that using pandas.qcut and np.random.permutation.", "API": [["pandas.qcut", [32, 43]]], "LVCDE": [], "RCDE": [["that", [21, 25], "split 'customer_id's into exact proportions for different groups using 'pandas.qcut' and 'np.random.permutation'."]]}
{"post_id": "32828988", "sentence": "Another way to do that would be to use numpy.concatenate .", "API": [["numpy.concatenate", [39, 56]]], "LVCDE": [], "RCDE": [["that", [18, 22], "the action of adding two elements to a numpy array, one at the beginning and one at the end"]]}
{"post_id": "55530662", "sentence": "The key function that helps you here is pandas.merge_asof with allow_exact_matches=False\n", "API": [["pandas.merge_asof", [40, 57]]], "LVCDE": [], "RCDE": []}
{"post_id": "41992237", "sentence": "Let me just mention that the whole data reading can be done in a much simpler way, using numpy.loadtxt.\n", "API": [["numpy.loadtxt", [89, 102]]], "LVCDE": [], "RCDE": []}
{"post_id": "22584930", "sentence": "If you look at the results, then you see that the constant is close to zero at approximately the default tolerance of scipy.optimize.leastsq.\n", "API": [["scipy.optimize.leastsq", [118, 140]]], "LVCDE": [], "RCDE": []}
{"post_id": "55620490", "sentence": "There are two similar functions that can help you: scipy.signal.argrelmin and scipy.signal.argrelmax.\n", "API": [["scipy.signal.argrelmax", [78, 100]]], "LVCDE": [], "RCDE": []}
{"post_id": "18253009", "sentence": "Perhaps you want to rename that variable; presumably you wanted to use the numpy.angle() function there.", "API": [["numpy.angle()", [75, 88]]], "LVCDE": [], "RCDE": [["that variable", [27, 40], "the variable defined by 'angle=40'"]]}
{"post_id": "45798172", "sentence": "Use the parse_dates parameter of pandas.read_sql to specify that DateVar column values are explicitly converted to datetime on dataframe load.\n", "API": [["pandas.read_sql", [33, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "74013313", "sentence": "In order to do that conversion, use pandas.to_datetime as follows", "API": [["pandas.to_datetime", [36, 54]]], "LVCDE": [], "RCDE": [["that conversion", [15, 30], "the process of changing the 'start_date' and 'end_date' columns from their current data type to a pandas datetime object"]]}
{"post_id": "74142029", "sentence": "You can pass the header argument into pandas.read_excel() that indicates how many rows are to be used as headers.\n", "API": [["pandas.read_excel()", [38, 57]]], "LVCDE": [], "RCDE": []}
{"post_id": "59568806", "sentence": "===\nThere's a collection of numpy.nan... functions that operate on arrays, omitting the nan.\n", "API": [["numpy.nan", [28, 37]]], "LVCDE": [], "RCDE": []}
{"post_id": "26344481", "sentence": "With pandas.DatetimeIndex, that is very simple:", "API": [["pandas.DatetimeIndex", [5, 25]]], "LVCDE": [], "RCDE": [["that", [27, 31], "the action of grouping a dataframe by financial year from April 1 to March 31 using pandas.DatetimeIndex"]]}
{"post_id": "24530544", "sentence": "if you wanted something more like a \"scatterplot that visually mirrors the dataframe's layout\"\nyou could try matplotlib.pyplot.spy\n", "API": [["matplotlib.pyplot.spy", [109, 130]]], "LVCDE": [], "RCDE": []}
{"post_id": "68978622", "sentence": "Simple slicing always creates a numpy.ndarray object that is a view over the original array.\n", "API": [["numpy.ndarray", [32, 45]]], "LVCDE": [], "RCDE": []}
{"post_id": "55213135", "sentence": "First we create a mask with pandas.DataFrame.isin\nAfter that we use np.where and ask for the opposite with ~\n", "API": [["pandas.DataFrame.isin", [28, 49]]], "LVCDE": [], "RCDE": []}
{"post_id": "42173579", "sentence": "or even better: Use numpy.isclose which already does that but allows relative and absolute tolerances and nan handling (if you need these) as well:", "API": [["numpy.isclose", [20, 33]]], "LVCDE": [], "RCDE": [["that", [53, 57], "the action of checking the closeness to a value by using the absolute difference and comparing it with a small number epsilon"]]}
{"post_id": "60900979", "sentence": "There's also a helper called numpy.result_type() that can tell you the same information without having to perform the binary operation:\n", "API": [["numpy.result_type()", [29, 48]]], "LVCDE": [], "RCDE": []}
{"post_id": "45168940", "sentence": "Here's a possible approach that uses scipy.ndimage.binary_dilation.\n", "API": [["scipy.ndimage.binary_dilation", [37, 66]]], "LVCDE": [], "RCDE": []}
{"post_id": "57095826", "sentence": "pandas.DataFrame.sample will return another pandas.DataFrame that is 2-dimensional\n", "API": [["pandas.DataFrame.sample", [0, 23]]], "LVCDE": [], "RCDE": []}
{"post_id": "42851551", "sentence": "Here's a method that takes in a list of items to add on the diagonal, using numpy.diag and taking advantage of the k parameter to specify which diagonal we want to modify.\n", "API": [["numpy.diag", [76, 86]]], "LVCDE": [], "RCDE": []}
{"post_id": "3685339", "sentence": "By default, numpy.loadtxt will ignore any lines that start with # (or whichever character is specified by the comments kwarg).\n", "API": [["numpy.loadtxt", [12, 25]]], "LVCDE": [], "RCDE": []}
{"post_id": "11885581", "sentence": "Note that for most applications, the plain one-dimensional array would work fine as both a row or column vector, but when coming from Matlab, you might prefer using numpy.matrix.\n", "API": [["numpy.matrix", [165, 177]]], "LVCDE": [], "RCDE": []}
{"post_id": "67232294", "sentence": "You can use pandas.Series.value_counts() on column to get the count of values appeared in that column.\n", "API": [["pandas.Series.value_counts()", [12, 40]]], "LVCDE": [], "RCDE": []}
{"post_id": "58437935", "sentence": "Equivalently: pandas.Series.replace has to be told that using regex is needed.\n", "API": [["pandas.Series.replace", [14, 35]]], "LVCDE": [], "RCDE": []}
{"post_id": "48044606", "sentence": "pandas.DataFrame.groupby has a sort argument that defaults to True.\n", "API": [["pandas.DataFrame.groupby", [0, 24]]], "LVCDE": [], "RCDE": []}
{"post_id": "68127710", "sentence": "You can do this by using pandas.DataFrame.idxmax:", "API": [["pandas.DataFrame.idxmax", [25, 48]]], "LVCDE": [], "RCDE": [["this", [11, 15], "finding the maximum value between 'col1', 'col2', 'col3' for every row in a dataframe and updating the last column 'max' with the column name of the maximum value"]]}
{"post_id": "69363460", "sentence": "You can use numpy.einsum function, in this case", "API": [["numpy.einsum", [12, 24]]], "LVCDE": [], "RCDE": [["this case", [38, 47], "the situation where the user wants to replicate the behavior of a for loop that performs a tensordot operation on two numpy arrays 'a' and 'b' in parallel, without actually using a for loop"]]}
{"post_id": "32082515", "sentence": "The equivalent way to do this in NumPy is to use the numpy.indices function.", "API": [["numpy.indices", [53, 66]]], "LVCDE": [], "RCDE": [["this", [25, 29], "the process of indexing 2D or 3D arrays similar to DM's i-variables in Python using NumPy"]]}
{"post_id": "42233806", "sentence": "I think you are looking for a permutation of the axes, numpy.transpose can get this job done:", "API": [["numpy.transpose", [55, 70]]], "LVCDE": [], "RCDE": [["this job", [79, 87], "the task of reshaping a numpy array of size 5000x32x32x3 to a numpy array of size 5000x3x32x32 while preserving the data."]]}
{"post_id": "42455847", "sentence": "You can control this using the fmt-parameter of numpy.savetxt.", "API": [["numpy.savetxt", [48, 61]]], "LVCDE": [], "RCDE": [["this", [16, 20], "the default behaviour of numpy.savetxt, which is to use scientific notation when dealing with arrays of dtype=np.float64"]]}
{"post_id": "46138501", "sentence": "An easy way to do this will be using numpy.linspace\nOP_BLOCK: (output omitted for annotation)\nExample:", "API": [["numpy.linspace", [37, 51]]], "LVCDE": [], "RCDE": [["this", [18, 22], "creating a sequence by specifying the start point, end point, and desired length of output in Python"]]}
{"post_id": "37663795", "sentence": "You can implement this with numpy.dot", "API": [["numpy.dot", [28, 37]]], "LVCDE": [], "RCDE": [["this", [18, 22], "the computation of the output layers in a neural network model"]]}
{"post_id": "71284632", "sentence": "One way to do this is to leverage pandas' pandas.DataFrame.groupby and pandas.DataFrame.groupby.GroupBy.apply functions.", "API": [["pandas.DataFrame.groupby", [42, 66]]], "LVCDE": [], "RCDE": [["this", [14, 18], "The task of calculating a difference score between two cells in the columns 'Negative Emotions - Mean', with the same values in the columns 'participant_id' and 'session'."]]}
{"post_id": "20543409", "sentence": "May be more appropriate way to do this is to use pandas.Series.map():", "API": [["pandas.Series.map()", [49, 68]]], "LVCDE": [], "RCDE": [["this", [34, 38], "the action of 'recoding' an integer variable into a float variable in a Pandas DataFrame"]]}
{"post_id": "71297399", "sentence": "if possible duplicated sets\nIn this case, you also need to compare with the sets of equal size, the best is to use frozenset and pandas.Series.duplicated:", "API": [["pandas.Series.duplicated", [129, 153]]], "LVCDE": [], "RCDE": [["this case", [31, 40], "The situation where the user is trying to filter a dataframe to remove rows where the values in column A are subsets of other values in column A. The user also wants to handle possible duplicated sets, which means they need to compare with sets of equal size. The recommended method for this is to use 'frozenset' and 'pandas.Series.duplicated'."]]}
{"post_id": "2809518", "sentence": "If you pass the file handle rather than the file name to numpy.savetxt() you avoid this race condition:", "API": [["numpy.savetxt()", [57, 72]]], "LVCDE": [], "RCDE": [["this race condition", [83, 102], "the situation where two open file handles are competing for the same filename, as in the original code snippet."]]}
{"post_id": "51144238", "sentence": "One way to do this is using numpy.where, which lets us select between two arrays depending on a condition.", "API": [["numpy.where", [28, 39]]], "LVCDE": [], "RCDE": [["this", [14, 18], "The process of selecting an index from a 2D Numpy array (e.g., A1), subtracting A1 element-wise from another array (A2) wherever both corresponding elements are nonzero, and where the selected value is less than the A1 element, subtracting that selected value instead of the A1 element."]]}
{"post_id": "32011629", "sentence": "The work-around solution for this problem is using the more generic scipy.integrate.ode function.", "API": [["scipy.integrate.od", [68, 86]]], "LVCDE": [], "RCDE": [["this problem", [29, 41], "The issue of wanting to include another time-dependent set of data into the calculations using Scipy's odeint function."]]}
{"post_id": "8505754", "sentence": "You can also use numpy.random.shuffle() and slicing, but this will be less efficient:", "API": [["numpy.random.shuffle()", [17, 39]]], "LVCDE": [], "RCDE": []}
{"post_id": "26288975", "sentence": "If M&gt;=N you could also use scipy.optimize.leastsq for this task:", "API": [["scipy.optimize.leastsq", [30, 52]]], "LVCDE": [], "RCDE": [["this task", [57, 66], "the task of finding the optimum values of variables \\(x1, x2,...,x5\\) in the function \\(yn = a_11*x1**2 + a_12*x2**2 + ... + a_m*xn**2\\) that satisfy the conditions of \\(y1, y2, y3\\)"]]}
{"post_id": "21836184", "sentence": "The location of this file is found by calling matplotlib.matplotlib_fname().", "API": [["matplotlib.matplotlib_fname()", [46, 75]]], "LVCDE": [], "RCDE": [["this file", [16, 25], "the matplotlib rcfile, a configuration file for matplotlib which can be used to set defaults for the matplotlib library"]]}
{"post_id": "33261924", "sentence": "you can do this with scipy.interpolate.interp2d and numpy.meshgrid.", "API": [["numpy.meshgrid", [52, 66]]], "LVCDE": [], "RCDE": [["this", [11, 15], "the process of interpolating a 2D array 'test' of dimensions 4x4 to a grid of shape 8x8 using scipy.interpolate.interp2d and numpy.meshgrid"]]}
{"post_id": "24805785", "sentence": "You can do this by selecting the row you want to operate on and using numpy.roll.", "API": [["numpy.roll", [70, 80]]], "LVCDE": [], "RCDE": [["this", [11, 15], "the action of modifying a 2D numpy array by rolling only one of its component rows"]]}
{"post_id": "63963105", "sentence": "The amazing numpy.argsort() function makes this task really simple.", "API": [["numpy.argsort()", [12, 27]]], "LVCDE": [], "RCDE": [["this task", [43, 52], "finding the index of the second highest number in each row of a numpy.ndarray"]]}
{"post_id": "42795371", "sentence": "numpy.isnan can then be used on this float array.", "API": [["numpy.isnan", [0, 11]]], "LVCDE": [], "RCDE": [["this float array", [32, 48], "the array 'A' which is converted to float type using the method 'astype(float)'."]]}
{"post_id": "62963076", "sentence": "Using numpy.logical_or.reduce, we can express this as", "API": [["numpy.logical_or", [6, 22]]], "LVCDE": [], "RCDE": [["this", [46, 50], "the action of applying the 'or' boolean operator to all arrays of a matrix"]]}
{"post_id": "55776915", "sentence": "An easy way to do this is by using numpy.array():", "API": [["numpy.array()", [35, 48]]], "LVCDE": [], "RCDE": [["this", [18, 22], "converting the index array 'ind' to a boolean array"]]}
{"post_id": "52035987", "sentence": "In order to do this with scipy, look at scipy.optimize.minimize with the SLSQP method.", "API": [["scipy.optimize.minimize", [40, 63]]], "LVCDE": [], "RCDE": [["this", [15, 19], "fitting two regions of experimental data with two different mathematical functions, ensuring they connect smoothly at a specific point"]]}
{"post_id": "48462832", "sentence": "numpy.loadtxt will auto separate by whitespace so this should work\n", "API": [["numpy.loadtxt", [0, 13]]], "LVCDE": [], "RCDE": []}
{"post_id": "55001336", "sentence": "You can do this using numpy.fromfunction():", "API": [["numpy.fromfunction()", [22, 42]]], "LVCDE": [], "RCDE": [["this", [11, 15], "applying a function on each point of a 3D numpy array"]]}
{"post_id": "75274561", "sentence": "You can approach this by using pandas.Series.str and pandas.Series.map.", "API": [["pandas.Series.map", [53, 70]]], "LVCDE": [], "RCDE": [["this", [17, 21], "the problem of changing the text variable in the data set, specifically, each row has categorical values in the object format that need to be changed, depending on the last character in the data set"]]}
{"post_id": "75274561", "sentence": "You can approach this by using pandas.Series.str and pandas.Series.map.", "API": [["pandas.Series.str", [31, 48]]], "LVCDE": [], "RCDE": [["this", [17, 21], "the task of changing the text variable in a dataset based on the last character of each row"]]}
{"post_id": "48296565", "sentence": "As suggested in the comments, this can be solved quickly by using scipy.signal.fftconvolve.", "API": [["scipy.signal.fftconvolve", [66, 90]]], "LVCDE": [], "RCDE": [["this", [30, 34], "the problem of computing the convolution product of two probability density functions efficiently"]]}
{"post_id": "41733570", "sentence": "An easy way to keep track of this is using the numpy.ndarray.shape property:", "API": [["numpy.ndarray.shape", [47, 66]]], "LVCDE": [], "RCDE": [["this", [29, 33], "the process of dimensionality reduction or changes in the shape of the numpy array when the sum operation is performed along an axis or multiple axes"]]}
{"post_id": "48353272", "sentence": "Using ix_ is preferable for this use case but numpy.meshgrid could also be used.", "API": [["numpy.meshgrid", [46, 60]]], "LVCDE": [], "RCDE": [["this use case", [28, 41], "The scenario where an individual wants to modify an original numpy array object based on some indices, without creating a copy"]]}
{"post_id": "57725664", "sentence": "You can do this kind of conversion while reading the file with pandas.read_csv rather than looping.", "API": [["pandas.read_csv", [63, 78]]], "LVCDE": [], "RCDE": [["this kind of conversion", [11, 34], "The conversion from the scientific notation with 'D' (like 1.23D+4) to the one with 'E' (like 1.23E+4)."]]}
{"post_id": "16240957", "sentence": "You can do this with numpy.in1d if you create structured array view.", "API": [["numpy.in1d", [21, 31]]], "LVCDE": [], "RCDE": [["this", [11, 15], "the process of using numpy.in1d to perform an efficient set operation on the rows of a 2D array (in this case, finding the intersection of the rows of 'K' and 'f(K)')."]]}
{"post_id": "51327628", "sentence": "You can get around this by making the function vectorized, using numpy.vectorize.", "API": [["numpy.vectorize", [65, 80]]], "LVCDE": [], "RCDE": [["this", [19, 23], "The problem that 'scipy.integrate.nquad' can only integrate a single (n-dimensional) function and will always return a single value, instead of applying to a multidimensional vector element-wise"]]}
{"post_id": "41135886", "sentence": "EDIT: \nA more recent simpler/better way of handling this problem with scikit-learn is using the class sklearn.preprocessing.OneHotEncoder", "API": [["sklearn.preprocessing.OneHotEncoder", [102, 137]]], "LVCDE": [], "RCDE": [["this problem", [52, 64], "The issue of handling unknown values for label encoding in scikit-learn, specifically when encoding of categorical variables via one-hot-encoder and unknown labels appear in the cross-validation step of the pipeline"]]}
{"post_id": "68264810", "sentence": "For avoiding the nested lists using comprehension, use numpy.zeros\nlook this solution:\nfor 2D:", "API": [["numpy.zeros", [55, 66]]], "LVCDE": [], "RCDE": [["this solution", [72, 85], "the solution for creating multidimensional arrays"]]}
{"post_id": "41312595", "sentence": "There is a method for this - pandas.DataFrame.drop_duplicates:", "API": [["pandas.DataFrame.drop_duplicates", [29, 61]]], "LVCDE": [], "RCDE": [["this", [22, 26], "extracting all unique combinations of values of columns Col1, Col2, and Col3 from a dataframe"]]}
{"post_id": "75314273", "sentence": "You can use numpy.argsort to vectorize this behavior.", "API": [["numpy.argsort", [12, 25]]], "LVCDE": [], "RCDE": [["this behavior", [39, 52], "the process of finding the indices in the 'ranks' array for all of the elements in 'field' using numpy's argwhere in a list comprehension, which is currently making the script extremely slow for large contest sizes and simulations"]]}
{"post_id": "10195347", "sentence": "Ideally, you'd just use plt.table, but in this case, it's easier to use matplotlib.table.Table directly:", "API": [["matplotlib.table.Table", [72, 94]]], "LVCDE": [], "RCDE": [["in this case", [39, 51], "the case where the user wants to draw a table (rectangle) as a plot with 96 individual cells (8 rows X 12 cols), color each alternative cell with a specific color (like a chess board but with different colors), and insert value for each cell from a pandas data frame or python dictionary. The user also wants to show the col and row labels on the side."]]}
{"post_id": "57404251", "sentence": "If this doesn't work, get rid of the error_kw and, before you create the figure, specify matplotlib.rcParams.update({'errorbar.capsize': 2}).", "API": [["matplotlib.rcParams.update({'errorbar.capsize': 2})", [89, 140]]], "LVCDE": [], "RCDE": [["this", [3, 7], "The suggestion to add 'markeredgewidth=10' to the barplot call in the code"]]}
{"post_id": "43941974", "sentence": "numpy.loadtxt() allows a dtype parameter, something like this will read mixed data:\n", "API": [["numpy.loadtxt()", [0, 15]]], "LVCDE": [], "RCDE": []}
{"post_id": "20627638", "sentence": "Using scipy, you can compute this with the ppf method of the scipy.stats.norm object.", "API": [["scipy.stats.norm object", [61, 84]]], "LVCDE": [], "RCDE": [["this", [29, 33], "the inverse of the cumulative distribution function (CDF) of the standard normal distribution"]]}
{"post_id": "70960874", "sentence": "If the target machine support 128 bit double-precision numbers, then you could try the numpy.longdouble type but I do not expect this to be the case.\n", "API": [["numpy.longdouble", [87, 103]]], "LVCDE": [], "RCDE": []}
{"post_id": "29319669", "sentence": "We can do this by applying numpy.unique on reversed version of the first column received from the file.", "API": [["numpy.unique", [27, 39]]], "LVCDE": [], "RCDE": [["this", [10, 14], "The process of finding the final size for each number in the given data by applying the numpy.unique function on the reversed version of the first column received from the file."]]}
{"post_id": "63835495", "sentence": "For this site, there's no need for beautifulsoup or requests\npandas.read_html creates a list of DataFrames for each &lt;table&gt; at the URL.", "API": [["pandas.read_html", [61, 77]]], "LVCDE": [], "RCDE": [["this site", [4, 13], "the website 'https://www.acf.hhs.gov/orr/resource/ffy-2012-13-state-of-colorado-orr-funded-programs'"]]}
{"post_id": "59786570", "sentence": "To show this I use scipy.signal.convolve which allows to use the full mode:", "API": [["scipy.signal.convolve", [19, 40]]], "LVCDE": [], "RCDE": [["this", [8, 12], "The process of performing a correlation operation (specifically convolution in this case) on a 3-dimensional image using scipy's ndimage.convolve function."]]}
{"post_id": "18793834", "sentence": "numpy.diff is useful in this case.", "API": [["numpy.diff", [0, 10]]], "LVCDE": [], "RCDE": [["this case", [24, 33], "the situation where a numpy array contains booleans and the user wants to determine whether that array contains only a single contiguous block of Trues, i.e., the sequence ...,True, False, ..., True... never occurs"]]}
{"post_id": "58805570", "sentence": "You can easily achieve this using a pandas.DataFrame.", "API": [["pandas.DataFrame", [36, 52]]], "LVCDE": [], "RCDE": [["this", [23, 27], "the process of converting a list of dictionaries with potentially inconsistent keys into a structured pandas DataFrame, which can then be easily exported to an Excel file"]]}
{"post_id": "36705673", "sentence": "You can achieve all this with one line, using numpy.savetxt():", "API": [["numpy.savetxt()", [46, 61]]], "LVCDE": [], "RCDE": [["this", [20, 24], "the process of writing a numpy array to a CSV file with headers"]]}
{"post_id": "70133533", "sentence": "numpy.ravel_multi_index is able to do this for you:", "API": [["numpy.ravel_multi_index", [0, 23]]], "LVCDE": [], "RCDE": [["this", [38, 42], "the task of creating a unique integer for each unique array and creating a histogram of those integers"]]}
{"post_id": "48742150", "sentence": "I Solved this first by writing a new file with \\t separations and then using pandas.read_csv('file_name',sep='\\t').", "API": [["pandas.read_csv('file_name',sep='\\t')", [77, 114]]], "LVCDE": [], "RCDE": [["this", [9, 13], "The problem of creating a DataFrame from a routing table data, where the data has varying number of columns due to the presence or absence of a prefix in the network column."]]}
{"post_id": "58807624", "sentence": "You could also use numpy.empty in this case, but I prefer the determinism of numpy.zeros.", "API": [["numpy.empty", [19, 30]]], "LVCDE": [], "RCDE": [["this case", [34, 43], "The situation where the user wants to create a new array of a certain size, replacing the for loop in the provided code with a numpy function for efficiency."]]}
{"post_id": "74891230", "sentence": "Last step is to save this to a .CSV using pandas.DataFrame.to_csv", "API": [["pandas.DataFrame.to_csv", [42, 65]]], "LVCDE": [], "RCDE": [["this", [21, 25], "the DataFrame 'out' which contains the difference between the last and first row of each CSV file"]]}
{"post_id": "74322452", "sentence": "I tried to explain how solve_ivp uses this at\nscipy.integrate.solve_ivp vectorized", "API": [["scipy.integrate.solve_ivp", [46, 71]]], "LVCDE": [], "RCDE": [["this", [38, 42], "refers to the 'vectorized' parameter in the scipy.optimize module."]]}
{"post_id": "59203201", "sentence": "You can work around this issue by passing a fully-formed pandas.IndexSlice as the subset argument:", "API": [["pandas.IndexSlice", [57, 74]]], "LVCDE": [], "RCDE": [["this issue", [20, 30], "The problem encountered when using the pandas.io.formats.style.Styler.format function in the Python pandas library. The user tries to format a specific row of a DataFrame, but encounters an error when following the documentation."]]}
{"post_id": "32624585", "sentence": "The quick way to accomplish this is by indexing with None (which is equivalent to numpy.newaxis):", "API": [["numpy.newaxis", [82, 95]]], "LVCDE": [], "RCDE": [["this", [28, 32], "The task of making the indexing arrays broadcastable by adding an axis to the first index 'i' to match the shape with the rest"]]}
{"post_id": "12584701", "sentence": "numpy.linalg.lstsq solves this for you.", "API": [["numpy.linalg.lstsq", [0, 18]]], "LVCDE": [], "RCDE": [["this", [26, 30], "the problem of finding coefficients (c1, c2, etc.) that best approximate a given array (array3) as a sum of other arrays (array1, array2, etc.) multiplied by these coefficients"]]}
{"post_id": "25717561", "sentence": "There is a pandas.Series method (where incidentally) for exactly this kind of task.", "API": [["pandas.Series", [11, 24]]], "LVCDE": [], "RCDE": [["this kind of task", [65, 82], "the task of replacing the 0 values with NaN values when all row values are zero in a pandas DataFrame, while maintaining other values in the row in the cases where all row values are not zero"]]}
{"post_id": "36161403", "sentence": "E.g., scipy.stats.beta.fit will fit the best parameters of the Beta distribution (all scipy distributions have this method).\n", "API": [["scipy.stats.beta.fit", [6, 26]]], "LVCDE": [], "RCDE": []}
{"post_id": "35073894", "sentence": "You could use numpy.fromfunction like this", "API": [["numpy.fromfunction", [14, 32]]], "LVCDE": [], "RCDE": [["this", [38, 42], "an example of how to use numpy.fromfunction to apply a function over all cells of a numpy array without looping."]]}
{"post_id": "66556000", "sentence": "You are probably looking for numpy.triu(), which supports exactly this behavior.", "API": [["numpy.triu()", [29, 41]]], "LVCDE": [], "RCDE": [["this behavior", [66, 79], "a NumPy function that takes a matrix and an integer k as parameters and returns a copy of the matrix with all elements below the kth diagonal set to zero"]]}
{"post_id": "54791423", "sentence": "Therefore, I have now fixed this by using just the matplotlib.figure.Figure class, as listed below (with a bit more nonsensical data, to avoid the use of pyplot completely).", "API": [["matplotlib.figure.Figure", [51, 75]]], "LVCDE": [], "RCDE": [["this", [28, 32], "The problem of the entire Tkinter-based program shutting down after finalizing a PDF report."]]}
{"post_id": "34965827", "sentence": "To convert this result to a list like the one you specified, you can use a combination of numpy.where and numpy.choose:", "API": [["numpy.where", [90, 101]]], "LVCDE": [], "RCDE": [["this result", [11, 22], "a boolean array 'samples' where each column corresponds to an option from the dictionary 'choices' and each row contains one try. The array 'samples' is the result of comparing random samples generated by the 'numpy.random.uniform' function to the probabilities in 'choices'. The comparison is done such that each column of the generated random samples is compared to the corresponding probability from 'choices'."]]}
{"post_id": "61392682", "sentence": "The best way I found to do this was to use scipy.optimize.curve_fit.", "API": [["scipy.optimize.curve_fit", [43, 67]]], "LVCDE": [], "RCDE": [["this", [27, 31], "fitting a lognormal distribution function to a set of particle size data that are binned by size and normalized by the bin width"]]}
{"post_id": "45939045", "sentence": "To make things easier, I often use matplotlib.cm.ScalarMappable (link to documentation) which handles this transformation automatically.", "API": [["matplotlib.cm.ScalarMappable", [35, 63]]], "LVCDE": [], "RCDE": [["this transformation", [102, 121], "the process of normalizing data correctly for a colormap, i.e., converting a data value into a value in the range [0-1] for getting a color value from a colormap"]]}
{"post_id": "64324510", "sentence": "One way to do this is to subclass pandas.DataFrame and add _metadata.", "API": [["pandas.DataFrame", [34, 50]]], "LVCDE": [], "RCDE": [["this", [14, 18], "Adding a description to a column in a pandas DataFrame"]]}
{"post_id": "26940240", "sentence": "You can compute this with scipy.stats.binom.expect:", "API": [["scipy.stats.binom", [26, 43]]], "LVCDE": [], "RCDE": [["this", [16, 20], "the expectation of the function 'f' with respect to a binomial distribution with 'n = 5000' and 'p = 0.5'"]]}
{"post_id": "35715035", "sentence": "numpy.where determines the locations in the array where this condition is satisfied.", "API": [["numpy.where", [0, 11]]], "LVCDE": [], "RCDE": [["this condition", [56, 70], "a logical AND operation between two numpy arrays, specifically where the values in array 'x' are 1 and the corresponding values in array 'y' are 4"]]}
{"post_id": "20627118", "sentence": "EDIT:\nIt looks like the numpy.bmat function was written just for this purpose:", "API": [["numpy.bmat", [24, 34]]], "LVCDE": [], "RCDE": [["this purpose", [65, 77], "combining four 2x2 matrices (Mat_1, Mat_2, Mat_3, Mat_4) into a single 4x4 matrix"]]}
{"post_id": "55236235", "sentence": "To avoid this, pandas.ExcelFile.parse method accepts an argument called converters, you could use this to tell Pandas the specific column data type by:", "API": [["pandas.ExcelFile.parse", [15, 37]]], "LVCDE": [], "RCDE": [["this", [9, 13], "the issue of Pandas interpreting and inferring column data types automatically, leading to the string format 101101114501700 being converted to 101101114501700.0 when the xls file is converted to csv format using pandas"]]}
{"post_id": "45912605", "sentence": "In this case sct.grab() returns a PIL Image, and numpy.array(Image) will thus convert the PIL Image object into a numpy ndarray.", "API": [["numpy.array(Image)", [49, 67]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "the context of using the mss library to capture a screenshot, convert it to a numpy array, and then apply the matchTemplate function from the cv2 library. The user is encountering an error because the data type of the image is not supported."]]}
{"post_id": "72223801", "sentence": "Then as per this answer you can use pandas.DataFrame.groupby in a list comprehension to return a list of split DataFrames.", "API": [["pandas.DataFrame.groupby", [36, 60]]], "LVCDE": [], "RCDE": [["this answer", [12, 23], "lice a DataFrame into many DataFrames when the sum of column 'Score' is greater than 50,000, involving cumulatively summing the 'Score', floor dividing it by 50,000, and shifting it up one cell."]]}
{"post_id": "30761316", "sentence": "numpy.meshgrid will do this for you.", "API": [["numpy.meshgrid", [0, 14]]], "LVCDE": [], "RCDE": [["this", [23, 27], "converting two 1D lists into separate 2D arrays, where each item is repeated for the length of the original lists"]]}
{"post_id": "58532920", "sentence": "From this point, calculating the euclidean distance using numpy is pretty straightforward using numpy.linalg.norm.", "API": [["numpy.linalg.norm", [96, 113]]], "LVCDE": [], "RCDE": [["this point", [5, 15], "the point in the process after the binary data is converted into a 32x32 numpy array using the provided code"]]}
{"post_id": "49120001", "sentence": "In this case, I would suggest working with the sklearn.linear_model.Ridge class and have the cross-validation of alpha at the GridSearchCV level, e.g.", "API": [["sklearn.linear_model.Ridge", [47, 73]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "the situation where the user is trying to use 'RidgeCV' in a pipeline with 'GridSearchCV' and 'SelectKBest' for hyperparameter optimization in a machine learning model. The user is having difficulty accessing the 'alpha' parameter of 'RidgeCV' after fitting the model."]]}
{"post_id": "25470132", "sentence": "For this purpose the numpy.nonzero is very useful.", "API": [["numpy.nonzero", [21, 34]]], "LVCDE": [], "RCDE": [["this purpose", [4, 16], "the need to store indices of elements in an array for easier and efficient Boolean indexing"]]}
{"post_id": "65854708", "sentence": "In this case, the hours and minutes in the 'total_time column, represent an absolute amount of time, so the column should be converted with pandas.to_timedelta.", "API": [["pandas.to_timedelta", [140, 159]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "the scenario where the user is trying to process time data stored in the 'total_time' column of a pandas DataFrame."]]}
{"post_id": "69102135", "sentence": "You can overcome this issue by replacing pandas' plot with matplotlib.pyplot.plot.", "API": [["matplotlib.pyplot.plot", [59, 81]]], "LVCDE": [], "RCDE": [["this issue", [17, 27], "the problem that when using pandas' plot function to create a line graph from a DataFrame with a DatetimeIndex, it only lists every other year on the x-axis, instead of listing each year"]]}
{"post_id": "63001638", "sentence": "To do this you can use numpy.random.choice.", "API": [["numpy.random.choice", [23, 42]]], "LVCDE": [], "RCDE": [["this", [6, 10], "The task of generating random samples from the IDs without replacement for each day in the schedule, using the numpy.random.choice function."]]}
{"post_id": "70759141", "sentence": "Yes, you can do this using scipy.optimize.minimize.", "API": [["scipy.optimize.minimize", [27, 50]]], "LVCDE": [], "RCDE": [["this", [16, 20], "find an alternate way of estimating a parameter beta in an ODE system"]]}
{"post_id": "31003889", "sentence": "you can do this with numpy.argmax and numpy.indices.", "API": [["numpy.argmax", [21, 33]]], "LVCDE": [], "RCDE": [["this", [11, 15], "the action of finding the location of the maximum values in a 3D numpy array (X) and creating a 2D array from the corresponding locations in another 3D numpy array (Y) with the same dimensions"]]}
{"post_id": "46625700", "sentence": "You can do this with a combination of numpy.unique, to find the unique classes, and masks to find elements in the array matching the given class:", "API": [["numpy.unique", [38, 50]]], "LVCDE": [], "RCDE": [["this", [11, 15], "the task of returning an amount of numpy arrays with labels, equal to the amount of unique classes in binary representation"]]}
{"post_id": "62783110", "sentence": "To be a bit cleaner, you can simplify this more by using the keepdims argument of numpy.sum to maintain the singleton dimension after you sum in the third dimension.", "API": [["numpy.sum", [82, 91]]], "LVCDE": [], "RCDE": [["this", [38, 42], "The process of normalizing an image"]]}
{"post_id": "52669783", "sentence": "The more numpythonic way to do this is to avoid broadcasting, and use the function designed for this: numpy.column_stack:", "API": [["numpy.column_stack", [102, 120]]], "LVCDE": [], "RCDE": [["this", [31, 35], "the task of concatenating numpy arrays 'a' and 'b'"]]}
{"post_id": "53597057", "sentence": "Since this question is tagged numpy but for some reason nobody posts the numpy solution... use numpy.cumsum.", "API": [["numpy.cumsum", [95, 107]]], "LVCDE": [], "RCDE": [["this question", [6, 19], "a question about creating a unique, chronological temperature profile from experimental data, specifically needing to create a new list where every item is the sum from the ones before."]]}
{"post_id": "57376914", "sentence": "A way to achieve this is with a matplotlib.patches.ConnectionPatch", "API": [["matplotlib.patches.ConnectionPatch", [32, 66]]], "LVCDE": [], "RCDE": [["this", [17, 21], "emphasizing the data points by drawing grid-like lines that are parallel to the axis but only fill the space between the axis and data points in a subplot"]]}
{"post_id": "27652427", "sentence": "You can compute this with \nnumpy.bincount.", "API": [["numpy.bincount", [27, 41]]], "LVCDE": [], "RCDE": [["this", [16, 20], "the process of creating a (N-by-#classes) matrix whose (i,j) element is the sum of distances from i-th point to its k-NN points with the class label 'j'."]]}
{"post_id": "28801050", "sentence": "Since you've tagged this question scipy, you could use scipy.stats.rankdata:", "API": [["scipy.stats.rankdata", [55, 75]]], "LVCDE": [], "RCDE": [["this question", [20, 33], "generating the rank order of a list, specifically the list named 'somelist'"]]}
{"post_id": "26086869", "sentence": "numpy.where gives the xs and ys to index this array to get all the True values.", "API": [["numpy.where", [0, 11]]], "LVCDE": [], "RCDE": [["this array", [41, 51], "The boolean array resulting from the comparison 'a_rownames == b_rownames[:, numpy.newaxis]'. This array is a result of broadcasting comparison between 'a_rownames' and 'b_rownames[:, numpy.newaxis]', where True indicates a match between row names in arrays 'a' and 'b'."]]}
{"post_id": "22009299", "sentence": "To get the upper triangle of this array, use numpy.triu:", "API": [["numpy.triu", [45, 55]]], "LVCDE": [], "RCDE": [["this array", [29, 39], "a 2D numpy array named 'dists' which contains the euclidean distances between pairs of points represented by the X and Y numpy arrays."]]}
{"post_id": "55031699", "sentence": "To read a .csv file using numpy, I think you can use numpy.genfromtext or numpy.loadtxt (from this question).", "API": [["numpy.loadtxt", [74, 87]]], "LVCDE": [], "RCDE": [["this question", [94, 107], "how to read CSV data into a record array in numpy"]]}
{"post_id": "53335131", "sentence": "Still, this can give you some hints on how to combine pandas.DataFrame.plot and matplotlib.pyplot.bar with different y axis.", "API": [["pandas.DataFrame.plot", [54, 75]]], "LVCDE": [], "RCDE": [["this", [7, 11], "'import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nwidth = 1\nfig, ax1 = plt.subplots()\nvalue_x = np.arange(cir['value'])\nax1.bar(x, cir['value'], width = 3*width)\nax2 = ax1.twinx()\ndf.unstack('Size').plot(kind='bar', stacked=True, ax=ax2, secondary_y=True, width=width)\nplt.show()'"]]}
{"post_id": "63451297", "sentence": "If you don't want uniform distribution of zeros and ones, this can be controlled directly using numpy.random.choice:\n", "API": [["numpy.random.choice", [96, 115]]], "LVCDE": [], "RCDE": []}
{"post_id": "65376977", "sentence": "One way to go about this is using numpy.linalg.lstsq:", "API": [["numpy.linalg.lstsq", [34, 52]]], "LVCDE": [], "RCDE": [["this", [20, 24], "the task of finding the values of a and b that fit the given experimental data in a linear equation Y=a*X+b, where X is a vector with 3 entries"]]}
{"post_id": "74537688", "sentence": "You can do this one with pandas.DataFrame.pivot :", "API": [["pandas.DataFrame.pivot", [25, 47]]], "LVCDE": [], "RCDE": [["this one", [11, 19], "the task of merging two dataframes such that df2 adds all its rows with matching 'on' values as new columns in df1"]]}
{"post_id": "60637924", "sentence": "Now you can fit a model on this data, let's fit sklearn.linear_model.LogisticRegression", "API": [["sklearn.linear_model.LogisticRegression", [48, 87]]], "LVCDE": [], "RCDE": [["this data", [27, 36], "Data portions are derived from a pandas DataFrame 'df' that contains three columns (X1, X2, Y). X1 and X2 are independent variables, and Y is the dependent variable."]]}
{"post_id": "7253436", "sentence": "In this situation numpy.unique can be used to generate an array of unique \"key\" values:", "API": [["numpy.unique", [18, 30]]], "LVCDE": [], "RCDE": [["this situation", [3, 17], "The situation where the user has a NumPy array with two columns and 'k' rows. One column serves as a numerical indicator (e.g., '2 = male', '1 = female', '0 = unknown') and the second column is a list of values or scores. The user wants to find the standard deviation of the values for all rows with each indicator (0, 1, 2)."]]}
{"post_id": "9515518", "sentence": "You can do this using numpy.ix_:", "API": [["numpy.ix_", [22, 31]]], "LVCDE": [], "RCDE": [["this", [11, 15], "the action of extracting specific rows and columns from a matrix using a single 'fancy' slice"]]}
{"post_id": "58807624", "sentence": "You could also use numpy.empty in this case, but I prefer the determinism of numpy.zeros.", "API": [["numpy.zeros", [77, 88]]], "LVCDE": [], "RCDE": [["this case", [34, 43], "the situation where the user wants to create an array of elements in a more efficient way than using a loop, specifically in the context of the provided code where an array of length 5012 is created and filled with zeros using numpy.zeros"]]}
{"post_id": "69759291", "sentence": "The use of numpy.matrix is discouraged, and using numpy.array also fixes this issue:", "API": [["numpy.array", [50, 61]]], "LVCDE": [], "RCDE": [["this issue", [73, 83], "The error 'could not broadcast input array from shape (3,1) into shape (3,)' that occurs in the line 'Q[:,i] = u / norm' of the QRfactorization function in the provided Python code"]]}
{"post_id": "7691018", "sentence": "The scipy.io.savemat can apparently take a dictionary of dictionaries of arrays, so this structure", "API": [["scipy.io.savemat", [4, 20]]], "LVCDE": [], "RCDE": [["this structure", [84, 98], "a complex data structure represented as a dictionary of dictionaries of arrays in Python. It is defined in the following lines of code:\n'data = {\n    'bigdata' : {\n        'a' : array([1, 2, 3]),\n        'b' : array([1, 2, 3]),\n        'c' : array([1, 2, 3]),\n     }\n}'"]]}
{"post_id": "71284632", "sentence": "One way to do this is to leverage pandas' pandas.DataFrame.groupby and pandas.DataFrame.groupby.GroupBy.apply functions.", "API": [["pandas.DataFrame.groupby", [42, 66]]], "LVCDE": [], "RCDE": [["this", [14, 18], "The process of calculating a difference score between two cells in the columns 'Negative Emotions - Mean'."]]}
{"post_id": "54969734", "sentence": "this can be done by setting the parameter markerfirstof the matplotlib.pyplot.legend to False.", "API": [["matplotlib.pyplot.legend", [60, 84]]], "LVCDE": [], "RCDE": [["this", [0, 4], "bringing the text on the left side of the marker/symbol in the legend"]]}
{"post_id": "41451082", "sentence": "You can try doing this in two ways:\nWith set_table_styles from pandas.DataFrame.style:", "API": [["pandas.DataFrame.style", [63, 85]]], "LVCDE": [], "RCDE": [["this", [18, 22], "the action of adding a background color to the header of tables in an email"]]}
{"post_id": "23037373", "sentence": "You can do this using sklearn.preprocessing.StandardScaler.", "API": [["sklearn.preprocessing.StandardScaler", [22, 58]]], "LVCDE": [], "RCDE": [["this", [11, 15], "scaling your columns to have mean 0 and variance 1"]]}
{"post_id": "43770002", "sentence": "One good solution for making this faster is pandas.DataFrame.eval():\nTL;DR", "API": [["pandas.DataFrame.eval()", [44, 67]]], "LVCDE": [], "RCDE": [["this", [29, 33], "the process of interpolating a column named 'interp' in a pandas DataFrame, where the calculation is done using the 'apply' method"]]}
{"post_id": "55006722", "sentence": "Then this can be called like this, using pandas.concat to assemble the partial results:", "API": [["pandas.concat", [41, 54]]], "LVCDE": [], "RCDE": [["this", [5, 9], "the method 'get_data' in the provided code snippet"]]}
{"post_id": "29102476", "sentence": "In this case, I'd use numpy.column_stack:", "API": [["numpy.column_stack", [22, 40]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "a scenario where the user has two 'records' shaped in a certain way (an array of numbers from 0 to 57, stacked vertically to form a 2D array), and they want to reshape or slice the data to create a new array where each pair of numbers (e.g., [0, 0], [1, 1], ..., [57, 57]) is on a separate row without having to iterate through the original array"]]}
{"post_id": "67263712", "sentence": "In this case you do not need to convert array to ints before feeding into numpy.flatnonzero.", "API": [["numpy.flatnonzero", [74, 91]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "the scenario where the user wants to find the indices of the differences between two numpy arrays of equal length"]]}
{"post_id": "59810129", "sentence": "You can try solving this using scipy.optimize.fsolve:", "API": [["scipy.optimize.fsolve", [31, 52]]], "LVCDE": [], "RCDE": [["this", [20, 24], "The system of non-linear equations defined in the function 'nonlin' in the code snippet"]]}
{"post_id": "35581607", "sentence": "To avoid this specific warning I would suggest using numpy.not_equal:", "API": [["numpy.not_equal", [53, 68]]], "LVCDE": [], "RCDE": [["this specific warning", [9, 30], "FutureWarning: comparison to `None` will result in an elementwise object comparison in the future, which is caused by the line of code 'assert self.lastobs != None'"]]}
{"post_id": "65844617", "sentence": "In fact, numpy has this built in as well, as numpy.bitwise_or:", "API": [["numpy.bitwise_or", [45, 61]]], "LVCDE": [], "RCDE": [["this", [19, 23], "the bitwise OR operator"]]}
{"post_id": "36795076", "sentence": "Use scipy.stats.expon to fit the expononential distribution to this data.", "API": [["scipy.stats.expon", [4, 21]]], "LVCDE": [], "RCDE": [["this data", [63, 72], "an array of numbers"]]}
{"post_id": "36052000", "sentence": "The fit method of scipy.stats.t returns (df, loc, scale), so this line", "API": [["scipy.stats.t", [18, 31]]], "LVCDE": [], "RCDE": [["this line", [61, 70], "the line of code 'pdf_fitted = t.pdf(x,loc=param[0],scale=param[1],df=param[2])'"]]}
{"post_id": "50558892", "sentence": "Using numpy.partition() is significantly faster than performing full sort for this purpose:", "API": [["numpy.partition()", [6, 23]]], "LVCDE": [], "RCDE": [["this purpose", [78, 90], "the purpose of getting the top N (maximal) args & values across an entire numpy matrix, as opposed to across a single dimension (rows / columns)"]]}
{"post_id": "62718386", "sentence": "You can use numpy.gradient for this in the following way:", "API": [["numpy.gradient", [12, 26]]], "LVCDE": [], "RCDE": [["this", [31, 35], "the task of calculating the gradient of a vector field in every point of a given array of coordinates"]]}
{"post_id": "50001232", "sentence": "You could do this way using numpy.newaxis:", "API": [["numpy.newaxis", [28, 41]]], "LVCDE": [], "RCDE": [["this way", [13, 21], "achieve pairwise multiplication of two arrays, where one array has two columns and the other only has one column."]]}
{"post_id": "68850866", "sentence": "You can also use numpy.digitize, but this gives you the indexes of the bins:\n", "API": [["numpy.digitize", [17, 31]]], "LVCDE": [], "RCDE": []}
{"post_id": "54736371", "sentence": "We can do this using pandas.DataFrame.iterrows:", "API": [["pandas.DataFrame.iterrows", [21, 46]]], "LVCDE": [], "RCDE": [["this", [10, 14], "The process of iterating through each row of a DataFrame to replace each NaN value with possible values for each column."]]}
{"post_id": "21708526", "sentence": "This will allow you to use numpy.concatenate, which is more appropriate for this case than numpy.append, and I think it's easier to read.", "API": [["numpy.concatenate", [27, 44]]], "LVCDE": [], "RCDE": [["This", [0, 4], "the method of iterating over the keys instead of iterating over the dictionaries in order to merge dictionaries of numpy arrays"]]}
{"post_id": "73110990", "sentence": "I like pandas.json_normalize for this because you don't need to supply the column names:", "API": [["pandas.json_normalize", [7, 28]]], "LVCDE": [], "RCDE": [["this", [33, 37], "The task of processing a list of dictionaries, where some dictionaries contain a list of dictionaries, to create a dataframe that has flattened the nested dictionaries and added new column headers"]]}
{"post_id": "72862775", "sentence": "You can use the pandas.Series.str.len function for this task:", "API": [["pandas.Series.str.len", [16, 37]]], "LVCDE": [], "RCDE": [["this task", [51, 60], "the task of selecting only the rows in a Pandas DataFrame where the string in 'col1' has 3 characters"]]}
{"post_id": "14130246", "sentence": "numpy.seterr provides an alternative to handling RuntimeWarning this way, though:", "API": [["numpy.seterr", [0, 12]]], "LVCDE": [], "RCDE": [["this way", [64, 72], "the method of using the 'warnings' module in Python to catch and handle the warning."]]}
{"post_id": "52838665", "sentence": "An easy way to do this is with numpy.correlate.", "API": [["numpy.correlate", [31, 46]]], "LVCDE": [], "RCDE": [["this", [18, 22], "The operation where elements from two lists (X and Y in the context) are multiplied in a certain pattern to generate a new list (Z)."]]}
{"post_id": "70754759", "sentence": "You could use the scipy.optimize.root for achieving this as described here.", "API": [["scipy.optimize.root", [18, 37]]], "LVCDE": [], "RCDE": [["this", [52, 56], "the task of finding the root of a function"]]}
{"post_id": "53335131", "sentence": "Still, this can give you some hints on how to combine pandas.DataFrame.plot and matplotlib.pyplot.bar with different y axis.", "API": [["pandas.DataFrame.plot", [54, 75]], ["matplotlib.pyplot.bar", [80, 101]]], "LVCDE": [], "RCDE": [["this", [7, 11], "'import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nwidth = 1\nfig, ax1 = plt.subplots()\nvalue_x = np.arange(cir['value'])\nax1.bar(x, cir['value'], width = 3*width)\nax2 = ax1.twinx()\ndf.unstack('Size').plot(kind='bar', stacked=True, ax=ax2, secondary_y=True, width=width)\nplt.show()'"]]}
{"post_id": "60553520", "sentence": "However, to get the correct plot in this case, you can use matplotlib.axes.Axes.plot function instead of pandas.DataFrame.plot as follows.", "API": [["matplotlib.axes.Axes.plot", [59, 84]]], "LVCDE": [], "RCDE": [["this case", [36, 45], "the situation where the user is trying to plot bars and a line on different y axes on the same chart using matplotlib, but the bars and line do not appear on the same chart"]]}
{"post_id": "51944022", "sentence": "One way to overcome this is to make the 'A' column an index and use loc on the newly generated pandas.DataFrame.", "API": [["pandas.DataFrame", [95, 111]]], "LVCDE": [], "RCDE": [["this", [20, 24], "The problem of selecting rows from a pandas DataFrame based on values in a column ('A' for instance) where the order from the input list of 'A' values is preserved"]]}
{"post_id": "70005371", "sentence": "The correct way to do this with pandas is with pandas.DataFrame.groupby and pandas.DataFrame.plot.", "API": [["pandas.DataFrame.plot", [76, 97]]], "LVCDE": [], "RCDE": [["this", [22, 26], "the problem of making a scatter plot with the geyser dataset from seaborn and coloring the points based on the 'kind' column, where the legend only shows 'long' but leaves out 'short', and a simpler way to color code the data that does not use a for-loop"]]}
{"post_id": "4091495", "sentence": "In Python 2, you can do this directly with numpy.fromstring:", "API": [["numpy.fromstring", [43, 59]]], "LVCDE": [], "RCDE": [["this", [24, 28], "the action of converting a string of binary data to a numpy array using the numpy.fromstring function in Python 2"]]}
{"post_id": "26442781", "sentence": "To fit this data to\na log-normal distribution using scipy.stats.lognorm, use:", "API": [["scipy.stats.lognorm", [52, 71]]], "LVCDE": [], "RCDE": [["this data", [7, 16], "the logarithm of which follows a normal distribution."]]}
{"post_id": "25454732", "sentence": "For each category users can change this behaviour with numpy.seterr.", "API": [["numpy.seterr", [55, 67]]], "LVCDE": [], "RCDE": [["this behaviour", [35, 49], "The behaviour refers to how Numpy treats floating point errors."]]}
{"post_id": "16491409", "sentence": "Then use numpy.correlate to calculate the autocorrelation, using the method described in the answers to this question.", "API": [["numpy.correlate", [9, 24]]], "LVCDE": [], "RCDE": [["this question", [104, 117], "how to calculate autocorrelation"]]}
{"post_id": "53746348", "sentence": "You can vectorize this with pandas.Series.where:", "API": [["pandas.Series.where", [28, 47]]], "LVCDE": [], "RCDE": [["this", [18, 22], "the operation of filling in the unknowns in the 'Country' column of the dataframe 'df' by getting the corresponding country from the 'city2country_mapping' dictionary for each city"]]}
{"post_id": "42707430", "sentence": "You should then be able to cross reference this list against the SKUs using a standard pandas.DataFrame.merge", "API": [["pandas.DataFrame.merge", [87, 109]]], "LVCDE": [], "RCDE": [["this list", [43, 52], "the dataframe created from the Excel file, which contains inventory data for each store and item."]]}
{"post_id": "48306699", "sentence": "I believe you can do this with a combination of numpy.where and numpy.all.", "API": [["numpy.where", [48, 59]]], "LVCDE": [], "RCDE": [["this", [21, 25], "the task of creating a 'null' class for the image segmentation model."]]}
{"post_id": "75367311", "sentence": "Using numpy.linalg.lstsq, this could look like:", "API": [["numpy.linalg.lstsq", [6, 24]]], "LVCDE": [], "RCDE": [["this", [26, 30], "the implementation of linear least squares using numpy.linalg.lstsq to speed up calculations as an alternative to numpy's polyfit"]]}
{"post_id": "53806018", "sentence": "To fix this you can DataFrame.reindex with pandas.date_range and fill missing entries with 0", "API": [["pandas.date_range", [43, 60]]], "LVCDE": [], "RCDE": [["this", [7, 11], "the issue of gaps in the data, where for example there is no 'msg_count' value at 2015-01-01 09:00"]]}
{"post_id": "22639759", "sentence": "You could also compute this with scipy.special.ndtr.", "API": [["scipy.special.ndtr", [33, 51]]], "LVCDE": [], "RCDE": [["this", [23, 27], "the computation of the survival function, S(x; s), of the Gaussian distribution using scipy.special.ndtr"]]}
{"post_id": "56217816", "sentence": "Okay, I just used matplotlib.pyplot.figure, and I could embed this on the canvas as well.", "API": [["matplotlib.pyplot.figure", [18, 42]]], "LVCDE": [], "RCDE": [["this", [62, 66], "the object created using matplotlib.pyplot.figure"]]}
{"post_id": "26179873", "sentence": "You could use numpy.all and index broadcasting for this", "API": [["numpy.all", [14, 23]]], "LVCDE": [], "RCDE": [["this", [51, 55], "The process of using numpy.all and index broadcasting to simplify the process of filtering and subsetting a numpy array."]]}
{"post_id": "73452741", "sentence": "Here is a way to do this kind of transformation by using pandas.DataFrame.explode :", "API": [["pandas.DataFrame.explode", [57, 81]]], "LVCDE": [], "RCDE": [["this kind of transformation", [20, 47], "The transformation of a pandas DataFrame where the number of rows is equal to the sum of a count column. Each row represents an instance of an event that took place during a given day. For example, if an event took place 3 times on Wednesday and 1 time on Thursday, there would be 3 'W' rows and 1 'Th' row."]]}
{"post_id": "67228619", "sentence": "or in this case just use .replace method of pandas.Series like so", "API": [["pandas.Series", [44, 57]]], "LVCDE": [], "RCDE": [["in this case", [3, 15], "the situation where a user is trying to convert string values in a column to binary values in a pandas DataFrame, but the initially proposed method (using a user-defined function with the .apply method) is causing errors"]]}
{"post_id": "51475384", "sentence": "For this you may use the inbuild mechanism matplotlib.dates.date2num", "API": [["matplotlib.dates.date2num", [43, 68]]], "LVCDE": [], "RCDE": [["this", [4, 8], "the need to convert a datetime object to a number in order to place text on plots using datetime as xlocation"]]}
{"post_id": "46304463", "sentence": "You can do this using pandas.cut", "API": [["pandas.cut", [22, 32]]], "LVCDE": [], "RCDE": [["this", [11, 15], "counting the number of pandas DataFrame rows in each bin and making a list of the counts"]]}
{"post_id": "73515052", "sentence": "You can use numpy.einsum (np.einsum('ijk,ik-&gt;ij', t, normal)) to get this result:", "API": [["numpy.einsum", [12, 24]]], "LVCDE": [], "RCDE": [["this result", [72, 83], "The result of the numpy.einsum operation, which computes the dot products between each row of the 2D array 't' and each corresponding 3D matrix in 'normal'. The result is another 2D array with shape (2, 10), where each element is the dot product of the corresponding row in 't' and matrix in 'normal'."]]}
{"post_id": "73431777", "sentence": "If this is truly multiclass, you're likely looking for sklearn.preprocessing.label_binarize():", "API": [["sklearn.preprocessing.label_binarize()", [55, 93]]], "LVCDE": [], "RCDE": [["this", [3, 7], "The issue of generating a ROC curve based on predictions from a classifier using the two best performing features in the data set, where a ValueError: multiclass format is not supported is encountered."]]}
{"post_id": "74720519", "sentence": "However, this could be more straight forward with pandas.concat.", "API": [["pandas.concat", [50, 63]]], "LVCDE": [], "RCDE": [["this", [9, 13], "the method of using an outer join to combine two dataframes and keep the newest data available"]]}
{"post_id": "67797557", "sentence": "You can do this with the at method of the numpy.add ufunc.", "API": [["numpy.add", [42, 51]]], "LVCDE": [], "RCDE": [["this", [11, 15], "improving the speed of counting the number of times an index appears in a matrix"]]}
{"post_id": "34965827", "sentence": "To convert this result to a list like the one you specified, you can use a combination of numpy.where and numpy.choose:", "API": [["numpy.choose", [106, 118]]], "LVCDE": [], "RCDE": [["this result", [11, 22], "a boolean array generated by comparing random samples to the probabilities in the 'choices' dictionary."]]}
{"post_id": "62998081", "sentence": "Try this with pandas.DataFrame.any:", "API": [["pandas.DataFrame.any", [14, 34]]], "LVCDE": [], "RCDE": [["this", [4, 8], "Codes:'df.groupby(['ChildID','MotherID']).agg(lambda x: 'Yes' if (x>4000).any() else 'No').reset_index()'"]]}
{"post_id": "63973195", "sentence": "Use pandas.to_datetime and specify unit='s', for this data.", "API": [["pandas.to_datetime", [4, 22]]], "LVCDE": [], "RCDE": [["this data", [49, 58], "the 'log_time' column in the dataframe, which contains unix timestamp"]]}
{"post_id": "40293189", "sentence": "Or even better use matplotlib.pyplot.hist2d to avoid this issue completely.", "API": [["matplotlib.pyplot.hist2d", [19, 43]]], "LVCDE": [], "RCDE": [["this issue", [53, 63], "The problem of the plot produced by the original code being rotated or not matching the input data, due to the image space convention of `plt.imshow` where `(0, 0)` is in the top right corner and a y-axis that is oriented downwards."]]}
{"post_id": "70681332", "sentence": "pandas.DataFrame.all is made for this kind of operation, just have to specify axis=1 to operate over columns:", "API": [["pandas.DataFrame.all", [0, 20]]], "LVCDE": [], "RCDE": [["this kind of operation", [33, 55], "The operation of adding a column at the end of a Boolean DataFrame, which says True if all the values in the column are True and False if any of the value in the column is False"]]}
{"post_id": "67956074", "sentence": "Something like this can be also achieved by numpy.lib.stride_tricks.as_strided", "API": [["numpy.lib.stride_tricks.as_strided", [44, 78]]], "LVCDE": [], "RCDE": [["this", [15, 19], "The process of splitting an array into multiple sub-arrays, where each sub-array has a length of 10 (bs), and the last 2 (ct) values of each sub-array are the first 2 values of the next sub-array."]]}
{"post_id": "21708526", "sentence": "This will allow you to use numpy.concatenate, which is more appropriate for this case than numpy.append, and I think it's easier to read.", "API": [["numpy.append", [91, 103]]], "LVCDE": [], "RCDE": [["This", [0, 4], "the method of iterating over the keys instead of iterating over the dictionaries"]]}
{"post_id": "67834557", "sentence": "Then you can create a DataFrame from this Series, using pandas.DataFrame.explode() :", "API": [["pandas.DataFrame.explode()", [56, 82]]], "LVCDE": [], "RCDE": [["this Series", [37, 48], "a pandas Series object defined in  's = df.set_index(\"Name\").apply(sundays, axis=1)'."]]}
{"post_id": "62287596", "sentence": "Anyone coming to this now may want to consider: pandas.Series.to_frame().", "API": [["pandas.Series.to_frame()", [48, 72]]], "LVCDE": [], "RCDE": [["this", [17, 21], "the problem of forcing matrix multiplication 'orientation' using Python Pandas, especially in the context of operations between DataFrames and Series"]]}
{"post_id": "57135346", "sentence": "In this case, the above result can also be\ncomputed in a vectorized calculation with scipy.special.rel_entr or scipy.special.kl_div.", "API": [["scipy.special.kl_div", [111, 131]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "The case where the user has a matrix (numpy 2d array) in which each row is a valid probability distribution and another vector (numpy 1d array), also a prob dist, and needs to compute KL divergence between each row of the matrix and the vector without using for loops"]]}
{"post_id": "42221868", "sentence": "You can use numpy.where instead apply, also for select by boolean indexing with column(s) is better use this solution:", "API": [["numpy.where", [12, 23]]], "LVCDE": [], "RCDE": []}
{"post_id": "68920365", "sentence": "For this purpose, you can use pandas.melt:", "API": [["pandas.melt", [30, 41]]], "LVCDE": [], "RCDE": [["this purpose", [4, 16], "reshaping the dataframe in a different format to make the desired plot"]]}
{"post_id": "17383336", "sentence": "You can use matplotlib.patches.ConnectionPatch to make this connection.", "API": [["matplotlib.patches.ConnectionPatch", [12, 46]]], "LVCDE": [], "RCDE": [["this connection", [55, 70], "the correlation between the boreholes (the subplots), specifically, drawing a line connecting the top and bottom of all the zoned areas across all the boreholes"]]}
{"post_id": "70604285", "sentence": "As suggested by joni, this is doable by using the scipy.optimize.minimize library.", "API": [["scipy.optimize.minimize", [50, 73]]], "LVCDE": [], "RCDE": [["this", [22, 26], "the method of finding the value of x such that P1+P2 equals some target and Q1+Q2 is minimum"]]}
{"post_id": "45453120", "sentence": "In this case, you're looking for scipy.stats.norm.ppf.", "API": [["scipy.stats.norm", [33, 49]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "the situation where the user is transitioning from R to Python and is trying to find an equivalent function in Python for the 'qnorm' function in R"]]}
{"post_id": "62370964", "sentence": "We could still use this with pandas.read_excel() if we set the encoding to None:", "API": [["pandas.read_excel()", [29, 48]]], "LVCDE": [], "RCDE": [["this", [19, 23], "the blob object obtained from the method 'download_blob().content_as_text(encoding=None)' of the blob client"]]}
{"post_id": "59280487", "sentence": "From the legend guide, you can do this by creating a \u2018proxy artist\u2019 for each line using matplotlib.lines.Line2D like so", "API": [["matplotlib.lines.Line2D", [88, 111]]], "LVCDE": [], "RCDE": [["this", [34, 38], "the action of matching the colors in the custom legend with the colors of the plotted lines."]]}
{"post_id": "61857708", "sentence": "Check out this section in the pandas docs on vectorized operations with pandas.Series (this applies to operations on individual columns of a DataFrame as well).", "API": [["pandas.Series", [72, 85]]], "LVCDE": [], "RCDE": [["this section", [10, 22], "the section in the pandas documentation that discusses vectorized operations with pandas.Series"]]}
{"post_id": "71341086", "sentence": "We need to understand how pandas.DataFrame.pivot_table works in order to solve this problem.", "API": [["pandas.DataFrame.pivot_table", [26, 54]]], "LVCDE": [], "RCDE": [["this problem", [79, 91], "the task of adding a pivot table in excel using python script with pandas to count the number of 'Met' and 'Missed' entries for each 'Priority'"]]}
{"post_id": "44298786", "sentence": "Consider using numpy.append for this task.", "API": [["numpy.append", [15, 27]]], "LVCDE": [], "RCDE": [["this task", [32, 41], "The task of creating a numpy array from the 'X' and 'Y' attributes of the 'activities_and_attractions' dataframe, where the original approach was to use a loop and the 'vstack' function of numpy. The original code was inefficient and only printed the last row of the dataframe."]]}
{"post_id": "64898859", "sentence": "You can use numpy.ufunc.accumulate, to accumulate the result of applying a function, maximum in this case, over all elements:", "API": [["numpy.ufunc.accumulate", [12, 34]]], "LVCDE": [], "RCDE": [["this case", [96, 105], "the function 'maximum' is applied to the numpy array [1, 2, 10, 5, 6, 12, 9, 9, 8] using numpy.ufunc.accumulate to keep only the maximum value seen so far as the array is traversed from beginning to end"]]}
{"post_id": "62108115", "sentence": "If you want to do this in more traditional way then use numpy.apply_along_axis(func, axis, arr) with axis 2", "API": [["numpy.apply_along_axis(func, axis, arr)", [56, 95]]], "LVCDE": [], "RCDE": [["this", [18, 22], "the process of extracting RGB-encoded pixel labels from a numpy array and converting the shape of the array from (256, 256, 3) to (256, 256, 1) using a custom function"]]}
{"post_id": "67532705", "sentence": "You might use numpy.where for this task", "API": [["numpy.where", [14, 25]]], "LVCDE": [], "RCDE": [["this task", [30, 39], "the task of retaining the maximum value in each row of a numpy array and changing all the other values to zero"]]}
{"post_id": "67528458", "sentence": "Once captured, you can read this data into pandas with pandas.read_csv", "API": [["pandas.read_csv", [55, 70]]], "LVCDE": [], "RCDE": [["this data", [28, 37], "the CSV data that records patient_id, date, scan_type, and volume of the tumor for each patient"]]}
{"post_id": "62963076", "sentence": "The usual way to do this would be to apply numpy.any along an axis:", "API": [["numpy.any", [43, 52]]], "LVCDE": [], "RCDE": [["this", [20, 24], "the act of applying the same boolean operator (specifically 'or') to all elements of an array without using a 'for' loop, as illustrated in the context by the operation 'b=a[0] | a[1] | a[2]'"]]}
{"post_id": "68419588", "sentence": "We will use torch.gather (similarly you can achieve this with numpy.take).", "API": [["numpy.take", [62, 72]]], "LVCDE": [], "RCDE": [["this", [52, 56], "the operation of constructing a new tensor based on two identically shaped tensors containing the indices and the values."]]}
{"post_id": "28502110", "sentence": "You can improve on this by using numpy.fromiter if you need \"leaner\" text I/O.", "API": [["numpy.fromiter", [33, 47]]], "LVCDE": [], "RCDE": [["this", [19, 23], "the inefficient way of loading a CSV file into a numpy array using np.loadtxt or by manually building up a list and then converting it to a numpy array"]]}
{"post_id": "55660036", "sentence": "Here's one way you could achieve this effect using reindexing and pandas.concat:", "API": [["pandas.concat", [66, 79]]], "LVCDE": [], "RCDE": [["this effect", [33, 44], "inserting the header of a dataframe into a new row when the value in the 'Strategy' column changes"]]}
{"post_id": "74552826", "sentence": "You can do this with pandas.DataFrame.sort_values :", "API": [["pandas.DataFrame.sort_values", [21, 49]]], "LVCDE": [], "RCDE": [["this", [11, 15], "sorting a DataFrame by two-level index where the order of 0-level index is determined by the sum of values from 'Value' column (in descending order), and the order of 1-level index is also determined by the values in 'Value' column"]]}
{"post_id": "29862365", "sentence": "You can extrapolate data with scipy.interpolate.UnivariateSpline as illustrated in this answer.", "API": [["scipy.interpolate.UnivariateSpline", [30, 64]]], "LVCDE": [], "RCDE": [["this answer", [83, 94], "a solution provided in an external link (https://stackoverflow.com/questions/1599754/is-there-easy-way-in-python-to-extrapolate-data-points-to-the-future) that illustrates how to extrapolate data using scipy.interpolate.UnivariateSpline"]]}
{"post_id": "50662490", "sentence": "Lastly you can achieve this with numpy.pad, specifying a pad width for each axis.", "API": [["numpy.pad", [33, 42]]], "LVCDE": [], "RCDE": [["this", [23, 27], "The act of achieving image resize by padding only on the top and bottom of the image"]]}
{"post_id": "9790857", "sentence": "numpy.bincount was introduced for this purpose:", "API": [["numpy.bincount", [0, 14]]], "LVCDE": [], "RCDE": [["this purpose", [34, 46], "the specific array operation where an array of floats 'w' and an array of indices 'idx' of the same length as 'w' are given, and the goal is to sum up all 'w' with the same 'idx' value and collect them in an array 'v'. This operation was originally performed as a loop, but the user was looking for a way to do this with array operations."]]}
{"post_id": "39394239", "sentence": "Then read this file using sklearn.datasets.load_svmlight_file", "API": [["sklearn.datasets.load_svmlight_file", [26, 61]]], "LVCDE": [], "RCDE": [["this file", [10, 19], "the CSV file that has been converted to libsvm format, which contains the processed document data"]]}
{"post_id": "3153537", "sentence": "If you don't plan on doing this process many times over, then I would recommend using Cython to speed up the numpy.where line.", "API": [["numpy.where", [109, 120]]], "LVCDE": [], "RCDE": [["this process", [27, 39], "The process of selecting a random value from each row of a numpy array, excluding the -1 values."]]}
{"post_id": "41875483", "sentence": "For this case we are using pandas.DataFrame.fillna method", "API": [["pandas.DataFrame.fillna", [27, 50]]], "LVCDE": [], "RCDE": [["this case", [4, 13], "The situation where a new column 'total energy consumption' is created in a pandas DataFrame. This column is calculated by multiplying 'energy_per_capita' and 'pop_2015' columns, using 'pop_2014' as a fallback value when 'pop_2015' is NaN."]]}
{"post_id": "43322098", "sentence": "In this example numpy.maximum will be much faster:", "API": [["numpy.maximum", [16, 29]]], "LVCDE": [], "RCDE": [["this example", [3, 15], "the case where numpy.maximum is used instead of numpy.vectorize for creating a numpy array based on a function called for each element"]]}
{"post_id": "46267570", "sentence": "You can do this in Python using the function scipy.stats.chi2_contingency (see URL_BLOCK: (output omitted for annotation) for documentation).", "API": [["scipy.stats.chi2_contingency", [45, 73]]], "LVCDE": [], "RCDE": [["this", [11, 15], "performing a chi-squared test for independence of the proportions between the different classes"]]}
{"post_id": "30355016", "sentence": "Another way to do this could be use as_index parameter of the pandas.DataFrame.groupby():", "API": [["pandas.DataFrame.groupby()", [62, 88]]], "LVCDE": [], "RCDE": [["this", [18, 22], "avoid losing access to the columns when grouping data in a pandas DataFrame"]]}
{"post_id": "44258486", "sentence": "You can achieve this by using pandas.DataFrame.where -", "API": [["pandas.DataFrame.where", [30, 52]]], "LVCDE": [], "RCDE": [["this", [16, 20], "the action of making 'member' column to be equal to 'Rank' when 'window' column equals 3, in a groupby statement grouping on 'id' in the DataFrame 'df'"]]}
{"post_id": "46683267", "sentence": "One way of doing this is to use scipy.interpolate.interp1d, the documentation can be found here.", "API": [["scipy.interpolate.interp1d", [32, 58]]], "LVCDE": [], "RCDE": [["this", [17, 21], "the task of finding 100 points on a curve by interpolating the data"]]}
{"post_id": "26288975", "sentence": "Directly using scipy.optimize.minimize() the code below solves this problem.", "API": [["scipy.optimize.minimize()", [15, 40]]], "LVCDE": [], "RCDE": [["this problem", [63, 75], "The problem refers to the issue the user is having with the package lmfit.minimize minimization procedure in Python."]]}
{"post_id": "73337066", "sentence": "We can do this using the matplotlib.colors.LinearSegmentedColormap.from_list() function.", "API": [["matplotlib.colors.LinearSegmentedColormap.from_list()", [25, 78]]], "LVCDE": [], "RCDE": [["this", [10, 14], "creating a custom colormap which starts at black and ends at green in matplotlib"]]}
{"post_id": "55561115", "sentence": "numpy.nanmean does this exactly.", "API": [["numpy.nanmean", [0, 13]]], "LVCDE": [], "RCDE": [["this", [19, 23], "computing the average of the array by excluding the 'nan' values"]]}
{"post_id": "68922312", "sentence": "You can use numpy.tile for this", "API": [["numpy.tile", [12, 22]]], "LVCDE": [], "RCDE": [["this", [27, 31], "the task of converting a 2D array to a 3D array by duplicating the values along the z-axis"]]}
{"post_id": "15976131", "sentence": "You can do this with numpy.lexsort", "API": [["numpy.lexsort", [21, 34]]], "LVCDE": [], "RCDE": [["this", [11, 15], "the act of sorting a structured numpy array in both ascending and descending order based on the column"]]}
{"post_id": "74016281", "sentence": "Another option for this last use case would be using pandas.Series.str.replace as", "API": [["pandas.Series.str.replace", [53, 78]]], "LVCDE": [], "RCDE": [["this last use case", [19, 37], "the task of updating the column 'final' in the dataframe 'df' to store the values of the column 'final', but without the spaces"]]}
{"post_id": "35301995", "sentence": "You can easily subclass matplotlib.colors.Normalize for this purpose.", "API": [["matplotlib.colors.Normalize", [24, 51]]], "LVCDE": [], "RCDE": [["this purpose", [56, 68], "the purpose of creating a custom normalization function in order to correctly display ticks and labels on a matplotlib colorbar"]]}
{"post_id": "30554550", "sentence": "You can use numpy.outer to help you do this outer product step.", "API": [["numpy.outer", [12, 23]]], "LVCDE": [], "RCDE": [["this outer product step", [39, 62], "an operation that performs the outer product of two vectors."]]}
{"post_id": "57862055", "sentence": "And for this task, you can use numpy.isin:", "API": [["numpy.isin", [31, 41]]], "LVCDE": [], "RCDE": [["this task", [8, 17], "the task of selecting N different values in a numpy array and setting other values to zero"]]}
{"post_id": "48306699", "sentence": "I believe you can do this with a combination of numpy.where and numpy.all.", "API": [["numpy.all", [64, 73]]], "LVCDE": [], "RCDE": [["this", [21, 25], "the process of creating a 'null' class in the ndarray for image segmentation model where the 'null' class represents the pixels not covered by the existing 5 classes"]]}
{"post_id": "16598397", "sentence": "You can use numpy.argsort to do this quickly:", "API": [["numpy.argsort", [12, 25]]], "LVCDE": [], "RCDE": [["this", [32, 36], "The process of sorting an array 'a' and keeping track of its corresponding vectors in another array 'b'."]]}
{"post_id": "54951507", "sentence": "If this is the case, then you can easily leverage numpy.unique to help:", "API": [["numpy.unique", [50, 62]]], "LVCDE": [], "RCDE": [["this", [3, 7], "the situation where the user wants to use the unique values of the 'quotient' array to categorize their data"]]}
{"post_id": "75242125", "sentence": "One way you could do this is using multiple calls to numpy.array_split", "API": [["numpy.array_split", [53, 70]]], "LVCDE": [], "RCDE": [["this", [21, 25], "the action of splitting a matrix into equally smaller matrices of m x n size, and if the matrix is not divisible by the given size, putting the remainder into a different matrix"]]}
{"post_id": "66727040", "sentence": "You can try something like this with numpy.full:", "API": [["numpy.full", [37, 47]]], "LVCDE": [], "RCDE": [["this", [27, 31], "a method to add a column of zeros to the beginning of a numpy array, defined as follows: \n\nx = 0\nnew = np.full((arr1.shape[0], arr1.shape[1] + 1), x)\nnew[:, 1:] = arr1"]]}
{"post_id": "41158156", "sentence": "You can do this with numpy.convolve, for example:", "API": [["numpy.convolve", [21, 35]]], "LVCDE": [], "RCDE": [["this", [11, 15], "the process of applying kernel convolution for smoothing a numpy array"]]}
{"post_id": "53613060", "sentence": "One way to solve this is to use the function pandas.pivot_table instead.", "API": [["pandas.pivot_table", [45, 63]]], "LVCDE": [], "RCDE": [["this", [17, 21], "the problem with the method pandas.DataFrame.pivot that does not handle duplicate values in the index"]]}
{"post_id": "38149245", "sentence": "This is called deconvolution: scipy.signal.deconvolve will do this for you.", "API": [["scipy.signal.deconvolve", [30, 53]]], "LVCDE": [], "RCDE": [["this", [62, 66], "the process of computing the inverse filter or deconvolution, which is the process of reversing the convolution operation in order to recover the original signal from the convolved signal"]]}
{"post_id": "49836904", "sentence": "You can do this via pandas.concat, sorting and dropping druplicates:", "API": [["pandas.concat", [20, 33]]], "LVCDE": [], "RCDE": [["this", [11, 15], "a method to scan df2 on df1 and store df2['values'] in df1['values'] if df2['ID'] matches df1['ID']"]]}
{"post_id": "54409317", "sentence": "For example, if you used pandas.read_csv, you should switch this to dask.dataframe.read_csv.", "API": [["pandas.read_csv", [25, 40]]], "LVCDE": [], "RCDE": [["this", [60, 64], "the method of creating a Dask DataFrame by first loading the data into a Pandas DataFrame"]]}
{"post_id": "42094658", "sentence": "A nice way to do this in one line using pandas.concat():", "API": [["pandas.concat()", [40, 55]]], "LVCDE": [], "RCDE": [["this", [17, 21], "the action of prepending a level to the MultiIndex of a DataFrame"]]}
{"post_id": "51029521", "sentence": "Since this is not currently possible, you can use scipy.ndimage.median instead.", "API": [["scipy.ndimage.median", [50, 70]]], "LVCDE": [], "RCDE": [["this", [6, 10], "The ability to use 'np.median' as a generalized ufunc in numpy."]]}
{"post_id": "61587919", "sentence": "You can use the pandas.read_csv() function to accomplish this very easily.", "API": [["pandas.read_csv()", [16, 33]]], "LVCDE": [], "RCDE": [["this", [57, 61], "read a text file and create a dataframe, while specifying the delimiter, column names, and rows to skip."]]}
{"post_id": "25454732", "sentence": "You can suppress this exception by calling numpy.seterr(invalid='warn'), or, alternatively, invalid='ignore'.", "API": [["numpy.seterr(invalid='warn')", [43, 71]]], "LVCDE": [], "RCDE": [["this exception", [17, 31], "The 'FloatingPointError: invalid value encountered in subtract' exception that the user encounters in their code"]]}
{"post_id": "36690244", "sentence": "To correct this issue, you can simply append numpy.array with a dtype specification for float64 as you did above", "API": [["numpy.array", [45, 56]]], "LVCDE": [], "RCDE": [["this issue", [11, 21], "The problem encountered in the Python code where a TypeError occurs when trying to use the numpy.linalg.solve function due to an incorrect type signature."]]}
{"post_id": "55764112", "sentence": "numpy.average indeed works pretty well for this task.", "API": [["numpy.average", [0, 13]]], "LVCDE": [], "RCDE": [["this task", [43, 52], "the task of building a semantic search engine by encoding objects in the database (into 512-dim vectors), encoding the query, augmenting objects with additional categories from Wikipedia, averaging all encoded vectors per object, and then using a k-NN algorithm to find results"]]}
{"post_id": "45155307", "sentence": "If you still wanna use np.where you could do something like this with pandas.isnull:", "API": [["pandas.isnull", [70, 83]]], "LVCDE": [], "RCDE": [["this", [60, 64], "Codes:'df3 = pd.DataFrame(np.where(pd.isnull(df1),np.nan,df2))'"]]}
{"post_id": "51753202", "sentence": "I made use of scipy.signal.detrend() to eliminate this and got a very appropriate looking DFT.", "API": [["scipy.signal.detrend()", [14, 36]]], "LVCDE": [], "RCDE": [["this", [50, 54], "the offset present in the signal that is causing issues with the huge peak at 0 Hz in the DFT and is also responsible for most of the noise in the transformed signal"]]}
{"post_id": "63622903", "sentence": "pandas.to_datetime should parse this happily if you tweak the strings slightly:", "API": [["pandas.to_datetime", [0, 18]]], "LVCDE": [], "RCDE": [["this", [32, 36], "the date and time strings in the 'datepicker' column of a dataframe, which are in the format '2019-09-04 16:00 UTC+3'"]]}
{"post_id": "40672660", "sentence": "Using this approach needs to have set matplotlib.rcParams['legend.numpoints'] = 1, otherwise two markers would appear in the legend.", "API": [["matplotlib.rcParams['legend.numpoints']", [38, 77]]], "LVCDE": [], "RCDE": [["this approach", [6, 19], "the creation of a plot with square markers and supplying it to the legend."]]}
{"post_id": "47490020", "sentence": "To reduce this undesired effect, use numpy.pad for more intelligent padding; reverting to mode='valid' for convolution.", "API": [["numpy.pad", [37, 46]]], "LVCDE": [], "RCDE": [["this undesired effect", [10, 31], "the boundary effect of zero-padding in the data smoothing process, which is visible when using numpy.convolve with mode='same'."]]}
{"post_id": "46263911", "sentence": "To this end, mpl_toolkits.axes_grid1.inset_locator.inset_axes may be used.", "API": [["mpl_toolkits.axes_grid1.inset_locator.inset_axes", [13, 61]]], "LVCDE": [], "RCDE": [["this end", [3, 11], "the goal of visually representing the relation between all five variables and the dependent variable in the dataset by displaying a small polar plot for each data point"]]}
{"post_id": "31003889", "sentence": "you can do this with numpy.argmax and numpy.indices.", "API": [["numpy.argmax", [21, 33]], ["numpy.indices", [38, 51]]], "LVCDE": [], "RCDE": [["this", [11, 15], "The process of finding the location of the maximum values in a 3D numpy array (X) and creating a 2D array from another 3D array (Y) with the same dimensions"]]}
{"post_id": "74553090", "sentence": "Then, you can use pandas.DataFrame.to_excel to put this dataframe in a spreadsheet:\nout.to_excel(&quot;path_to_the_file.xlsx&quot;, index=False)", "API": [["pandas.DataFrame.to_excel", [18, 43]]], "LVCDE": [], "RCDE": [["this dataframe", [51, 65], "the 'out' dataframe, which is created by using pandas Series, extractall, reset_index, str.split, rename, and applymap methods to transform the string data 'str1' into a dataframe structure with columns 'X' and 'Y'"]]}
{"post_id": "39239941", "sentence": "To perform the \u03c72 test on this data, you can use scipy.stats.chi2_contingency:", "API": [["scipy.stats.chi2_contingency", [49, 77]]], "LVCDE": [], "RCDE": [["this data", [26, 35], "The data referred to here is the result of two experiments, represented in a contingency table. Condition A and Condition B each have two results, with Condition A having 25 and 75, and Condition B having 100 and 100. The total counts for each result are 125 and 175 respectively. The data represents the counts of what was observed during an experiment, and the user wants to perform a \u03c72 test on this data to determine if there is a significant difference between the success rates of Condition A and Condition B."]]}
{"post_id": "65505379", "sentence": "You can use numpy.argsort multiple times to handle a matrix, as suggested in this answer on SO.", "API": [["numpy.argsort", [12, 25]]], "LVCDE": [], "RCDE": [["this answer", [77, 88], "https://stackoverflow.com/questions/17814502/rank-values-in-a-2d-array-preserve-array-shape-and-indices"]]}
{"post_id": "33966967", "sentence": "Here is an example showing how you can use numpy.linalg.lstsq for this task:", "API": [["numpy.linalg.lstsq", [43, 61]]], "LVCDE": [], "RCDE": [["this task", [66, 75], "finding a least-squares solution for the coefficients 'a' in the equation 'z = (a0 + a1*x + a2*y + a3*x**2 + a4*x**2*y + a5*x**2*y**2 + a6*y**2 + a7*x*y**2 + a8*x*y)', given arrays 'x', 'y', and 'z' of length 20"]]}
{"post_id": "74634650", "sentence": "In this case using numpy.multiply instead of * solves the issue:", "API": [["numpy.multiply", [19, 33]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "a situation in Python programming where the user is trying to use a type hint with a Numpy array and encounters an error. The user expects to be able to use a generic type hint for a Numpy array, but encounters an error when the multiplication operation is used."]]}
{"post_id": "70005371", "sentence": "The correct way to do this with pandas is with pandas.DataFrame.groupby and pandas.DataFrame.plot.", "API": [["pandas.DataFrame.groupby", [47, 71]], ["pandas.DataFrame.plot", [76, 97]]], "LVCDE": [], "RCDE": [["this", [22, 26], "the process of making a scatter plot with the geyser dataset from seaborn, coloring the points based on the 'kind' column, and avoiding the use of a for-loop"]]}
{"post_id": "42961333", "sentence": "The most straight forward way to do this is use pandas.crosstab which gives you a frequency table of the factors:", "API": [["pandas.crosstab", [48, 63]]], "LVCDE": [], "RCDE": [["this", [36, 40], "creating a new DataFrame that represents prediction accuracy from an existing DataFrame containing predicted and actual class memberships"]]}
{"post_id": "29092204", "sentence": "Unless I'm missing something, this case calls for a simple invocation of numpy.interp.", "API": [["numpy.interp", [73, 85]]], "LVCDE": [], "RCDE": [["this case", [30, 39], "the situation where the user wants to convert y to a linearly spaced array and find the corresponding interpolated values of x"]]}
{"post_id": "57135346", "sentence": "In this case, the above result can also be\ncomputed in a vectorized calculation with scipy.special.rel_entr or scipy.special.kl_div.", "API": [["scipy.special.rel_entr", [85, 107]]], "LVCDE": [], "RCDE": [["this case", [3, 12], "the situation where the user has a 2d numpy array (matrix) where each row is a valid probability distribution, and another 1d numpy array (vector), which is also a probability distribution, and needs to compute the KL divergence between each row of the matrix and the vector without using for loops"]]}
{"post_id": "74259826", "sentence": "We can use numpy.random.normal to generate this data.", "API": [["numpy.random.normal", [11, 30]]], "LVCDE": [], "RCDE": [["this data", [43, 52], "a toy dataset that resembles a Gaussian trend with arbitrary peaks."]]}
{"post_id": "63208073", "sentence": "If you want to use the seaborn.countplot function for this purpose, you may use an array instead of a pandas.DataFrame as the input data:", "API": [["seaborn.countplot", [23, 40]], ["pandas.DataFrame", [102, 118]]], "LVCDE": [], "RCDE": [["this purpose", [54, 66], "the user's intention to show how many times a certain dog breed was mentioned, specifically for breeds 'golden_retriever', 'labrador_retriever', and 'chihuahua', with counts of 266, 265, and 179 respectively, using seaborn and matplotlib inline to create a bar chart"]]}
{"post_id": "58485882", "sentence": "I think this can be solved only using pandas.DataFrame.merge.", "API": [["pandas.DataFrame.merge", [38, 60]]], "LVCDE": [], "RCDE": [["this", [8, 12], "the problem of retrieving a desired dataset from the given pandas DataFrame 'df', the frequency count 'df_count', and the list of paper pairs 'pair'. The desired dataset should be in a specific format where 'paper1' and 'paper2' represent the first two elements of each list of 'pair', 'freq1' and 'freq2' represent the frequency count of each paper done by 'df_count', and 'common' is a number of 'reference' both 'paper1' and 'paper2' cite in common."]]}
{"post_id": "71818572", "sentence": "IIUC, this is a perfect use case for numpy.argsort:", "API": [["numpy.argsort", [37, 50]]], "LVCDE": [], "RCDE": [["this", [6, 10], "sorting the 2D and 1D numpy arrays based on the 1D-array values"]]}
{"post_id": "66197103", "sentence": "Solved it using numpy.pad()", "API": [["numpy.pad()", [16, 27]]], "LVCDE": [], "RCDE": [["it", [7, 9], "the problem of distorting an image along the y axis using values generated from overlaying sine waves"]]}
{"post_id": "53807091", "sentence": "You can do it with numpy.repeat:", "API": [["numpy.repeat", [19, 31]]], "LVCDE": [], "RCDE": [["it", [11, 13], "generating an array containing the indices of the first array, where the index i is repeated counts[i] times"]]}
{"post_id": "7717456", "sentence": "The simple, brute force way to do it is to just use numpy.where.", "API": [["numpy.where", [52, 63]]], "LVCDE": [], "RCDE": [["it", [34, 36], "finding the 'every' location of all the unique elements in a 2D numpy array"]]}
{"post_id": "75623143", "sentence": "should do just that\nyou can then load it with pandas.read_pickle(\"mydf.pkl\")", "API": [["pandas.read_pickle(\"mydf.pkl\")", [46, 76]]], "LVCDE": [], "RCDE": [["it", [38, 40], "the pickled dataframe 'ndf'"]]}
{"post_id": "51702409", "sentence": "The correct way to do it will be using the pandas.DataFrame.assign method", "API": [["pandas.DataFrame.assign", [43, 66]]], "LVCDE": [], "RCDE": [["it", [22, 24], "Adding a unique_id column to the pandas DataFrame that contains href URL from the HTML"]]}
{"post_id": "35735195", "sentence": "You can implement it using repeated calls to numpy.random.hypergeometric.", "API": [["numpy.random.hypergeometric", [45, 72]]], "LVCDE": [], "RCDE": [["it", [18, 20], "The process of drawing a specific number of balls from an urn containing balls of different colors, in a way that simulates drawing from an urn without replacement."]]}
{"post_id": "73524951", "sentence": "One of the ways to do it is to use pandas.merge with a left-join:\nOP_BLOCK: (output omitted for annotation)", "API": [["pandas.merge", [35, 47]]], "LVCDE": [], "RCDE": [["it", [22, 24], "the process of using VLOOKUP on a new column in Python, equivalent to the process in Excel"]]}
{"post_id": "47654314", "sentence": "One way to do it would be using pandas.DataFrame.apply() and pandas.Series.map() like this:", "API": [["pandas.Series.map()", [61, 80]]], "LVCDE": [], "RCDE": [["it", [14, 16], "the task of replacing all non-tuple elements in the dataframe with tuples"]]}
{"post_id": "47654314", "sentence": "One way to do it would be using pandas.DataFrame.apply() and pandas.Series.map() like this:", "API": [["pandas.DataFrame.apply()", [32, 56]]], "LVCDE": [], "RCDE": [["it", [14, 16], "the task of replacing all elements in the data frame to be tuples"]]}
{"post_id": "69025436", "sentence": "Here is how I've solved it using pandas.MultiIndex:", "API": [["pandas.MultiIndex", [33, 50]]], "LVCDE": [], "RCDE": [["it", [24, 26], "the problem of making a cartesian product of an input table using pandas"]]}
{"post_id": "34444338", "sentence": "Based on the warning, it looks like normalize refers to sklearn.preprocessing.normalize.", "API": [["sklearn.preprocessing.normalize", [56, 87]]], "LVCDE": [], "RCDE": []}
{"post_id": "44734241", "sentence": "You can do it in a single line using numpy.clip()", "API": [["numpy.clip()", [37, 49]]], "LVCDE": [], "RCDE": [["it", [11, 13], "compute the operation where if an element of x is smaller than 3, replace it with 3, and if an element of x is bigger than 7, replace it with 7, in a single line of code"]]}
{"post_id": "47802741", "sentence": "Try it with numpy.bincount:", "API": [["numpy.bincount", [12, 26]]], "LVCDE": [], "RCDE": [["it", [4, 6], "the Python expression involving the numpy.histogram function, specifically 'np.histogram(np.sum(np.random.randint(0, 2, size=(n, N)), axis=0), bins=n+1)[0]'"]]}
{"post_id": "73970103", "sentence": "I test it and working well, added alternative solution with numpy.select:", "API": [["numpy.select", [60, 72]]], "LVCDE": [], "RCDE": [["it", [7, 9], "the issue the questioner had with using the numpy.where function to modify certain values in a pandas dataframe"]]}
{"post_id": "47122827", "sentence": "Edit1: To do it inside the array (as mentioned in the comment) you can use numpy.vstack:", "API": [["numpy.vstack", [75, 87]]], "LVCDE": [], "RCDE": [["it", [13, 15], "The task of appending each iteration into a single array"]]}
{"post_id": "21955379", "sentence": "Maybe you should compare it to the numerical approximation of the gradient with scipy.optimize.check_grad.", "API": [["scipy.optimize.check_grad", [80, 105]]], "LVCDE": [], "RCDE": [["it", [25, 27], "the gradient computation in the logistic regression implementation"]]}
{"post_id": "36447684", "sentence": "To write it shorter you can use numpy.percentile (numpy docs).", "API": [["numpy.percentile", [32, 48]]], "LVCDE": [], "RCDE": [["it", [9, 11], "the original code provided by the questioner, 'idx_pos = np.where(x > 0.)\nidx_pos_sorted = np.argsort(x[idx_pos])\nn = len(idx_pos[0])\nn_punc = int(n*percentage)\nx[idx_pos[0][idx_pos_sorted[n_punc:]], idx_pos[1][idx_pos_sorted[n_punc:]]] = (\n  x[idx_pos[0][idx_pos_sorted[n_punc]], idx_pos[1][idx_pos_sorted[n_punc]]])'"]]}
{"post_id": "73261016", "sentence": "You can specify it as numpy.typing.NDArray with an entry type.", "API": [["numpy.typing.NDArray", [22, 42]]], "LVCDE": [], "RCDE": [["it", [16, 18], "the type-hint for OpenCV images in Python functions"]]}
{"post_id": "36384223", "sentence": "In order to make it work with scipy.optimize.minimize you will need to keep the variable argument at the last position:", "API": [["scipy.optimize.minimize", [30, 53]]], "LVCDE": [], "RCDE": [["it", [17, 19], "the function 'fun'"]]}
{"post_id": "66836610", "sentence": "Try using numpy.array and then just divide it by 255, as in:", "API": [["numpy.array", [10, 21]]], "LVCDE": [], "RCDE": [["it", [43, 45], "the numpy array 'loaded_image' that stores the image data"]]}
{"post_id": "62377970", "sentence": "Convert it with pandas.to_datetime", "API": [["pandas.to_datetime", [16, 34]]], "LVCDE": [], "RCDE": [["it", [8, 10], "Orange table containing datetime and numbers that the user is trying to convert to a DataFrame"]]}
{"post_id": "66876796", "sentence": "By using scipy.optimize.minimize, you could do it like this:", "API": [["scipy.optimize.minimize", [9, 32]]], "LVCDE": [], "RCDE": [["it", [47, 49], "The process of defining a constraint that will make the 'b' value less than zero in the context of using Ordinary Least Squares linear regression method to fit 'x' and 'y'."]]}
{"post_id": "51144189", "sentence": "And you can of course read it back with numpy.fromfile - URL_BLOCK: (output omitted for annotation)", "API": [["numpy.fromfile", [40, 54]]], "LVCDE": [], "RCDE": [["it", [27, 29], "the file 'test.bin' which contains the flattened array data"]]}
{"post_id": "3685339", "sentence": "If you want it to be human readable, look into numpy.savetxt.", "API": [["numpy.savetxt", [47, 60]]], "LVCDE": [], "RCDE": [["it", [12, 14], "the 4x11x14 numpy array the user is trying to write to a file"]]}
{"post_id": "54400575", "sentence": "For analysing you can just load it with numpy.load", "API": [["numpy.load", [40, 50]]], "LVCDE": [], "RCDE": [["it", [32, 34], "the file containing multiple numpy arrays saved using numpy.savez"]]}
{"post_id": "67996161", "sentence": "Another way to do it using pandas.DataFrame.apply and pandas.DataFrame.explode:", "API": [["pandas.DataFrame.explode", [54, 78]]], "LVCDE": [], "RCDE": [["it", [18, 20], "the task of normalizing the data vertically, specifically breaking out the genre from the given movie dataset"]]}
{"post_id": "75200418", "sentence": "Here is one way to do it with the help of ast.literal_eval and pandas.Series.str :", "API": [["pandas.Series.str", [63, 80]]], "LVCDE": [], "RCDE": [["it", [22, 24], "the task of transforming a string-type pandas dataframe column into separate columns representing the sum of the values of 'min' and 'max'"]]}
{"post_id": "60377951", "sentence": "You should make it so the indexes are not the same, then use zip, numpy.hstack, pandas.concat and DataFrame.reindex:", "API": [["pandas.concat", [80, 93]]], "LVCDE": [], "RCDE": [["it", [16, 18], "the process of changing the indexes of the two dataframes (df1 and df2) so that they are not the same"]]}
{"post_id": "32409506", "sentence": "This can be done using pandas.ExcelFile.parse's converters option with a function derived from pandas.to_datetime as the functions in the converters dict and enabling it with coerce=True to force errors to NaT.", "API": [["pandas.ExcelFile.parse", [23, 45]], ["pandas.to_datetime", [95, 113]]], "LVCDE": [], "RCDE": [["This", [0, 4], "The method of converting dates in an Excel document while parsing its sheets"]]}
{"post_id": "74946903", "sentence": "You can change it to numpy.int_, or just int.", "API": [["numpy.int_", [21, 31]]], "LVCDE": [], "RCDE": [["it", [15, 17], "the dtype 'numpy.int' in the code, which was deprecated in NumPy 1.20 and removed in NumPy 1.24"]]}
{"post_id": "52991726", "sentence": "You need define it before by numpy.random.seed, also list comprehension is not necessary, because is possible use numpy.random.choice with parameter size:", "API": [["numpy.random.seed", [29, 46]]], "LVCDE": [], "RCDE": [["it", [16, 18], "the random seed for the numpy's random choice function"]]}
{"post_id": "43925741", "sentence": "You should definitely use numpy.save, you can still do it in-memory:", "API": [["numpy.save", [26, 36]]], "LVCDE": [], "RCDE": [["it", [55, 57], "the process of using numpy.save to organize and compress numpy array data in memory"]]}
{"post_id": "67808429", "sentence": "You can do it using pandas.Grouper by grouping the Date and Identificator values within 24 hrs, and then picking last value for each Identificator within a group.", "API": [["pandas.Grouper", [20, 34]]], "LVCDE": [], "RCDE": [["it", [11, 13], "The task of deleting all lines in a dataframe where the status is 'FAILED'"]]}
{"post_id": "20179962", "sentence": "The result is pandas.DataFrame, but if you want to get it as numpy array, you can use values attribute:", "API": [["pandas.DataFrame", [14, 30]]], "LVCDE": [], "RCDE": [["it", [55, 57], "The result of the operation, which is a pandas.DataFrame object that contains the sum of the 'amount' column grouped by 'person' and 'itemCode'"]]}
{"post_id": "35853953", "sentence": "Since you want to put it in a pandas dataframe, just use pandas.read_hdf.", "API": [["pandas.read_hdf", [57, 72]]], "LVCDE": [], "RCDE": [["it", [22, 24], "the data from .dat format files with an internal structure constructed as hdf5"]]}
{"post_id": "75121619", "sentence": "In a Pandorable way, you can do it with pandas.DataFrame.squeeze and pandas.DataFrame.mul :", "API": [["pandas.DataFrame.mul", [69, 89]]], "LVCDE": [], "RCDE": [["it", [32, 34], "the creation of a new dataframe by applying an equation to the values of two other dataframes"]]}
{"post_id": "52991726", "sentence": "You need define it before by numpy.random.seed, also list comprehension is not necessary, because is possible use numpy.random.choice with parameter size:", "API": [["numpy.random.seed", [29, 46]]], "LVCDE": [], "RCDE": [["it", [16, 18], "the random seed for numpy's random choice function"]]}
{"post_id": "27697254", "sentence": "The correct way to do it would be to compare individually each pair of arrays using for example numpy.array_equal:", "API": [["numpy.array_equal", [96, 113]]], "LVCDE": [], "RCDE": [["it", [22, 24], "The task of comparing each pair of arrays, which was previously done incorrectly in the loop provided in the context."]]}
{"post_id": "23231843", "sentence": "Therefore, we can attack it using scipy.optimize.fmin_slsqp()", "API": [["scipy.optimize.fmin_slsqp()", [34, 61]]], "LVCDE": [], "RCDE": [["it", [25, 27], "the problem of minimizing the sum of square of errors, with 3 linear constrain functions"]]}
{"post_id": "66450410", "sentence": "You can do it with pandas.to_datetime and pandas.Series.dt.strftime:", "API": [["pandas.Series.dt.strftime", [42, 67]]], "LVCDE": [], "RCDE": [["it", [11, 13], "The task of converting date strings in the 'Date' column of the Avocados dataset to a single string value representing the year"]]}
{"post_id": "72552567", "sentence": "I was able to fix it using pandas.NA instead which fillna, for some reason, recognizes as blanks to fill with ffill\nFix:", "API": [["pandas.NA", [27, 36]]], "LVCDE": [], "RCDE": [["it", [18, 20], "the issue of assigning an empty cell (blank/nan) in the 'np.where' condition to a pandas dataframe"]]}
{"post_id": "66848625", "sentence": "To fix it convert df2[&quot;DateTime&quot;] to datetime with pandas.to_datetime:", "API": [["pandas.to_datetime", [61, 79]]], "LVCDE": [], "RCDE": [["it", [7, 9], "the issue of incompatible data types when merging the two DataFrames (df1 and df2)"]]}
{"post_id": "15934081", "sentence": "If you want to catch it you can:\nUse numpy.seterr(all='raise') which will directly raise the exception.", "API": [["numpy.seterr(all='raise')", [37, 62]]], "LVCDE": [], "RCDE": [["it", [21, 23], "the warning that the user sees when a division by zero occurs in their Python code, specifically in the context of using numpy"]]}
{"post_id": "37690817", "sentence": "However, if you feel you need to use it then you can filter a matrix using numpy.compress.", "API": [["numpy.compress", [75, 89]]], "LVCDE": [], "RCDE": [["it", [37, 39], "the 'matrix' type in numpy"]]}
{"post_id": "67968816", "sentence": "Apply pandas.Series to it and save results into label_df:", "API": [["pandas.Series", [6, 19]]], "LVCDE": [], "RCDE": [["it", [23, 25], "the output of the function 'check_labels' applied to the DataFrame 'dd'"]]}
{"post_id": "33709455", "sentence": "Lots of ways to do this, but assuming it is already a pandas Series, the easiest way is pandas.to_datetime:", "API": [["pandas.to_datetime", [88, 106]]], "LVCDE": [], "RCDE": [["this", [19, 23], "the process of converting the axis to type datetime in a pandas Series"]]}
{"post_id": "56947411", "sentence": "Construct it as sparse tridiagonal matrix from the start, using scipy.sparse.diags", "API": [["scipy.sparse.diags", [64, 82]]], "LVCDE": [], "RCDE": [["it", [10, 12], "the dense matrix constructed using the Toeplitz constructor"]]}
{"post_id": "55867479", "sentence": "but if using an older versionyou need to do it manually, using matplotlib.dates.date2num, e.g.", "API": [["matplotlib.dates.date2num", [63, 88]]], "LVCDE": [], "RCDE": [["it", [44, 46], "the process of manually converting datetime objects to numbers that represent the number of days since 0001-01-01 UTC, plus 1"]]}
{"post_id": "13442866", "sentence": "If you need it as a 3D array afterwards, just use numpy.dstack to create it.", "API": [["numpy.dstack", [50, 62]]], "LVCDE": [], "RCDE": [["it", [12, 14], "a Python list of 2D arrays, which represents a dynamic medical image - a series of 2D images at different time points"]]}
{"post_id": "59767415", "sentence": "And pass it as the ax parameter of pandas.DataFrame.plot,", "API": [["pandas.DataFrame.plot", [35, 56]]], "LVCDE": [], "RCDE": [["it", [9, 11], "the 'axes' instance 'ax' created by 'fig, ax = plt.subplots()'"]]}
{"post_id": "16781247", "sentence": "You'll have to numpy.copy it each time.", "API": [["numpy.copy", [15, 25]]], "LVCDE": [], "RCDE": [["it", [26, 28], "the 'pos' array"]]}
{"post_id": "28578391", "sentence": "You can get it using plain vectors using numpy.outer():", "API": [["numpy.outer()", [41, 54]]], "LVCDE": [], "RCDE": [["it", [12, 14], "the 'outer product' of two vectors, which is a matrix resulting from the multiplication of two vectors"]]}
{"post_id": "66449494", "sentence": "You can do it with pandas.DataFrame.groupby and with DataFrameGroupBy.aggregate:", "API": [["pandas.DataFrame.groupby", [19, 43]]], "LVCDE": [], "RCDE": [["it", [11, 13], "the task of efficiently and elegantly aggregating the column 'value' by 'id' considering multiple aggregate functions on the same column in a pandas DataFrame"]]}
{"post_id": "44400754", "sentence": "Taking it to another level\nWe can use numpy.bincount to speed things.", "API": [["numpy.bincount", [38, 52]]], "LVCDE": [], "RCDE": [["it", [7, 9], "the process of converting the original dataframe to the desired format"]]}
{"post_id": "71975048", "sentence": "You can't do it with the python hash function, but there is a helper function in pandas: pandas.util.hash_pandas_object.", "API": [["pandas.util.hash_pandas_object", [89, 119]]], "LVCDE": [], "RCDE": [["it", [13, 15], "removing duplicates from a list of pandas dataframes"]]}
{"post_id": "67996161", "sentence": "Another way to do it using pandas.DataFrame.apply and pandas.DataFrame.explode:", "API": [["pandas.DataFrame.apply", [27, 49]]], "LVCDE": [], "RCDE": [["it", [18, 20], "the process of normalizing the data vertically, specifically to break out the 'genres' field from the provided dataset"]]}
{"post_id": "71093094", "sentence": "And if you want to keep it a 2D array then you can use numpy.reshape", "API": [["numpy.reshape", [55, 68]]], "LVCDE": [], "RCDE": [["it", [24, 26], "the tensor that is a two-dimensional array, specifically 't = torch.tensor([[17, 0], [93, 0], [0, 0], [21, 0], [19, 0]])'"]]}
{"post_id": "52430643", "sentence": "I fixed it by setting the replacement value to be a numpy.datetime64 from the start.", "API": [["numpy.datetime64", [52, 68]]], "LVCDE": [], "RCDE": [["it", [8, 10], "the issue of incorrect handling of dates by numpy where in the context of replacing null dates with a static date in a pandas DataFrame"]]}
{"post_id": "35691637", "sentence": "You get it into long-form data with the pandas.melt function.", "API": [["pandas.melt", [40, 51]]], "LVCDE": [], "RCDE": [["it", [8, 10], "the dataframe contains columns 'season', 'A', 'B', 'C', 'D' and various rows of data"]]}
{"post_id": "40197049", "sentence": "You may do it using numpy.copy() since you are having numpy object.", "API": [["numpy.copy()", [20, 32]]], "LVCDE": [], "RCDE": [["it", [11, 13], "creating a copy of the object, specifically 'imageOriginal_3d'"]]}
{"post_id": "57434090", "sentence": "To put it at the beginning use numpy.insert:", "API": [["numpy.insert", [31, 43]]], "LVCDE": [], "RCDE": [["it", [7, 9], "the action of inserting a temp value at the beginning of the 'currentStep' array"]]}
{"post_id": "48916959", "sentence": "If you want to solve it with linear programming, you can use scipy.optimize.linprog.", "API": [["scipy.optimize.linprog", [61, 83]]], "LVCDE": [], "RCDE": [["it", [21, 23], "the transportation cost flow problem described in the context, where the goal is to minimize overall transportation cost from 5 carriers across more than 3000 transport lanes"]]}
{"post_id": "58993286", "sentence": "EDIT: \nMaking it work with scipy.optimize.fsolve", "API": [["scipy.optimize.fsolve", [27, 48]]], "LVCDE": [], "RCDE": [["it", [14, 16], "the process of making the system of nonlinear equations work"]]}
{"post_id": "7432957", "sentence": "You can do it using numpy.histogramdd but, as you say, the method proposed by @jozzas won't work.", "API": [["numpy.histogramdd", [20, 37]]], "LVCDE": [], "RCDE": [["it", [11, 13], "creating a histogram volume of the input channels of a 3D rgb image using numpy"]]}
{"post_id": "73978111", "sentence": "And next you can convert it to numpy.array.", "API": [["numpy.array", [31, 42]]], "LVCDE": [], "RCDE": [["it", [25, 27], "the result of the operation 'set(tuples_a) - set(tuples_b)', which is a set of tuples"]]}
{"post_id": "25597805", "sentence": "If you want to compute it without calling chi2_contingency, you can use scipy.stats.contingency.expected_freq.", "API": [["chi2_contingency", [42, 58]], ["scipy.stats.contingency.expected_freq", [72, 109]]], "LVCDE": [], "RCDE": [["it", [23, 25], "the expected array in a chi square test or Fisher's exact test"]]}
{"post_id": "51767603", "sentence": "Try explicitly converting it to a Numpy array using pandas.Series.values:", "API": [["pandas.Series.values", [52, 72]]], "LVCDE": [], "RCDE": [["it", [26, 28], "the data extracted from the DataFrame using the pandas library, specifically, the data stored in the variables x1, y1, x2, and y2, which are of the Pandas Series type"]]}
{"post_id": "66483255", "sentence": "To convert it to a pandas.DataFrame you need to use the .summary_frame() method.", "API": [["pandas.DataFrame", [19, 35]]], "LVCDE": [], "RCDE": [["it", [11, 13], "the Wald Test table, which is an instance of the 'WaldTestResults' object, as created by the 'wald_test_terms()' method on the 'LogModel' object in the code"]]}
{"post_id": "40872535", "sentence": "You can customize it through \"numpy.set_printoptions\" function.", "API": [["numpy.set_printoptions", [30, 52]]], "LVCDE": [], "RCDE": [["it", [18, 20], "the way the numpy array is printed"]]}
{"post_id": "69232117", "sentence": "then put it into pandas.DataFrame using io.StringIO as follows", "API": [["pandas.DataFrame", [17, 33]]], "LVCDE": [], "RCDE": [["it", [9, 11], "the cleaned text data, which is the text from the .txt file after being cleaned by the regular expression to replace certain double quotes with apostrophes"]]}
{"post_id": "9199364", "sentence": "You can set it by, for example pylab.figure(facecolor=SOME_COLOR, ...) or matplotlib.rcParams['figure.facecolor'] = SOME_COLOR.", "API": [["matplotlib.rcParams", [74, 93]]], "LVCDE": [], "RCDE": [["it", [12, 14], "the background colour of the canvas in the Matplotlib graphing tool used in a PyGTK/Twisted app"]]}
{"post_id": "61136047", "sentence": "You can achieve it by using dictionary comprehension and pandas.Series.map() method.", "API": [["pandas.Series.map()", [57, 76]]], "LVCDE": [], "RCDE": [["it", [16, 18], "the task of doing 'reverse-mapping' on the dictionary to create a new 'col2' in the DataFrame, which maps the values in 'col1' to the corresponding keys in the dictionary"]]}
{"post_id": "52685393", "sentence": "Here is how to do it using scipy.sparse.diags.", "API": [["scipy.sparse.diags", [27, 45]]], "LVCDE": [], "RCDE": [["it", [18, 20], "The task of replacing the 'toeplitz' function and the 'fliplr' function without converting from sparse to dense matrices in the provided Numpy/Scipy code"]]}
{"post_id": "25046619", "sentence": "That would make it much easier to read a column from the file with e.g. a numpy.memmap.", "API": [["numpy.memmap", [74, 86]]], "LVCDE": [], "RCDE": [["it", [16, 18], "The output file written in binary format"]]}
{"post_id": "54544948", "sentence": "If it is the case, the delimiter= option of numpy.genfromtxt could help you.", "API": [["numpy.genfromtxt", [44, 60]]], "LVCDE": [], "RCDE": [["it", [3, 5], "The situation where the columns in the text file don't have the same width, specifically 10 for the first four fields, then 11"]]}
{"post_id": "66450410", "sentence": "You can do it with pandas.to_datetime and pandas.Series.dt.strftime:", "API": [["pandas.to_datetime", [19, 37]]], "LVCDE": [], "RCDE": [["it", [11, 13], "the process of converting the 'Date' column in the dataset from a date format to a year-only format"]]}
{"post_id": "63494564", "sentence": "You can achieve it with pandas.pivot:", "API": [["pandas.pivot", [24, 36]]], "LVCDE": [], "RCDE": [["it", [16, 18], "the transformation of the dataframe 'data' to a new dataframe with Computer names as column headers"]]}
{"post_id": "58784402", "sentence": "You would address it by calling \"numpy.linalg.slogdet\" instead of \"numpy.linalg.det\".", "API": [["numpy.linalg.det", [67, 83]]], "LVCDE": [], "RCDE": [["it", [18, 20], "the problem of overflow from the calculation of the power mean**n"]]}
{"post_id": "63986433", "sentence": "Or make it a pandas.Series and use to_dict:", "API": [["pandas.Series", [13, 26]]], "LVCDE": [], "RCDE": [["it", [8, 10], "a dataframe (df) created from a csv file"]]}
{"post_id": "49016204", "sentence": "I solved it using sklearn.model_selection.StratifiedKFold.", "API": [["sklearn.model_selection.StratifiedKFold", [18, 57]]], "LVCDE": [], "RCDE": [["it", [9, 11], "the issue of the RandomForestClassifier being trained with only one category due to the manner in which the data was split into chunks"]]}
{"post_id": "74364390", "sentence": "You can achieve it with numpy.frompyfunc.", "API": [["numpy.frompyfunc", [24, 40]]], "LVCDE": [], "RCDE": [["it", [16, 18], "the task of implementing a tie breaker system for sorting an array, where the sorting is first based on the number of 10s, then on the number of 9s, 8s, etc., in each row"]]}
{"post_id": "16483149", "sentence": "Then to read it back, use numpy.fromstring:", "API": [["numpy.fromstring", [26, 42]]], "LVCDE": [], "RCDE": [["it", [13, 15], "the string 'VIstring' which represents a numpy array of floats converted to a string"]]}
{"post_id": "73192638", "sentence": "You could do it by placing the *** with matplotlib.axes.Axes.text .", "API": [["matplotlib.axes.Axes.text", [40, 65]]], "LVCDE": [], "RCDE": [["it", [13, 15], "Adding stars to certain variables in a matplotlib.pyplot bar plot"]]}
{"post_id": "57654177", "sentence": "I solved it by using scipy.interpolate.LinearNDInterpolator.", "API": [["scipy.interpolate.LinearNDInterpolator", [21, 59]]], "LVCDE": [], "RCDE": [["it", [9, 11], "the problem of using Scipy interpolate2d option to predict values in points outside of 'training' points"]]}
{"post_id": "21190090", "sentence": "Not sure if it can be done with numpy.sort, but you can use numpy.argsort for sure:", "API": [["numpy.sort", [32, 42]], ["numpy.argsort", [60, 73]]], "LVCDE": [], "RCDE": [["it", [12, 14], "the task of sorting each array in increasing order according to the second column, leaving the NaN values out"]]}
{"post_id": "65758590", "sentence": "You can do it with pandas.DataFrame.apply:", "API": [["pandas.DataFrame.apply", [19, 41]]], "LVCDE": [], "RCDE": [["it", [11, 13], "the task of creating a new column 'positive_review_contents' in a dataframe which contains the value from the column 'review_contents', and the value is corresponding with column 'individual_rate' where it is equal and higher than 4. For the new column 'positive_review_contents', some corresponding rows, where the 'individual_rate' is below 4, will be Null so the user could assign that Null value to 'No Positive'"]]}
{"post_id": "59823480", "sentence": "If so, I would suggest loading it though pandas.read_pickle which has additional logic to preserve backwards compatibility with older pickle files.", "API": [["pandas.read_pickle", [41, 59]]], "LVCDE": [], "RCDE": [["it", [31, 33], "the pickle file"]]}
{"post_id": "40555989", "sentence": "If you want to collapse it all into one DataFrame, you can simply use pandas.concat:", "API": [["pandas.concat", [70, 83]]], "LVCDE": [], "RCDE": [["it", [24, 26], "a dictionary where the keys are the sheet names, and the values are the DataFrames for each sheet, created by the pandas.read_excel function"]]}
{"post_id": "41043935", "sentence": "The function numpy.minimum does it for you, if you pass an axis argument.", "API": [["numpy.minimum", [13, 26]]], "LVCDE": [], "RCDE": [["it", [32, 34], "computes the minimum of the dist(C_i, X_j) over all i"]]}
{"post_id": "61412370", "sentence": "It has a very simple interface to downsample arrays by applying a function such as numpy.mean.", "API": [["numpy.mean", [83, 93]]], "LVCDE": [], "RCDE": [["It", [0, 2], "the function `block_reduce` in the `scikit-image` module"]]}
{"post_id": "74066513", "sentence": "The fastest of them all was using pandas.DataFrame.apply.", "API": [["pandas.DataFrame.apply", [34, 56]]], "LVCDE": [], "RCDE": [["them", [15, 19], "different methods tried to solve the problem of creating a new column in a dataframe based on certain conditions"]]}
{"post_id": "47822490", "sentence": "Use pandas.to_datetime before you merge them into one DataFrame/Series.", "API": [["pandas.to_datetime", [4, 22]]], "LVCDE": [], "RCDE": [["them", [40, 44], "the csv files containing date and time information in two different formats"]]}
