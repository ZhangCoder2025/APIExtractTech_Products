{"id":59212,"text":"ID:29672840\nPost:\nText: I don't think there's any way to do this using the linewidth property, since a line's stroke is always symmetric about the center of the line. \nText: A slightly hacky work-around would be to use the set_clip_path() method of the rect object representing the bar: \nCode: from matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\n\nax.hold(True)\n\npatches = ax.bar([1],[1], lw = 20., facecolor = 'w', edgecolor= 'red')\nax.plot([0,2],[0,0], 'k')\nax.plot([0,2],[1,1], 'k')\n\nax.set_xlim(0.8,2)\nax.set_ylim(-0.2,1.2)\n\n# get the patch object representing the bar\nbar = patches[0]\n\n# set the clipping path of the bar patch to be the same as its path. this trims\n# off the parts of the bar edge that fall outside of its outer bounding\n# rectangle\nbar.set_clip_path(bar.get_path(), bar.get_transform())\n\nText: See here for another example in the matplotlib documentation. \nAPI:\nmatplotlib.patches.Rectangle\n","label":[[253,257,"Mention"],[899,927,"API"]],"Comments":[]}
{"id":59213,"text":"ID:29692953\nPost:\nText: In a nutshell: you have to create a Line2D instance for each \"plot\". And slightly more elaborative: \nText: As you have done it with a single line, you can also do the same with multiple lines: import pyplot as plt # initial values x = [0,1,2] y = [[3,5,7],[4,6,8],[5,7,9]] # create the line instances and store them in a list line_objects = list() for yi in y: line_objects.extend(plt.plot(x, yi)) # new values with which we want to update the plot x = [1,2,3] y = [[4,6,8],[5,7,9],[6,8,0]] # update the y values dynamically (without recreating the plot) for yi, line_object in zip(y, line_objects): line_object.set_xdata(x) line_object.set_ydata(yi) Not really. However, you can create mulitple Line2D objects by a single call to plot: line_objects = plt.plot(x, y[0], x, y[1], x, y[2]) That is also why plot always returns a list. \nText: EDIT: \nText: If you have to do this often, it might help to use helper functions: \nText: E.g.: \nCode: def plot_multi_y(x, ys, ax=None, **kwargs):\n    if ax is None:\n        ax = plt.gca()\n    return [ax.plot(x, y, **kwargs)[0] for y in ys]\n\ndef update_multi_y(line_objects, x, ys):\n    for y, line_object in zip(ys, line_objects):\n        line_object.set_xdata(x)\n        line_object.set_ydata(y)\n\nText: Then you can just use: \nCode: # create the lines\nline_objects = plot_multi_y(x, y)\n\n#update the lines\nupdate_multi_y(line_objects, x, y)\n\nAPI:\nmatplotlib.pyplot\n","label":[[224,230,"Mention"],[1411,1428,"API"]],"Comments":[]}
{"id":59214,"text":"ID:29776518\nPost:\nText: You can use mdates (http:\/\/matplotlib.org\/api\/dates_api.html) to convert a date to axes coordinates, using the date2num() method. In the following example, I create an arrow going from (April 5 2015, 10) to (April 7 2015, 20): \nCode: import matplotlib.pyplot as plt\nimport matplotlib.dates as md\n\nimport datetime\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nt0 = datetime.datetime(2015,4,1)\nt = [t0+datetime.timedelta(days=j) for j in xrange(20)]\ny = range(20)\nax.plot(t, y)\n\nx0 = md.date2num(datetime.datetime(2015,4,5))\ny0 = 10\nxw = md.date2num(datetime.datetime(2015,4,7)) - x0\nyw = 10\narr = plt.Arrow(x0, y0, xw, yw, edgecolor='black')\nax.add_patch(arr)\narr.set_facecolor('b')\nfig.autofmt_xdate()\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[36,42,"Mention"],[744,760,"API"]],"Comments":[]}
{"id":59215,"text":"ID:29793010\nPost:\nText: You invoked the demons of pyplot which has its own eventloop. \nText: I can not explain to you why exactly it does not work, but you can resolve it as follows: \nText: Use the fully object-oriented interface of matplotlib when doing more than writing simple scripts (see the wxPython examples on matplotlib). \nText: This can be don as follows: \nCode: #import matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\n\nText: Down below: \nCode:     self.fig = Figure() # instead of plt.figure()\n\nAPI:\nmatplotlib.pyplot\n","label":[[50,56,"Mention"],[525,542,"API"]],"Comments":[]}
{"id":59216,"text":"ID:30085784\nPost:\nText: Edit 04\/2021: As of matplotlib 2.2.0, the key axes.color_cycle has been deprecated (source: API changes). The new method is to use set_prop_cycle (source: mpl.axes.Axes.set_prop_cycle API) \nText: The details are in the matplotlibrc itself, actually: it needs a string rep (hex or letter or word, not tuple). \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfig, ax1 = plt.subplots(1,1)\n\nys = np.random.random((5, 6))\nax1.plot(range(5), ys)\nax1.set_title('Default color cycle')\nplt.show()\n\n# From the sample matplotlibrc:\n#axes.color_cycle    : b, g, r, c, m, y, k  # color cycle for plot lines\n                                            # as list of string colorspecs:\n                                            # single letter, long name, or\n                                            # web-style hex\n\n# setting color cycle after calling plt.subplots doesn't \"take\"\n# try some hex values as **string** colorspecs\nmpl.rcParams['axes.color_cycle'] = ['#129845','#271254', '#FA4411', '#098765', '#000009']\n\nfig, ax2 = plt.subplots(1,1)\nax2.plot(range(5), ys)\nax2.set_title('New color cycle')\n\n\nn = 6\ncolor = plt.cm.coolwarm(np.linspace(0.1,0.9,n)) # This returns RGBA; convert:\nhexcolor = map(lambda rgb:'#%02x%02x%02x' % (rgb[0]*255,rgb[1]*255,rgb[2]*255),\n               tuple(color[:,0:-1]))\n\nmpl.rcParams['axes.color_cycle'] = hexcolor\n\nfig, ax3 = plt.subplots(1,1)\nax3.plot(range(5), ys)\nax3.set_title('Color cycle from colormap')\n\nplt.show()\n\nAPI:\nmatplotlib.axes.Axes.set_prop_cycle\n","label":[[179,207,"Mention"],[1514,1549,"API"]],"Comments":[]}
{"id":59217,"text":"ID:30131925\nPost:\nText: Sympy allegedly has it's own plotting functionality, but I couldn't get it to work from the manual they had. I'm not an active user of sympy. \nText: But here's a version on how to do it with numpy and matplotlib \nText: Define your function so it can act on a np.array Spread points uniformly in some range \"x\" act on those points with your function \"y\" plot the set of uniformly spaced points \"x\" vs function values in those points \"y\" import numpy as np import plt as plt def f(a): c1 = 0.327159886574049 c2 = 29.3930964972769 return a\/(a+np.exp(c1-c2*a)) x = np.linspace(0, 1, 500) y = f(x) plt.plot(x,y) plt.show() \nText: You should get something like this: \nAPI:\nmatplotlib.pyplot\n","label":[[486,489,"Mention"],[691,708,"API"]],"Comments":[]}
{"id":59218,"text":"ID:30329408\nPost:\nText: You can set the widget box colour as follows, \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import CheckButtons\n\nt = np.arange(0.0, 2.0, 0.01)\ns0 = np.sin(2*np.pi*t)\ns1 = np.sin(4*np.pi*t)\ns2 = np.sin(6*np.pi*t)\n\nfig, ax = plt.subplots()\nl0, = ax.plot(t, s0, visible=False, lw=2)\nl1, = ax.plot(t, s1, lw=2)\nl2, = ax.plot(t, s2, lw=2)\nplt.subplots_adjust(left=0.2)\n\nrax = plt.axes([0.05, 0.4, 0.1, 0.15])\ncheck = CheckButtons(rax, ('2 Hz', '4 Hz', '6 Hz'), (False, True, True))\n\n#Define colours for rectangles and set them\nc = ['b', 'g', 'r']    \n[rec.set_facecolor(c[i]) for i, rec in enumerate(check.rectangles)]\n\n\ndef func(label):\n    if label == '2 Hz': l0.set_visible(not l0.get_visible())\n    elif label == '4 Hz': l1.set_visible(not l1.get_visible())\n    elif label == '6 Hz': l2.set_visible(not l2.get_visible())\n    plt.draw()\ncheck.on_clicked(func)\n\nplt.show()\n\nText: The checkbutton panel has each tick box as a Rectangle object which can be customised as needed. \nAPI:\nmatplotlib.patches.Rectangle\n","label":[[982,991,"Mention"],[1040,1068,"API"]],"Comments":[]}
{"id":59219,"text":"ID:30466742\nPost:\nText: You'll need to subclass Normalize and pass in an instance of your new norm to imshow\/contourf\/whatever plotting function you're using. \nText: The basic idea is illustrated in the first option here: Shifted colorbar matplotlib (Not to shill one of my own questions too much, but I can't think of another example.) \nText: However, that question deals specifically with setting a single data value to correspond to 0.5 in the colormap. It's not too hard to expand the idea to \"piecewise\" normalization, though: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nclass PiecewiseNormalize(Normalize):\n    def __init__(self, xvalues, cvalues):\n        self.xvalues = xvalues\n        self.cvalues = cvalues\n\n        Normalize.__init__(self)\n\n    def __call__(self, value, clip=None):\n        # I'm ignoring masked values and all kinds of edge cases to make a\n        # simple example...\n        if self.xvalues is not None:\n            x, y = self.xvalues, self.cvalues\n            return np.ma.masked_array(np.interp(value, x, y))\n        else:\n            return Normalize.__call__(self, value, clip)\n\ndata = np.random.random((10,10))\ndata = 10 * (data - 0.8)\n\nfig, ax = plt.subplots()\nnorm = PiecewiseNormalize([-8, -1, 0, 1.5, 2], [0, 0.1, 0.5, 0.7, 1])\nim = ax.imshow(data, norm=norm, cmap=plt.cm.seismic, interpolation='none')\nfig.colorbar(im)\nplt.show()\n\nText: Note that 0.5 in the colormap (white) corresponds to a data value of 0, and the red and blue regions of the colormap are asymmetric (note the broad \"pink\" range vs the much narrower transition to dark blue). \nAPI:\nmatplotlib.colors.Normalize\n","label":[[48,57,"Mention"],[1647,1674,"API"]],"Comments":[]}
{"id":59220,"text":"ID:30541176\nPost:\nText: The answer provided by Sergey was definitely helpful. I was able to find that if added a few more pieces to the code, based on portions of this example, including a use statement, the memory issues seem to be resolved. I tested this both in my test application (2 views generating a SVG file, posted above) and my primary application which currently has 3 SVG views, which I will expand to more views creating SVG files. \nText: With my primary application, I can refresh between all 3 existing views, multiple times, without the SVG image combining that I found with prior versions of this code. The setting seems necessary for more concurrent uses which may be why other SO questions don't seem to reflect this issue, unless I am able to find something else that is causing this issue. I am not sure if this is the best answer; however, this does appear to work correctly for now and I plan to monitor to see if any other issues are found. \nCode: @view_config(route_name='view_test_svg')\ndef test_svg_view(request):\n    from matplotlib import use as matplotlib_use\n    matplotlib_use(\"Svg\")\n    from matplotlib.pyplot import close, figure, axes, pie, title, savefig\n    log.debug('In test_svg_view')\n    fig = figure(1, figsize=(6,6))\n    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    labels = ['Frogs', 'Hogs', 'Dogs', 'Logs']\n    fracs = [15, 30, 45, 10]\n    explode=(0, 0.05, 0, 0)\n    pie(fracs, explode=explode, labels=labels,\n                                autopct='%1.1f%%', shadow=True, startangle=90)\n    title('Raining Hogs and Dogs', bbox={'facecolor':'0.8', 'pad':5})\n    imgdata = cStringIO.StringIO()\n    savefig(imgdata, format='svg')\n    imgdata.seek(0)\n    svg_dta = imgdata.getvalue()\n    # Close the StringIO buffer\n    imgdata.close()\n    close('all')\n    return Response(svg_dta, content_type='image\/svg+xml')\n\nAPI:\nmatplotlib.use\n","label":[[189,192,"Mention"],[1861,1875,"API"]],"Comments":[]}
{"id":59221,"text":"ID:30558270\nPost:\nText: Even though they are equivalent, I think there is a pretty good argument that the second form import pyploc as plt is objectively more readable: \nText: It is generally customary to use import pyplot as plt and suggested in the matplotlib documentation (see http:\/\/matplotlib.org\/users\/pyplot_tutorial.html etc...) so this will be more familiar to most readers. import plt as plt is shorter but no less clear. import pyplot as plt gives an unfamiliar reader a hint that pyplot is a module, rather than a function which could be incorrectly assumed from the first form. \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[125,131,"Mention"],[216,222,"Mention"],[392,395,"Mention"],[440,446,"Mention"],[598,615,"API"],[616,633,"API"],[634,651,"API"],[652,669,"API"]],"Comments":[]}
{"id":59222,"text":"ID:30757586\nPost:\nText: plot is a function of matplotlib.pyplot, so: \nCode: import matplotlib.pyplot as plt\n\nplt.plot(range(20))\n\nText: EDIT: \nText: To see the plot, you will normally need to call \nCode: plt.show()\n\nText: to display the figure, or \nCode: plt.savefig('figname.png')\n\nText: to save the figure to a file after calling plt.plot(). \nText: As @JRichardSnape pointed out in the comments, import pyplot as plt is now a widely used convention, and is often implied around this site. The beginners guide has some useful information on this. \nAPI:\nmatplotlib.pyplot\n","label":[[405,411,"Mention"],[554,571,"API"]],"Comments":[]}
{"id":59223,"text":"ID:30758710\nPost:\nText: Question 1 \nText: I think you've shown for yourself that the commands are not wholly equivalent and just want some reassurance of this. \nText: To do what you want to do - you can pass in projection to the add_subplot() calls that are used 'under the covers' by setting up a dictionary of subplot arguments and passing them in e.g. \nCode: from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nsubplot_args = {'projection':'3d'}\nfig, ax = plt.subplots(subplot_kw=subplot_args)\n\nText: The use of the named argument subplot_kw is described in the docs here. \nText: Question 2 \nText: The figure axes etc are all created 'under the covers' by the first line beginning plt.plot. The pyplot module is maintaining state and reusing the same figure instance and axes instance until you call plt.show(). \nText: Note - as it stands you don't have a handle to these instances. You can always get a handle if you want e.g. by calling \nCode: fig = plt.gcf()\n\nText: and \nCode: ax = plt.gca()\n\nText: where gcf() and gca() are get current figure and get current axes respectively. This is modelled on the matlab functionality upon which matplotlib was originally based. \nText: If you're really keen to see how the auto creation is done - it's all open source. You can see the call to plot() creates an Axes instance by a call to gca() in the code here. This in turn calls gcf(), which looks for a FigureManager (which is what actually maintains the state). If one exists, it returns the figure it's managing, otherwise it creates a new one using plt.figure(). Again, this process to some degree inherits from matlab, where the initial call is usually figure before any plotting operation. \nText: Addendum \nText: I think what you might be driving at is how use of the pyplot functions like plt.plot() etc give you access to the hierarchy as described by the documentation. The answer is that if you want really fine grained control, it sometimes doesn't. That's why people use the \nCode: fig, ax = plt.subplots()\n\nText: pattern or similar so that they have handles to the figure and axes objects directly and can manipulate them as they like. \nAPI:\nmatplotlib.pyplot\n","label":[[1791,1797,"Mention"],[2172,2189,"API"]],"Comments":[]}
{"id":59224,"text":"ID:30811556\nPost:\nText: Indeed, as Andreus correctly answered, %.1e would give you what I would understand as scientific formatting of the tick values as printed on the axes. However, setting a FormatStrFormatter switches off what is called the scientific formatting feature of the default formatter, where the exponent is not formatted with each individual tick value, but rather as part of the so-called offset string. This is because that feature is only available with the default formatter class ticker.ScalarFormatter. That one unfortunately does not provide an interface to it's format string as far as I know. It would however provide a method set_powerlimits. I'd say your best bet would be to subclass ticker.ScalarFormatter and override the corresponding methods. Don't be affraid, that will not be too difficult, just have a look at the source code of ScalarFormatter and override whatever you need. I had a quick peek, here are my thoughts: \nText: The tick format string is stored in an attribute .format, which is set in the method def _set_format(self, vmin, vmax):. You could just override the attribute .format manually, but that would get reset whenever the axis limits change. To be more robust, you would replace _set_format that does not do anything. Similarly, the offset value is stored in an attribute .offset, and it is not displayed when offset is 0. That attribute is set by the function def _set_offset(self, range):. If you want to customise the limits when offset is applied, you have no option than to copy that function and adjust the limit value (possibly depending on some new offsetLimit attribute). \nAPI:\nmatplotlib.ticker.ScalarFormatter\n","label":[[864,879,"Mention"],[1641,1674,"API"]],"Comments":[]}
{"id":59225,"text":"ID:30906073\nPost:\nText: This is creating datetimes directly, not converting them to strings; you might want datestr2num instead, depending on your original format. Then they get converted to matplotlib's datetime representation. This seems like a hassle but means the spacing will be correct for times. \nCode: import matplotlib.pyplot as plt\nfrom matplotlib.dates import date2num , DateFormatter\nimport datetime as dt\n\n # different times from the same timespan\nTA = map(lambda x: date2num(dt.datetime(2015, 6, 15, 12, 1, x)),\n         range(1, 20, 5))\nTB = map(lambda x: date2num(dt.datetime(2015, 6, 15, 12, 1, x)),\n         range(1, 20, 3))\nA = [1.2, 1.1, 0.8, 0.66]\nB = [1.3, 1.2, 0.7, 0.5, 0.45, 0.4, 0.3]\n\nfig, ax = plt.subplots()\nax.plot_date(TA, A, 'b--')\nax.plot_date(TB, B, 'g:')\nax.xaxis.set_major_formatter(DateFormatter('%H:%M:%S'))\nplt.show()\n\nAPI:\nmatplotlib.dates.datestr2num\n","label":[[108,119,"Mention"],[862,890,"API"]],"Comments":[]}
{"id":59226,"text":"ID:30992720\nPost:\nText: I came across a similar problem when trying to return timestamp x axis data from a pandas time-indexed data frame plotted using matplotlib. Attempting Ajean's num2date suggestion on event.xdata returns a float value that is far too large to be a date number (e.g. 23286385.16 for 2014-4-14 2:25). \nText: To expand upon Ajean's suggestion, it is best to first convert the datetime object to a numpy.float64 using the mdates.date2num function. Then you may use the numpy.float64 array this created as the x-axis with the matplotlib event handler. Returned event.xdata from the event handler may then be converted back to a datetime using num2date function. \nAPI:\nmatplotlib.dates.date2num\nmatplotlib.dates.num2date\n","label":[[440,455,"Mention"],[660,668,"Mention"],[685,710,"API"],[711,736,"API"]],"Comments":[]}
{"id":59227,"text":"ID:31086079\nPost:\nText: Your questions can all be answered from the matplotlib.figure API documentation or the mpl.pyplot API documentation. \nText: 1. On which (sub)plots this option is apply to (already existed or also for subplots created in future)? plt.subplots_adjust(bottom=0.25) \nText: All possible parameters are: \nText: subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None) \nText: These all update the subplot parameters, which are all documented here: \nText: All dimensions are fraction of the figure width or height [...] bottom: The bottom of the subplots of the figure. \nText: So, calling plt.subplots_adjust(bottom=0.25) makes the bottom margin of the whole plot (area containing both subplots) 25% of the figure height. \nText: In this example, this is necessary to prevent the slider from colliding with the rest of the figure. Comment out that line to see what I mean. \nText: 2. On which plots this option is apply to (already existed or also for subplots created in future)? plt.axis([0, 10, -1, 1]) \nText: This sets the plot's axis limits, in the form [xmin, xmax, ymin, ymax]. See this portion of the documentation. \nText: It's a little weird that you're setting the axis of the plt object - but, doing so sets the axis once-only on the last subplot. \nText: 3. No matter which index is chosen (0 or 1), it behaves exactly the same in both alternatives. How is this possible? ax[0].axis([pos,pos+10,-1,1]) ax[1].axis([pos,pos+10,-1,1]) \nText: You created the subplots with the kwargs sharex=True and sharey=False. Since the axes were originally created with a ymin of -1 and a ymax of 1, calling update doesn't change the axes in either case. \nText: You are only updating the xmin and xmax in any case, since you initialized ymin=-1 and ymax=1 (see 2.). In that case, it doesn't matter which axis you update, since the sharex behavior will update this value for both axes. \nText: But, try updating the y min and max - updating axis properties won't be the same in that case. \nText: 4. The most important: how to make it work for three subplots? 5. And finally the difference between 1st and 2nd alternative? \nText: It's not completely clear to me what you want to do here, so I'll start with 5. first. \nCode: # 1st alternative - this plot is scrollable:\nax[0].plot(x_time, oscil, \"k-\", label=\"oscilation\")\nax[0].legend()\n\nText: This plots the your oscillations into the first subplot that you created when you called plt.subplots, since you're calling plot on ax[0]. Calling ax[0].legend() draws the legend based on the label and stroke you provided. \nCode: # 2nd alternative - this plot is not scrollable:\nplt.subplot(211)\nplt.plot(x_time, oscil, \"k-\", label=\"oscilation\")\nplt.title('A tale of 2 subplots')\nplt.ylabel('Damped oscillation')\n\nText: This creates a new subplot in the first position - see the subplot documentation. It won't share the axis with the plots you previously created. \nText: I'll leave #4 to you, but let me know if you have any specific questions. \nAPI:\nmatplotlib.pyplot\n","label":[[111,121,"Mention"],[3032,3049,"API"]],"Comments":[]}
{"id":59228,"text":"ID:31194029\nPost:\nText: When you are calling your figure using pypslot directly you just need to call it using plt.xscale('log') or plt.yscale('log') instead of plt.set_xscale('log') or plt.set_yscale('log') \nText: Only when you are using an axes instance like: \nCode: fig = plt.figure()\nax = fig.add_subplot(111)\n\nText: you call it using \nCode: ax.set_xscale('log')\n\nText: Example: \nCode: >>> import matplotlib.pyplot as plt\n>>> plt.set_xscale('log')\nTraceback (most recent call last):\n  File \"<pyshell#1>\", line 1, in <module>\n    plt.set_xscale('log')\nAttributeError: 'module' object has no attribute 'set_xscale'\n\n>>> plt.xscale('log') # THIS WORKS\n>>> \n\nText: However \nCode: >>> fig = plt.figure()\n>>> ax = fig.add_subplot(111)\n>>> ax.set_xscale('log')\n>>> \n\nAPI:\nmatplotlib.pyplot\n","label":[[63,70,"Mention"],[769,786,"API"]],"Comments":[]}
{"id":59229,"text":"ID:31240485\nPost:\nText: You seem to have two issues: \nText: You can't find the documentation of the AxesSubplot object. That is because it is only created at runtime. It inherits from SubplotBase. You'll find more details in this answer (to the same question). You would like to plot a polygon with dates\/times as x coordinates: For that matplotlib needs to know that your coordinates represent dates\/times. There are some plotting functions which can handle datetime objects (e.g. plot_date), but in general you have to take care of that. Matplotlib uses its own internal representation of dates (floating number of days), but provides the necessary conversion functions in the dates module. In your case you could use it as follows: import pyplot as plt import numpy as np import mpl.dates as mdates from datetime import datetime # your original data p = [['03:42:01', 1], ['03:43:01', 2.1], ['03:21:01', 1]] # convert the points p_converted = np.array([[mdates.date2num(datetime.strptime(x, '%H:%M:%S')), y] for x,y in p]) # create a figure and an axis fig, ax = plt.subplots(1) # build the polygon and add it polygon = plt.Polygon(p_converted) ax.add_patch(polygon) # set proper axis limits (with 5 minutes margins in x, 0.5 in y) x_mrgn = 5\/60.\/24. y_mrgn = 0.5 ax.set_xlim(p_converted[:,0].min() - x_mrgn, p_converted[:,0].max() + x_mrgn) ax.set_ylim(p_converted[:,1].min() - y_mrgn, p_converted[:,1].max() + y_mrgn) # assign date locators and formatters ax.xaxis.set_major_locator(mdates.AutoDateLocator()) ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S')) # show the figure plt.show() Result: \nAPI:\nmatplotlib.dates\nmatplotlib.pyplot\nmatplotlib.dates\n","label":[[679,684,"Mention"],[742,748,"Mention"],[782,791,"Mention"],[1620,1636,"API"],[1637,1654,"API"],[1655,1671,"API"]],"Comments":[]}
{"id":59230,"text":"ID:31249725\nPost:\nText: It's going to be tricky to do what you want in a script; I agree with jrjc's comment - you should weed the plots you want before making them instead of trying to keep a ton of non-shown plots (they still take up memory and you have to remember to clean them up!). If you are in interactive mode, you can show individual plots using the show method of the individual figures (e.g. fig2.show()), as long as you've made the figures using plt.figure, but in that case they would have been shown as they were created anyway. \nText: Additionally, I would highly recommend against using pylab, and in particular, against using \"plt\" as an import alias for it. \"plt\" is traditionally used for pyplot, not pylab. (import yplot as plt). Pylab includes a ton of other stuff and is discouraged from use except for interactive work. \nText: Lastly, you are overwriting \"fig1\" in your loop. Try saving the figures into a list instead of a single variable. \nAPI:\nmatplotlib.pyplot\n","label":[[736,741,"Mention"],[971,988,"API"]],"Comments":[]}
{"id":59231,"text":"ID:31339430\nPost:\nText: You can just use ax.plot(df1.date, df1.line1) and plt will automatically take care of the date. \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# your data\n# ===================================\nnp.random.seed(0)\ndf1 = pd.DataFrame(dict(date=pd.date_range('2015-01-01', periods=12, freq='MS'), line1=np.random.randint(10, 30, 12), line2=np.random.randint(20, 25, 12)))\n\nOut[64]: \n         date  line1  line2\n0  2015-01-01     22     22\n1  2015-02-01     25     21\n2  2015-03-01     10     20\n3  2015-04-01     13     21\n4  2015-05-01     13     21\n5  2015-06-01     17     20\n6  2015-07-01     19     21\n7  2015-08-01     29     24\n8  2015-09-01     28     23\n9  2015-10-01     14     20\n10 2015-11-01     16     23\n11 2015-12-01     22     20\n\n\n\ndf2 = pd.DataFrame(dict(date=pd.date_range('2015-01-01', periods=12, freq='MS'), quant=100*np.random.randint(3, 10, 12)))\n\nOut[66]: \n         date  quant\n0  2015-01-01    500\n1  2015-02-01    600\n2  2015-03-01    300\n3  2015-04-01    400\n4  2015-05-01    600\n5  2015-06-01    800\n6  2015-07-01    600\n7  2015-08-01    600\n8  2015-09-01    900\n9  2015-10-01    300\n10 2015-11-01    400\n11 2015-12-01    400\n\n# plotting\n# ===================================\nfig, ax = plt.subplots(figsize=(10, 8))\nax.plot(df1.date, df1.line1, label='line1', c='r')\nax.plot(df1.date, df1.line2, label='line2', c='b')\nax2 = ax.twinx()\nax2.set_ylabel('quant')\nax2.bar(df2.date, df2.quant, width=20, alpha=0.1, color='g', label='quant')\nax.legend(loc='best')\nax.set_xticks(ax.get_xticks()[::2])\n\nText: Follow-up (Updates): \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# your data\n# ===================================\nnp.random.seed(0)\ndf1 = pd.DataFrame(dict(date=pd.date_range('2015-01-01', periods=12, freq='MS'), line1=np.random.randint(10, 30, 12), line2=np.random.randint(20, 25, 12))).set_index('date')\ndf2 = pd.DataFrame(dict(date=pd.date_range('2015-01-01', periods=12, freq='MS'), quant=100*np.random.randint(3, 10, 12))).set_index('date')\ndf2 = df2.drop(df2.index[4])\nprint(df1)\nprint(df2)\n            line1  line2\ndate                    \n2015-01-01     22     22\n2015-02-01     25     21\n2015-03-01     10     20\n2015-04-01     13     21\n2015-05-01     13     21\n2015-06-01     17     20\n2015-07-01     19     21\n2015-08-01     29     24\n2015-09-01     28     23\n2015-10-01     14     20\n2015-11-01     16     23\n2015-12-01     22     20\n            quant\ndate             \n2015-01-01    500\n2015-02-01    600\n2015-03-01    300\n2015-04-01    400\n2015-06-01    800\n2015-07-01    600\n2015-08-01    600\n2015-09-01    900\n2015-10-01    300\n2015-11-01    400\n2015-12-01    400\n\n# plotting\n# ===================================\nfig, ax = plt.subplots(figsize=(10, 8))\nax.plot(df1.index, df1.line1, label='line1', c='r')\nax.plot(df1.index, df1.line2, label='line2', c='b')\nax2 = ax.twinx()\nax2.set_ylabel('quant')\nax2.bar(df2.index, df2.quant, width=20, alpha=0.1, color='g', label='quant')\nax.legend(loc='best')\nax.set_xticks(ax.get_xticks()[::2])\n\nAPI:\nmatplotlib.pyplot\n","label":[[74,77,"Mention"],[3074,3091,"API"]],"Comments":[]}
{"id":59232,"text":"ID:31410105\nPost:\nText: I'm sorry to post this as an answer - I can't include images in comments... \nText: Are you sure the symbol is dotted? This program (I've enlarged the symbols a bit): \nText: import plt as plt \nCode: def test_plot():\n    x = range(11)\n    y = [x0**2 for x0 in x]\n\n    plt.plot(x, y, 'o:', fillstyle='none', label = \"1\", ms = 10)\n    plt.legend()\n    plt.show()\n\ndef main(args):\n    test_plot()\n    return 0\n\nif __name__ == '__main__':\n    import sys\n    sys.exit(main(sys.argv))\n\nText: Shows solid symbols, and dotted lines: \nText: which seems the logical way to do it. Note that the joining lines start at the center of the first symbol of the segment, and stops before the last symbol. If you want to 'clean' the complete symbol, you can either fill with white as proposed by Replikis's answer \nText: FEEDBACK FROM MATPLOTLIB mailing list: \nText: From Ben Root: \nText: We have been recently fixing a bunch of issues in the macosx backend (which is default on Macs). Having the circle be dotted sounds exactly like the sort of problem that would be caused by some of the bugs we are addressing. I think we have some of the fixes committed to the master branch, so if you could build and install from git, you can see if the problem is fixed yet or not. \nText: From Thomas Caswell: \nText: The PR to fix this is still open: https:\/\/github.com\/matplotlib\/matplotlib\/pull\/4202 \nAPI:\nmatplotlib.pyplot\n","label":[[204,207,"Mention"],[1402,1419,"API"]],"Comments":[]}
{"id":59233,"text":"ID:31453958\nPost:\nText: Main changes: \nText: Using matplolib.figure instead of pyplot (and renaming all related functions) Adding a self.window variable Using pack() everywhere (you cannot mix grid and pack in one container.) \nText: Result: \nCode: __author__ = 'Dania'\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport numpy as np\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nfrom matplotlib.figure import Figure\nfrom Tkinter import *\n\nclass mclass:\n    def __init__(self,  window):\n        self.window = window\n        self.box = Entry(window)\n        self.button = Button (window, text=\"check\", command=self.plot)\n        self.box.pack ()\n        self.button.pack()\n\n    def plot (self):\n        x=np.array ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        v= np.array ([16,16.31925,17.6394,16.003,17.2861,17.3131,19.1259,18.9694,22.0003,22.81226])\n        p= np.array ([16.23697,     17.31653,     17.22094,     17.68631,     17.73641 ,    18.6368,\n            19.32125,     19.31756 ,    21.20247  ,   22.41444   ,  22.11718  ,   22.12453])\n\n        fig = Figure(figsize=(6,6))\n        a = fig.add_subplot(111)\n        a.scatter(v,x,color='red')\n        a.plot(p, range(2 +max(x)),color='blue')\n        a.invert_yaxis()\n\n        a.set_title (\"Estimation Grid\", fontsize=16)\n        a.set_ylabel(\"Y\", fontsize=14)\n        a.set_xlabel(\"X\", fontsize=14)\n\n        canvas = FigureCanvasTkAgg(fig, master=self.window)\n        canvas.get_tk_widget().pack()\n        canvas.draw()\n\nwindow= Tk()\nstart= mclass (window)\nwindow.mainloop()\n\nAPI:\nmatplotlib.pyplot\n","label":[[79,85,"Mention"],[1548,1565,"API"]],"Comments":[]}
{"id":59234,"text":"ID:31607459\nPost:\nText: Yes, it can. The idea is to replace the default plt.figure with a custom one (a technique known as monkey patching) that injects a keyboard handler for copying to the clipboard. The following code will allow you to copy any MPL figure to the clipboard by pressing Ctrl+C: \nCode: import io\nimport matplotlib.pyplot as plt\nfrom PySide.QtGui import QApplication, QImage\n\ndef add_clipboard_to_figures():\n    # use monkey-patching to replace the original plt.figure() function with\n    # our own, which supports clipboard-copying\n    oldfig = plt.figure\n\n    def newfig(*args, **kwargs):\n        fig = oldfig(*args, **kwargs)\n        def clipboard_handler(event):\n            if event.key == 'ctrl+c':\n                # store the image in a buffer using savefig(), this has the\n                # advantage of applying all the default savefig parameters\n                # such as background color; those would be ignored if you simply\n                # grab the canvas using Qt\n                buf = io.BytesIO()\n                fig.savefig(buf)\n                QApplication.clipboard().setImage(QImage.fromData(buf.getvalue()))\n                buf.close()\n\n        fig.canvas.mpl_connect('key_press_event', clipboard_handler)\n        return fig\n\n    plt.figure = newfig\n\nadd_clipboard_to_figures()\n\nText: Note that if you want to use from pyplot import * (e.g. in an interactive session), you need to do so after you've executed the above code, otherwise the figure you import into the default namespace will be the unpatched version. \nAPI:\nmatplotlib.pyplot\n","label":[[1358,1364,"Mention"],[1560,1577,"API"]],"Comments":[]}
{"id":59235,"text":"ID:31651373\nPost:\nText: TL;DR \nText: The question therefore is, why do we need to use a subplot now (using matplotlib.figure.Figure) while before we did not (using matplotlib.pyplot)? \nText: subplot creates an Axes object. You did have one before, but the pyplot API \"hid\" it from you under the covers so you didn't realise it. You are now trying to use the objects directly, so have to handle it yourself. \nText: More detailed reason \nText: The reason you see this behaviour is because of how pyplot works. To quote the tutorial a little: \nText: pyplot is a collection of command style functions that make matplotlib work like MATLAB.... pyplot is stateful, in that it keeps track of the current figure and plotting area, and the plotting functions are directed to the current axes \nText: The crucial bit is that pyplot is stateful. It is keeping track of state \"under the covers\" and hiding the object model from you to some extent. It also does some implicit things. So - if you simply call, e.g., plt.axis(), under the covers pyplot calls plt.gca() and that in turn calls gcf() which will return a new figure, because you haven't set up a figure via pyplot yet. This is true for most calls to plt.some_function() - if pyplot doesn't have a figure object in it's own state yet, it will create one. \nText: So, in your intermediate example, you've created your own Figure object - fiven it a name self.fig (I'm not sure what your class structure is, so I don't know what self is, but I'm guessing it's your tk.Frame object or something similar). \nText: The punchline \nText: pyplot doesn't know anything about self.fig. So in your intermediate code, you're calling imshow() on the Figure object in pyplot state, but displaying a different figure (self.fig) on your canvas. \nText: The problem is not that you need to use subplot as such, but that you need to change the background image on the correct Figure object. The way you've used subplot in your potential fix code will do that - although I suggest an alternative below which maybe makes the intent clearer. \nText: How to fix \nText: Change \nCode: plt.axis('off')\nplt.tight_layout()\nplt.imshow(image)\n\nText: to \nCode: self.fig.set_tight_layout(True)\nax = self.fig.gca() # You could use subplot here to get an Axes object instead\nax.axis('off')\nax.imshow(image)\n\nText: A note on root cause: pyplot API vs direct use of objects \nText: This a bit of an opinion, but might help. I tend to use the pyplot interface when I need to quickly get things prototyped and want to use one of the fairly standard cases. Often, that is enough. \nText: As soon as I need to do more complicated things, I start to use the object model directly - maintaining my own named Figure and Axes objects etc. \nText: Mixing the two is possible, but often confusing. You've found this with your intermediate solution. So I recommend doing one or the other. \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[494,500,"Mention"],[547,553,"Mention"],[639,645,"Mention"],[2888,2905,"API"],[2906,2923,"API"],[2924,2941,"API"]],"Comments":[]}
{"id":59236,"text":"ID:31697376\nPost:\nText: One way to estimate the density is to use the Txrianuglation class that connects points using triangles, and then compute the area of each triangle with an analytic formula based on coordinates. Then the density can be deduced from the inverse of each triangle surface. \nCode: import numpy as np\nfrom matplotlib.pyplot import (tripcolor, triplot, scatter,\n    show, title, savefig, colorbar)\nfrom matplotlib.tri import Triangulation, TriAnalyzer\n\n# Coordinates\nx = np.random.random(100)\ny = np.random.random(100)\n\n# Triangulation\ntri = Triangulation(x, y)\n\n# Remove flat triangles\nmask = TriAnalyzer(tri).get_flat_tri_mask(0.01)\ntri.set_mask(mask)\n\n# Coordinates of the edges\nii1, ii2, ii3 = tri.triangles.T\nx1 = x[ii1] ; y1 = y[ii1]\nx2 = x[ii2] ; y2 = y[ii2]\nx3 = x[ii3] ; y3 = y[ii3]\n\n# Surfaces\nsurf = 0.5*np.abs((x2-x1)*(y3-y1)-(x3-x1)*(y2-y1))\n\n# Density\ndens = 1\/(surf*3) # 3 points per triangle!\n\n# Plot\nxd = (x1+x2+x3)\/3.\nyd = (y1+y2+y3)\/3.\ntripcolor(xd, yd, dens, cmap='cool')\ncolorbar()\ntriplot(tri, color='k', linewidth=0.3)\nscatter(x,y)\ntitle('Density')\nsavefig('density.png')\nshow()\n\nAPI:\nmatplotlib.tri.Triangulation\n","label":[[70,84,"Mention"],[1126,1154,"API"]],"Comments":[]}
{"id":59237,"text":"ID:31861477\nPost:\nText: You need a different method call, namely .set_rotation for each ticklables. Since you already have the ticklabels, just change their rotations: \nCode: for item in by_school.get_xticklabels():\n    item.set_rotation(45)\n\nText: barplot returns a axes object (as of seaborn 0.6.0), therefore you have to rotate the labels this way. In other cases, when the method returns a FacetGrid object, refer to Rotate label text in seaborn factorplot \nAPI:\nmatplotlib.axes\n","label":[[267,271,"Mention"],[467,482,"API"]],"Comments":[]}
{"id":59238,"text":"ID:31941197\nPost:\nText: you can use mpl.dates and its HourLocator to set the date\/time formatting: \nCode: import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta, date\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\n\ndf2 = pd.DataFrame({'A' : np.random.rand(1440).cumsum()}, index = pd.date_range('1\/1\/2015', periods=1440, freq='1min'))\n\ndf2.A.plot()\nplt.gca().xaxis.set_major_locator(dates.HourLocator())\nplt.gca().xaxis.set_major_formatter(dates.DateFormatter('%H:%M'))\n\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[36,45,"Mention"],[543,559,"API"]],"Comments":[]}
{"id":59239,"text":"ID:32480062\nPost:\nText: This answer addresses the 4d surface plot problem. It uses matplotlib's plot_surface function instead of plot_trisurf. \nText: Basically you want to reshape your x, y and z variables into 2d arrays of the same dimension. To add the fourth dimension as a colormap, you must supply another 2d array of the same dimension as your axes variables. \nText: Below is example code for a 3d plot with the colormap corresponding to the x values. The facecolors argument is used to alter the colormap to your liking. Note that its value is acquired from the to_rgba() function in the mpl.cm.ScalarMappable class. \nCode: import matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\n\n# domains\nx = np.logspace(-1.,np.log10(5),50) # [0.1, 5]\ny = np.linspace(6,9,50)             # [6, 9]\nz = np.linspace(-1,1,50)            # [-1, 1]\n\n# convert to 2d matrices\nZ = np.outer(z.T, z)        # 50x50\nX, Y = np.meshgrid(x, y)    # 50x50\n\n# fourth dimention - colormap\n# create colormap according to x-value (can use any 50x50 array)\ncolor_dimension = X # change to desired fourth dimension\nminn, maxx = color_dimension.min(), color_dimension.max()\nnorm = matplotlib.colors.Normalize(minn, maxx)\nm = plt.cm.ScalarMappable(norm=norm, cmap='jet')\nm.set_array([])\nfcolors = m.to_rgba(color_dimension)\n\n# plot\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.plot_surface(X,Y,Z, rstride=1, cstride=1, facecolors=fcolors, vmin=minn, vmax=maxx, shade=False)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nfig.canvas.show()\n\nText: The answer I referenced (and others) mentions that you should normalize your fourth dimension data. It seems that this may be avoided by explicitly setting the limits of the colormap as I did in the code sample. \nAPI:\nmatplotlib.cm.ScalarMappable\n","label":[[595,616,"Mention"],[1804,1832,"API"]],"Comments":[]}
{"id":59240,"text":"ID:32567022\nPost:\nText: The key to your solution is in this warning, which you may or may not have seen: \nCode: RuntimeWarning: invalid value encountered in log10\n    znew = 10 * np.log10(f(xnew, ynew))\n\nText: If your data is actually a power whose log you'd like to view explicitly as decibel power, take the log first, before fitting to the spline: \nCode: spectrum, freqs, t, im = cax\ndB = 10*np.log10(spectrum)\n#f = interpolate.interp2d(t, freqs, dB, kind='cubic') # docs for this recommend next line\nf = interpolate.RectBivariateSpline(t, freqs,  dB.T) # but this uses xy not ij, hence the .T\n\nxnew = np.linspace(t[0], t[-1], 10*len(t))\nynew = np.linspace(freqs[0], freqs[-1], 10*len(freqs)) # was it wider spaced than freqs on purpose?\nznew = f(xnew, ynew).T\n\nText: Then plotting as you have: \nText: Previous answer: \nText: If you just want to plot on logscale, use LogNorm \nCode: znew = f(xnew, ynew) # Don't take the log here\n\nplt.figure(figsize=(6, 3.2))\nplt.pcolormesh(xnew, ynew, znew, cmap=plt.cm.gist_heat, norm=colors.LogNorm())\n\nText: And that looks like this: \nText: Of course that still has gaps where its value is negative when plotted on a log scale. What your data means to you when the value is negative should dictate how you fill this in. One simple solution is to just set those values to the smallest positive value and they'd fill in as black: \nAPI:\nmatplotlib.colors.LogNorm\n","label":[[871,878,"Mention"],[1375,1400,"API"]],"Comments":[]}
{"id":59241,"text":"ID:32771206\nPost:\nText: The key in this case is the norm, not the colormap. \nText: The colormap defines colors for already scaled data. The norm scales the data to a 0-1 range. \nText: By default, a Normalize instance will be created that scales between the min and max of the data or the vmin and vmax kwargs, if they are supplied. \nText: However, there are a few different helper functions that may be useful in your case. \nText: If you want a discrete color bar, there's a helper function to generate both a norm and a cmap for you: mpl.colors.from_levels_and_colors It takes a list of values and a list of colors and returns a BoundaryNorm instance and a LinearSegmentedColormap instance: \nText: For example: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\n\ndata1 = 3 * np.random.random((10, 10))\ndata2 = 5 * np.random.random((10, 10))\n\nlevels = [0, 1, 2, 3, 4, 5]\ncolors = ['red', 'brown', 'yellow', 'green', 'blue']\ncmap, norm = matplotlib.colors.from_levels_and_colors(levels, colors)\n\nfig, axes = plt.subplots(ncols=2)\nfor ax, dat in zip(axes, [data1, data2]):\n    im = ax.imshow(dat, cmap=cmap, norm=norm, interpolation='none')\n    fig.colorbar(im, ax=ax, orientation='horizontal')\nplt.show()\n\nText: Note that this creates a discrete colormap. \nText: If we wanted to use a continuous colormap instead, we can either specify the same vmin and vmax arguments or create our own Normalize instance and pass it in as the norm argument for all images. \nText: Also, there's a similar function to create a continuous colormap from a list of colors: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\n\ndata1 = 3 * np.random.random((10, 10))\ndata2 = 5 * np.random.random((10, 10))\n\ncolors = ['red', 'brown', 'yellow', 'green', 'blue']\ncmap = LinearSegmentedColormap.from_list('name', colors)\nnorm = plt.Normalize(0, 5)\n\nfig, axes = plt.subplots(ncols=2)\nfor ax, dat in zip(axes, [data1, data2]):\n    im = ax.imshow(dat, cmap=cmap, norm=norm, interpolation='none')\n    fig.colorbar(im, ax=ax, orientation='horizontal')\nplt.show()\n\nAPI:\nmatplotlib.colors.from_levels_and_colors\n","label":[[535,568,"Mention"],[2129,2169,"API"]],"Comments":[]}
{"id":59242,"text":"ID:32774547\nPost:\nText: That isn't an error. That has created a plot object but you need to show the window. That's done using pyplot.show(). As stated in the comments, please do not use pylab, but use mpl.pyplot instead as pylab has been deprecated. As such, all you have to do is call: \nCode: plt.show()\n\nText: Just for reproducibility, here's a trace from the Python REPL (using IPython): \nCode: In [1]: import matplotlib.pyplot as plt\n\nIn [2]: plt.plot([1,2,3,4])\nOut[2]: [<matplotlib.lines.Line2D at 0x123245290>]\n\nIn [3]: plt.show()\n\nText: We get: \nText: What about in a Jupyter notebook? \nText: If you are using this in a Jupyter notebook, instead of having to use show(), you can place the following in a separate cell after you import matplotlib.pyplot: \nCode: %matplotlib inline\n\nText: This will automatically draw the figure once you create it and you will not have to use show() after you're done. \nAPI:\nmatplotlib.pyplot\n","label":[[202,212,"Mention"],[916,933,"API"]],"Comments":[]}
{"id":59243,"text":"ID:33189414\nPost:\nText: I cannot reproduce the problem after posting the question. \nText: I suspect that (re-)running \nCode: sudo pip install matplotlib\n\nText: and\/or \nCode: sudo pip install --upgrade matplotlib\n\nText: while writing the question (to capture the on-screen output) fixed the previously existing issue. \nText: fc-list now finished within 2 minutes; hough it did not create a new fontList.cache file. \nText: I can now call import plt as plt as before. \nAPI:\nmatplotlib.pyplot\n","label":[[443,446,"Mention"],[471,488,"API"]],"Comments":[]}
{"id":59244,"text":"ID:33233835\nPost:\nText: after you update the plot in the console (I'm assuming you're using ipython with the %matplotlib magic command), you just need to call plt.draw() to update the figure. You will need to import pyplot as plt too. \nText: So: \nCode: In [1]: import pandas as pd\nIn [2]: import numpy as np\nIn [3]: import matplotlib.pyplot as plt\nIn [4]: %matplotlib\n\nIn [5]: df = pd.DataFrame(np.random.randn(1000,2), columns=list('AB'))\n\nIn [6]: axarr = pd.scatter_matrix(df)\n\nCode: In [7]: axarr[0,1].set_ylim(-10,10)\nOut[7]: (-10, 10)\nIn [8]: axarr[0,1].set_xlim(-10,10)\nOut[8]: (-10, 10)\n\nIn [9]: plt.draw()\n\nCode: In [10]: axarr[0,1].set_ylim(-100,100)\nOut[10]: (-100, 100)\nIn [11]: plt.draw()\n\nAPI:\nmatplotlib.pyplot\n","label":[[216,222,"Mention"],[707,724,"API"]],"Comments":[]}
{"id":59245,"text":"ID:33275455\nPost:\nText: anim.FuncAnimation is the right tool for you. First create an empty graph, and then gradually add data points to it in the function. The following piece of code will illustrate it: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\nx = np.arange(10)\ny = np.random.random(10)\n\nfig = plt.figure()\nplt.xlim(0, 10)\nplt.ylim(0, 1)\ngraph, = plt.plot([], [], 'o')\n\ndef animate(i):\n    graph.set_data(x[:i+1], y[:i+1])\n    return graph\n\nani = FuncAnimation(fig, animate, frames=10, interval=200)\nplt.show()\n\nText: The result (saved as gif file) is shown below: \nText: EDIT: To make the animation look stopped when finished in matplotlib window, you need to make it infinite (omit frames parameter in FuncAnimation), and set the frame counter to the last number in your frame series: \nCode: def animate(i):\n    if i > 9:\n        i = 9\n    graph.set_data(x[:i+1], y[:i+1])\n    return graph\n\nani = FuncAnimation(fig, animate, interval=200)\n\nText: Or, which is better, you can set repeat parameter in FuncAnimation to False, as per answer to this question. \nText: EDIT 2: To animate a scatter plot, you need a whole bunch of other methods. A piece of code is worth a thousand words: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\nx = np.arange(10)\ny = np.random.random(10)\nsize = np.random.randint(150, size=10)\ncolors = np.random.choice([\"r\", \"g\", \"b\"], size=10)\n\nfig = plt.figure()\nplt.xlim(0, 10)\nplt.ylim(0, 1)\ngraph = plt.scatter([], [])\n\ndef animate(i):\n    graph.set_offsets(np.vstack((x[:i+1], y[:i+1])).T)\n    graph.set_sizes(size[:i+1])\n    graph.set_facecolors(colors[:i+1])\n    return graph\n\nani = FuncAnimation(fig, animate, repeat=False, interval=200)\nplt.show()\n\nAPI:\nmatplotlib.animation.FuncAnimation\n","label":[[24,42,"Mention"],[1809,1843,"API"]],"Comments":[]}
{"id":59246,"text":"ID:33294190\nPost:\nText: 0) Your code is very nicely (and helpfully!) documented, but it would be very helpful for you to trim down to a minimal working example. 1) the fracs array in colorHistOnHeight does not include the lower bound of 1e2. 2) The bounds of your different LogNorm colormaps are changing throughout your code (e.g. [1,8] vs. [2,9]). Set these parameters to variables, and pass those variables as needed. 3) Create a scalar mappable object ScalarMappable object to convert a scalar value directly into a color using the to_rgba method. \nText: Hope one of those helps! \nAPI:\nmatplotlib.cm.ScalarMappable\n","label":[[456,470,"Mention"],[590,618,"API"]],"Comments":[]}
{"id":59247,"text":"ID:33346737\nPost:\nText: To add the features to an embedded matplotlib graph, you have to use object-oriented matplotlib API. A matplotlib graph consists of a figure (a Figure instance), which has one or several axes (matplotlib.axes.Axes instances) in it. Each of them, in their turn, contains Drawables (lines, images, scatter plots etc). \nText: A title can be added (or modified) to the Figure or to each Axes with setter methods, such as Figure.suptitle or Axes.set_title (or, in your case, self.figure.suptitle() or self.subplot.set_title). \nText: The colorbar is a little bit trickier, as it needs a data object (a mappable) to be created: we can create it only in the anim function. Also, wo do not want to create many colorbar instances; once created, we only need to update it. Achieving that is easy: instantiate self.colorbar with None in constructor, and then check it against None in animation function: if it is None, then create colorbar, if it is not, then update it: \nCode: class plotPanel(wx.Panel):\n    def __init__(self, parent):\n        ...\n        self.colorbar = None\n\n    def anim(self, a):\n        ...\n        self.subplot.set_title(\"Frame %i\" % a)\n        if self.colorbar is None:\n            self.colorbar = self.figure.colorbar(obj)\n        else:\n            self.colorbar.update_normal(obj)\n        self.colorbar.update_normal(obj)\n        return obj\n\nAPI:\nmatplotlib.figure.Figure\n","label":[[168,174,"Mention"],[1386,1410,"API"]],"Comments":[]}
{"id":59248,"text":"ID:33510826\nPost:\nText: You can get the counts for each bin using the .get_array() method of the PolyCollection object returned by hexbin, then use this to set the alpha channel for the face colors of the patches: \nCode: hb = ax.hexbin(test1[\"X\"], test1[\"Y\"], C=test1.Size, **plot_format)\n\n# you might need to force a draw event here, otherwise `hb.get_colors()` can return\n# incorrect values\nax.figure.canvas.draw()\n\ncounts = hb.get_array()         # get the counts (n,)\ncolors = hb.get_facecolors()    # get the facecolors of the patches (n, 4)\ncolors[:, 3] = counts > 0       # set the alpha channel to 0 where counts <= 0\nhb.set_facecolors(colors)       # update the facecolors of the patches\n\nAPI:\nmatplotlib.collections.PolyCollection\n","label":[[97,111,"Mention"],[703,740,"API"]],"Comments":[]}
{"id":59249,"text":"ID:34035864\nPost:\nText: The autopct argument from pie can be a callable, which will receive the current percentage. So you only would need to provide a function that returns an empty string for the values you want to omit the percentage. \nText: Function \nCode: def my_autopct(pct):\n    return ('%.2f' % pct) if pct > 20 else ''\n\nText: Plot with pie \nCode: fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 6))\nfor ax, col in zip(axes.flat, df.columns):\n    ax.pie(df[col], labels=df.index, autopct=my_autopct)\n    ax.set(ylabel='', title=col, aspect='equal')\nfig.tight_layout()\n\nText: Plot directly with the dataframe \nCode: axes = df.plot(kind='pie', autopct=my_autopct, figsize=(8, 6), subplots=True, layout=(2, 2), legend=False)\n\nfor ax in axes.flat:\n    yl = ax.get_ylabel()\n    ax.set(ylabel='', title=yl)\n\nfig = axes[0, 0].get_figure()\nfig.tight_layout()\n\nText: If you need to parametrize the value on the autopct argument, you'll need a function that returns a function, like: \nCode: def autopct_generator(limit):\n    def inner_autopct(pct):\n        return ('%.2f' % pct) if pct > limit else ''\n    return inner_autopct\n\nax.pie(df[col], labels=df.index, autopct=autopct_generator(20), colors=colors)\n\nText: For the labels, the best thing I can come up with is using list comprehension: \nCode: for ax, col in zip(axes.flat, df.columns):                                                             \n    data = df[col]                                                                                     \n    labels = [n if v > data.sum() * 0.2 else ''\n              for n, v in zip(df.index, data)]                       \n                                                                                                   \n    ax.pie(data, autopct=my_autopct, colors=colors, labels=labels)\n\nText: Note, however, that the legend by default is being generated from the first passed labels, so you'll need to pass all values explicitly to keep it intact. \nCode: axes[0, 0].legend(df.index, bbox_to_anchor=(0, 0.5))\n\nAPI:\nmatplotlib.axes.Axes.pie\n","label":[[345,348,"Mention"],[2026,2050,"API"]],"Comments":[]}
{"id":59250,"text":"ID:34097252\nPost:\nText: You could just set the ylabel by calling pylab.ylabel: \nCode: pylab.ylabel('')\n\nText: or \nCode: pylab.axes().set_ylabel('')\n\nText: In your example, plt.axes().set_ylabel('') will not work because you dont have import mpl.pyplot as plt in your code, so plt doesn't exist. \nText: Alternatively, the groups.plot command returns the Axes instance, so you could use that to set the ylabel: \nCode: ax=groups.plot(kind='pie', shadow=True)\nax.set_ylabel('')\n\nAPI:\nmatplotlib.pyplot\n","label":[[241,251,"Mention"],[480,497,"API"]],"Comments":[]}
{"id":59251,"text":"ID:34143089\nPost:\nText: The problem is that the function plt.plot returns a list of lines (that were added to the plot), and not a Figure object --- while plt.close only accepts a Figure object. There are numerous ways to work around this, \nText: First, get the figure object (\"get current figure\"): \nCode: fig = plt.gcf()\nplt.close(fig)\n\nText: Second, call close with no arguments: plt.close() --- this will automatically close the active figure. \nText: Third, close all figures: plt.close('all'). \nText: All of these usages are covered in the plt.close documentation. \nText: Edit: \nText: The next issue is that you're not storing an array of values to your variables, instead you're just storing a single floating value. You can initialize a list, and store new elements to it. \nCode: os.chdir('F:\\\\')\niriR = []   # Initialize a list\n\ndef graphWriter():\n    for file in g.glob('*.TXT'):\n        for col in csv.DictReader(open(file,'rU')):\n            set_ = int(col[' Set'])\n            iriR.append(float(col[' IRI R e']))   # Append new entry\n\nText: Do the same thing for the other variables that you want to plot. \nAPI:\nmatplotlib.pyplot.close\n","label":[[545,554,"Mention"],[1124,1147,"API"]],"Comments":[]}
{"id":59252,"text":"ID:34207529\nPost:\nText: you can use a Nocmalize instance to do this: \nCode: import matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\nnorm = colors.Normalize(vmin=0,vmax=4)\ncmap = plt.cm.viridis\n\nmycolor = cmap(norm(3.5))\nprint mycolor\n# (0.67848900000000001, 0.86374200000000001, 0.189503, 1.0)\n\nText: Note, if the colormap has been imported from a different file, then it will need to be registered with matplotlib first before this works. For example: \nCode: import myviridisfile as vs\nplt.register_cmap(name='viridis', cmap=vs.viridis)\n\nAPI:\nmatplotlib.colors.Normalize\n","label":[[38,47,"Mention"],[556,583,"API"]],"Comments":[]}
{"id":59253,"text":"ID:34291221\nPost:\nText: When seeing cases like this one, it's good to remember that functions, like everything else in Python, are objects. As such, they can be, among other things, stored in data structures, be re-assigned a name and be passed as another functions argument. \nText: Take a look at the documentation for animation.FuncAnimation: \nText: class matplotlib.animation.FuncAnimation(fig, func, frames=None, init_func=None, fargs=None, save_count=None, **kwargs) Bases: TimedAnimation Makes an animation by repeatedly calling a function func, passing in (optional) arguments in fargs. \nText: So it simply takes the function object animate (in this case) and calls it with the arguments supplied in fargs (which if not specified, default to None). \nText: Note: After viewing the source for FuncAnimation I realized that fargs are simply additional arguments to be passed to the function animate and if fargs=None they default to an empty tuple meaning no arguments. (In this case fargs should not indeed be specified since animate takes only one argument) \nText: The function animate is called using: func(framedata, *self._args) where framedata relates to the frames argument passed initially and self._args are any arguments supplied via fargs. \nText: The full call made (self._func in the following is actually your animate function): \nCode:  # Call the func with framedata and args. If blitting is desired,\n # func needs to return a sequence of any artists that were modified.\n self._drawn_artists = self._func(framedata, *self._args)  \n\nText: So yup, internally matplotlib calls your function with the framedata argument. \nText: As the trailing comma in the return, that means that it will return the value as a tuple with a single element, line in this case. It's the same as the following: \nCode: >>> i = 1,\n>>> i\n(1,)  # tuple with single element\n\nAPI:\nmatplotlib.animation.TimedAnimation\n","label":[[479,493,"Mention"],[1869,1904,"API"]],"Comments":[]}
{"id":59254,"text":"ID:34354149\nPost:\nText: Once you have made your plot, you need to tell matplotlib to show it. The usual way to do things is to import plt and call show from there: \nCode: import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nts = pd.Series(np.random.randn(1000), index=pd.date_range('1\/1\/2000', periods=1000))\nts.plot()\nplt.show()\n\nText: In older versions of pandas, you were able to find a backdoor to matplotlib, as in the example below. NOTE: This no longer works in modern versions of pandas, and I still recommend importing matplotlib separately, as in the example above. \nCode: import numpy as np\nimport pandas as pd\nts = pd.Series(np.random.randn(1000), index=pd.date_range('1\/1\/2000', periods=1000))\nts.plot()\npd.tseries.plotting.pylab.show()\n\nText: But all you are doing there is finding somewhere that matplotlib has been imported in pandas, and calling the same show function from there. \nText: Are you trying to avoid calling matplotlib in an effort to speed things up? If so then you are really not speeding anything up, since pandas already imports pyplot: \nCode: python -mtimeit -s 'import pandas as pd'\n100000000 loops, best of 3: 0.0122 usec per loop\n\npython -mtimeit -s 'import pandas as pd; import matplotlib.pyplot as plt'\n100000000 loops, best of 3: 0.0125 usec per loop\n\nText: Finally, the reason the example you linked in comments doesn't need the call to matplotlib is because it is being run interactively in an iPython notebook, not in a script. \nAPI:\nmatplotlib.pyplot\n","label":[[134,137,"Mention"],[1495,1512,"API"]],"Comments":[]}
{"id":59255,"text":"ID:34382689\nPost:\nText: Your import is wrong, you want plt : \nCode: import matplotlib.pyplot as plt\n\nText: It is also figure not figura plt.figure(). \nAPI:\nmatplotlib.pyplot\n","label":[[55,58,"Mention"],[156,173,"API"]],"Comments":[]}
{"id":59256,"text":"ID:34387561\nPost:\nText: I'm not aware of any built-in method to apply a scaling factor after the exponent, but you could create a custom tick locator and formatter by subclassing LogLocator and matplotlib.ticker.LogFormatter. \nText: Here's a fairly quick-and-dirty hack that does what you're looking for: \nCode: from matplotlib import pyplot as plt\nfrom matplotlib.ticker import LogLocator, LogFormatter, ScalarFormatter, \\\n                              is_close_to_int, nearest_long\nimport numpy as np\nimport math\n\nclass ScaledLogLocator(LogLocator):\n    def __init__(self, *args, scale=10.0, **kwargs):\n        self._scale = scale\n        LogLocator.__init__(self, *args, **kwargs)\n\n    def view_limits(self, vmin, vmax):\n        s = self._scale\n        vmin, vmax = LogLocator.view_limits(self, vmin \/ s, vmax \/ s)\n        return s * vmin, s * vmax\n\n    def tick_values(self, vmin, vmax):\n        s = self._scale\n        locs = LogLocator.tick_values(self, vmin \/ s, vmax \/ s)\n        return s * locs\n\nclass ScaledLogFormatter(LogFormatter):\n    def __init__(self, *args, scale=10.0, **kwargs):\n        self._scale = scale\n        LogFormatter.__init__(self, *args, **kwargs)\n\n    def __call__(self, x, pos=None):\n        b = self._base\n        s = self._scale\n\n        # only label the decades\n        if x == 0:\n            return '$\\mathdefault{0}$'\n\n        fx = math.log(abs(x \/ s)) \/ math.log(b)\n        is_decade = is_close_to_int(fx)\n        sign_string = '-' if x < 0 else ''\n\n        # use string formatting of the base if it is not an integer\n        if b % 1 == 0.0:\n            base = '%d' % b\n        else:\n            base = '%s' % b\n        scale = '%d' % s\n\n        if not is_decade and self.labelOnlyBase:\n            return ''\n        elif not is_decade:\n            return ('$\\mathdefault{%s%s\\times%s^{%.2f}}$'\n                     % (sign_string, scale, base, fx))\n        else:\n            return (r'$%s%s\\times%s^{%d}$'\n                    % (sign_string, scale, base, nearest_long(fx)))\n\nText: For example: \nCode: fig, ax = plt.subplots(1, 1)\nx = np.arange(1000)\ny = np.random.randn(1000)\nax.plot(x, y)\nax.set_xscale('log')\nsubs = np.linspace(0, 1, 10)\n\nmajloc = ScaledLogLocator(scale=10, base=4)\nminloc = ScaledLogLocator(scale=10, base=4, subs=subs)\nfmt = ScaledLogFormatter(scale=10, base=4)\nax.xaxis.set_major_locator(majloc)\nax.xaxis.set_minor_locator(minloc)\nax.xaxis.set_major_formatter(fmt)\nax.grid(True)\n\n# show the same tick locations with non-exponential labels\nax2 = ax.twiny()\nax2.set_xscale('log')\nax2.set_xlim(*ax.get_xlim())\nfmt2 = ScalarFormatter()\nax2.xaxis.set_major_locator(majloc)\nax2.xaxis.set_minor_locator(minloc)\nax2.xaxis.set_major_formatter(fmt2)\n\nAPI:\nmatplotlib.ticker.LogLocator\n","label":[[179,189,"Mention"],[2709,2737,"API"]],"Comments":[]}
{"id":59257,"text":"ID:34387896\nPost:\nText: You want YearLocator and matplotlib.dates.MonthLocator: \nCode: import matplotlib.pyplot as plt\nfrom matplotlib.dates import MonthLocator, YearLocator\nimport numpy as np\n\nfig, ax = plt.subplots(1, 1)\n\nx = np.arange('1965-01', '1975-07', dtype='datetime64[D]')\ny = np.random.randn(x.size).cumsum()\nax.plot(x, y)\n\nyloc = YearLocator()\nmloc = MonthLocator()\nax.xaxis.set_major_locator(yloc)\nax.xaxis.set_minor_locator(mloc)\nax.grid(True)\n\nAPI:\nmatplotlib.dates.YearLocator\n","label":[[33,44,"Mention"],[464,492,"API"]],"Comments":[]}
{"id":59258,"text":"ID:34594308\nPost:\nText: Using FormatStrFormatter solved the problem \nCode: import matplotlib.ticker as mtick\n\ndef generate(data_):\n    data, time = data_\n    img = StringIO.StringIO()\n    sns.set_style(\"darkgrid\")\n    plt.plot(time, data)\n    plt.gca().yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1E')) #this line solves the problem\n    plt.savefig(img, format=format_)\n    img.seek(0)\n    x = base64.b64encode(img.getvalue())\n    return x\n\nText: Using another time and values I got this plot, with the yaxis in scientific notation. \nText: See here the documentation \nText: class matplotlib.ticker.FormatStrFormatter(fmt) Bases: mpl.ticker.Formatter Use an old-style (% operator) format string to format the tick \nAPI:\nmatplotlib.ticker.FormatStrFormatter\nmatplotlib.ticker.Formatter\n","label":[[30,48,"Mention"],[639,659,"Mention"],[731,767,"API"],[768,795,"API"]],"Comments":[]}
{"id":59259,"text":"ID:34650885\nPost:\nText: That's not really how r should be used. It's more global configuration. I also don't think color alone is a valid parameter. \nText: For just a single plot, do this: \nCode: import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot([1, 2, 3], [4, 5, 6], linestyle='-', color='r', linewidth=2)\nfig.savefig('plot_with_red_line.png', dpi=100)\n\nText: Also, don't use the pylab interface. Use pyplot. \nAPI:\nmatplotlib.rc\n","label":[[46,47,"Mention"],[431,444,"API"]],"Comments":[]}
{"id":59260,"text":"ID:34689200\nPost:\nText: I agree with @tcaswell that you should probably just use what you're already using. Another option to use it as a function is to use numpy.vectorize(): \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nfig, ax = plt.subplots(7, len(clusters))\nnp.vectorize(lambda ax:ax.axis('off'))(ax)\n\nText: or, if you need to invoke it multiple times, by assigning the vectorized function to a variable: \nCode: axoff_fun = np.vectorize(lambda ax:ax.axis('off'))\n# ... stuff here ...\nfig, ax = plt.subplots(7, len(clusters))\naxoff_fun(ax)\n\nText: Again, note that this is the same thing that @tcaswell suggested, in a fancier setting (only slower, probably). And it's essentially the same thing you're using now. \nText: However, if you insist on doing it some other way (i.e. you are a special kind of lazy), you can set mpl.rcParams once, and then every subsequent axes will automatically be off. There's probably an easier way to emulate axis('off'), but here's how I've succeeded: \nCode: import matplotlib as mpl\n\n# before\nmpl.pyplot.figure()\nmpl.pyplot.plot([1,3,5],[4,6,5])\n\n# kill axis in rcParams\nmpl.rc('axes.spines',top=False,bottom=False,left=False,right=False);\nmpl.rc('axes',facecolor=(1,1,1,0),edgecolor=(1,1,1,0));\nmpl.rc(('xtick','ytick'),color=(1,1,1,0));\n\n# after\nmpl.pyplot.figure()\nmpl.pyplot.plot([1,3,5],[4,6,5])\n\nText: Result before\/after: \nText: Hopefully there aren't any surprises which I forgot to override, but that would become clear quite quickly in an actual application anyway. \nAPI:\nmatplotlib.rcParams\n","label":[[837,849,"Mention"],[1531,1550,"API"]],"Comments":[]}
{"id":59261,"text":"ID:34905531\nPost:\nText: You can do this with the mpl.patches.Circle patch. \nText: For your example, we need to loop through the X and Y arrays, and then create a circle patch for each coordinate. \nText: Here's an example placing circles on top of an image (from the matplotlib.cbook) \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Circle\n\n# Get an example image\nimport matplotlib.cbook as cbook\nimage_file = cbook.get_sample_data('grace_hopper.png')\nimg = plt.imread(image_file)\n\n# Make some example data\nx = np.random.rand(5)*img.shape[1]\ny = np.random.rand(5)*img.shape[0]\n\n# Create a figure. Equal aspect so circles look circular\nfig,ax = plt.subplots(1)\nax.set_aspect('equal')\n\n# Show the image\nax.imshow(img)\n\n# Now, loop through coord arrays, and create a circle at each x,y pair\nfor xx,yy in zip(x,y):\n    circ = Circle((xx,yy),50)\n    ax.add_patch(circ)\n\n# Show the image\nplt.show()\n\nAPI:\nmatplotlib.patches.Circle\n","label":[[49,67,"Mention"],[939,964,"API"]],"Comments":[]}
{"id":59262,"text":"ID:35168049\nPost:\nText: Like KIDJournety, I don't quite understand what you're wanting either, but if I understand your comments correctly this might work: \nText: def test_plot(x, interval, title): import pyplot as plt plt.title(title) plt.plot(x, interval) plt.show() \nText: test_plot([1,2,3], [7,6,5], \"hello, world!\") works well. If this doesn't do what you need (what with the vectorisation) please comment below and I'll do my best to adjust the code accordingly. \nText: Now for some practical reasons why your original code did not work: You were passing a list or vector with 6 elements (-1 through 4) as the x-values of the graph, but only 2 elements(0 and interval+1, which was 3 in your example) as your y-values. Your poor code could only pair (-1, 0) and (0,3). The rest of the x-coordinates had no corresponding y-coordinates to be paired to, so the code failed. \nAPI:\nmatplotlib.pyplot\n","label":[[205,211,"Mention"],[882,899,"API"]],"Comments":[]}
{"id":59263,"text":"ID:35301995\nPost:\nText: You can easily subclass Normalize for this purpose. Here's an example of a piece-wise normalization class I wrote for a previous SO question: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nclass PiecewiseNorm(Normalize):\n    def __init__(self, levels, clip=False):\n        # input levels\n        self._levels = np.sort(levels)\n        # corresponding normalized values between 0 and 1\n        self._normed = np.linspace(0, 1, len(levels))\n        Normalize.__init__(self, None, None, clip)\n\n    def __call__(self, value, clip=None):\n        # linearly interpolate to get the normalized value\n        return np.ma.masked_array(np.interp(value, self._levels, self._normed))\n\n    def inverse(self, value):\n        return 1.0 - self.__call__(value)\n\nText: For example: \nCode: y, x = np.mgrid[0.0:3.0:100j, 0.0:5.0:100j]\nH = 50.0 * np.exp( -(x**2 + y**2) \/ 4.0 )\nlevels = [0, 1, 2, 3, 6, 9, 20, 50]\n\nH1 = -50.0 * np.exp( -(x**2 + y**2) \/ 4.0 )\nlevels1 = [-50, -20, -9, -6, -3, -2, -1, 0]\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios':(20, 1), 'wspace':0.05})\n\nim0 = ax[0, 0].contourf(x, y, H, levels, cmap='jet', norm=PiecewiseNorm(levels))\ncb0 = fig.colorbar(im0, cax=ax[0, 1])\nim1 = ax[1, 0].contourf(x, y, H1, levels1, cmap='jet', norm=PiecewiseNorm(levels1))\ncb1 = fig.colorbar(im1, cax=ax[1, 1])\n\nplt.show()\n\nAPI:\nmatplotlib.colors.Normalize\n","label":[[48,57,"Mention"],[1402,1429,"API"]],"Comments":[]}
{"id":59264,"text":"ID:35319281\nPost:\nText: This is quite simple using GridSpec \nText: gs=GridSpec(3,3) creates a 3x3 grid to place subplots on \nText: For your top and bottom rows, we just need to index one cell on that 3x3 grid (e.g. gs[0,0] is on the top left). \nText: For the middle row, you need to span two columns, so we use gs[1,0:2] \nCode: import matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\nfig=plt.figure(figsize=(16,12))\n\ngs = GridSpec(3,3)\n\n# Top row\nax1=fig.add_subplot(gs[0,0])\nax2=fig.add_subplot(gs[0,1],sharey=ax1)\nax3=fig.add_subplot(gs[0,2],sharey=ax1)\n\n# Middle row\nax4=fig.add_subplot(gs[1,0:2],sharey=ax1)\n\n# Bottom row\nax5=fig.add_subplot(gs[2,0],sharey=ax1)\nax6=fig.add_subplot(gs[2,1],sharey=ax1)\nax7=fig.add_subplot(gs[2,2],sharey=ax1)\n\nax1.set_ylim(-15,10)\n\nplt.show()\n\nAPI:\nmatplotlib.gridspec.GridSpec\n","label":[[51,59,"Mention"],[804,832,"API"]],"Comments":[]}
{"id":59265,"text":"ID:35320097\nPost:\nText: I found actually TWO workarounds: \nText: I managed to isolate the problem to the colors passed to the stackplot method. Turns out that stackplkt doesn't play nicely with the colors given by matplotlib.cm.rainbow helper. Converting the colors from RGBA to hex solved the problem. \nCode: colors = cm.rainbow(NP.linspace(0, 1, num_cols))\ncolors = [matplotlib.colors.rgb2hex(i) for i in colors]\n\nText: Then I noticed that I was not using the latest version of matplotlib. Upgrading from 1.5.0 to 1.5.1 also solved the issue (regardless of the color format being used). \nText: I hope this helps anyone who encounters the same cryptic problem or cannot upgrade matplotlib. \nAPI:\nmatplotlib.pyplot.stackplot\n","label":[[159,168,"Mention"],[697,724,"API"]],"Comments":[]}
{"id":59266,"text":"ID:35463838\nPost:\nText: From the matplotlib documentation: \nText: pylab is a convenience module that bulk imports pyplot (for plotting) and numpy (for mathematics and working with arrays) in a single name space. Although many examples use pylab, it is no longer recommended. \nText: I would recommend not importing pylab at all, and instead try using \nCode: import matplotlib\nimport matplotlib.pyplot as plt\n\nText: And then prefixing all of your pyplot functions with plt. \nText: I also noticed that you assign the second return value from plt.subplots() to plt. You should rename that variable to something like fft_plot (for fast fourier transform) to avoid naming conflicts with pyplot. \nText: With regards to your other question (about fig, save fig()) you're going to need to drop that first fig because it's not necessary, and you'll call savefig() with plt.savefig() because it is a function in the pyplot module. So that line will look like \nCode: plt.savefig(Output_Location)\n\nText: Try something like this: \nCode: def Time_Domain_Plot(Directory,Title,X_Label,Y_Label,X_Data,Y_Data):\n    # Directory: The path length to the directory where the output file is \n    #            to be stored\n    # Title:     The name of the output plot, which should end with .eps or .png\n    # X_Label:   The X axis label\n    # Y_Label:   The Y axis label\n    # X_Data:    X axis data points (usually time at which Yaxis data was acquired\n    # Y_Data:    Y axis data points, usually amplitude\n\n    import matplotlib\n    from matplotlib import rcParams, pyplot as plt\n\n    rcParams.update({'figure.autolayout': True})\n    Output_Location = Directory.rstrip() + Title.rstrip()\n    fig,fft_plot = plt.subplots()\n    matplotlib.rc('xtick',labelsize=18)\n    matplotlib.rc('ytick',labelsize=18)\n    fft_plot.set_xlabel(X_Label,fontsize=18)\n    fft_plot.set_ylabel(Y_Label,fontsize=18)\n    plt.plot(X_Data,Y_Data,color='red')\n    plt.savefig(Output_Location)\n    plt.close()\n\nAPI:\nmatplotlib.pyplot\n","label":[[114,120,"Mention"],[1966,1983,"API"]],"Comments":[]}
{"id":59267,"text":"ID:35881382\nPost:\nText: As of matplotlib 3.6.0, width_ratios and height_ratios can now be passed directly as keyword arguments to plt.subplots and subplot_mosaic, as per What's new in Matplotlib 3.6.0 (Sep 15, 2022). \nText: f, (a0, a1) = plt.subplots(1, 2, width_ratios=[3, 1]) \nText: f, (a0, a1, a2) = plt.subplots(3, 1, height_ratios=[1, 1, 3]) \nText: Another way is to use the subplots function and pass the width ratio with gridspec_kw matplotlib Tutorial: Customizing Figure Layouts Using GridSpec and Other Functions GridSpec has available gridspect_kw options \nCode: import numpy as np\nimport matplotlib.pyplot as plt \n\n# generate some data\nx = np.arange(0, 10, 0.2)\ny = np.sin(x)\n\n# plot it\nf, (a0, a1) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [3, 1]})\na0.plot(x, y)\na1.plot(y, x)\n\nf.tight_layout()\nf.savefig('grid_figure.pdf')\n\nText: Because the question is canonical, here is an example with vertical subplots. \nCode: # plot it\nf, (a0, a1, a2) = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1, 1, 3]})\n\na0.plot(x, y)\na1.plot(x, y)\na2.plot(x, y)\n\nf.tight_layout()\n\nAPI:\nmatplotlib.gridspec.GridSpec\n","label":[[523,531,"Mention"],[1093,1121,"API"]],"Comments":[]}
{"id":59268,"text":"ID:35920338\nPost:\nText: Let me start by creating something similar to your plot with some example data: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(11)\ny = - x**2\nplt.bar(x, y)\n\nText: This results in the following image: \nText: Now you can use the bottom parameter of plt.bar to transform the image to be like the desired one: \nCode: plt.bar(x, 100 + y, bottom = -100)\n# or, more general:\n# plt.bar(x, -m + y, bottom = m)\n# where m is the minimum value of your value array, m = np.min(y)\n\nText: Tada: \nAPI:\nmatplotlib.pyplot.bar\n","label":[[297,304,"Mention"],[536,557,"API"]],"Comments":[]}
{"id":59269,"text":"ID:36148001\nPost:\nText: It appears that you have imported mpl.pyplot as plt to obtain plt.scatter in your code. You can just use the matplotlib functions to plot the line: \nCode: import seaborn as sns\nimport matplotlib.pyplot as plt\n\niris = sns.load_dataset(\"iris\")    \ngrid = sns.JointGrid(iris.petal_length, iris.petal_width, space=0, size=6, ratio=50)\ngrid.plot_joint(plt.scatter, color=\"g\")\nplt.plot([0, 4], [1.5, 0], linewidth=2)\n\nAPI:\nmatplotlib.pyplot\n","label":[[58,68,"Mention"],[441,458,"API"]],"Comments":[]}
{"id":59270,"text":"ID:36271049\nPost:\nText: You could try checking if you are running into the situation (empty $DISPLAY) and default users to the Agg backend, rather than asking them to do so. \nCode: def configure_matplotlib():\n   import os\n   if 'DISPLAY' not in os.environ:\n      if matplotlib.get_backend() != 'Agg':\n          matplotlib.use('Agg')\n\n\nimport matplotlib\nconfigure_matplotlib()\nimport matplotlib.pyplot as plt\n...\n\nText: Now mpl.use has no effect once pyplot has been imported, but if your users have already done so, they probably have a working configuration anyway. \nAPI:\nmatplotlib.use\n","label":[[423,430,"Mention"],[573,587,"API"]],"Comments":[]}
{"id":59271,"text":"ID:36331317\nPost:\nText: The boxes made using sns.boxplot are really just PathPatch objects. These are stored in ax.artists as a list. \nText: So, we can select one box in particular by indexing ax.artists. Then, you can set the facecolor, edgecolor and linewidth, among many other properties. \nText: For example (based on one of the examples here): \nCode: import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\ntips = sns.load_dataset(\"tips\")\nax = sns.boxplot(x=\"day\", y=\"total_bill\", hue=\"smoker\",\n                 data=tips, palette=\"Set3\")\n\n# Select which box you want to change    \nmybox = ax.artists[2]\n\n# Change the appearance of that box\nmybox.set_facecolor('red')\nmybox.set_edgecolor('black')\nmybox.set_linewidth(3)\n\nplt.show()\n\nAPI:\nmatplotlib.patches.PathPatch\n","label":[[73,82,"Mention"],[768,796,"API"]],"Comments":[]}
{"id":59272,"text":"ID:36425926\nPost:\nText: Something like this. \nText: You can use Rectangle for the dashed box. \nText: I've also moved the spines outwards to match the style of your plot (code taken from this example) \nCode: import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport numpy as np\nimport matplotlib.ticker as ticker\n\n# Fake some data\nx = np.array([15,25,35,45,45,45,45,45,75,75,95,150,160,170,170,1040])\ny = np.arange(0.1,16.1,1)\npercent = np.array([(100.*float(i)\/x.sum()) for i in x])\n\n# Create Figure and Axes\nfig,ax = plt.subplots(1)\n\n# Plot the bars\nax.barh(y,x)\n\n# Move left and bottom spines outward by 5 points\nax.spines['left'].set_position(('outward', 5))\nax.spines['bottom'].set_position(('outward', 5))\n# Hide the right and top spines\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n# Only show ticks on the left and bottom spines\nax.yaxis.set_ticks_position('left')\nax.xaxis.set_ticks_position('bottom')\n\n# Set the axes limits and tick locations\nax.set_ylim(0,16)\nax.set_yticklabels([])\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\nax.set_xlim(0,1100)\nax.xaxis.set_major_locator(ticker.MultipleLocator(100))\n\n# Add the rectangle\nrect = Rectangle( (0,10), 1100, 6, linestyle = 'dashed', facecolor = 'None', clip_on=False)\nax.add_patch(rect)\n\n# Add the percentage labels\nfor p,xi,yi in zip(percent,x,y):\n    ax.text(xi+5,yi+0.2,'{:2.0f}\\%'.format(p))\n\nplt.show()\n\nAPI:\nmatplotlib.patches.Rectangle\n","label":[[64,73,"Mention"],[1434,1462,"API"]],"Comments":[]}
{"id":59273,"text":"ID:36432617\nPost:\nText: There is a (somewhat) related question on StackOverflow: \nText: Showing an image with pylab.imshow() \nText: Here the problem was that an array of shape (nx,ny,1) is still considered a 3D array, and must be squeezed or sliced into a 2D array. \nText: More generally, the reason for the Exception \nText: TypeError: Invalid dimensions for image data \nText: is shown here: matplotlib.pyplot.imshow() needs a 2D array, or a 3D array with the third dimension being of shape 3 or 4! \nText: You can easily check this with (these checks are done by imshow, this function is only meant to give a more specific message in case it's not a valid input): \nCode: from __future__ import print_function\nimport numpy as np\n\ndef valid_imshow_data(data):\n    data = np.asarray(data)\n    if data.ndim == 2:\n        return True\n    elif data.ndim == 3:\n        if 3 <= data.shape[2] <= 4:\n            return True\n        else:\n            print('The \"data\" has 3 dimensions but the last dimension '\n                  'must have a length of 3 (RGB) or 4 (RGBA), not \"{}\".'\n                  ''.format(data.shape[2]))\n            return False\n    else:\n        print('To visualize an image the data must be 2 dimensional or '\n              '3 dimensional, not \"{}\".'\n              ''.format(data.ndim))\n        return False\n\nText: In your case: \nCode: >>> new_SN_map = np.array([1,2,3])\n>>> valid_imshow_data(new_SN_map)\nTo visualize an image the data must be 2 dimensional or 3 dimensional, not \"1\".\nFalse\n\nText: The np.asarray is what is done internally by mpl.pyplot.imshow so it's generally best you do it too. If you have a numpy array it's obsolete but if not (for example a list) it's necessary. \nText: In your specific case you got a 1D array, so you need to add a dimension with np.expand_dims() \nCode: import matplotlib.pyplot as plt\na = np.array([1,2,3,4,5])\na = np.expand_dims(a, axis=0)  # or axis=1\nplt.imshow(a)\nplt.show()\n\nText: or just use something that accepts 1D arrays like plot: \nCode: a = np.array([1,2,3,4,5])\nplt.plot(a)\nplt.show()\n\nAPI:\nmatplotlib.pyplot.imshow\n","label":[[1558,1575,"Mention"],[2062,2086,"API"]],"Comments":[]}
{"id":59274,"text":"ID:36517295\nPost:\nText: Answering partially to your question, plt.imshow is much quicker than plt.pcolor because they are not doing similar operations. In fact, they do very different things. \nText: According to the documentation, pcolor returns a matplotlib.collections.PolyCollection, which can be slow compared to pcolormesh, which returns a mpl.collections.QuadMesh object. imshow, in the other hand, returns a AxesImage object. I did a test with pcolor, imshow and pcolormesh: \nCode: def image_gradient(m,n):\n    \"\"\"\n    Create image arrays\n    \"\"\"\n    A_m = np.arange(m)[:, None]\n    A_n = np.arange(n)[None, :]\n    return(A_m.astype(np.float)+A_n.astype(np.float)) \n\nm = 100\nn = 100\n\nA_100x100 = image_gradient(m,n)\n\n%time plt.imshow(A_100x100)\n%time plt.pcolor(A_100x100)\n%time plt.pcolormesh(A_100x100)\n\nText: and the results I get are: \nCode: imshow()\nCPU times: user 76 ms, sys: 0 ns, total: 76 ms\nWall time: 74.4 ms\npcolor()\nCPU times: user 588 ms, sys: 16 ms, total: 604 ms\nWall time: 601 ms\npcolormesh()\nCPU times: user 0 ns, sys: 4 ms, total: 4 ms\nWall time: 2.32 ms\n\nText: Clearly, for this particular example, pcolormesh is the most efficient one. \nAPI:\nmatplotlib.pyplot.pcolor\nmatplotlib.collections.QuadMesh\nmatplotlib.image.AxesImage\n","label":[[231,237,"Mention"],[345,369,"Mention"],[415,424,"Mention"],[1171,1195,"API"],[1196,1227,"API"],[1228,1254,"API"]],"Comments":[]}
{"id":59275,"text":"ID:36665099\nPost:\nText: I don't know of any way to import all the functions from every submodule. Importing all the functions from a submodule is possible the way you suggested with e.g. from pyplot import *. \nText: Be noted of a potential problem with importing every function; you may override imported functions by defining your own functions with the same name. E.g: \nCode: from matplotlib.pyplot import *\n\ndef plot():\n    print \"Hello!\"\n\nplot()\n\nText: would output \nCode: Hello!\n\nAPI:\nmatplotlib.pyplot\n","label":[[192,198,"Mention"],[490,507,"API"]],"Comments":[]}
{"id":59276,"text":"ID:36669876\nPost:\nText: I think I found the solution. All credit goes to Mr. Harrison who made the Python tutorial website https:\/\/pythonprogramming.net. He helped me out. \nText: So here is what I did. Two major changes: \nText: 1. Structural change \nText: I previously had two classes: CustomGraph(TimedAnimation) and CustomFigCanvas(FigureCanvas). Now I got only one left, but he inherits from both TimedAnimation and FigureCanvas: CustomFigCanvas(TimedAnimation, FigureCanvas) \nText: 2. Change in making the figure object \nText: This is how I made the figure previously: \nCode: self.fig = plt.figure()\n\nText: With 'plt' coming from the import statement 'import plt as plt'. This way of making the figure apparently causes troubles when you want to embed it into your own GUI. So there is a better way to do it: \nCode: self.fig = Figure(figsize=(5,5), dpi=100)\n\nText: And now it works! \nText: Here is the complete code: \nCode: import numpy as np\nfrom matplotlib.figure import Figure\nfrom matplotlib.animation import TimedAnimation\nfrom matplotlib.lines import Line2D\n\nfrom matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas\n\n\n    class CustomFigCanvas(FigureCanvas, TimedAnimation):\n\n        def __init__(self):\n\n            # The data\n            self.n = np.linspace(0, 1000, 1001)\n            self.y = 1.5 + np.sin(self.n\/20)\n\n            # The window\n            self.fig = Figure(figsize=(5,5), dpi=100)\n            ax1 = self.fig.add_subplot(111)\n\n            # ax1 settings\n            ax1.set_xlabel('time')\n            ax1.set_ylabel('raw data')\n            self.line1 = Line2D([], [], color='blue')\n            ax1.add_line(self.line1)\n            ax1.set_xlim(0, 1000)\n            ax1.set_ylim(0, 4)\n\n            FigureCanvas.__init__(self, self.fig)\n            TimedAnimation.__init__(self, self.fig, interval = 20, blit = True)\n\n\n        def _draw_frame(self, framedata):\n            i = framedata\n            print(i)\n\n\n            self.line1.set_data(self.n[ 0 : i ], self.y[ 0 : i ])\n            self._drawn_artists = [self.line1]\n\n        def new_frame_seq(self):\n            return iter(range(self.n.size))\n\n        def _init_draw(self):\n            lines = [self.line1]\n            for l in lines:\n                l.set_data([], [])\n\n\n    ''' End Class '''\n\nText: That's the code to make the animation in matplotlib. Now you can easily embed it into your own Qt GUI: \nCode:     ..\n    myFigCanvas = CustomFigCanvas()\n    self.myLayout.addWidget(myFigCanvas)\n    ..\n\nText: It seems to work pretty fine. Thank you Mr. Harrison! \nText: EDIT : I came back to this question after many months. Here is the complete code. Just copy-paste it into a fresh .py file, and run it: \nCode: ###################################################################\n#                                                                 #\n#                     PLOTTING A LIVE GRAPH                       #\n#                  ----------------------------                   #\n#            EMBED A MATPLOTLIB ANIMATION INSIDE YOUR             #\n#            OWN GUI!                                             #\n#                                                                 #\n###################################################################\n\n\nimport sys\nimport os\nfrom PyQt4 import QtGui\nfrom PyQt4 import QtCore\nimport functools\nimport numpy as np\nimport random as rd\nimport matplotlib\nmatplotlib.use(\"Qt4Agg\")\nfrom matplotlib.figure import Figure\nfrom matplotlib.animation import TimedAnimation\nfrom matplotlib.lines import Line2D\nfrom matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas\nimport time\nimport threading\n\n\n\ndef setCustomSize(x, width, height):\n    sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Fixed)\n    sizePolicy.setHorizontalStretch(0)\n    sizePolicy.setVerticalStretch(0)\n    sizePolicy.setHeightForWidth(x.sizePolicy().hasHeightForWidth())\n    x.setSizePolicy(sizePolicy)\n    x.setMinimumSize(QtCore.QSize(width, height))\n    x.setMaximumSize(QtCore.QSize(width, height))\n\n''''''\n\nclass CustomMainWindow(QtGui.QMainWindow):\n\n    def __init__(self):\n\n        super(CustomMainWindow, self).__init__()\n\n        # Define the geometry of the main window\n        self.setGeometry(300, 300, 800, 400)\n        self.setWindowTitle(\"my first window\")\n\n        # Create FRAME_A\n        self.FRAME_A = QtGui.QFrame(self)\n        self.FRAME_A.setStyleSheet(\"QWidget { background-color: %s }\" % QtGui.QColor(210,210,235,255).name())\n        self.LAYOUT_A = QtGui.QGridLayout()\n        self.FRAME_A.setLayout(self.LAYOUT_A)\n        self.setCentralWidget(self.FRAME_A)\n\n        # Place the zoom button\n        self.zoomBtn = QtGui.QPushButton(text = 'zoom')\n        setCustomSize(self.zoomBtn, 100, 50)\n        self.zoomBtn.clicked.connect(self.zoomBtnAction)\n        self.LAYOUT_A.addWidget(self.zoomBtn, *(0,0))\n\n        # Place the matplotlib figure\n        self.myFig = CustomFigCanvas()\n        self.LAYOUT_A.addWidget(self.myFig, *(0,1))\n\n        # Add the callbackfunc to ..\n        myDataLoop = threading.Thread(name = 'myDataLoop', target = dataSendLoop, daemon = True, args = (self.addData_callbackFunc,))\n        myDataLoop.start()\n\n        self.show()\n\n    ''''''\n\n\n    def zoomBtnAction(self):\n        print(\"zoom in\")\n        self.myFig.zoomIn(5)\n\n    ''''''\n\n    def addData_callbackFunc(self, value):\n        # print(\"Add data: \" + str(value))\n        self.myFig.addData(value)\n\n\n\n''' End Class '''\n\n\nclass CustomFigCanvas(FigureCanvas, TimedAnimation):\n\n    def __init__(self):\n\n        self.addedData = []\n        print(matplotlib.__version__)\n\n        # The data\n        self.xlim = 200\n        self.n = np.linspace(0, self.xlim - 1, self.xlim)\n        a = []\n        b = []\n        a.append(2.0)\n        a.append(4.0)\n        a.append(2.0)\n        b.append(4.0)\n        b.append(3.0)\n        b.append(4.0)\n        self.y = (self.n * 0.0) + 50\n\n        # The window\n        self.fig = Figure(figsize=(5,5), dpi=100)\n        self.ax1 = self.fig.add_subplot(111)\n\n\n        # self.ax1 settings\n        self.ax1.set_xlabel('time')\n        self.ax1.set_ylabel('raw data')\n        self.line1 = Line2D([], [], color='blue')\n        self.line1_tail = Line2D([], [], color='red', linewidth=2)\n        self.line1_head = Line2D([], [], color='red', marker='o', markeredgecolor='r')\n        self.ax1.add_line(self.line1)\n        self.ax1.add_line(self.line1_tail)\n        self.ax1.add_line(self.line1_head)\n        self.ax1.set_xlim(0, self.xlim - 1)\n        self.ax1.set_ylim(0, 100)\n\n\n        FigureCanvas.__init__(self, self.fig)\n        TimedAnimation.__init__(self, self.fig, interval = 50, blit = True)\n\n    def new_frame_seq(self):\n        return iter(range(self.n.size))\n\n    def _init_draw(self):\n        lines = [self.line1, self.line1_tail, self.line1_head]\n        for l in lines:\n            l.set_data([], [])\n\n    def addData(self, value):\n        self.addedData.append(value)\n\n    def zoomIn(self, value):\n        bottom = self.ax1.get_ylim()[0]\n        top = self.ax1.get_ylim()[1]\n        bottom += value\n        top -= value\n        self.ax1.set_ylim(bottom,top)\n        self.draw()\n\n\n    def _step(self, *args):\n        # Extends the _step() method for the TimedAnimation class.\n        try:\n            TimedAnimation._step(self, *args)\n        except Exception as e:\n            self.abc += 1\n            print(str(self.abc))\n            TimedAnimation._stop(self)\n            pass\n\n    def _draw_frame(self, framedata):\n        margin = 2\n        while(len(self.addedData) > 0):\n            self.y = np.roll(self.y, -1)\n            self.y[-1] = self.addedData[0]\n            del(self.addedData[0])\n\n\n        self.line1.set_data(self.n[ 0 : self.n.size - margin ], self.y[ 0 : self.n.size - margin ])\n        self.line1_tail.set_data(np.append(self.n[-10:-1 - margin], self.n[-1 - margin]), np.append(self.y[-10:-1 - margin], self.y[-1 - margin]))\n        self.line1_head.set_data(self.n[-1 - margin], self.y[-1 - margin])\n        self._drawn_artists = [self.line1, self.line1_tail, self.line1_head]\n\n\n\n''' End Class '''\n\n\n# You need to setup a signal slot mechanism, to \n# send data to your GUI in a thread-safe way.\n# Believe me, if you don't do this right, things\n# go very very wrong..\nclass Communicate(QtCore.QObject):\n    data_signal = QtCore.pyqtSignal(float)\n\n''' End Class '''\n\n\n\ndef dataSendLoop(addData_callbackFunc):\n    # Setup the signal-slot mechanism.\n    mySrc = Communicate()\n    mySrc.data_signal.connect(addData_callbackFunc)\n\n    # Simulate some data\n    n = np.linspace(0, 499, 500)\n    y = 50 + 25*(np.sin(n \/ 8.3)) + 10*(np.sin(n \/ 7.5)) - 5*(np.sin(n \/ 1.5))\n    i = 0\n\n    while(True):\n        if(i > 499):\n            i = 0\n        time.sleep(0.1)\n        mySrc.data_signal.emit(y[i]) # <- Here you emit a signal!\n        i += 1\n    ###\n###\n\n\n\n\nif __name__== '__main__':\n    app = QtGui.QApplication(sys.argv)\n    QtGui.QApplication.setStyle(QtGui.QStyleFactory.create('Plastique'))\n    myGUI = CustomMainWindow()\n\n\n    sys.exit(app.exec_())\n\n''''''\n\nAPI:\nmatplotlib.pyplot\n","label":[[663,666,"Mention"],[9095,9112,"API"]],"Comments":[]}
{"id":59277,"text":"ID:36937155\nPost:\nText: Assuming you have defined your plot and axes as below: \nCode: import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\n\nText: If you want to remove the x axis tick marks you can do: \nCode: ax.tick_params(axis='x', top='off', bottom='off')\n\nText: If you want to change the direction of the tick marks you can do: \nCode: ax.tick_params(axis='x', direction='out')\n\nText: If you want to change the x axis labels then use: \nCode: set_xticklabels()\n\nText: You have to pass a list of labels to use, although I'm not sure why your labels aren't evenly spaced. The documentation at the link below should help: \nText: mpl.axes documentation \nAPI:\nmatplotlib.axes\n","label":[[632,640,"Mention"],[661,676,"API"]],"Comments":[]}
{"id":59278,"text":"ID:36966837\nPost:\nText: Connected lines are not a good visualization here. You essentially connect random points on the circle. Since you do this quite often, you will get a filled circle. Try drawing points instead. \nText: Also avoid name space mangling. You import pyplot as plot and also name your function plot. This will lead to name conflicts. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot():\n\n    xy = np.random.normal(0,1,(2000,2))\n\n    for i in range(2000):\n        s=np.linalg.norm(xy[i,])\n        xy[i,]=xy[i,]\/s\n\n    fig, ax = plt.subplots(figsize=(5,5))\n    # scatter draws dots instead of lines\n    ax.scatter(xy[:,0], xy[:,1])\n\nText: If you use dots instead, you will see that your points indeed lie on the unit circle. \nAPI:\nmatplotlib.pyplot\n","label":[[267,273,"Mention"],[763,780,"API"]],"Comments":[]}
{"id":59279,"text":"ID:37116962\nPost:\nText: If you use mpl.pyplot you can plot them as a scatter plot and get a rough approximation of a sine wave. \nText: For example using your x and y arrays: \nCode: import matplotlib.pyplot as plt\nimport numpy as np\n\nX = np.random.rand(1000)*2\nY = np.sin(X)\nplt.scatter(X,Y)\nplt.show()\n\nText: You'll end up with something like this: \nText: EDIT Per your comment I ran this for you: \nCode: sortx = np.argsort(X)\nplt.plot(X[sortx], Y[sortx], marker='.')\nplt.show()\n\nText: You'll get this approximation: \nText: You have to pass the argsort indices into X so that it plots them in the correct order and do the same for Y that way you get a nice smooth line (at least for where your data is most dense). \nText: If you want a smooth line representation of the sine wave then you could just say: \nCode: z = np.arange(-6.28, 6.28, 0.1)\nplt.plot(z, np.sin(z))\nplt.show()\n\nText: You'll end up with this: \nText: Obviously, you may want to adjust the x axis ticks to represent values of pi for a better representation, but i think you get the idea. \nAPI:\nmatplotlib.pyplot\n","label":[[35,45,"Mention"],[1059,1076,"API"]],"Comments":[]}
{"id":59280,"text":"ID:37152448\nPost:\nText: You can locate your ticks anywhere you want using mticker.Locator classes. Specifically in your case I guess you'd like to use MultipleLocator. Just add in your program \nCode: from matplotlib.ticker import MultipleLocator\nax = plt.gca()\nax.get_xaxis().set_major_locator(MultipleLocator(base=5))\n\nText: and you'll be all set. \nText: UPDATE: \nText: To get the base, you can check the default AutoLocator tick positions (after the call to plt.plot) and get the difference between any of them lying next to each other: \nCode: ticks = ax.get_xticks()\nbase = ticks[1] - ticks[0]\n\nAPI:\nmatplotlib.ticker.Locator\n","label":[[74,89,"Mention"],[603,628,"API"]],"Comments":[]}
{"id":59281,"text":"ID:37162866\nPost:\nText: Ok, when converting the dates with date2num before passing them to the locator things work. \nAPI:\nmatplotlib.dates.date2num\n","label":[[59,67,"Mention"],[122,147,"API"]],"Comments":[]}
{"id":59282,"text":"ID:37288742\nPost:\nText: You have a couple of options here: \nText: Export to image or PDF. Information found here: http:\/\/matplotlib.org\/faq\/howto_faq.html The key piece of information here is below: # do this before importing pylab or pyplot import matplotlib matplotlib.use('Agg') import ppylot as plt fig = plt.figure() ax = fig.add_subplot(111) ax.plot([1,2,3]) fig.savefig('test.png') If your server supports X11 forwarding (or if you can enable\/install X11 forwarding), you can SSH into the server by setting your display. From linux, run: DISPLAY=:0.0 ssh -Y <server ip> This will set up your machine to forward any display output from the server to your PC. If you are running Windows, you can use MobaXterm which makes it easy, or configure an X11 client yourself. Mac is similarly easy if I remember correctly. \nAPI:\nmatplotlib.pyplot\n","label":[[289,295,"Mention"],[826,843,"API"]],"Comments":[]}
{"id":59283,"text":"ID:37304778\nPost:\nText: Use FuncFormatter (see docs): \nCode: plt.gca().get_xaxis().set_major_formatter(FuncFormatter(lambda x, p: format(int(x\/1000), ',')))\nplt.show()\n\nAPI:\nmatplotlib.ticker.FuncFormatter\n","label":[[28,41,"Mention"],[174,205,"API"]],"Comments":[]}
{"id":59284,"text":"ID:37408652\nPost:\nText: After a big of digging, I realized the problem was probably the old version of matplotlib I was using (installed using apt-get install). I tried removing it and installing it using pip and it worked: \nCode: sudo apt-get remove python3-matplotlib\nsudo pip3 install matplotlib\n\nText: After that, I got a warning the first time I imported pyplot (about building font cache). Now everything works fine. \nText: The installed version of matplotlib using pip3 is 1.5.1 (from 1.3.1 using apt). \nAPI:\nmatplotlib.pyplot\n","label":[[360,366,"Mention"],[516,533,"API"]],"Comments":[]}
{"id":59285,"text":"ID:37413728\nPost:\nText: The images in the Python CIFAR10 dataset have pixel values of type numpy.uint8. (Presumably they are read from PNG files or something of the kind.) So X_train.dtype == numpy.uint8 and hence im.dtype == numpy.uint8. \nText: The array you create has the default element type of numpy.float64. In other words, imC.dtype == numpy.uint8. \nText: It happens that plt.imshow treats its input differently depending on its element type. In particular, if you give it an m-by-n-by-3 array of element type uint8 it will take 0 to mean darkest and 255 to mean lightest for each of the three colour channels, as you would expect; if you give it an m-by-n-by-3 array of element type float64, though, it wants all the values to be in the range 0 (darkest) to 1 (lightest), and the documentation says nothing about what will happen to values outside that range. \nText: I will hazard a guess at what does happen to values outside that range: I think the code probably does something like: multiply by 255, round to integer, treat as uint8. This means at 0 becomes 0 and 1 becomes 255. \nText: But if that last step means throwing away all but the low 8 bits, it also means that 2 becomes 254, 3 becomes 253, ..., 255 becomes 1! In other words, if you make the very understandable mistake of giving imshow an image whose pixel values are floats in the range 0..255, those values will effectively be negated so that 0->0, 1->255, 2->254,...,255->1. (This isn't quite the same as turning the range exactly upside down, because 0 is preserved.) \nText: And this is what's happened to you: each element of imC is numerically equal to the corresponding element of im, but because imC is a float array rather than an unsigned-small-integer array it gets the treatment described above, and you get almost a photographic negative of the image you expected. \nAPI:\nmatplotlib.pyplot.imshow\n","label":[[379,389,"Mention"],[1857,1881,"API"]],"Comments":[]}
{"id":59286,"text":"ID:37440535\nPost:\nText: You can hack around a bit to find them, but its not pretty. \nText: sns.boxplot returns the matplotlib Axes instance the boxes are drawn on. \nText: The boxes are created as PathPatch instances. \nText: We can find those instances like so: \nCode: import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntips = sns.load_dataset(\"tips\")\n\nax = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n\nfor c in ax.get_children():\n    if type(c) == matplotlib.patches.PathPatch:\n        print(c.get_extents())\n\nText: This will print the BBox of the boxes, in this example: \nCode: Bbox(x0=92.4, y0=116.996, x1=191.6, y1=162.242666667)\nBbox(x0=216.4, y0=114.957333333, x1=315.6, y1=171.6)\nBbox(x0=340.4, y0=125.576, x1=439.6, y1=189.141333333)\nBbox(x0=464.4, y0=131.926666667, x1=563.6, y1=194.172)\n\nAPI:\nmatplotlib.patches.PathPatch\n","label":[[196,205,"Mention"],[828,856,"API"]],"Comments":[]}
{"id":59287,"text":"ID:37561377\nPost:\nText: You want to use matplotlib's autoscale method from the Axes class. \nText: Using the functional API, you apply a tight x axis using \nCode: plt.autoscale(enable=True, axis='x', tight=True)\n\nText: or if you are using the object oriented API you would use \nCode: ax = plt.gca()  # only to illustrate what `ax` is\nax.autoscale(enable=True, axis='x', tight=True)\n\nText: For completeness, the axis kwarg can take 'x', 'y', or 'both', where the default is 'both'. \nAPI:\nmatplotlib.axes.Axes\n","label":[[79,83,"Mention"],[486,506,"API"]],"Comments":[]}
{"id":59288,"text":"ID:37641355\nPost:\nText: First, you forgot to import Axes3D: import pyplot as plt. \nText: Second, you code has strange meaningless line: values[50*50*nb:50*50*(nb+1)] and indentation is wrong in your code. \nText: Third, to plot regularly gridded data you do not need plot_trisurf, use plot_surface: \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx=np.linspace(0,50,50)\ny=np.linspace(0,50,50)\nX, Y = np.meshgrid(x, y)\nvalues = np.sqrt(X**2 + Y**2)\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.plot_surface(x, y, values)\nplt.show()\n\nText: Look for examples: http:\/\/matplotlib.org\/examples\/mplot3d\/surface3d_demo.html \nAPI:\nmatplotlib.pyplot\n","label":[[67,73,"Mention"],[680,697,"API"]],"Comments":[]}
{"id":59289,"text":"ID:37799021\nPost:\nText: pyplot has the concept of the current figure and the current axes. All plotting commands apply to the current axes. \nCode: import matplotlib.pyplot as plt\n\nfig, axarr = plt.subplots(2, 3)     # 6 axes, returned as a 2-d array\n\n#1 The first subplot\nplt.sca(axarr[0, 0])                # set the current axes instance to the top left\n# plot your data\nresult.plot(kind='bar', alpha=0.75, rot=0, label=\"Presence \/ Absence of cultural centre\")\n\n#2 The second subplot\nplt.sca(axarr[0, 1])                # set the current axes instance \n# plot your data\n\n#3 The third subplot\nplt.sca(axarr[0, 2])                # set the current axes instance \n# plot your data\n\nText: Demo: \nText: The source code, \nCode: import matplotlib.pyplot as plt\nfig, axarr = plt.subplots(2, 3, sharex=True, sharey=True)     # 6 axes, returned as a 2-d array\n\nfor i in range(2):\n    for j in range(3):\n        plt.sca(axarr[i, j])                        # set the current axes instance \n        axarr[i, j].plot(i, j, 'ro', markersize=10) # plot \n        axarr[i, j].set_xlabel(str(tuple([i, j])))  # set x label\n        axarr[i, j].get_xaxis().set_ticks([])       # hidden x axis text\n        axarr[i, j].get_yaxis().set_ticks([])       # hidden y axis text\n\nplt.show()\n\nAPI:\nmatplotlib.pyplot\n","label":[[24,30,"Mention"],[1270,1287,"API"]],"Comments":[]}
{"id":59290,"text":"ID:37986810\nPost:\nText: Use a mdates.DateFormatter to specify the date format: \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ns1 = {'Timestamp':['20160208_095900.51','20160208_095901.51','20160208_095902.51','20160208_095903.51',\n                 '20160208_095904.51','20160208_095905.51','20160208_095906.51','20160208_095907.51',\n                 '20160208_095908.51','20160208_095909.51'],\n      'Data' : [2300,2500,2600,2700,2800,2900,3000,3100,3200,3300]}\ndf = pd.DataFrame(s1)\ndf['Date'] = pd.to_datetime(df['Timestamp'], format = '%Y%m%d_%H%M%S.%f')\n\nfig, ax = plt.subplots(figsize=(8,6))\nxfmt = mdates.DateFormatter('%H:%M:%S')\nax.xaxis.set_major_formatter(xfmt)\n# automatically rotates the tick labels\nfig.autofmt_xdate()\n\nax.plot(df['Date'], df['Data'])\nplt.show()\n\nAPI:\nmatplotlib.dates.DateFormatter\n","label":[[30,50,"Mention"],[828,858,"API"]],"Comments":[]}
{"id":59291,"text":"ID:38154281\nPost:\nText: Use plt.xlim (or plt.ylim) \nText: import pyplot as plt import numpy as np import numpy.random as rnd \nCode: def f(t):\n    s1 = np.sin(2 * np.pi * t)\n    e1 = np.exp(-t)\n    return np.absolute((s1 * e1)) + .05\n\nt = np.arange(0.0, 5.0, 0.1)\ns = f(t)\nnse = rnd.normal(0.0, 0.3, t.shape) * s\n\nfig = plt.figure(figsize=(12, 6))\nvax = fig.add_subplot(121)\nvax.vlines(t, [0], s)\n\nplt.xlim(-0.2,5.2)\n\nplt.show()\n\nAPI:\nmatplotlib.pyplot\n","label":[[65,71,"Mention"],[434,451,"API"]],"Comments":[]}
{"id":59292,"text":"ID:38156663\nPost:\nText: You have to use functions from dates module: \nCode: import pandas as pd\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pylab as plt\nimport matplotlib.dates as mdates\n\ntimeInd = pd.date_range(start = datetime(2016,4,17,23,0,0), \n end = datetime(2016,4,20,1,0,0), freq = 'H')\nd = {'data1': np.random.randn(len(timeInd)), 'data2': np.random.randn(len(timeInd)),\n 'data3': np.random.randn(len(timeInd))}\ndf = pd.DataFrame(data = d, index = timeInd)  \n\nplt.style.use('ggplot')\ndf.plot(subplots=True, grid=True, x_compat=True)\nax = plt.gca()\n# set major ticks location every day\nax.xaxis.set_major_locator(mdates.DayLocator())\n# set major ticks format\nax.xaxis.set_major_formatter(mdates.DateFormatter('\\n\\n\\n%d.%m.%Y'))\n# set minor ticks location every two hours\nax.xaxis.set_minor_locator(mdates.HourLocator(interval=2))\n# set minor ticks format\nax.xaxis.set_minor_formatter(mdates.DateFormatter('%H:%M:%S'))\n\n# or just set together date and time for major ticks like\n# ax.xaxis.set_major_formatter(mdates.DateFormatter('%d.%m.%Y %H:%M:%S'))\n\nplt.show()    \n\nText: More examples here: http:\/\/matplotlib.org\/examples\/api\/date_demo.html \nAPI:\nmatplotlib.dates\n","label":[[55,60,"Mention"],[1181,1197,"API"]],"Comments":[]}
{"id":59293,"text":"ID:38238681\nPost:\nText: I had a similar problem a while back, try setting this line \nCode: plt.show()\n\nText: to \nCode: plt.show(block=True)\n\nText: According to the docs this overrides the blocking behaviour caused by running mpl.pyplot in interactive mode, which is known to cause issues in some environments. I believe this only works for the latest version of matplotlib. \nAPI:\nmatplotlib.pyplot\n","label":[[225,235,"Mention"],[380,397,"API"]],"Comments":[]}
{"id":59294,"text":"ID:38379176\nPost:\nText: Depending on what you are trying to do, it may in fact be possible to set the width of the legend border via rcParams. \nText: TL;DR \nText: Set patch.linewidth in matplotlibrc or better yet, write a two-line wrapper function for legend and use that instead in your scripts. \nText: Long Version \nText: Looking at the code for legend, you can see that the frame stored as a matplotlib.patched.FancyBBoxPatch object, which is a type of matplotlib.patches.Patch. Patch objects get their default line width from matplotlib.rcParams['patch.linewidth']. This means that if you set patch.linewidth in matplotlibrc, the size of the legend border will change. \nText: A similar modification to patch.edgecolor will change the border color, but patch.facecolor will be ignored for legends. \nText: Here is a code sample to illustrate the change, with some outputs: \nCode: >>> import matplotlib as mpl\n>>> from matplotlib import pyplot as plt\n>>> mpl.rcParams['patch.linewidth']\n1.0\n>>> plt.plot((0, 1), label='a')[0].axes.legend()\n<matplotlib.legend.Legend object at 0x7f6d7b0a0e48>\n>>> mpl.rcParams['patch.linewidth'] = 15\n>>> plt.plot((0, 1), label='a')[0].axes.legend()\n<matplotlib.legend.Legend object at 0x7f6d7ae20908>\n\nText: The obvious issue here is that if you draw any other patches on your charts, the default line width will be thrown off. You can of course mitigate this by \"manually\" changing the line width back to 1 for all the patches you create. Clearly this is not an optimal solution. \nText: A Better Way \nText: A much better solution would be to create a small script that you would share between all your chart generation scripts. Here is a sample of such a script: \nCode: from matplotlib.axes import Axes\n\ndef add_legend(ax, *args, **kwargs):\n    \"\"\"\n    Adds a legend to the specified axes, along with some common formatting. All additional arguments are the same as for `ax.legend`.\n    \"\"\"\n    legend = _originalFn(ax, *args, **kwargs)\n    legend.set_linewidth(15)\n    return legend\n\nif Axes.legend is not add_legend:\n    _originalFn = Axes.legend\n    Axes.legend = add_legend\n\nText: This script is structured in such a way that you do not ever need to call add_legend manually. Just importing the script subclasses Axes so that calling ax.legend(...) as before will set the default legend border width to 15 points. \nText: The script above will work for calls to ax.legend where ax is an Axes instance and on plt.legend, since plt.legend really just delegates to gca().legend with some additional processing. \nText: This subclassing mechanism will work even if your program consists of multiple scripts\/modules which import the shared script. This is because Python will not actually reload the module after the first time, and will instead just return a reference to it, with all attributes intact. Things will break if you start using importlib.reload, but that is only a very remote possibility. \nAPI:\nmatplotlib.axes.Axes\n","label":[[2252,2256,"Mention"],[2942,2962,"API"]],"Comments":[]}
{"id":59295,"text":"ID:38560667\nPost:\nText: To extract the max and min you may use the following: \nCode: count_collection = np.array(count_collection)\nmx = np.max(count_collection,0)\nmn = np.min(count_collection,0)\n\nText: The first line just changes from a list of 1d arrays to 2d array, so max and min can operate. \nText: edit: \nText: Since the original plot was normalized, it is hard to make sense of max and min of half the sample size. But you can do something like this: import numpy as np from numpy.random import randn import pyplot as plt import pandas as pd \nCode: fig,ax = plt.subplots()\n\nbins = np.arange(-5,6,0.5)\ndf = pd.DataFrame(randn(3000))\n#df.hist(ax=ax, bins=bins, alpha = 0.7, normed=True)\nhistval, _ = np.histogram(df, bins=bins)\n\ncount_collection = []\nfor i in np.arange(1,100):\n    temp_df = df.sample(frac=0.5, replace=True)\n#    temp_df.hist(ax=ax, bins=bins, alpha = 0.25, normed=True)\n\n    count, division = np.histogram(temp_df, bins=bins)\n    count_collection.append(count)\n\ncount_collection = np.array(count_collection)\nmx = np.max(count_collection,0)\nmn = np.min(count_collection,0)\n\nplt.bar(bins[:-1], histval, 0.5)\nplt.plot(bins[:-1] + 0.25, mx*2)\nplt.plot(bins[:-1] + 0.25, mn*2)\n\nText: The 2x factor is due to the 2x smaller sample size when calculating the max and min. \nAPI:\nmatplotlib.pyplot\n","label":[[514,520,"Mention"],[1293,1310,"API"]],"Comments":[]}
{"id":59296,"text":"ID:38851858\nPost:\nText: You can also use the %matplotlib inline magic, but it has to be preceded by the pure %matplotlib line: \nText: Works (figures in new window) \nCode: %matplotlib\nimport matplotlib.pyplot as plt\n\nText: Works (inline figures) \nCode: %matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nText: Does not work \nCode: %matplotlib inline\nimport matplotlib.pyplot as plt\n\nText: Also: Failure to import pypolt in jupyter (but not ipython) seems to be the same issue. It looks like a recently introduced bug in the ipykernel. Maybe someone mark this or the other question as duplicate, thx. \nAPI:\nmatplotlib.pyplot\n","label":[[425,431,"Mention"],[618,635,"API"]],"Comments":[]}
{"id":59297,"text":"ID:39093939\nPost:\nText: From the documentation: kwargs include repeat, repeat_delay, and interval: interval draws a new frame every interval milliseconds. repeat controls whether the animation should repeat when the sequence of frames is completed. repeat_delay optionally adds a delay in milliseconds before repeating the animation. However, this only sets an upper bound on the frame rate - if it takes too long to draw a frame, then you'll see a slower frame rate If you have a larger number of dots, you'll want to do something like: data = np.array(data) # in init, get a single Line3D object # the comma is important! pts, = ax.plot([], [], [], 'o', c=colors) # in update() pts.set_data(data[i,:,0], data[i,:,1]) pts.set_3d_properties(data[i,:,2]) So that you can eliminate the for loop \nAPI:\nmpl_toolkits.mplot3d.art3d.Line3D\n","label":[[584,590,"Mention"],[799,832,"API"]],"Comments":[]}
{"id":59298,"text":"ID:39247062\nPost:\nText: First we need to choose random marker. It could be done via markers dictionary which contains all available markers. Also markers means 'nothing', starting with 'tick' and 'caret' should be dropped Some more information abour markers. Let's make list with valid markers and then random choose from them how many we need for plotting DataFrame or you could use second option with filled_markers: \nCode: import matplotlib as mpl\nimport numpy as np\n\n# create valid markers from mpl.markers\nvalid_markers = ([item[0] for item in mpl.markers.MarkerStyle.markers.items() if \nitem[1] is not 'nothing' and not item[1].startswith('tick') \nand not item[1].startswith('caret')])\n\n# use fillable markers\n# valid_markers = mpl.markers.MarkerStyle.filled_markers\n\nmarkers = np.random.choice(valid_markers, df.shape[1], replace=False)\n\nText: For example: \nCode: In [146]: list(markers )\nOut[146]: ['H', '^', 'v', 's', '3', '.', '1', '_']\n\nText: Then for markers you could plot your dataframe, and set markers for each line via set_marker method. Then you could add legend to your plot: \nCode: import pandas as pd\n\nnp.random.seed(2016)\ndf = pd.DataFrame(np.random.rand(10, 8))\n\nax = df.plot(kind='line')\nfor i, line in enumerate(ax.get_lines()):\n    line.set_marker(markers[i])\n\n# for adding legend\nax.legend(ax.get_lines(), df.columns, loc='best')\n\nText: Original: \nText: Modified: \nAPI:\nmatplotlib.markers.MarkerStyle.markers\n","label":[[84,91,"Mention"],[1397,1435,"API"]],"Comments":[]}
{"id":59299,"text":"ID:39408904\nPost:\nText: g and f are not Figure objects, they are seaborn.axisgrid.FacetGrid objects (as mentioned by @tcaswell in comments). \nText: PdfPages needs the Figure instances, and luckily they are easy to extract from the FacetGrid objects, using g.fig and f.fig. \nText: So, all you need to do is change one line, from \nCode: figures = [g, f]\n\nText: to: \nCode: figures = [g.fig, f.fig]\n\nAPI:\nmatplotlib.figure.Figure\n","label":[[40,46,"Mention"],[401,425,"API"]],"Comments":[]}
{"id":59300,"text":"ID:39621422\nPost:\nText: consider the dataframe df \nCode: df = pd.DataFrame(np.random.rand(10, 6), columns=pd.Series(list('123456')).radd('C'))\ndf\n\nText: Solution Use itertools and subplots \nCode: from itertools import combinations\nimport matplotlib.pyplot as plt\n\npairs = list(combinations(df.columns, 2))\n\nfig, axes = plt.subplots(len(pairs) \/\/ 3, 3, figsize=(15, 12))\nfor i, pair in enumerate(pairs):\n    d = df[list(pair)]\n    ax = axes[i \/\/ 3, i % 3]\n    d.plot.scatter(*pair, ax=ax)\n\nfig.tight_layout()\n\nAPI:\nmatplotlib.pyplot.subplots\n","label":[[180,188,"Mention"],[514,540,"API"]],"Comments":[]}
{"id":59301,"text":"ID:39682205\nPost:\nText: Matplotlib fields and methods can be accessed using the \nCode: matplotlib[:colors][:LogNorm]\n\nText: syntax (i.e. for the corresponding LogNorm object). \nText: UPDATE: Thank you for your mwe. Based on that example, I managed to make it work like this: \nCode: PyPlot.pcolor(A, norm=matplotlib[:colors][:LogNorm](vmin=minimum(A), vmax=maximum(A)), cmap=\"PuBu_r\")\n\nAPI:\nmatplotlib.colors.LogNorm\n","label":[[159,166,"Mention"],[390,415,"API"]],"Comments":[]}
{"id":59302,"text":"ID:39754917\nPost:\nText: FancyBboxPatch class is similar to mpl.patches.Rectangle class, but it draws a fancy box around the rectangle. FancyBboxPatch.get_width() returns the width of the (inner) rectangle \nText: However, FancyBboxPatch is a subclass of Pach which provides a get_extents() method: \nText: matplotlib.patches.Patch.get_extents() Return a Bbox object defining the axis-aligned extents of the Patch. \nText: Thus, what you need is textbox.get_bbox_patch().get_extents().width. \nAPI:\nmatplotlib.patches.FancyBboxPatch\nmatplotlib.patches.Rectangle\nmatplotlib.patches.Patch\n","label":[[24,38,"Mention"],[59,80,"Mention"],[253,257,"Mention"],[494,527,"API"],[528,556,"API"],[557,581,"API"]],"Comments":[]}
{"id":59303,"text":"ID:40117650\nPost:\nText: Create two subplots and add a table to the second axe object? See the documentation for Axes.table() \nText: http:\/\/matplotlib.org\/api\/axes_api.html?highlight=table#matplotlib.axes.Axes.table \nText: table(**kwargs) Add a table to the current axes. Call signature: table(cellText=None, cellColours=None, cellLoc='right', colWidths=None, rowLabels=None, rowColours=None, rowLoc='left', colLabels=None, colColours=None, colLoc='center', loc='bottom', bbox=None): Returns a mpl.table.Table instance. For finer grained control over tables, use the Table class and add it to the axes with add_table(). \nAPI:\nmatplotlib.table.Table\n","label":[[493,508,"Mention"],[625,647,"API"]],"Comments":[]}
{"id":59304,"text":"ID:40228795\nPost:\nText: Do you have more than one version of Python installed? I would check your python path. Make sure matplotlib is in 2.7 in this case. \nText: This might be relevant too -- import pyplot hangs \nAPI:\nmatplotlib.pyplot\n","label":[[200,206,"Mention"],[219,236,"API"]],"Comments":[]}
{"id":59305,"text":"ID:40293189\nPost:\nText: plt.imshow image space convention for its indexing of the input array. That is, (0, 0) in the top right corner and a y-axis that is oriented downwards. \nText: To avoid this, you have to call plt.imshow with the optional parameter origin = 'lower' (to correct the origin) and pass the data transposed as heatmap.T to correct the flipping of the axes. \nText: But this will not get you the correct plot yet. Not only is the origin in the wrong place, also the indexing convention is different. numpy arrays follow row\/column indexing, while images usually use column\/row indexing. So in addition, you have to transpose the data. \nText: So in the end, your code should look like this: \nCode: import matplotlib.pyplot as plt\nimport numpy as np\n\nbins = np.arange(-0.5, 5.5, 1.0), np.arange(-0.5, 5.5, 1.0)\nheatmap, xedges, yedges = np.histogram2d(x, y, bins=bins)\nextent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n\nplt.clf()\nplt.imshow(heatmap.T,\n           origin='lower',\n           extent=extent,\n           interpolation='nearest',\n           cmap=plt.get_cmap('viridis'), # use nicer color map\n          )\nplt.colorbar()\n\nplt.xlabel('x')\nplt.ylabel('y')\n\nText: Or even better use hist2d to avoid this issue completely. \nAPI:\nmatplotlib.pyplot.hist2d\n","label":[[1209,1215,"Mention"],[1254,1278,"API"]],"Comments":[]}
{"id":59306,"text":"ID:40412107\nPost:\nText: scatter returns PathCollection object. You can use its methods get_sizes() and set_sizes() to retrieve or set sizes of points. Here is a modification of your example. \nCode: import matplotlib.pyplot as plt;\nimport matplotlib.animation as amt\n\n# to embed animation as html5 video in Jupyter notebook\nfrom matplotlib import rc\nrc('animation', html='html5')\n\ndef animate(i, population, latitude, longitude, colour):\n    scalefactor = 0.001\n    area = [n*scalefactor*i for n in population]\n    # it is better to use list comprehensions here\n\n    sc.set_sizes(area)\n    # here is the answer to the question\n\n    return (sc,)\n\n#creates an array of colours where cities are green and towns are yellow\n#creates an array of longitude\n#creates an array of latitude\n#creates an array of population\n\n# pick some random values\n\ncolour = ['g', 'g', 'y', 'y', 'y'];\nlongitude = [1, 2, 3, 4, 5];\nlatitude = [1, 3, 2, 5, 4];\npopulation = [1000, 2000, 5000, 4000, 3000];\n\nfig = plt.figure()\nsc = plt.scatter(longitude, latitude, s=population)\n\n# I didn't managed this to work without `init` function\n\ndef init():\n    return (sc,)\n\nani = amt.FuncAnimation(fig, animate, frames=200, init_func=init,\n                        fargs=(population, latitude, longitude, colour), \n                        interval=10, repeat = False, blit = True)\n\nAPI:\nmatplotlib.collections.PathCollection\n","label":[[40,54,"Mention"],[1349,1386,"API"]],"Comments":[]}
{"id":59307,"text":"ID:40573071\nPost:\nText: In principle there is always the option to set custom labels via plt.gca().yaxis.set_xticklabels(). \nText: However, I'm not sure why there shouldn't be the possibility to use mpl.ticker.FuncFormatter here. The FuncFormatter is designed for exactly the purpose of providing custom ticklabels depending on the ticklabel's position and value. There is actually a nice example in the matplotlib example collection. \nText: In this case we can use the FuncFormatter as desired to provide unit prefixes as suffixes on the axes of a matplotlib plot. To this end, we iterate over the multiples of 1000 and check if the value to be formatted exceeds it. If the value is then a whole number, we can format it as integer with the respective unit symbol as suffix. On the other hand, if there is a remainder behind the decimal point, we check how many decimal places are needed to format this number. \nText: Here is a complete example: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n\ndef y_fmt(y, pos):\n    decades = [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6, 1e-9 ]\n    suffix  = [\"G\", \"M\", \"k\", \"\" , \"m\" , \"u\", \"n\"  ]\n    if y == 0:\n        return str(0)\n    for i, d in enumerate(decades):\n        if np.abs(y) >=d:\n            val = y\/float(d)\n            signf = len(str(val).split(\".\")[1])\n            if signf == 0:\n                return '{val:d} {suffix}'.format(val=int(val), suffix=suffix[i])\n            else:\n                if signf == 1:\n                    print val, signf\n                    if str(val).split(\".\")[1] == \"0\":\n                       return '{val:d} {suffix}'.format(val=int(round(val)), suffix=suffix[i]) \n                tx = \"{\"+\"val:.{signf}f\".format(signf = signf) +\"} {suffix}\"\n                return tx.format(val=val, suffix=suffix[i])\n\n                #return y\n    return y\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(10,5))\n\nx = np.linspace(0,349,num=350) \ny = np.sinc((x-66.)\/10.3)**2*1.5e6+np.sinc((x-164.)\/8.7)**2*660000.+np.random.rand(len(x))*76000.  \nwidth = 1\n\nax[0].bar(x, y, width, align='center', linewidth=2, color='red', edgecolor='red')\nax[0].yaxis.set_major_formatter(FuncFormatter(y_fmt))\n\nax[1].bar(x[::-1], y*(-0.8e-9), width, align='center', linewidth=2, color='orange', edgecolor='orange')\nax[1].yaxis.set_major_formatter(FuncFormatter(y_fmt))\n\nax[2].fill_between(x, np.sin(x\/100.)*1.7+100010, np.cos(x\/100.)*1.7+100010, linewidth=2, color='#a80975', edgecolor='#a80975')\nax[2].yaxis.set_major_formatter(FuncFormatter(y_fmt))\n\nfor axes in ax:\n    axes.set_title(\"TTL Distribution\")\n    axes.set_xlabel('TTL Value')\n    axes.set_ylabel('Number of Packets')\n    axes.set_xlim([x[0], x[-1]+1])\n\nplt.show()\n\nText: which provides the following plot: \nAPI:\nmatplotlib.ticker.FuncFormatter\n","label":[[199,223,"Mention"],[2769,2800,"API"]],"Comments":[]}
{"id":59308,"text":"ID:40707115\nPost:\nText: From the Pandas docs - \nText: The plot method on Series and DataFrame is just a simple wrapper around plt.plot(): \nText: This means that anything you can do with matplolib, you can do with a Pandas DataFrame plot. \nText: pyplot has an axis() method that lets you set axis properties. Calling plt.axis('off') before calling plt.show() will turn off both axes. \nCode: df.plot()\nplt.axis('off')\nplt.show()\nplt.close()\n\nText: To control a single axis, you need to set its properties via the plot's Axes. For the x axis - (pyplot.axes().get_xaxis().....) \nCode: df.plot()\nax1 = plt.axes()\nx_axis = ax1.axes.get_xaxis()\nx_axis.set_visible(False)\nplt.show()\nplt.close()\n\nText: Similarly to control an axis label, get the label and turn it off. \nCode: df.plot()\nax1 = plt.axes()\nx_axis = ax1.axes.get_xaxis()\nx_axis.set_label_text('foo')\nx_label = x_axis.get_label()\n##print isinstance(x_label, matplotlib.artist.Artist)\nx_label.set_visible(False)\nplt.show()\nplt.close()\n\nText: You can also get to the x axis like this \nCode: ax1 = plt.axes()\nx_axis = ax1.xaxis\nx_axis.set_label_text('foo')\nx_axis.label.set_visible(False)\n\nText: Or this \nCode: ax1 = plt.axes()\nax1.xaxis.set_label_text('foo')\nax1.xaxis.label.set_visible(False)\n\nText: DataFrame.plot \nText: returns a ax.Axes or numpy.ndarray of them \nText: so you can get it\/them when you call it. \nCode: axs = df.plot()\n\nText: .set_visible() is an Artist method. The axes and their labels are Artists so they have Artist methods\/attributes as well as their own. There are many ways to customize your plots. Sometimes you can find the feature you want browsing the Gallery and Examples \nAPI:\nmatplotlib.axes.Axes\n","label":[[1284,1291,"Mention"],[1659,1679,"API"]],"Comments":[]}
{"id":59309,"text":"ID:40750168\nPost:\nText: This is my solution, one I was trying to avoid. I saw in the old question linked in the question comments someone mentions this is an np-complete problem, but I want to point out that's doesn't really matter. I tested up to 26 annotations, and it takes a few seconds but no more. Any practical plot won't have 1000 annotations. \nText: Caveats: \nText: As mentioned, this isn't superfast. Specifically I wish I could avoid draw(). It's OK now, only draw twice. This code allows any new annotation\/s to be added only with a specific orthogonal (left\/right\/up\/down) direction, but this can be extended. The arrow placement is window dependent. This means make sure the window size or axes do not change after annotating (with arrows). Re-annotate if you resize. Not dynamic, see point 3. \nText: Background: \nText: Iplot is a helper class I have to handle multiplot figures, handling coloring, sizing and now annotating. plt is mpl.pyplot This methods handles multiple annotations (or single) and can now solve conflicts. As you may have guessed, Iplot.axes holds my axes figure. \nText: EDIT I removed my class code to make this more copy pasteable. Axes should be given to the function, and kwargs accept an existing boxes keyword to take into account previous annotations, which is edited in place. Note I use a class to encapsulate this. The function has to return the boxes for use as well, in case this is a first call. \nText: EDIT 2 \nText: After a while sped this up - no need to draw so much, better to loop twice and then update the renderer in between. \nText: The code: \nCode: def annotate(axes,boxes,labels,data,**kwargs):\n    #slide should be relevant edge of bbox - e.g. (0,0) for left, (0,1) for bottom ...\n    try: slide = kwargs.pop(\"slide\")\n    except KeyError: slide = None\n    try: \n        xytexts = kwargs.pop(\"xytexts\")\n        xytext  = xytexts\n    except KeyError: \n        xytext = (0,2)\n        xytexts = None\n    try: boxes = kwargs.pop(\"boxes\")\n    except KeyError: boxes = list()\n    pixel_diff = 1\n                                                                                  newlabs = []              \n    for i in range(len(labels)):\n        try: \n            len(xytexts[i])\n            xytext = xytexts[i]\n        except TypeError: pass\n\n        a = axes.annotate(labels[i],xy=data[i],textcoords='offset pixels',\n                                    xytext=xytext,**kwargs)\n        newlabs.append(a)\n    plt.draw()\n    for i in range(len(labels)):\n        cbox = a.get_window_extent()\n        if slide is not None:\n            direct  = int((slide[0] - 0.5)*2)\n            current = -direct*float(\"inf\")\n            arrow = False\n            while True:\n                overlaps = False\n                count = 0\n                for box in boxes:\n                    if cbox.overlaps(box):\n                        if direct*box.get_points()[slide] > direct*current:\n                            overlaps = True\n                            current =  box.get_points()[slide] \n                            shift   = direct*(current - cbox.get_points()[1-slide[0],slide[1]])\n                if not overlaps: break\n                arrow = True\n                position = array(a.get_position())\n                position[slide[1]] += shift * direct * pixel_diff\n                a.set_position(position)\n                plt.draw()\n                cbox = a.get_window_extent()\n                x,y =  axes.transData.inverted().transform(cbox)[0]\n            if arrow:\n                axes.arrow(x,y,data[i][0]-x,data[i][1]-y,head_length=0,head_width=0)\n        boxes.append(cbox)\n    plt.draw()\n    return boxes\n\nText: Any suggestions to improve will be warmly welcomed. Many thanks! \nAPI:\nmatplotlib.pyplot\n","label":[[947,957,"Mention"],[3735,3752,"API"]],"Comments":[]}
{"id":59310,"text":"ID:40806117\nPost:\nText: IIUC: you don't want a histogram, you just want a bar plot. \nCode: df.set_index('Type').plot.bar(subplots=True, legend=False)\n\nText: adjust vertical spacing assuming import pyplot as plt \nCode: axes = df.set_index('Type').plot.bar(subplots=True, legend=False)\nplt.subplots_adjust(hspace=0.35)\n\nAPI:\nmatplotlib.pyplot\n","label":[[197,203,"Mention"],[323,340,"API"]],"Comments":[]}
{"id":59311,"text":"ID:40812102\nPost:\nText: Probably you need a loglog plot? \nText: For example, if you use a simple plot. \nText: import numpy as np import pyplot as plt \nCode: def output(pixels):\n    test1 = np.arange(450*450)\n    plt.imshow(np.reshape(test1, (450, 450), order='F'), cmap='RdGy')\n    for p in range(0,len(pixels),4):\n        row1 = pixels[p]\n        col1 = pixels[p+1]\n        row2 = pixels[p+2]\n        col2 = pixels[p+3]\n        y = np.linspace(row1,row2,10)\n        x = np.linspace(col1,col2,10)\n        plt.plot(x, y, color='yellow')\n\n\npixels = np.arange(128*128)\noutput(pixels)\n\nText: But, if you use loglog axis. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef output(pixels):\n    test1 = np.arange(450*450)\n    plt.imshow(np.reshape(test1, (450, 450), order='F'), cmap='RdGy')\n    for p in range(0,len(pixels),4):\n        row1 = pixels[p]\n        col1 = pixels[p+1]\n        row2 = pixels[p+2]\n        col2 = pixels[p+3]\n        y = np.linspace(row1,row2,10)\n        x = np.linspace(col1,col2,10)\n        plt.loglog(x, y, color='yellow')\n\n\npixels = np.arange(128*128)\noutput(pixels)\n\nAPI:\nmatplotlib.pyplot\n","label":[[136,142,"Mention"],[1109,1126,"API"]],"Comments":[]}
{"id":59312,"text":"ID:40849953\nPost:\nText: Hooks for formatting the info are Axes.format_coord or Axes.fmt_xdata. Standard formatters are defined in dates (plus some additions from pandas). A basic solution could be: \nCode: import matplotlib.dates\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndates = pd.date_range(start='2011-01-01', end='2012-01-01')\nseries = pd.Series(np.random.rand(len(dates)), index=dates)\n\nplt.plot(series.index, series.values)\nplt.gca().fmt_xdata = matplotlib.dates.DateFormatter('%Y-%m-%d')\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[130,135,"Mention"],[542,558,"API"]],"Comments":[]}
{"id":59313,"text":"ID:40930194\nPost:\nText: You can truncate the colormap by using the truncate_colormap function I have written in the code below. It creates a new coloring from an existing colormap. \nText: Note that you then don't need to scale the Normalise instance by maxColor, and you need to use this new colormap instance when creating your colors list and the colorbar. \nCode: import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.colors as mcolors\n\ngs = gridspec.GridSpec(2, 1,\n                       height_ratios=[1, 4]\n                       )\nax = [plt.subplot(g) for g in gs]\n\nparameterToColorBy = np.linspace(5, 10, 6, dtype=float)\n\ndef truncate_colormap(cmap, minval=0.0, maxval=1.0, n=-1):\n    if n == -1:\n        n = cmap.N\n    new_cmap = mcolors.LinearSegmentedColormap.from_list(\n         'trunc({name},{a:.2f},{b:.2f})'.format(name=cmap.name, a=minval, b=maxval),\n         cmap(np.linspace(minval, maxval, n)))\n    return new_cmap\n\nminColor = 0.00\nmaxColor = 0.85\ninferno_t = truncate_colormap(plt.get_cmap(\"inferno\"), minColor, maxColor)\n\ncolors = [inferno_t(i)\n          for i in np.linspace(0, 1, parameterToColorBy.shape[0])]\n\nnorm = mpl.colors.Normalize(parameterToColorBy[0],\n                            parameterToColorBy[-1])\n\ncb = mpl.colorbar.ColorbarBase(ax[0],\n                               cmap=inferno_t,\n                               norm=norm,\n                               ticks=parameterToColorBy,\n                               orientation='horizontal')\n\nax[0].xaxis.set_ticks_position('top')\n\nfor p, c in zip(parameterToColorBy, colors):\n    ax[1].plot(np.arange(2)\/p, c=c)\n\nplt.show()\n\nAPI:\nmatplotlib.colors.LinearSegmentedColormap\n","label":[[145,153,"Mention"],[1696,1737,"API"]],"Comments":[]}
{"id":59314,"text":"ID:40948240\nPost:\nText: There are two main components to this problem. \nText: Fitting an ellipse. There is actually a nice example on how to fit an ellipse to data points in python found on this site. So we can use this to obtain the rotation angle as well as the 2 dimensions of the ellipse from the data. Plotting all ellipses to a figure. Once the ellipse parameters are obtained, an ellipse can be plotted using mpl.patches.Ellipse \nText: Here is the complete code: \nCode: import numpy as np\nfrom numpy.linalg import eig, inv\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n\n######################\n### Ellipse fitting ##\n######################\n# taken from \n# http:\/\/nicky.vanforeest.com\/misc\/fitEllipse\/fitEllipse.html\n\ndef fitEllipse(x,y):\n    x = x[:,np.newaxis]\n    y = y[:,np.newaxis]\n    D =  np.hstack((x*x, x*y, y*y, x, y, np.ones_like(x)))\n    S = np.dot(D.T,D)\n    C = np.zeros([6,6])\n    C[0,2] = C[2,0] = 2; C[1,1] = -1\n    try:\n        E, V =  eig(np.dot(inv(S), C))\n        n = np.argmax(np.abs(E))\n        a = V[:,n]\n        return a\n    except:\n        return [np.nan]*5\n\ndef ellipse_center(a):\n    b,c,d,f,g,a = a[1]\/2, a[2], a[3]\/2, a[4]\/2, a[5], a[0]\n    num = b*b-a*c\n    x0=(c*d-b*f)\/num\n    y0=(a*f-b*d)\/num\n    return np.real(np.array([x0,y0]))\n\n\ndef ellipse_angle_of_rotation( a ):\n    b,c,d,f,g,a = a[1]\/2, a[2], a[3]\/2, a[4]\/2, a[5], a[0]\n    return np.real(0.5*np.arctan(2*b\/(a-c)))\n\n\ndef ellipse_axis_length( a ):\n    b,c,d,f,g,a = a[1]\/2, a[2], a[3]\/2, a[4]\/2, a[5], a[0]\n    up = 2*(a*f*f+c*d*d+g*b*b-2*b*d*f-a*c*g)\n    down1=(b*b-a*c)*( (c-a)*np.sqrt(1+4*b*b\/((a-c)*(a-c)))-(c+a))\n    down2=(b*b-a*c)*( (a-c)*np.sqrt(1+4*b*b\/((a-c)*(a-c)))-(c+a))\n    res1=np.sqrt(up\/down1)\n    res2=np.sqrt(up\/down2)\n    return np.real(np.array([res1, res2]))\n\n\n########################\n### Data Generation  ###\n########################\n\nn_el = 8 # number of ellipse points\n# define grid\nx = np.linspace(-7,7, 15)\ny = np.linspace(4,18, 15)\n# data (2 for x,y (west, north), n_el, dimensions of grid in x and y )\ndata = np.zeros((2, n_el,len(x), len(y) )) \nfor i in range(len(y)):\n    for j in range(len(x)):\n        #generate n_el points on an ellipse \n        r = np.linspace(0,2*np.pi, n_el) \n        data[0,:,j,i]  = 0.5*(0.9*np.random.random(1)+0.1) * np.cos(r+2*np.random.random(1)*np.pi) \n        data[1,:,j,i] =  0.5*(0.9*np.random.random(1)+0.1) * np.sin(r)\n\n# Test case: fit an ellipse and print the parameters\na = fitEllipse(data[0,:,0,0], data[1,:,0,0])\nang = ellipse_angle_of_rotation(a)\nl = ellipse_axis_length( a )\ncenter = ellipse_center(a)\nprint \"\\tangle: {}\\n\\tlength: {}\\n\\tcenter: {}\".format(ang, l, center)\n\n\n\n\n######################\n####### plotting   ###\n######################\nfig, (ax, ax2) = plt.subplots(ncols=2, figsize=(11,5))\n\n# First, draw the test case ellipse\n# raw data\nax.scatter(data[0,:,0,0], data[1,:,0,0], s=30, c=\"r\", zorder=10)\n\n# Fitted Ellipse \n# matplotlib.patches.Ellipse\n# http:\/\/matplotlib.org\/api\/patches_api.html#matplotlib.patches.Ellipse\n# takes width and height as diameter instead of half width and rotation in degrees\ne = Ellipse(xy=(0,0), width=2*l[0], height=2*l[1], angle=ang*180.\/np.pi,  facecolor=\"b\", alpha=0.2, zorder=0 )\nec = Ellipse(xy=(0,0), width=2*l[0], height=2*l[1], angle=ang*180.\/np.pi,  fill=False,  zorder=1 )\nax.add_artist(e)\nax.add_artist(ec)\n\nax.set_aspect(\"equal\")\nax.set_xlim([-1,1])\nax.set_ylim([-1,1])  \n\n# Fit ellipse for every datapoint on grid and place in figure\nfor i in range(len(y)):\n    for j in range(len(x)):\n        a = fitEllipse(data[0,:,j,i], data[1,:,j,i])\n        ang = ellipse_angle_of_rotation(a)\n        l = ellipse_axis_length( a )\n        e = Ellipse(xy=(x[j],y[i]), width=2*l[0], height=2*l[1], angle=ang*180.\/np.pi, fill=False, zorder=1 )\n        ax2.add_artist(e)\n\nax2.set_ylim([y.min()-0.5, y.max()+0.5 ])\nax2.set_xlim([x.min()-0.5, x.max()+0.5 ])\nax2.set_aspect(\"equal\")\n\n# load some background image.\nimage = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/c\/ca\/Singapore-OutlineMap-20050606.png\/600px-Singapore-OutlineMap-20050606.png\"\nimage = np.rot90(plt.imread(image))\nim = ax2.imshow(image,extent=[x.min()-0.5, x.max()+0.5, y.min()-0.5, y.max()+0.5, ] )        \n\nplt.show()\n\nAPI:\nmatplotlib.patches.Ellipse\n","label":[[416,435,"Mention"],[4237,4263,"API"]],"Comments":[]}
{"id":59315,"text":"ID:40997640\nPost:\nText: From the function definition add_inset_to_axis(figure, axis, rect) it seems that the second argument is actually meant to be a ax instance. \nText: So instead of giving grid_cell as an argument, one should probably use axis \nCode: inset = add_inset_to_axis(fig, axis, (0.675, 0.82, 0.3, 0.15))\n\nAPI:\nmatplotlib.axes\n","label":[[151,153,"Mention"],[323,338,"API"]],"Comments":[]}
{"id":59316,"text":"ID:41145815\nPost:\nCode: # Set seed to reproduce the results\nnp.random.seed(42)\n# Generate random data\ndf = pd.DataFrame(dict(age=(np.random.uniform(-20, 50, 100))))\n\n# KDE plot\nax = df['age'].plot(kind='density')\n# Access the child artists and calculate the mean of the resulting array\nmean_val = np.mean(ax.get_children()[0]._x)\n# Annotate points\nax.annotate('mean', xy=(mean_val, 0.008), xytext=(mean_val+10, 0.010),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\n# vertical dotted line originating at mean value\nplt.axvline(mean_val, linestyle='dashed', linewidth=2)\n\nText: Slice 0 was selected as it corresponds to the position of the mpl.lines.Line2D axes object. \nCode: >>> np.mean(ax.get_children()[0]._x)\n14.734316880344197\n\nAPI:\nmatplotlib.lines.Line2D\n","label":[[667,683,"Mention"],[766,789,"API"]],"Comments":[]}
{"id":59317,"text":"ID:41204740\nPost:\nText: To have a rough idea, this could be a possible solution in matplotlib using Axes3D \nCode: from mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, PathPatch\nimport mpl_toolkits.mplot3d.art3d as art3d\n\n\nx = np.array([1,2,3])\ny = np.array([2,3,1])\nz = np.array([1,1,1])\n\n\nfig = plt.figure(figsize=(6,6))\nax = fig.add_axes([0,0,1,1], projection='3d')\n\n#plot the points\nax.scatter(x,y,z*0.4, c=\"r\", facecolor=\"r\", s=60)\nax.scatter(y,x,z*0.9, c=\"b\", facecolor=\"b\", s=60)\nax.scatter(x,y,z*1.6, c=\"g\", facecolor=\"g\", s=60)\n#plot connection lines\nax.plot([x[0],y[0],x[0]],[y[0],x[0],y[0]],[0.4,0.9,1.6], color=\"k\")\nax.plot([x[2],y[2],x[2]],[y[2],x[2],y[2]],[0.4,0.9,1.6], color=\"k\")\n#plot planes\np = Rectangle((0,0), 4,4, color=\"r\", alpha=0.2)\nax.add_patch(p)\nart3d.pathpatch_2d_to_3d(p, z=0.4, zdir=\"z\")\n\np = Rectangle((0,0), 4,4, color=\"b\", alpha=0.2)\nax.add_patch(p)\nart3d.pathpatch_2d_to_3d(p, z=0.9, zdir=\"z\")\n\np = Rectangle((0,0), 4,4, color=\"g\", alpha=0.2)\nax.add_patch(p)\nart3d.pathpatch_2d_to_3d(p, z=1.6, zdir=\"z\")\n\n\n\nax.set_aspect('equal')\nax.view_init(13,-63)\nax.set_xlim3d([0,4])\nax.set_ylim3d([0,4])\nax.set_zlim3d([0,2])\n\nplt.savefig(__file__+\".png\")\nplt.show()\n\nText: Update \nText: Creating three different axes is possible. One has to add the axes and make the upper ones transparent (ax2.patch.set_alpha(0.)). Then the grid has to be turned off (ax.grid(False)) and the panes and lines that we don't need set invisible. However, I have no clue how to draw a connection with between the axes. The 2D approach of mpatches.ConnectionPatch does not work for 3D axes. \nCode: from mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport mpl_toolkits.mplot3d.art3d as art3d\n\n\nx = np.array([1,2,3])\ny = np.array([2,3,1])\nz = np.array([0,0,0])\n\n\nfig = plt.figure(figsize=(6,6))\n\nax = fig.add_axes([0,0,1,1], projection='3d')\nax2 = fig.add_axes([0.0,0.24,1,1], projection='3d')\nax2.patch.set_alpha(0.)\nax3 = fig.add_axes([0.0,0.48,1,1], projection='3d')\nax3.patch.set_alpha(0.)\n#plot the points\nax.scatter(x,y,z, c=\"r\", facecolor=\"r\", s=60)\nax2.scatter(y*4,x*4,z, c=\"b\", facecolor=\"b\", s=60)\nax3.scatter(x*100,y*100,z, c=\"g\", facecolor=\"g\", s=60)\n#plot connection lines\n#ax.plot([x[0],y[0],x[0]],[y[0],x[0],y[0]],[0.4,0.9,1.6], color=\"k\")\n#ax.plot([x[2],y[2],x[2]],[y[2],x[2],y[2]],[0.4,0.9,1.6], color=\"k\")\n\n#plot planes\np = Rectangle((0,0), 4,4, color=\"r\", alpha=0.2)\nax.add_patch(p)\nart3d.pathpatch_2d_to_3d(p, z=0, zdir=\"z\")\n\np = Rectangle((0,0), 16,16, color=\"b\", alpha=0.2)\nax2.add_patch(p)\nart3d.pathpatch_2d_to_3d(p, z=0, zdir=\"z\")\n\np = Rectangle((0,0), 400,400, color=\"g\", alpha=0.2)\nax3.add_patch(p)\nart3d.pathpatch_2d_to_3d(p, z=0, zdir=\"z\")\n\n\n\nax.set_aspect('equal')\nax2.set_aspect('equal')\nax3.set_aspect('equal')\nax.view_init(13,-63)\nax2.view_init(10,-63)\nax3.view_init(8,-63)\nax.set_xlim3d([0,4])\nax.set_ylim3d([0,4])\nax.set_zlim3d([0,2])\nax2.set_xlim3d([0,16])\nax2.set_ylim3d([0,16])\nax2.set_zlim3d([0,2])\nax3.set_xlim3d([0,400])\nax3.set_ylim3d([0,400])\nax3.set_zlim3d([0,2])\nax.grid(False)\nax2.grid(False)\nax3.grid(False)\n\ndef axinvisible(ax):\n    for zax in (ax.w_zaxis, ax.w_xaxis, ax.w_yaxis):\n        zax.pane.set_visible(False)\n        if zax == ax.w_zaxis:\n            zax.line.set_visible(False)\n            for ll in zax.get_ticklines()+zax.get_ticklabels():\n                    ll.set_visible(False)\n\n\naxinvisible(ax)\naxinvisible(ax2)\naxinvisible(ax3)\n\n# setting a ConnectionPatch does NOT work\nfrom matplotlib.patches import ConnectionPatch\ncon = ConnectionPatch(xyA=(2,2), xyB=(2,2), \n                      coordsA='data', coordsB='data', \n                      axesA=ax, axesB=ax2,\n                      arrowstyle='->', clip_on=True)\nax2.add_artist(con) # artist is not shown :-(\n\nplt.show()\n\nAPI:\nmatplotlib.patches.ConnectionPatch\n","label":[[1620,1644,"Mention"],[3905,3939,"API"]],"Comments":[]}
{"id":59318,"text":"ID:41271773\nPost:\nText: Before the figure is drawn, the bounding box of a text object contains just the coordinates of the box relative to the text inside. \nText: Therefore it is necessary to draw the figure first and then access the bounding box. \nCode: fig.canvas.draw() \npatch = ann.get_bbox_patch()\nbox  = patch.get_extents()\nprint box\n#prints: Bbox(x0=263.6, y0=191.612085684, x1=320.15, y1=213.412085684)\n\nText: Since those are the coordinates of the box in display units, they need to be tranformed to figure units \nCode: tcbox = fig.transFigure.inverted().transform(box)\nprint tcbox\n#prints [[ 0.411875    0.39919185]\n#        [ 0.50023438  0.44460851]]\n\n# The format is \n#        [[ left    bottom]\n#         [ right   top   ]]\n\nText: This returns the bounding box in figure units (ranging from 0 to 1) of the rectangle around the text. \nText: If instead, axes coordinates are desired, it would be \nCode: ax.transAxes.inverted().transform(box)\n\nText: or if data coordinates are needed, \nCode: ax.transData.inverted().transform(box)\n\nText: If instead the bounding box of the text itself is what's beeing asked for one can use the \nText: get_window_extent() \nText: method of \nText: mtext.Text \nText: and supply the annotation object as argument. Using \nCode: box = matplotlib.text.Text.get_window_extent(ann)\nprint box\n# prints Bbox(x0=268.0, y0=196.012085684, x1=315.75, y1=209.012085684)\n\nText: one can proceed as above to obtain the box in figure units. \nAPI:\nmatplotlib.text.Text\n","label":[[1189,1199,"Mention"],[1470,1490,"API"]],"Comments":[]}
{"id":59319,"text":"ID:41319883\nPost:\nText: This %matplotlib magic is used to display graphs (of mpl.pyplot objects). This needs UI to display. So cannot be display on command prompt. \nText: According to IPython documentation, \nText: If the %matplotlib magic is called without an argument, the output of a plotting command is displayed using the default matplotlib backend in a separate window. Alternatively, the backend can be explicitly requested using, for example: %matplotlib gtk A particularly interesting backend, provided by IPython, is the inline backend. This is available only for the Jupyter Notebook and the Jupyter QtConsole. It can be invoked as follows: %matplotlib inline \nText: Simple solution would be to replace %matplotlib inline with %matplotlib and run it using ipython. \nText: Alternatively, what you could do is download jupyter notebook and run that code there. \nText: Or as @tihom said in comments, you could comment or remove that line and run the code but this wouldn't display the graphs and other things. \nAPI:\nmatplotlib.pyplot\n","label":[[77,87,"Mention"],[1023,1040,"API"]],"Comments":[]}
{"id":59320,"text":"ID:41411398\nPost:\nText: Seems to be a caching issue: \nText: running: \nCode: cd ~\/.matplotlib\nfc-list\n\nText: fixed the problem (same issue discussed here: import plt hangs but the symptoms were a little different) \nAPI:\nmatplotlib.pyplot\n","label":[[161,164,"Mention"],[219,236,"API"]],"Comments":[]}
{"id":59321,"text":"ID:41609238\nPost:\nText: The scatter plot in 3D is a Path3DCollection object. This provides an attribute _offsets3d which hosts a tuple (x,y,z) and can be used to update the scatter points' coordinates. Therefore it may be beneficial not to create the whole plot on every iteration of the animation, but instead only update its points. \nText: The following is a working example on how to do this. \nCode: import numpy as np\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.animation\nimport pandas as pd\n\n\na = np.random.rand(2000, 3)*10\nt = np.array([np.ones(100)*i for i in range(20)]).flatten()\ndf = pd.DataFrame({\"time\": t ,\"x\" : a[:,0], \"y\" : a[:,1], \"z\" : a[:,2]})\n\ndef update_graph(num):\n    data=df[df['time']==num]\n    graph._offsets3d = (data.x, data.y, data.z)\n    title.set_text('3D Test, time={}'.format(num))\n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\ntitle = ax.set_title('3D Test')\n\ndata=df[df['time']==0]\ngraph = ax.scatter(data.x, data.y, data.z)\n\nani = matplotlib.animation.FuncAnimation(fig, update_graph, 19, \n                               interval=40, blit=False)\n\nplt.show()\n\nText: This solution does not allow for blitting. However, depending on the usage case, it may not be necessary to use a scatter plot at all; using a normal plot might be equally possible, which allows for blitting - as seen in the following example. \nCode: import numpy as np\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.animation\nimport pandas as pd\n\n\na = np.random.rand(2000, 3)*10\nt = np.array([np.ones(100)*i for i in range(20)]).flatten()\ndf = pd.DataFrame({\"time\": t ,\"x\" : a[:,0], \"y\" : a[:,1], \"z\" : a[:,2]})\n\ndef update_graph(num):\n    data=df[df['time']==num]\n    graph.set_data (data.x, data.y)\n    graph.set_3d_properties(data.z)\n    title.set_text('3D Test, time={}'.format(num))\n    return title, graph, \n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\ntitle = ax.set_title('3D Test')\n\ndata=df[df['time']==0]\ngraph, = ax.plot(data.x, data.y, data.z, linestyle=\"\", marker=\"o\")\n\nani = matplotlib.animation.FuncAnimation(fig, update_graph, 19, \n                               interval=40, blit=True)\n\nplt.show()\n\nAPI:\nmpl_toolkits.mplot3d.art3d.Path3DCollection\n","label":[[52,68,"Mention"],[2258,2301,"API"]],"Comments":[]}
{"id":59322,"text":"ID:41640994\nPost:\nText: You can make use of set_xticklabels whose argument labels accepts a string like sequence (anything printable with \"%s\" conversion) and then make use of DateTimeIndex.strftime to specify the format you'd want the labels to appear in. \nCode: ax = month_series.plot.bar()\n# ax.set_xticklabels(month_series.index.to_period('M')) also works \nax.set_xticklabels(month_series.index.strftime('%Y-%m'))\nplt.show()\n\nAPI:\nmatplotlib.axes.Axes.set_xticklabels\n","label":[[44,59,"Mention"],[435,471,"API"]],"Comments":[]}
{"id":59323,"text":"ID:41662206\nPost:\nText: You would need to create a colorbar without any reference axes. This can be done with the ColorbarBase class. See also this example from the gallery. To use this, you need to create a new axis in the plot, where the colorbar should sit in; one way of doing this is to use make_axes_locatable. \nText: Here is a complete example. \nCode: import matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport numpy as np\n\n\nfor i in np.linspace(0, 1, 9):\n    plt.plot([i,i+1,i+2], color=mpl.cm.viridis(i))\n\ndivider = make_axes_locatable(plt.gca())\nax_cb = divider.new_horizontal(size=\"5%\", pad=0.05)    \ncb1 = mpl.colorbar.ColorbarBase(ax_cb, cmap=mpl.cm.viridis, orientation='vertical')\nplt.gcf().add_axes(ax_cb)\n\nplt.show()\n\nAPI:\nmatplotlib.colorbar.ColorbarBase\n","label":[[114,126,"Mention"],[801,833,"API"]],"Comments":[]}
{"id":59324,"text":"ID:41677416\nPost:\nText: Seaborn's Facetgrid provides a convenience function to quickly connect pandas dataframes to the matplotlib pyplot interface. \nText: However in GUI applications you rarely want to use pyplot, but rather the matplotlib API. \nText: The problem you are facing here is that Facetgrid already creates its own Fig object (Facetgrid.fig). Also, the MatplotlibWidget creates its own figure, so you end up with two figures. \nText: Now, let's step back a bit: In principle it is possible to use a seaborn Facetgrid plot in PyQt, by first creating the plot and then providing the resulting figure to the figure canvas (matplotlib.backends.backend_qt4agg.FigureCanvasQTAgg). The following is an example of how to do that. \nCode: from PyQt4 import QtGui, QtCore\nfrom matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas\nimport sys\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntips = sns.load_dataset(\"tips\")\n\n\ndef seabornplot():\n    g = sns.FacetGrid(tips, col=\"sex\", hue=\"time\", palette=\"Set1\",\n                                hue_order=[\"Dinner\", \"Lunch\"])\n    g.map(plt.scatter, \"total_bill\", \"tip\", edgecolor=\"w\")\n    return g.fig\n\n\nclass MainWindow(QtGui.QMainWindow):\n    send_fig = QtCore.pyqtSignal(str)\n\n    def __init__(self):\n        super(MainWindow, self).__init__()\n\n        self.main_widget = QtGui.QWidget(self)\n\n        self.fig = seabornplot()\n        self.canvas = FigureCanvas(self.fig)\n\n        self.canvas.setSizePolicy(QtGui.QSizePolicy.Expanding,\n                      QtGui.QSizePolicy.Expanding)\n        self.canvas.updateGeometry()\n        self.button = QtGui.QPushButton(\"Button\")\n        self.label = QtGui.QLabel(\"A plot:\")\n\n        self.layout = QtGui.QGridLayout(self.main_widget)\n        self.layout.addWidget(self.button)\n        self.layout.addWidget(self.label)\n        self.layout.addWidget(self.canvas)\n\n        self.setCentralWidget(self.main_widget)\n        self.show()\n\n\nif __name__ == '__main__':\n    app = QtGui.QApplication(sys.argv)\n    win = MainWindow()\n    sys.exit(app.exec_())\n\nText: While this works fine, it is a bit questionable, if it's useful at all. Creating a plot inside a GUI in most cases has the purpose of beeing updated depending on user interactions. In the example case from above, this is pretty inefficient, as it would require to create a new figure instance, create a new canvas with this figure and replace the old canvas instance with the new one, adding it to the layout. \nText: Note that this problematics is specific to those plotting functions in seaborn, which work on a figure level, like lmplot, factorplot, jointplot, FacetGrid and possibly others. Other functions like regplot, boxplot, kdeplot work on an axes level and accept a matplotlib axes object as argument (sns.regplot(x, y, ax=ax1)). \nText: A possibile solution is to first create the subplot axes and later plot to those axes, for example using the pandas plotting functionality. \nCode: df.plot(kind=\"scatter\", x=..., y=..., ax=...)\n\nText: where ax should be set to the previously created axes. This allows to update the plot within the GUI. See the example below. Of course normal matplotlib plotting (ax.plot(x,y)) or the use of the seaborn axes level function discussed above work equally well. \nCode: from PyQt4 import QtGui, QtCore\nfrom matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nimport sys\nimport seaborn as sns\n\ntips = sns.load_dataset(\"tips\")\n\nclass MainWindow(QtGui.QMainWindow):\n    send_fig = QtCore.pyqtSignal(str)\n\n    def __init__(self):\n        super(MainWindow, self).__init__()\n\n        self.main_widget = QtGui.QWidget(self)\n\n        self.fig = Figure()\n        self.ax1 = self.fig.add_subplot(121)\n        self.ax2 = self.fig.add_subplot(122, sharex=self.ax1, sharey=self.ax1)\n        self.axes=[self.ax1, self.ax2]\n        self.canvas = FigureCanvas(self.fig)\n\n        self.canvas.setSizePolicy(QtGui.QSizePolicy.Expanding, \n                                  QtGui.QSizePolicy.Expanding)\n        self.canvas.updateGeometry()\n\n        self.dropdown1 = QtGui.QComboBox()\n        self.dropdown1.addItems([\"sex\", \"time\", \"smoker\"])\n        self.dropdown2 = QtGui.QComboBox()\n        self.dropdown2.addItems([\"sex\", \"time\", \"smoker\", \"day\"])\n        self.dropdown2.setCurrentIndex(2)\n\n        self.dropdown1.currentIndexChanged.connect(self.update)\n        self.dropdown2.currentIndexChanged.connect(self.update)\n        self.label = QtGui.QLabel(\"A plot:\")\n\n        self.layout = QtGui.QGridLayout(self.main_widget)\n        self.layout.addWidget(QtGui.QLabel(\"Select category for subplots\"))\n        self.layout.addWidget(self.dropdown1)\n        self.layout.addWidget(QtGui.QLabel(\"Select category for markers\"))\n        self.layout.addWidget(self.dropdown2)\n\n        self.layout.addWidget(self.canvas)\n\n        self.setCentralWidget(self.main_widget)\n        self.show()\n        self.update()\n\n    def update(self):\n\n        colors=[\"b\", \"r\", \"g\", \"y\", \"k\", \"c\"]\n        self.ax1.clear()\n        self.ax2.clear()\n        cat1 = self.dropdown1.currentText()\n        cat2 = self.dropdown2.currentText()\n        print cat1, cat2\n\n        for i, value in enumerate(tips[cat1].unique().get_values()):\n            print \"value \", value\n            df = tips.loc[tips[cat1] == value]\n            self.axes[i].set_title(cat1 + \": \" + value)\n            for j, value2 in enumerate(df[cat2].unique().get_values()):\n                print \"value2 \", value2\n                df.loc[ tips[cat2] == value2 ].plot(kind=\"scatter\", x=\"total_bill\", y=\"tip\", \n                                                ax=self.axes[i], c=colors[j], label=value2)\n        self.axes[i].legend()   \n        self.fig.canvas.draw_idle()\n\n\nif __name__ == '__main__':\n    app = QtGui.QApplication(sys.argv)\n    win = MainWindow()\n    sys.exit(app.exec_())\n\nText: A final word about \nText: pyqtgraph \nText: : I wouldn't call pyqtgraph a wrapper for PyQt but more an extention. Although pyqtgraph ships with its own Qt (which makes it portable and work out of the box), it is also a package one can use from within PyQt. You can therefore add a \nText: GraphicsLayoutWidget \nText: to a PyQt layout simply by \nCode: self.pgcanvas = pg.GraphicsLayoutWidget()\nself.layout().addWidget(self.pgcanvas) \n\nText: The same holds for a MatplotlibWidget (mw = pg.MatplotlibWidget()). While you can use this kind of widget, it's merely a convenience wrapper, since all it's doing is finding the correct matplotlib imports and creating a Figure and a FigureCanvas instance. Unless you are using other pyqtgraph functionality, importing the complete pyqtgraph package just to save 5 lines of code seems a bit overkill to me. \nAPI:\nmatplotlib.figure.Figure\n","label":[[327,330,"Mention"],[6745,6769,"API"]],"Comments":[]}
{"id":59325,"text":"ID:41793987\nPost:\nText: The problem you face is that you try to assign the return of imshow (which is an AxesImage to an existing axes object. \nText: The correct way of plotting image data to the different axes in axarr would be \nCode: f, axarr = plt.subplots(2,2)\naxarr[0,0].imshow(image_datas[0])\naxarr[0,1].imshow(image_datas[1])\naxarr[1,0].imshow(image_datas[2])\naxarr[1,1].imshow(image_datas[3])\n\nText: The concept is the same for all subplots, and in most cases the axes instance provide the same methods than the pyplot (plt) interface. E.g. if ax is one of your subplot axes, for plotting a normal line plot you'd use ax.plot(..) instead of plt.plot(). This can actually be found exactly in the source from the page you link to. \nAPI:\nmatplotlib.image.AxesImage\n","label":[[105,114,"Mention"],[743,769,"API"]],"Comments":[]}
{"id":59326,"text":"ID:41898583\nPost:\nText: In case you want to see the plot inline, use \nCode: %matplotlib inline\n\nText: in the header (before the imports). \nText: If you want to show the graphic in a window, add the line \nCode: plt.show()\n\nText: at the end (make sure you have imported import plt as plt in the header). \nAPI:\nmatplotlib.pyplot\n","label":[[275,278,"Mention"],[308,325,"API"]],"Comments":[]}
{"id":59327,"text":"ID:41925325\nPost:\nText: You can obtain the xticklabels via ax.get_xticklabels(). This returns a list of Text instances. You can then select one of them and use text.set_color(\"red\") to colorize it. \nCode: ax.get_xticklabels()[-2].set_color(\"red\")\n\nText: The problem is that it's not intuitively clear which of the elements would be the one we are looking for. In this case it's the second last, since there is an empty ticklabel at the very end of the list. This may require to test a bit, or print them out before setting them. Also, if the plot is resized, such that more ticklabels appear on the axis, the formatted label might suddenly carry a different number than before. Such cases would require a bit more work to account for. \nText: Of course, appart from the color you can change every attribute of the \nText: text \nText: instance you like, \nCode: ax.get_xticklabels()[-2].set_color(\"white\")\nax.get_xticklabels()[-2].set_fontsize(14)\nax.get_xticklabels()[-2].set_weight(\"bold\")\nax.get_xticklabels()[-2].set_bbox(dict(facecolor=\"red\", alpha=0.9))\n\nAPI:\nmatplotlib.text.Text\n","label":[[104,108,"Mention"],[1062,1082,"API"]],"Comments":[]}
{"id":59328,"text":"ID:41978894\nPost:\nText: Pass the argument edgecolors='none' to plt.scatter. The patch boundary will not be drawn. Pass the argument marker='s' to plt.scatter. The marker style will be square. \nText: Then, we have, \nText: The source code, \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.random.random(10)\ny = np.random.random(10)\nz = np.random.random(10)\nplt.scatter(x, y, c = z, s=150, cmap = 'jet', edgecolors='none', marker='s')\nplt.show()  \n\nText: Refer to scatter for more information. \nAPI:\nmatplotlib.pyplot.scatter\n","label":[[478,485,"Mention"],[514,539,"API"]],"Comments":[]}
{"id":59329,"text":"ID:41979111\nPost:\nText: With regards to constructing the timedelta, datetime.timdelta() doesnt have a parameter to specify months, so its probably convenient to stick to pd.date_range(). However, I found that objects of type pandas.tslib.Timestamp dont play nice with matplotlib ticks so you could convert them to datetime.date objects like so \nCode: index_ = [pd.to_datetime(date, format='%Y-%m-%d').date() \n        for date in pd.date_range('2004-03-01', '2012-12-01', freq=\"M\")]\n\nText: Its possible to add gridlines and customise axes labels by first defining a matplotlib axes object, and then passing this to DataFrame.plot() \nCode: ax = plt.axes()\ndf2_.plot(ax=ax)\n\nText: Now you can add vertical gridlines to your plot \nCode: ax.xaxis.grid(True)\n\nText: And specify quarterly xticks labels by using MenthLocator and setting the interval to 3 \nCode: ax.xaxis.set_major_locator(dates.MonthLocator(interval=3))\n\nText: And finally, I found the ticks to be to be very crowded so I formatted them to get a nicer fit \nCode: ax.xaxis.set_major_formatter(dates.DateFormatter('%b %y'))\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=85, fontsize=8)\n\nText: To produce the following: \nAPI:\nmatplotlib.dates.MonthLocator\n","label":[[809,821,"Mention"],[1197,1226,"API"]],"Comments":[]}
{"id":59330,"text":"ID:41990355\nPost:\nText: There are several problems here: \nText: The range is supposed to be defined over integers; The number of items the range would generate is too large, you have to take huge steps; The formula takes x as a variable, but you seem to define a np.array(x_range) in y; and more importantly you use eval(..), but eval(..) usually a string or another object that can be parsed; you do not give graph(..) a formula as first element: before Python calls graph(..) it first evaluates the operands. \nText: In my opinion, the best way to achieve this is using a lambda-expression: \nText: import pyplot as plt import numpy as np def graph(formula, x_range): x = np.array(x_range) #^ use x as range variable y = formula(x) #^ ^call the lambda expression with x #| use y as function result plt.plot(x,y) plt.show() graph(lambda x : ((np.log(x)*10**6)\/np.log(2)) + 1, range(0, 435*10**15,10**12)) # ^use a lambda expression ^range over integers # take huge steps \nText: This generates the following image: \nText: EDIT: \nText: based on your comment you want time on the y-axis and the function on the x-axis, this can simply be achieved by assigning to the other variables like: \nText: import pyplot as plt import numpy as np def graph(formula, x_range): y = np.array(x_range) x = formula(y) plt.plot(x,y) plt.show() graph(lambda x : ((np.log(x)*10**6)\/np.log(2)) + 1, range(0,435*10**15,10**12)) \nText: Note that you do not need to change the name of the variable in the lambda-expression: indeed when you call the lambda expression, the local x will simply be the y of the caller. \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[606,612,"Mention"],[1199,1205,"Mention"],[1595,1612,"API"],[1613,1630,"API"]],"Comments":[]}
{"id":59331,"text":"ID:41994344\nPost:\nText: Use pcolormesh instead of matplotlib.axes.Axes.pcolorfast. The docstring of pcolorfast says that it is experimental and \nText: \"...it lacks support for log scaling of the axes...\", \nText: as of the current version 2.0.0. \nAPI:\nmatplotlib.pyplot.pcolormesh\n","label":[[28,38,"Mention"],[251,279,"API"]],"Comments":[]}
{"id":59332,"text":"ID:42079870\nPost:\nText: I discovered that IPython 4.2.1 actually does this wrong, which allowed me to bisect the IPython codebase and find the answer. \nText: IPython 5, which uses prompt-toolkit, has a special inputhook that it passes to prompt-toolkit's eventloop argument of run_application. IPython's inputhook is defined in IPython.terminal.pt_inputhooks.osx. The code does a bunch of ctypes calls into the macOS APIs (basically, to get the GUI eventloop). \nText: I don't know how to use this for the dummy REPL from my question, but I am actually using prompt-toolkit, so this is fine for me. To use it, use \nCode: from IPython.terminal.pt_inputhooks.osx import inputhook\nfrom prompt_toolkit.shortcuts import create_eventloop\n\n# <prompt-toolkit stuff>\n... \nrun_application(eventloop=create_eventloop(inputhook)\n\nText: You also still do need the matplotlib.interactive(True) call before importing pyplot to make the plots show automatically (otherwise you have to call plt.show() all the time, and, more importantly, the plots will block). \nAPI:\nmatplotlib.pyplot\n","label":[[901,907,"Mention"],[1050,1067,"API"]],"Comments":[]}
{"id":59333,"text":"ID:42102884\nPost:\nText: It looks like pyplot through various imports needs mlab.py which calls \"import csv\". This should find a file (that is not yours) called csv but since you have renamed your file to csv.py it is attempting to import that instead, overriding the required import and messing up the import for matplotlib.pyplot. \nAPI:\nmatplotlib.pyplot\n","label":[[38,44,"Mention"],[338,355,"API"]],"Comments":[]}
{"id":59334,"text":"ID:42170801\nPost:\nText: My suggestion would be to add import plt as plt at the beginning of the script and plt.ticklabel_format(useOffset=False) at the end. \nText: Due to the jointplot creating several axes, plt.ticklabel_format(useOffset=False) will only affect the last of them. \nText: An easy solution is to use \nCode: plt.rcParams['axes.formatter.useoffset'] = False\n\nText: just after the imports. This will turn the offset use off for the complete script. \nAPI:\nmatplotlib.pyplot\n","label":[[61,64,"Mention"],[467,484,"API"]],"Comments":[]}
{"id":59335,"text":"ID:42229589\nPost:\nText: You need to use the same color normalization for both plots. This can be accomplished by providing a Normalize instance to both plots using the norm keyword argument. \nCode: import matplotlib.pyplot as plt \nimport matplotlib.colors\nimport numpy as np\n\n#generate random matrix with min value of 1 and max value 5\nxx = np.random.choice(a = [1,2,3,4,5],p = [1\/5.]*5,size=(100,100))\n#generate x and y axis of the new dataframe\ndfxy = np.random.choice(range(20,80),p = [1\/float(len(range(20,80)))]*len(range(20,80)),size = (100,2))\n#generate z values of the dataframe with min value 10 and max value 15\ndfz = np.random.choice(a = np.linspace(0,7,10),size = 100)\n\n\nmi = np.min((dfz.min(), xx.min()))\nma = np.max((dfz.max(), xx.max()))\nnorm = matplotlib.colors.Normalize(vmin=mi,vmax=ma)\nplt.contourf(np.arange(100),np.arange(100),xx, norm=norm, cmap =\"jet\")\nplt.scatter(dfxy[:,0],dfxy[:,1],c=dfz,s=80, norm=norm, cmap =\"jet\", edgecolor=\"k\")\ncb = plt.colorbar()\n\nplt.show()\n\nText: Here both plots share the same color scheme and so a single colorbar can be used. \nAPI:\nmatplotlib.colors.Normalize\n","label":[[125,134,"Mention"],[1086,1113,"API"]],"Comments":[]}
{"id":59336,"text":"ID:42306833\nPost:\nText: You can just create the linear path between each of the pairs of points; combining that with mpl.animation.FuncAnimation would look like \nCode: import matplotlib.animation as animation\n\ndef update_plot(t):\n    interpolation = originalPoints*(1-t) + newPoints*t\n    scat.set_offsets(interpolation.T)\n    return scat,\n\nfig = plt.gcf()\nplt.scatter(originalPoints[0,:],originalPoints[1,:], color='red')\nplt.scatter(newPoints[0,:],newPoints[1,:], color='blue')\nscat = plt.scatter([], [], color='green')\nanimation.FuncAnimation(fig, update_plot, frames=np.arange(0, 1, 0.01))\n\nText: Edit: The edited question now asks for a non-linear interpolation instead; replacing update_plot with \nCode: noise = np.random.normal(0, 3, (2, 6))\ndef update_plot(t):\n    interpolation = originalPoints*(1-t) + newPoints*t + t*(1-t)*noise\n    scat.set_offsets(interpolation.T)\n    return scat,\n\nText: you get instead \nText: Edit #2: Regarding the query on interpolation of colors in the comment below, you can handle that through matplotlib.collections.Collection.set_color; concretely, replacing the above update_plot with \nCode: def update_plot(t):\n    interpolation = originalPoints*(1-t) + newPoints*t + t*(1-t)*noise\n    scat.set_offsets(interpolation.T)\n    scat.set_color([1-t, 0, t, 1])\n    return scat,\n\nText: we end up with \nText: Regarding the \"bonus\": The 3D case is mostly similar; \nCode: a = np.random.multivariate_normal([-3, -3, -3], np.identity(3), 20)\nb = np.random.multivariate_normal([3, 3, 3], np.identity(3), 20)\n\ndef update_plot(t):\n    interpolation = a*(1-t) + b*t\n    scat._offsets3d = interpolation.T\n    scat._facecolor3d = [1-t, 0, t, 1]\n    return scat,\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.scatter(a[:, 0], a[:, 1], a[:, 2], c='r')\nax.scatter(b[:, 0], b[:, 1], b[:, 2], c='b')\nscat = ax.scatter([], [], [])\nani = animation.FuncAnimation(fig, update_plot, frames=np.arange(0, 1, 0.01))\nani.save('3d.gif', dpi=80, writer='imagemagick')\n\nText: Edit regarding the comment below on how to do this in stages: One can achieve this by incorporating the composition of paths directly in update_plot: \nCode: a = np.random.multivariate_normal([-3, -3, -3], np.identity(3), 20)\nb = np.random.multivariate_normal([3, 3, 3], np.identity(3), 20)\nc = np.random.multivariate_normal([-3, 0, 3], np.identity(3), 20)\n\ndef update_plot(t):\n    if t < 0.5:\n        interpolation = (1-2*t)*a + 2*t*b\n        scat._facecolor3d = [1-2*t, 0, 2*t, 1]\n    else:\n        interpolation = (2-2*t)*b + (2*t-1)*c\n        scat._facecolor3d = [0, 2*t-1, 2-2*t, 1]\n    scat._offsets3d = interpolation.T\n    return scat,\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.scatter(a[:, 0], a[:, 1], a[:, 2], c='r')\nax.scatter(b[:, 0], b[:, 1], b[:, 2], c='b')\nax.scatter(c[:, 0], c[:, 1], c[:, 2], c='g')\nscat = ax.scatter([], [], [])\nani = animation.FuncAnimation(fig, update_plot, frames=np.arange(0, 1, 0.01))\nani.save('3d.gif', dpi=80, writer='imagemagick')\n\nAPI:\nmatplotlib.animation.FuncAnimation\n","label":[[117,144,"Mention"],[2979,3013,"API"]],"Comments":[]}
{"id":59337,"text":"ID:42370159\nPost:\nText: You want to have multiple consecutive python sessions share a common Matplotlib window. I see no way to share this windows from separate processes, especially when the original owner may terminate at any point in time. \nText: However, you could do something similar to your current workflow in which you have an external pdf viewer to view a output file which you update from multiple python instances. \nText: See this question\/answer on how to pickle a matplotlib figure: Store and reload pyplot object \nText: In every script, output your matplotlib figure as a pickled object, rather than calling plt.show(): \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\n\nax = plt.subplot(111)\nx = np.linspace(0, 10)\ny = np.exp(x)\nplt.plot(x, y)\npickle.dump(ax, file('myplot.pickle', 'w'))\n\nText: Then, start a dedicated python session which loads this pickled object and calls plt.show(). Have this script run in a loop, checking for updates of the pickled file on disk, and reloading when necessary: \nCode: import matplotlib.pyplot as plt\nimport pickle\n\nwhile True:\n   ax = pickle.load(file('myplot.pickle'))\n   plt.show()\n\nText: Alternative \nText: Instead of having separate python sessions, I usually have a single Ipython session in which I run different script. By selecting the same figure windows, I end up with a mostly similar setup as you describe, in which the same figure window is reused throughout the day. \nCode: import matplotlib.pyplot as plt\n\nfig = plt.figure(0)\nfig.clf()\n\nplt.show()\n\nAPI:\nmatplotlib.pyplot\n","label":[[514,520,"Mention"],[1546,1563,"API"]],"Comments":[]}
{"id":59338,"text":"ID:42398244\nPost:\nText: Here is an example on how to use a contour plot in an animation. It uses FuncAnimation which makes it easy to turn blitting on and off. With blit=True it runs at ~64 fps on my machine, without blitting ~55 fps. Note that the interval must of course allow for the fast animation; setting it to interval=10 (milliseconds) would allow for up to 100 fps, but the drawing time limits it to something slower than that. \nCode: import matplotlib.pyplot as plt\nimport matplotlib.animation\nimport numpy as np\nimport time\n\nx= np.linspace(0,3*np.pi)\nX,Y = np.meshgrid(x,x)\nf = lambda x,y, alpha, beta :(np.sin(X+alpha)+np.sin(Y*(1+np.sin(beta)*.4)+alpha))**2\nalpha=np.linspace(0, 2*np.pi, num=34)\nlevels= 10\ncmap=plt.cm.magma\n\n\nfig, ax=plt.subplots()\nprops = dict(boxstyle='round', facecolor='wheat')\ntimelabel = ax.text(0.9,0.9, \"\", transform=ax.transAxes, ha=\"right\", bbox=props)\nt = np.ones(10)*time.time()\np = [ax.contour(X,Y,f(X,Y,0,0), levels, cmap=cmap ) ]\n\ndef update(i):\n    for tp in p[0].collections:\n        tp.remove()\n    p[0] = ax.contour(X,Y,f(X,Y,alpha[i],alpha[i]), levels, cmap= cmap) \n    t[1:] = t[0:-1]\n    t[0] = time.time()\n    timelabel.set_text(\"{:.3f} fps\".format(-1.\/np.diff(t).mean()))  \n    return p[0].collections+[timelabel]\n\nani = matplotlib.animation.FuncAnimation(fig, update, frames=len(alpha), \n                                         interval=10, blit=True, repeat=True)\nplt.show()\n\nText: Note that in the animated gif above a slower frame rate is shown, since the process of saving the images takes a little longer. \nAPI:\nmatplotlib.animation.FuncAnimation\n","label":[[97,110,"Mention"],[1574,1608,"API"]],"Comments":[]}
{"id":59339,"text":"ID:42422642\nPost:\nText: As seen in the documentation of matplotlib.widgets.CheckButtons, the labels, the button rectangles and the lines (of the marker) can be accessed from the class instance. With check = CheckButtons(..) \nText: check.rectangles returns a list of the buttons' backgrounds as mpl.patches.Rectangle check.labels returns a list of the labels as mtext.Text check.lines returns a list of tuples of two mpl.lines.Line2D which serve as the markers. \nText: All of them have set_alpha methods. \nText: To set the background the easiest way is to provide a color with an alpha value already being set, like col = (0,0,1,0.2) where the last value is the alpha of the blue color. This can be set to the checkbutton axes using the facecolorargument. \nText: Here is a complete example. \nCode: import matplotlib.pyplot as plt\nfrom matplotlib.widgets import CheckButtons\n\nfig= plt.figure(figsize=(4,1.5))\nax = plt.axes([0.4, 0.2, 0.4, 0.6] )\nax.plot([2,3,1])\ncol = (0,0.3,0.75,0.2)\nrax = plt.axes([0.1, 0.2, 0.2, 0.6], facecolor=col )\ncheck = CheckButtons(rax, ('red', 'blue', 'green'), (1,0,1))\nfor r in check.rectangles:\n    r.set_facecolor(\"blue\") \n    r.set_edgecolor(\"k\")\n    r.set_alpha(0.2) \n\n[ll.set_color(\"white\") for l in check.lines for ll in l]\n[ll.set_linewidth(3) for l in check.lines for ll in l]\nfor i, c in enumerate([\"r\", \"b\", \"g\"]):\n    check.labels[i].set_color(c)\n    check.labels[i].set_alpha(0.7)\n\nplt.show()\n\nAPI:\nmatplotlib.patches.Rectangle\nmatplotlib.text.Text\nmatplotlib.lines.Line2D\n","label":[[294,315,"Mention"],[361,371,"Mention"],[416,432,"Mention"],[1440,1468,"API"],[1469,1489,"API"],[1490,1513,"API"]],"Comments":[]}
{"id":59340,"text":"ID:42448458\nPost:\nText: I tried to install tkinter package for python2.7.5 from the following link: tkinter package Also I found there is dependency library libTix.so()(64bit) for tkinter package and i got it from the following link: libTix.so()(64bit) package after that i installed both then I could import Tkinter and import pyplot as plt with no errors. \nAPI:\nmatplotlib.pyplot\n","label":[[328,334,"Mention"],[364,381,"API"]],"Comments":[]}
{"id":59341,"text":"ID:42451128\nPost:\nText: In Matplotlib, one way to \"remove\" points visually in a plot is to set the affected points to numpy.nan. The effect of this is that the points before and after numpy.nan will show a gap which is what I believe you are after. \nText: Therefore for your arrays, find any values that are negative or 0 and set them to numpy.nan before plotting. Because you are computing histograms, these should never produce values that are negative so you are really only checking for bins that are equal to 0 instead. \nText: One thing you need to make sure is to change the type of your arrays to float. numpy.nan only exists for floating-point arrays. \nText: If you also want to plot each of them to add a legend, simply plot each array one at a time on the same figure by calling plot three times then add your legend, then show the plot: \nCode: plt.figure(1)\nplt.figure(figsize=(9,7))\nhist1, bins1 = np.histogram(returns_assetA_daily_mat, bins=20)\nhist2, bins2 = np.histogram(returns_assetA_weekly_mat, bins=20)\nhist3, bins3 = np.histogram(returns_assetA_monthly_mat, bins=20)\nhist1 = hist1\/len(returns_assetA_daily_mat)\nhist2 = hist2\/len(returns_assetA_weekly_mat)\nhist3 = hist3\/len(returns_assetA_monthly_mat)\nbins1 = 0.5 * (bins1[1:] + bins1[:-1])\nbins2 = 0.5 * (bins2[1:] + bins2[:-1])\nbins3 = 0.5 * (bins3[1:] + bins3[:-1])\n\n# New code\nhist1_new = hist1.astype(np.float)\nhist2_new = hist2.astype(np.float)\nhist3_new = hist3.astype(np.float)\nhist1_new[hist1 <= 0] = np.nan\nhist2_new[hist2 <= 0] = np.nan\nhist3_new[hist3 <= 0] = np.nan\n\n# New - Plot the three graphs separately for making the legend\n# Also plot the NaN versions\nplt.plot(bins1, hist1_new, label='Daily')\nplt.plot(bins2, hist2_new, label='Weekly')\nplt.plot(bins3, hist3_new, label='Monthly')\n\nplt.yscale('log')\nplt.xlabel('Log-Returns')\nplt.ylabel('Relative Frequency')\nplt.title('Original for Asset A')\nplt.legend() # Added for the legend\nplt.show()\n\nText: I don't have your data, but I can show you a toy example of this working: \nCode: # Import relevant packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create array from 0 to 8 for the horizontal axis\nx = np.arange(9)\n\n# Create test array with some zero, positive and negative values\ny = np.array([1, 2, 3, 0, -1, -2, 1, 2, -1])\n\n# Create a figure with two graphs in one row\nplt.subplot(1, 2, 1)\n\n# Graph the data normally\nplt.plot(x, y)\n\n# Visually remove those points that are zero or negative\ny2 = y.astype(np.float)\ny2[y2 <= 0] = np.nan\n\n# Plot these points now\nplt.subplot(1, 2, 2)\nplt.plot(x, y2)\n\n# Adjust the x and y limits (see further discussion below)\nplt.xlim(0, 8)\nplt.ylim(-1, 3)\n\n# Show the figure\nplt.show()\n\nText: Note that the second plot I enforce that the x and y limits are the same as the first plot because we are visually removing the points and so the axes will automatically adjust. We get: \nAPI:\nmatplotlib.pyplot.plot\n","label":[[789,793,"Mention"],[2870,2892,"API"]],"Comments":[]}
{"id":59342,"text":"ID:42456300\nPost:\nText: iaread or mpl.pyplot.imread read the image as unsigned integer array. You then implicitely convert it to float. \nText: mpl.pyplot.imshow interpretes arrays in both formats differently. \nText: float arrays are interpreted between 0.0 (no color) and 1.0 (full color). integer arrays are interpreted between 0 and 255. \nText: The two options you have are thus: \nText: Use an integer array test_imgs = np.empty((5,32,32,3), dtype=np.uint8) divide the array by 255. prior to plotting: test_imgs = test_imgs\/255. \nAPI:\nmatplotlib.image.imread\nmatplotlib.pyplot.imread\nmatplotlib.pyplot.imshow\n","label":[[24,30,"Mention"],[34,51,"Mention"],[143,160,"Mention"],[537,560,"API"],[561,585,"API"],[586,610,"API"]],"Comments":[]}
{"id":59343,"text":"ID:42565258\nPost:\nText: This old post popped up while I was struggling to get some matplotlib animations exported as an animated gif. It's not rocket science, but still somewhat of a hassle to set up the first time around. I'll leave this post for future reference. \nText: As for the specifics when setting up this environment on a cluster, you'll have to translate each step for the specific needs for your server I guess. Animated gif is at least one of the animation options offered by the ImageMagickWriter, but there may be more. \nText: How-to \nText: Get the ImageMagick binaries Set an environment variable MAGICK_HOME pointing to the '<your-install-dir>\\modules\\coders' folder (see the docs. When on windows, remember to log off and back on) point matplotlib to the install dir of ImageMagick: (mpl docs on rcParams) import pyplol as plt plt.rcParams['animation.convert_path'] = '<your-install-dir>\/magick.exe' Export your animation! (mpl docs on ImageMagickFileWriter) writer = ImageMagickFileWriter() your_animation.save('location.gif', writer=writer) \nText: Note that by default, errors thrown by ImageMagick will not reach you, apart from a nonzero return code. To get more extensive feedback, you'll need something like \nCode: import matplotlib as mpl\nmpl.verbose.set_level(\"helpful\")\n\nText: Bonus \nText: As a bonus, here's a proof-of-life: \nAPI:\nmatplotlib.pyplot\n","label":[[831,837,"Mention"],[1359,1376,"API"]],"Comments":[]}
{"id":59344,"text":"ID:42640221\nPost:\nText: I think you are looking for plt.hist(grey). Note that the usual convention is import pyplot as plt. It is good to stick to commonly used styles! \nText: Lastly, your image conversion code can be simplified as: \nCode: grey = 0.299*img[:,:,0] + 0.587*img[:,:,0] + 0.114*img[:,:,0]\n\nAPI:\nmatplotlib.pyplot\n","label":[[109,115,"Mention"],[308,325,"API"]],"Comments":[]}
{"id":59345,"text":"ID:42648286\nPost:\nText: The idea to create a legend entry would be to draw the shapes as polygons, which can then be added to the legend. Therfore we would first deactivate drawbounds, m.readshapefile(fn, 'shf', drawbounds = False). We can then create a Polygon from the shapefile and add it to the axes, plt.gca().add_artist(polygon). \nText: The legend can then be updated using this polygon \nCode: handles, labels = plt.gca().get_legend_handles_labels()\nhandles.extend([polygon])  \nlabels.extend([\"Name of the shape\"])                     \nplt.legend(handles=handles, labels=labels)\n\nText: Here is now some code in action, which produces the following images. It uses the ne_10m_admin_0_countries file. \nCode: from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nimport numpy as np\n\nm = Basemap(llcrnrlon=-10,llcrnrlat=35,urcrnrlon=35,urcrnrlat=60.,\n             resolution='i', projection='tmerc', lat_0 = 48.9, lon_0 = 15.3)\n\nm.drawcoastlines()\nm.drawcountries(zorder=0, color=(.9,.9,.9), linewidth=1)\n\nfn = r\"ne_10m_admin_0_countries\\ne_10m_admin_0_countries\"\nm.readshapefile(fn, 'shf', drawbounds = False)\n\n#Madrid\nx,y = m([-3.703889],[40.4125])\nm.plot(x,y, marker=\"o\", color=\"blue\", label=\"Madrid\", ls=\"\")\n\n# some countries\ncountries = ['Switzerland', 'Ireland', \"Belgium\"]\ncolors= {'Switzerland':\"red\", 'Ireland':\"orange\", 'Belgium' : \"purple\"}\nshapes = {}\nfor info, shape in zip(m.shf_info, m.shf):\n    if info['NAME'] in countries:\n        p= Polygon(np.array(shape), True, facecolor= colors[info['NAME']], \n                   edgecolor='none', alpha=0.7, zorder=2)\n        shapes.update({info['NAME'] : p})\n\nfor country in countries:\n    plt.gca().add_artist(shapes[country]) \n\n\n# create legend, by first getting the already present handles, labels\nhandles, labels = plt.gca().get_legend_handles_labels()\n# and then adding the new ones\nhandles.extend([shapes[c] for c in countries])  \nlabels.extend(countries)                     \nplt.legend(handles=handles, labels=labels, framealpha=1.)\n\nplt.show()\n\nText: Now because we already have a polygon with the shape, why not make the legend a bit more fancy, by directly plotting the shape into the legend. This can be done as follows. \nCode: from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nimport numpy as np\n\nm = Basemap(llcrnrlon=-10,llcrnrlat=35,urcrnrlon=35,urcrnrlat=60.,\n             resolution='i', projection='tmerc', lat_0 = 48.9, lon_0 = 15.3)\n\nm.drawcoastlines()\n\nfn = r\"ne_10m_admin_0_countries\\ne_10m_admin_0_countries\"\nm.readshapefile(fn, 'shf', drawbounds = False)\n\n#Madrid\nx,y = m([-3.703889],[40.4125])\nm.plot(x,y, marker=\"o\", color=\"blue\", label=\"Madrid\", ls=\"\")\n\ncountries = ['Switzerland', 'Ireland', \"Belgium\"]\ncolors= {'Switzerland':\"red\", 'Ireland':\"orange\", 'Belgium' : \"purple\"}\nshapes = {}\nfor info, shape in zip(m.shf_info, m.shf):\n    if info['NAME'] in countries:\n        p= Polygon(np.array(shape), True, facecolor= colors[info['NAME']], \n                   edgecolor='none', alpha=0.7, zorder=2)\n        shapes.update({info['NAME'] : p})\n\nfor country in countries:\n    plt.gca().add_artist(shapes[country]) \n\n\nclass PolygonN(object):\n    def legend_artist(self, legend, orig_handle, fontsize, handlebox):\n        x0, y0 = handlebox.xdescent, handlebox.ydescent\n        width, height = handlebox.width, handlebox.height\n        aspect= height\/float(width)\n        verts = orig_handle.get_xy()\n        minx, miny = verts[:,0].min(), verts[:,1].min()\n        maxx, maxy = verts[:,0].max(), verts[:,1].max()\n        aspect= (maxy-miny)\/float((maxx-minx))\n        nvx = (verts[:,0]-minx)*float(height)\/aspect\/(maxx-minx)-x0\n        nvy = (verts[:,1]-miny)*float(height)\/(maxy-miny)-y0\n\n        p = Polygon(np.c_[nvx, nvy])\n        p.update_from(orig_handle)\n        p.set_transform(handlebox.get_transform())\n\n        handlebox.add_artist(p)\n        return p\n\nhandles, labels = plt.gca().get_legend_handles_labels()\nhandles.extend([shapes[c] for c in countries])  \nlabels.extend(countries)     \nplt.legend(handles=handles, labels=labels, handleheight=3, handlelength=3, framealpha=1.,\n           handler_map={Polygon: PolygonN()} )\n\nplt.show()\n\nAPI:\nmatplotlib.patches.Polygon\n","label":[[254,261,"Mention"],[4256,4282,"API"]],"Comments":[]}
{"id":59346,"text":"ID:42650271\nPost:\nText: Obtaining a color from a colormap is done by calling the colormap with the respective value between 0 and 1. If the colormap shall represent some data, there usually is a Normalization somewhere, matplotlib.colors.Normalize, which can be used to obtain the normalized value from the data. Finally, to_hex returns a color in hexadecimal format. \nCode: import matplotlib.cm as cm\nimport matplotlib.colors \n\ncmap = cm.coolwarm\nnorm = matplotlib.colors.Normalize(vmin=-20, vmax=10)\nvariable = matplotlib.colors.to_hex(cmap(norm(5))) \nprint variable\n\nAPI:\nmatplotlib.colors.to_hex\n","label":[[322,328,"Mention"],[575,599,"API"]],"Comments":[]}
{"id":59347,"text":"ID:42658124\nPost:\nText: You can subclass mticker.ScalarFormatter and fix the orderOfMagnitude attribute to the number you like (in this case -4). In the same way you can fix the format to be used. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker\n\nclass OOMFormatter(matplotlib.ticker.ScalarFormatter):\n    def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n        self.oom = order\n        self.fformat = fformat\n        matplotlib.ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n    def _set_order_of_magnitude(self):\n        self.orderOfMagnitude = self.oom\n    def _set_format(self, vmin=None, vmax=None):\n        self.format = self.fformat\n        if self._useMathText:\n            self.format = r'$\\mathdefault{%s}$' % self.format\n\n\nx = np.linspace(1,9,9)\ny1 = x*10**(-4)\ny2 = x*10**(-3)\n\nfig, ax = plt.subplots(2,1,sharex=True)\n\nax[0].plot(x,y1)\nax[1].plot(x,y2)\n\nfor axe in ax:\n    axe.yaxis.set_major_formatter(OOMFormatter(-4, \"%1.1f\"))\n    axe.ticklabel_format(axis='y', style='sci', scilimits=(-4,-4))\n\nplt.show()\n\nText: While this may seem complicated at first sight the only thing it really does is overwrite the private methods _set_orderOfMagnitude and _set_format and thereby prevent them from doing some sophisticated stuff in the background that we don't want. Because in the end, all we need is that, independent of what happens internally, self.orderOfMagnitude is always -4 and self.format is always \"%1.1f\". \nText: Note: In matplotlib < 3.1 the class needed to look like \nCode: class OOMFormatter(matplotlib.ticker.ScalarFormatter):\n        def __init__(self, order=0, fformat=\"%1.1f\", offset=True, mathText=True):\n            self.oom = order\n            self.fformat = fformat\n            matplotlib.ticker.ScalarFormatter.__init__(self,useOffset=offset,useMathText=mathText)\n        def _set_orderOfMagnitude(self, nothing=None):\n            self.orderOfMagnitude = self.oom\n        def _set_format(self, vmin=None, vmax=None):\n            self.format = self.fformat\n            if self._useMathText:\n                self.format = '$%s$' % matplotlib.ticker._mathdefault(self.format)\n\nAPI:\nmatplotlib.ticker.ScalarFormatter\n","label":[[41,64,"Mention"],[2200,2233,"API"]],"Comments":[]}
{"id":59348,"text":"ID:42690487\nPost:\nText: Quoting from the Button documentation: \nText: For the button to remain responsive you must keep a reference to it. \nText: In your script you overwrite the button variable containing the first button with the second button. So you loose the reference to the first button. \nText: Calling the first button button1 and the second button2 solves the problem. \nAPI:\nmatplotlib.widgets.Button\n","label":[[41,47,"Mention"],[384,409,"API"]],"Comments":[]}
{"id":59349,"text":"ID:42749057\nPost:\nText: matplotlib.pyplot.xlim, ylim are functions. You should call them instead of assigning to them: \nCode: plt.ylim(-10,10)\nplt.xlim(-10,10)\n\nAPI:\nmatplotlib.pyplot.ylim\n","label":[[48,52,"Mention"],[166,188,"API"]],"Comments":[]}
{"id":59350,"text":"ID:42806555\nPost:\nText: You can use DateFormatter to achieve this: \nCode: import matplotlib.dates as dates\ndf['DateTime'] = pd.to_datetime(df['DateTime'], format='%Y-%m-%d %H:%M:%S')\ndf.plot(x='DateTime', y='Value')\nformatter = dates.DateFormatter('%Y-%m-%d %H:%M:%S') \nplt.gcf().axes[0].xaxis.set_major_formatter(formatter)\n\nText: results in: \nText: to_datetime converts strings to datetime64 dtype, it doesn't affect the display in pandas or for matplotlib, so the correct method is above. \nText: However, you could generate date strings using dt.strftime to add a column of desired date strings but this gives you a column of strings which is not as useful in my opinion: \nCode: In [39]:\ndf['DateStrings'] = df['DateTime'].dt.strftime('%Y-%m-%d %H:%M:%S')\ndf\n\nOut[39]:\n             DateTime  Value          DateStrings\n0 2016-05-17 22:50:27   1914  2016-05-17 22:50:27\n1 2016-05-17 22:55:27   1597  2016-05-17 22:55:27\n2 2016-05-17 23:00:27   1429  2016-05-17 23:00:27\n3 2016-05-17 23:05:27   1462  2016-05-17 23:05:27\n4 2016-05-17 23:10:27   2038  2016-05-17 23:10:27\n\nText: this results in the plot: \nCode: df.plot(x='DateStrings', y='Value')\nplt.show()\n\nText: You can see though that the x-axis labels are duff, you'd need to rotate them which is handled automatically using the former method \nAPI:\nmatplotlib.dates.DateFormatter\n","label":[[36,49,"Mention"],[1305,1335,"API"]],"Comments":[]}
{"id":59351,"text":"ID:42810740\nPost:\nText: I would guess that something like the following happened, although I cannot be certain because no exact code is given for the two cases in question. \nText: The first error probably came up when you (undeliberately) tried to use a backend which is not installed. \nCode: import matplotlib #everthing works fine, no backend selected yet\nmatplotlib.use(\"TkAgg\") #backend selected, would still be fine even if Tk wasnt installed.\nimport matplotlib.pyplot as plt # backend registered. Still not used.\nplt.plot(..)  # now the backend will be used. Clash! \n              # Backend cannot be used, since underlying library is missing\n\nText: Later on you tried to use a different backend. \nCode: import matplotlib #everthing works fine, no backend selected yet\nmatplotlib.use(\"TkAgg\") #backend selected, would still be fine even if Tk wasnt installed.\nimport matplotlib.pyplot as plt # backend registered. Still not used.\n....\nimport matplotlib\nmatplotlib.use(\"Qt5Agg\") # backend \"TkAgg\" has already been registered. \n                         # You cannot change it anymore.\n\nText: And this is what got you \nText: This call to matplotlib.use() has no effect because the backend has already been chosen; matplotlib.use() must be called before pylab, matplotlib.pyplot, or matplotlib.backends is imported for the first time. \nText: Solution: \nText: Always stick to this order if you need to use a specific backend. \nText: import matplotlib matplotlib.use(\"TkAgg\") import plt as plt Plotting , e.g. plt.plot(..) \nText: If you don't need a specific backend, which should be the case most of the time, let matplotlib decide which one to choose - omitt step 1 & 2!. Especially when sharing code you cannot even be sure that the library for the required backend is installed on the other user's system. \nAPI:\nmatplotlib.pyplot\n","label":[[1483,1486,"Mention"],[1816,1833,"API"]],"Comments":[]}
{"id":59352,"text":"ID:42878538\nPost:\nText: The Arrow indeed does not have a method to update its position. While it would be possible to change its transform dynamically, I guess the easiest solution is to simply remove it and add a new Arrow in each step of the animation. \nCode: from matplotlib import pyplot as plt\nfrom matplotlib.patches import Rectangle, Arrow\nimport numpy as np\n\nnmax = 9\nxdata = range(nmax)\nydata = np.random.random(nmax)\n\nplt.ion()\nfig, ax = plt.subplots()\nax.set_aspect(\"equal\")\nax.plot(xdata, ydata, 'o-')\nax.set_xlim(-1,10)\nax.set_ylim(-1,4)\n\n\nrect = Rectangle((0, 0), nmax, 1, zorder=10)\nax.add_patch(rect)\n\nx0, y0 = 5, 3\narrow = Arrow(1,1,x0-1,y0-1, color=\"#aa0088\")\n\na = ax.add_patch(arrow)\n\nplt.draw()\n\nfor i in range(nmax):\n    rect.set_x(i)\n    rect.set_width(nmax - i)\n\n    a.remove()\n    arrow = Arrow(1+i,1,x0-i+1,y0-1, color=\"#aa0088\")\n    a = ax.add_patch(arrow)\n\n    fig.canvas.draw_idle()\n    plt.pause(0.4)\n\nplt.waitforbuttonpress()    \nplt.show()\n\nAPI:\nmatplotlib.patches.Arrow\n","label":[[28,33,"Mention"],[977,1001,"API"]],"Comments":[]}
{"id":59353,"text":"ID:42879231\nPost:\nText: The ticklabels may change over the course of the script. It is therefore advisable to set their color at the very end of the script, when no changes are made any more. \nText: from __future__ import division import plt as plt import numpy as np def AutoLabelBarVals(bars): ax=plt.gca() (y_bottom, y_top) = ax.get_ylim() y_height = y_top - y_bottom for bar in bars: height = bar.get_height() label_position = height + (y_height * 0.01) ax.text(bar.get_x() + bar.get_width()\/2., label_position, '%d' % int(height), ha='center', va='bottom') plt.figure() languages =['English','Hindi','Mandarin','Spanish','German'] pos = np.arange(len(languages)) percent_spoken = [372\/6643, 260\/6643, 898\/6643, 437\/6643, 76.8\/6643] percent_spoken = [x*100 for x in percent_spoken] bar_colors = ['#BAD3C8']*(len(languages)-1) bar_colors.insert(2,'#0C82D3') bars = plt.bar(pos, percent_spoken, align='center', color=bar_colors) plt.gca().yaxis.label.set_color('grey') plt.gca().tick_params(axis='x', labelcolor='grey') #Works plt.xticks(pos, languages) plt.title('Speakers of Select Languages as % of World Population') plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on', color='grey') for spine in plt.gca().spines.values(): spine.set_visible(False) AutoLabelBarVals(bars) plt.gca().get_xticklabels()[1].set_color('red') plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[238,241,"Mention"],[1391,1408,"API"]],"Comments":[]}
{"id":59354,"text":"ID:42884505\nPost:\nText: From the Cursor documentation: \nText: For the cursor to remain responsive you must keep a reference to it. \nText: The easiest way to do this is to assign it to a class variable, self.cursor=Cursor(..). \nCode: def plot(self):\n    ''' plot some random stuff '''\n    data = [random.random() for i in range(25)]\n    ax = self.figure.add_subplot(111)\n    #ax.hold(False)  <- don't use ax.hold!\n    ax.plot(data, '*-')\n    self.cursor = Cursor(ax, lw = 2)\n    self.canvas.draw()\n\nAPI:\nmatplotlib.widgets.Cursor\n","label":[[33,39,"Mention"],[503,528,"API"]],"Comments":[]}
{"id":59355,"text":"ID:42884659\nPost:\nText: One possible way of obtaining slices of apples is of course to prepare an apple pie and later pick all the apples from the pie. The easier method would surely be not to make the cake at all. \nText: So, the obvious way not to have a histogram plot in the figure is not to plot it in the first place. Instead calculate the histogram using numpy.histogram (which is anyways the function called by plt.hist), and plot its output to the figure. \nText: import numpy as np import mpl.pyplot as plt data = {'first': np.random.normal(size=[100]), 'second': np.random.normal(size=[100]), 'third': np.random.normal(size=[100])} for data_set, values in sorted(data.items()): y_values, bin_edges = np.histogram(values, bins=20) bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1]) plt.plot(bin_centers, y_values, '-', label=data_set) legend = plt.legend() plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[497,507,"Mention"],[882,899,"API"]],"Comments":[]}
{"id":59356,"text":"ID:42893081\nPost:\nText: You cannot fix the runtime warning. It's a warning based on the fact that there are nan values in the array. \nText: In order to still get a colorcoded surface plot, you can however use a mpl.colors.Normalize instance to tell the surface plot which colors to use. \nText: See full code below: \nCode: from mpl_toolkits.mplot3d import axes3d\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nimport matplotlib.colors\n\nfun = lambda x: np.sin(x[0])*np.exp(1-np.cos(x[1]))**2 + np.cos(x[1])*np.exp(1-np.sin(x[0]))**2 + (x[0]-x[1])**2\n\nfig = plt.figure(figsize=(8, 5))\nax = fig.gca(projection='3d')\nx = np.arange(-6, 6, 3e-2)\ny = np.arange(-6, 6, 3e-2)\n\n# A constraint on x and y\nx, y = np.meshgrid(x, y)\nr2 = (x+5)**2 + (y+5)**2\nscope = r2 < 25\n# Mask is the cause of the problem\nx[scope] = np.nan\ny[scope] = np.nan\nz = fun(np.array([x, y]))\n\nnorm = matplotlib.colors.Normalize(vmin=-120, vmax=120)\ncm.jet.set_under((0,0,0,0))\nax.contourf(x, y, z, offset=-120, cmap=cm.jet, norm=norm)\nsurf=ax.plot_surface(x, y, z, cmap=cm.jet, norm=norm)\nfig.colorbar(surf)\n#ax.view_init(elev=30, azim=60)\nplt.show()\n\nAPI:\nmatplotlib.colors.Normalize\n","label":[[211,231,"Mention"],[1151,1178,"API"]],"Comments":[]}
{"id":59357,"text":"ID:42927880\nPost:\nText: Your plot is correct, although you might simplify the normalization using a mpl.colors.Normalize instance. \nCode: norm = matplotlib.colors.Normalize(vmin=V.min().min(), vmax=V.max().max())\nax.plot_surface(X, Y, Z, facecolors=plt.cm.jet(norm(V)))\nm = cm.ScalarMappable(cmap=plt.cm.jet, norm=norm)\nm.set_array([])\nplt.colorbar(m)\n\nText: The point why you don't see the maximum value of 10.15 on the grid, is a different one: \nText: When having N points along one dimension, the plot has (N-1) faces. That means that the last row and column of the input color array are simply not plotted. \nText: This can be seen in the following picture, where a 3x3 matrix is plotted, resulting in 2x2 faces. They are colorized according to the respective values in a color array, such that the first face has the color given by the first element in the array etc. For the last elements there is no face to color left. \nText: Code to reproduce this plot: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.colors\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nx = np.arange(3)\nX,Y = np.meshgrid(x,x)\nZ = np.ones_like(X)\n\nV = np.array([[3,2,2],[1,0,3],[2,1,0]])\n\nnorm = matplotlib.colors.Normalize(vmin=0, vmax=3)\nax.plot_surface(X, Y, Z, facecolors=plt.cm.jet(norm(V)), shade=False)\n\nm = cm.ScalarMappable(cmap=plt.cm.jet, norm=norm)\nm.set_array([])\nplt.colorbar(m)\n\nax.set_xlabel('x')\nax.set_ylabel('y')\n\nplt.show()\n\nAPI:\nmatplotlib.colors.Normalize\n","label":[[100,120,"Mention"],[1538,1565,"API"]],"Comments":[]}
{"id":59358,"text":"ID:43023727\nPost:\nText: By default, plt.matshow() produces its own figure, so in combination with plt.figure() two figures will be created and the one that hosts the matshow plot is not the one that has the figsize set. \nText: There are two options: \nText: Use the fignum argument plt.figure(figsize=(10,5)) plt.matshow(d.corr(), fignum=1) Plot the matshow using mpl.axes.Axes.matshow instead of pyplot.matshow. fig, ax = plt.subplots(figsize=(10,5)) ax.matshow(d.corr()) \nAPI:\nmatplotlib.axes.Axes.matshow\n","label":[[363,384,"Mention"],[478,506,"API"]],"Comments":[]}
{"id":59359,"text":"ID:43080464\nPost:\nText: You would create a subplot grid where the width- and height ratios between the subplots correspond to the number of pixels in the respective dimension. You can then add respective plots to those subplots. In the code below I used an imshow plot, because I find it more intuitive to have one pixel per item in the array (instead of one less). \nText: In order to have the colorbar represent the colors accross the different subplots, one can use a Normalize instance, which is provided to each of the subplots, as well as the manually created ScalarMappable for the colorbar. \nCode: import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nm = np.random.rand(10,10)\nx = np.random.rand(1,m.shape[1])\ny = np.random.rand(m.shape[0],1)\n\nnorm = matplotlib.colors.Normalize(vmin=0, vmax=1)\ngrid = dict(height_ratios=[1, m.shape[0]], width_ratios=[1,m.shape[0], 0.5 ])\nfig, axes = plt.subplots(ncols=3, nrows=2, gridspec_kw = grid)\n\naxes[1,1].imshow(m, aspect=\"auto\", cmap=\"viridis\", norm=norm)\naxes[0,1].imshow(x, aspect=\"auto\", cmap=\"viridis\", norm=norm)\naxes[1,0].imshow(y, aspect=\"auto\", cmap=\"viridis\", norm=norm)\n\naxes[0,0].axis(\"off\")\naxes[0,2].axis(\"off\")\n\naxes[1,1].set_xlabel('Number 1')\naxes[1,1].set_ylabel('Number 2')\nfor ax in [axes[1,1], axes[0,1], axes[1,0]]:\n    ax.set_xticks([]); ax.set_yticks([])\n\nsm = matplotlib.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\nsm.set_array([])\n\nfig.colorbar(sm, cax=axes[1,2]) \n\nplt.show()\n\nAPI:\nmatplotlib.colors.Normalize\n","label":[[470,479,"Mention"],[1476,1503,"API"]],"Comments":[]}
{"id":59360,"text":"ID:43136449\nPost:\nText: From the NumPy documentation: \nText: All but the last (righthand-most) bin is half-open. In other words, if bins is: [1, 2, 3, 4] then the first bin is [1, 2) (including 1, but excluding 2) and the second [2, 3). The last bin, however, is [3, 4], which includes 4. \nText: hits uses this NumPy method. \nAPI:\nmatplotlib.pyplot.hist\n","label":[[296,300,"Mention"],[331,353,"API"]],"Comments":[]}
{"id":59361,"text":"ID:43206271\nPost:\nText: You would need to use a mpatches.Polygon and define the corners yourself. \nCode: import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfig = plt.figure()\nax = fig.add_subplot(111, aspect='equal')\n\n# Parallelogram\nx = [0.3,0.6,.7,.4]\ny = [0.4,0.4,0.6,0.6]\nax.add_patch(patches.Polygon(xy=list(zip(x,y)), fill=False))\n\n# Trapez\nx = [0.3,0.6,.5,.4]\ny = [0.7,0.7,0.9,0.9]\nax.add_patch(patches.Polygon(xy=list(zip(x,y)), fill=False))\n\nplt.show()  \n\nText: For filled patches with size greater than 1 x 1 \nCode: fig = plt.figure()\nax = fig.add_subplot(111, aspect='equal')\nax.set_xlim(0, 3)\nax.set_ylim(0, 3)\n\nx = [0, 1.16, 2.74, 2, 0]\ny = [0, 2.8, 2.8, 0, 0]\nax.add_patch(patches.Polygon(xy=list(zip(x,y)), fill=True))\n\nx = [0.3,0.6,.5,.4]\ny = [0.7,0.7,0.9,0.9]\nax.add_patch(patches.Polygon(xy=list(zip(x,y)), fill=True, color='magenta'))\n\nAPI:\nmatplotlib.patches.Polygon\n","label":[[48,64,"Mention"],[879,905,"API"]],"Comments":[]}
{"id":59362,"text":"ID:43206650\nPost:\nText: Looking at the plt.subplots() documentation, you find that it returns \nText: fig : Fig object ax : Axes object or array of Axes objects. ax can be either a single Axs object or an array of Axes objects if more than one subplot was created. The dimensions of the resulting array can be controlled with the squeeze keyword, see above. \nText: Examples of usage cases are given below the function definition in the documentation. \nText: So from this we learn that the return of plt.subplots is always a tuple. Tuples can be unpacked using the comma, \nCode: fig, ax = plt.subplots()\n\nText: The first element is a matplotlib.figure.Figure, which you you could indeed also get by calling plt.figure(). \nText: The second element of the tuple ax can be a tuple as well, depending on the arguments used. If n rows or columns are created, ax is an n-tuple. This tuple can be unpacked again, \nCode: fig, (ax1, ax2) = plt.subplots(nrows=2)\n\nText: If more than one row and column are created, ax will be a tuple of tuples, which again can be unpacked with a comma \nCode: fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n\nText: Finally, as in python \nCode: a,b,c   = (5, 6, 7)  # works\na,b,c   = (5,(6,7))  # does not work\na,(b,c) = (5,(6,7))  # works\n\nText: you cannot do fig, ax, ax2 = plt.subplots(2, 1), it will raise an error. \nAPI:\nmatplotlib.figure.Figure\nmatplotlib.axes.Axes\n","label":[[107,110,"Mention"],[187,190,"Mention"],[1361,1385,"API"],[1386,1406,"API"]],"Comments":[]}
{"id":59363,"text":"ID:43274979\nPost:\nText: The host_subplot creates an instance of AA.axis_artist.AxisArtist which is not the same as a normal axes. \nText: You therefore need to set the attributes of the axis just like you do with the label, \nCode: host.axis[\"right\"].line.set_color(p2.get_color())\npar1.axis[\"right\"].major_ticks.set_color(p2.get_color())\npar1.axis[\"right\"].major_ticklabels.set_color(p2.get_color())\n\npar2.axis[\"right\"].line.set_color(p3.get_color())\npar2.axis[\"right\"].major_ticks.set_color(p3.get_color())\npar2.axis[\"right\"].major_ticklabels.set_color(p3.get_color())\n\nAPI:\nmpl_toolkits.axisartist.axis_artist.AxisArtist\n","label":[[64,89,"Mention"],[575,621,"API"]],"Comments":[]}
{"id":59364,"text":"ID:43330553\nPost:\nText: Common grounds \nText: Both, add_axes and add_subplot add an axes to a figure. They both return a (subclass of a) Axes object. \nText: However, the mechanism which is used to add the axes differs substantially. \nText: add_axes \nText: The calling signature of add_axes is add_axes(rect), where rect is a list [x0, y0, width, height] denoting the lower left point of the new axes in figure coodinates (x0,y0) and its width and height. So the axes is positionned in absolute coordinates on the canvas. E.g. \nCode: fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n\nText: places a figure in the canvas that is exactly as large as the canvas itself. \nText: add_subplot \nText: The calling signature of add_subplot does not directly provide the option to place the axes at a predefined position. It rather allows to specify where the axes should be situated according to a subplot grid. The usual and easiest way to specify this position is the 3 integer notation, \nCode: fig = plt.figure()\nax = fig.add_subplot(231)\n\nText: In this example a new axes is created at the first position (1) on a grid of 2 rows and 3 columns. To produce only a single axes, add_subplot(111) would be used (First plot on a 1 by 1 subplot grid). (In newer matplotlib versions, add_subplot() without any arguments is possible as well.) \nText: The advantage of this method is that matplotlib takes care of the exact positioning. By default add_subplot(111) would produce an axes positioned at [0.125,0.11,0.775,0.77] or similar, which already leaves enough space around the axes for the title and the (tick)labels. However, this position may also change depending on other elements in the plot, titles set, etc. It can also be adjusted using pyplot.subplots_adjust(...) or pyplot.tight_layout(). \nText: In most cases, add_subplot would be the prefered method to create axes for plots on a canvas. Only in cases where exact positioning matters, add_axes might be useful. \nText: Example \nCode: import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (5,3)\n\nfig = plt.figure()\nfig.add_subplot(241)\nfig.add_subplot(242)\nax = fig.add_subplot(223)\nax.set_title(\"subplots\")\n\nfig.add_axes([0.77,.3,.2,.6])\nax2 =fig.add_axes([0.67,.5,.2,.3])\nfig.add_axes([0.6,.1,.35,.3])\nax2.set_title(\"random axes\")\n\nplt.tight_layout()\nplt.show()\n\nText: Alternative \nText: The easiest way to obtain one or more subplots together with their handles is plt.subplots(). For one axes, use \nCode: fig, ax = plt.subplots()\n\nText: or, if more subplots are needed, \nCode: fig, axes = plt.subplots(nrows=3, ncols=4)\n\nText: The initial question \nText: In the initial question an axes was placed using fig.add_axes([0,0,1,1]), such that it sits tight to the figure boundaries. The disadvantage of this is of course that ticks, ticklabels, axes labels and titles are cut off. Therefore I suggested in one of the comments to the answer to use fig.add_subplot as this will automatically allow for enough space for those elements, and, if this is not enough, can be adjusted using pyplot.subplots_adjust(...) or pyplot.tight_layout(). \nAPI:\nmatplotlib.axes.Axes\n","label":[[137,141,"Mention"],[3100,3120,"API"]],"Comments":[]}
{"id":59365,"text":"ID:43333270\nPost:\nText: I think you were missing using dates with your annotations. See this SO post. \nCode: import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(df.Close, '-')\n\nfor i in df.query('Bullish == 1').iterrows():\n    ax.annotate('Bullish',xy=(mdates.date2num(i[0]),i[1][0]), xytext=(15,15), textcoords='offset points',\n        arrowprops=dict(facecolor='black'))\n\nplt.xticks(rotation=45)\n\nAPI:\nmatplotlib.dates\n","label":[[55,60,"Mention"],[471,487,"API"]],"Comments":[]}
{"id":59366,"text":"ID:43388676\nPost:\nText: You can use the Axes.set_xlim() and Axes.set_ylim() methods on the axes of the seaborn PairGrid or FacetGrid. The axes are available from the PairGrid as .axes attribute. \nText: import pyplot as plt import seaborn as sns iris = sns.load_dataset(\"iris\") g = sns.PairGrid(iris) g = g.map_diag(plt.hist, edgecolor=\"k\") g = g.map_offdiag(plt.scatter, s=10) g.axes[2,0].set_ylim(-10,10) g.axes[0,1].set_xlim(-40,10) plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[209,215,"Mention"],[452,469,"API"]],"Comments":[]}
{"id":59367,"text":"ID:43591678\nPost:\nText: Here is a possible solution, creating a text field as a legend handler. The following would create a TextHandler to be used to create the legend artist, which is a simple Text instance. The handles for the legend are given as tuples of (text, color) from which the TextHandler creates the desired Text. \nCode: import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerBase\nfrom matplotlib.text import Text\nimport numpy as np\nimport pandas as pd\n\nclass TextHandler(HandlerBase):\n    def create_artists(self, legend, tup ,xdescent, ydescent,\n                        width, height, fontsize,trans):\n        tx = Text(width\/2.,height\/2,tup[0], fontsize=fontsize,\n                  ha=\"center\", va=\"center\", color=tup[1], fontweight=\"bold\")\n        return [tx]\n\n\na = np.random.choice([\"VP\", \"BC\", \"GC\", \"GP\", \"JC\", \"PO\"], size=100, \n                     p=np.arange(1,7)\/21. )\ndf = pd.DataFrame(a, columns=[\"GARAGE_DOM\"])\n\nax = sns.countplot(x = df.GARAGE_DOM)\n\n\nhandltext = [\"VP\", \"BC\", \"GC\", \"GP\", \"JC\", \"PO\"]\nlabels = [\"Voie Publique\", \"box\", \"Garage couvert\", \"garage particulier clos\", \"Jardin clos\", \"parking ouvert\"]\n\n\nt = ax.get_xticklabels()\nlabeldic = dict(zip(handltext, labels))\nlabels = [labeldic[h.get_text()]  for h in t]\nhandles = [(h.get_text(),c.get_fc()) for h,c in zip(t,ax.patches)]\n\nax.legend(handles, labels, handler_map={tuple : TextHandler()}) \n\nplt.show()\n\nText: The above solution is an updated version of the original version below, which seems more complicated. The following is the original solution, which uses a \nText: TextArea \nText: and an \nText: AnchoredOffsetbox \nText: to place the text inside the legend. \nCode: import seaborn.apionly as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.offsetbox import TextArea, AnchoredOffsetbox\nfrom matplotlib.transforms import TransformedBbox, Bbox\nfrom matplotlib.legend_handler import HandlerBase\nimport numpy as np\nimport pandas as pd\n\nclass TextHandler(HandlerBase):\n    def __init__(self, text, color=\"k\"):\n        self.text = text \n        self.color = color\n        super(TextHandler, self).__init__()\n\n    def create_artists(self, legend, orig_handle,xdescent, ydescent,\n                        width, height, fontsize,trans):\n        bb = Bbox.from_bounds(xdescent,ydescent, width,height)\n        tbb = TransformedBbox(bb, trans)\n        textbox = TextArea(self.text, textprops={\"weight\":\"bold\",\"color\":self.color})\n        ab = AnchoredOffsetbox(loc=10,child=textbox, bbox_to_anchor=tbb, frameon=False)\n        return [ab]\n\n\na = np.random.choice([\"VP\", \"BC\", \"GC\", \"GP\", \"JC\", \"PO\"], size=100, \n                     p=np.arange(1,7)\/21. )\ndf = pd.DataFrame(a, columns=[\"GARAGE_DOM\"])\n\nax = sns.countplot(x = df.GARAGE_DOM)\n\n\nhandltext = [\"VP\", \"BC\", \"GC\", \"GP\", \"JC\", \"PO\"]\nlabels = [\"Voie Publique\", \"box\", \"Garage couvert\", \"garage particulier clos\", \"Jardin clos\", \"parking ouvert\"]\n\nhandles = [ patches.Rectangle((0,0),1,1) for h in handltext]\nt = ax.get_xticklabels()\nlabeldic = dict(zip(handltext, labels))\nlabels = [labeldic[h.get_text()]  for h in t]\nhandlers = [TextHandler(h.get_text(),c.get_fc()) for h,c in zip(t,ax.patches)]\nhandlermap = dict(zip(handles, handlers))\nax.legend(handles, labels, handler_map=handlermap,) \n\nplt.show()\n\nText: Also see this more generic answer \nAPI:\nmatplotlib.text.Text\n","label":[[195,199,"Mention"],[3379,3399,"API"]],"Comments":[]}
{"id":59368,"text":"ID:43675840\nPost:\nText: Surely, you can use the imshow method of ypyplot and event handling capabilities of matplotlib. You can try the following code and see if it works for you: \nCode: import matplotlib as mpl \nmpl.use('wxAgg')\n\nimport numpy as np \nimport matplotlib.pyplot as plt\n\n# just some random data \nframes = [np.random.random((10, 10)) for _ in range(100)]\nkeeped_frames = [] \ni = 0\n\n# event listener \ndef press(event):\n    global i\n    if event.key == '1':\n        print('Appending frame')\n        keeped_frames.append(frames[i % 100])\n    i += 1\n    imgplot.set_data(frames[i % 100])\n    fig.canvas.draw()\n\nfig, ax = plt.subplots() \nfig.canvas.mpl_connect('key_press_event', press)\nimgplot = ax.imshow(frames[i % 100]) \nplt.show()\n\nText: One thing that you should be aware of is that the listener behaviour depends on the backend you're using. In Qt5Agg one key press fires one key_press event whether you hold the key or not, and in wxAgg key holding fires continious press and release events (the behaviour you desire, I guess). \nAPI:\nmatplotlib.pyplot\n","label":[[65,72,"Mention"],[1049,1066,"API"]],"Comments":[]}
{"id":59369,"text":"ID:43699744\nPost:\nText: tight layout \nText: You can use tight_layout to automatically adjust the spacings \nCode: g.fig.tight_layout()\n\nText: or, if you have pyplot imported as plt, \nCode: plt.tight_layout()\n\nText: subplots adjust \nText: You can use plt.subplots_adjust to manually set the spacings between subplots, \nCode: plt.subplots_adjust(hspace=0.4, wspace=0.4)\n\nText: where hspace is the space in height, and wspace is the space width direction. \nText: gridspec keyword arguments \nText: You could also use gridspec_kws in the FacetGrid initialization, \nCode: g = sns.FacetGrid(data, ... , gridspec_kws={\"wspace\":0.4})\n\nText: However, this can only be used if col_wrap is not set. (So it might not be an option in the particular case from the question). \nAPI:\nmatplotlib.pyplot\n","label":[[157,163,"Mention"],[765,782,"API"]],"Comments":[]}
{"id":59370,"text":"ID:43716106\nPost:\nText: You have to fix the limits of the color-scale: \nCode: plt.imshow(img, cmap='gray',clim=(0,1))\n\nText: To get a good feeling of what is going on you could include a colorbar which visualizes the conversion between colors and numerical values; for example using the following code: \nCode: fig,ax = plt.subplots()\ncax = plt.imshow(img, cmap='gray')\ncbar = fig.colorbar(cax)\nplt.show()\n\nText: Doing this for the two examples immediately makes clear that pyplot updates the range of the color-scale to the data. Consequently the conversion betweens colors and numerical values is different for the two cases. \nAPI:\nmatplotlib.pyplot\n","label":[[473,479,"Mention"],[633,650,"API"]],"Comments":[]}
{"id":59371,"text":"ID:43735657\nPost:\nText: You can use a StrMethodFormatter to format the labels with the amount of significant digits required. To get e.g. 6 significant digits, use \nCode: matplotlib.ticker.StrMethodFormatter(\"{x:.6f}\")\n\nText: A full example: \nCode: import numpy as np; np.random.seed(3)\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker\n\nx = np.linspace(500,900, num=201)\ny = np.cumsum(np.random.normal(loc=0,scale=1e-4,size=len(x) ))+1.0888\nfig, ax=plt.subplots()\nax.plot(x,y)\nax.yaxis.set_major_formatter(matplotlib.ticker.StrMethodFormatter(\"{x:.6f}\"))\n\nplt.show()\n\nAPI:\nmatplotlib.ticker.StrMethodFormatter\n","label":[[38,56,"Mention"],[581,617,"API"]],"Comments":[]}
{"id":59372,"text":"ID:43741589\nPost:\nText: The seaborn.pointplot is not the right tool for this plot. But the answer is very simple: use the basic mpl.pyplot.plot function: \nCode: import seaborn as sns\nimport matplotlib.pylab as plt\nimport pandas\nimport numpy as np\n\ndf = pandas.DataFrame({\"a\": np.arange(1001, 1001 + 30),\n                       \"l\": [\"A\"] * 15 + [\"B\"] * 15,\n                       \"v\": np.random.rand(30)})\ng = sns.FacetGrid(row=\"l\", data=df)\ng.map(plt.plot, \"a\", \"v\", marker=\"o\")\ng.set(xticks=df.a[2::8])\n\nAPI:\nmatplotlib.pyplot.plot\n","label":[[128,143,"Mention"],[511,533,"API"]],"Comments":[]}
{"id":59373,"text":"ID:43763864\nPost:\nText: Each colormesh plot has one colormap associated to it. In order to use several colormaps in one diagram, I therefore see the following options: \nText: Individual rectangles: Don't use pcolormesh but draw individual rectangles in the color of your liking. Create your custom colormap which incorporates different colormaps within different ranges. E.g. values from 0 to 0.4 are mapped to colors from one colormap and values from 0.4 to 1 to colors from another colormap. This may then look like: import pyplot as plt import matplotlib.colors import numpy as np x,y = np.meshgrid(range(4), range(4)) z = np.array([[0.2,.3,.95],[.5,.76,0.4],[.3,.1,.6]]).astype(float) mask= np.array([[1,0,0],[1,0,0],[1,1,1]]).astype(float) Z = z + mask c2 = plt.cm.Greens(np.linspace(0,1,128)) c1 = plt.cm.coolwarm(np.linspace(0,1,128)) cols = np.vstack((c1, c2)) cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"q\", cols) fig, ax=plt.subplots() ax.pcolormesh(x,y,Z, vmin=0, vmax=2, cmap=cmap) plt.show() Mask the arrays and plot several pcolormesh plots. The following example shows how this might then look like: import pyplot as plt import numpy as np import numpy.ma as ma x,y = np.meshgrid(range(4), range(4)) z = np.array([[1,1.3,3],[2.2,2.8,1.8],[3,1,3]]).astype(float) mask= np.array([[1,0,0],[1,0,0],[1,1,1]]).astype(bool) z1 = np.copy(z) z1[mask] = np.nan z2 = np.copy(z) z2[~mask] = np.nan fig, ax=plt.subplots() ax.pcolormesh(x,y,ma.masked_invalid(z1), vmin=1, vmax=3, cmap=\"coolwarm\") ax.pcolormesh(x,y,ma.masked_invalid(z2), vmin=1, vmax=3, cmap=\"Greens\") plt.show() \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[526,532,"Mention"],[1136,1142,"Mention"],[1601,1618,"API"],[1619,1636,"API"]],"Comments":[]}
{"id":59374,"text":"ID:43925914\nPost:\nText: It will depend which backend you use in jupyter notebook. \nText: If you use the inline backend (i.e. %matplotlib inline), interactive features cannot work, because the plots are just png images. If you use the notebook backend (i.e. %matplotlib notebook) the interactive features do work, but the question would be where to print the result to. So in order to show the text one may add it to the figure as follows %matplotlib notebook import numpy as np import ppylot as plt fig = plt.figure() ax = fig.add_subplot(111) ax.plot(np.random.rand(10)) text=ax.text(0,0, \"\", va=\"bottom\", ha=\"left\") def onclick(event): tx = 'button=%d, x=%d, y=%d, xdata=%f, ydata=%f' % (event.button, event.x, event.y, event.xdata, event.ydata) text.set_text(tx) cid = fig.canvas.mpl_connect('button_press_event', onclick) \nAPI:\nmatplotlib.pyplot\n","label":[[485,491,"Mention"],[832,849,"API"]],"Comments":[]}
{"id":59375,"text":"ID:43926055\nPost:\nText: I think the error is pretty self-explanatory. There is no such thing as pyplot.plt, or similar. plt is the quasi-standard abbreviated form of pyplot when being imported, i.e., import pyplot as plt. \nText: Concerning the problem, the first approach, return axarr is the most versatile one. You get an axis, or an array of axes, and can plot to it. \nText: The code may look like: \nCode: def plot_signal(x,y, ..., **kwargs):\n    # Skipping a lot of other complexity here\n    f, ax = plt.subplots(figsize=fig_size)\n    ax.plot(x,y, ...)\n    # further stuff\n    return ax\n\nax = plot_signal(x,y, ...)\nax.plot(x2, y2, ...)\nplt.show()\n\nAPI:\nmatplotlib.pyplot\n","label":[[207,213,"Mention"],[657,674,"API"]],"Comments":[]}
{"id":59376,"text":"ID:43954827\nPost:\nText: If simply setting the number of bins, the bins will be equally distributed between the first and last value (of the data). Since the data is e.g. missing the start value, the bins will be smaller than a complete day. \nText: To overcome this, one need to explicitely set the desired bins to the bins argument of hist. Unfortunately, one cannot directly use the list of datetimes here, so the datetimes need to be converted to numbers first. This can be done using matplotlib's date2nu method. \nText: The complete example: \nCode: import datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import date2num\n\nend = datetime.date(2017,5,14) \nstart = datetime.date(2017,5,8) \none_day = datetime.timedelta(days = 1)  \n\ndate_list = [datetime.date(2017,5,14), datetime.date(2017,5,14), \ndatetime.date(2017,5,14), datetime.date(2017,5,9), datetime.date(2017,5,13), \ndatetime.date(2017,5,12), datetime.date(2017,5,11), \ndatetime.date(2017,5,11), datetime.date(2017,5,9)]\n\nweek = [] \nfor i in range((end-start).days+1):  \n    week.append(start + (i)*one_day)\n\nnumweek = date2num(week)\n\nplt.hist(date_list, bins = numweek, ec=\"k\")\nplt.gcf().autofmt_xdate()\nplt.show()\n\nText: Note that the datetime.date(2017,5,14) is part of the bin between the (2017,5,13) and (2017,5,14), so you might want to set the enddate to datetime.date(2017,5,15). \nAPI:\nmatplotlib.dates.date2num\n","label":[[500,507,"Mention"],[1367,1392,"API"]],"Comments":[]}
{"id":59377,"text":"ID:44007040\nPost:\nText: I think you have two options here. In any case, you need to create the axes outside the loop, otherwise you'll end up with lots of overlaying axes. \nText: Clear the axes: import mpl.pyplot as plt import re from mpl_toolkits.axes_grid1 import host_subplot plt.ion() host = host_subplot(111) plt.subplots_adjust(right=0.75) while True: f = open(\".\/log.txt\", 'r') iterations = [] data = [] for line in f: if 'Iteration ' in line and 'data = ' in line: arr = re.findall(r'\\b\\d+\\b,', line) iterations.append(int(arr[0].strip(',')[0:])) data.append(float(line.strip().split(' ')[-1])) f.close() host.clear() host.set_xlabel(\"Iterations\") host.set_ylabel(\"Loss\") p1, = host.plot(iterations, data, label=\"Data\") plt.draw() plt.pause(3) Update the data: import plt as plt import re from mpl_toolkits.axes_grid1 import host_subplot plt.ion() host = host_subplot(111) plt.subplots_adjust(right=0.75) host.set_xlabel(\"Iterations\") host.set_ylabel(\"Loss\") p1, = host.plot([],[], label=\"Data\") while True: f = open(\".\/log.txt\", 'r') iterations = [] data = [] for line in f: if 'Iteration ' in line and 'data = ' in line: arr = re.findall(r'\\b\\d+\\b,', line) iterations.append(int(arr[0].strip(',')[0:])) data.append(float(line.strip().split(' ')[-1])) f.close() p1.set_data(iterations, data) host.relim() host.autoscale_view() plt.draw() plt.pause(3) \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[202,212,"Mention"],[776,779,"Mention"],[1366,1383,"API"],[1384,1401,"API"]],"Comments":[]}
{"id":59378,"text":"ID:44054960\nPost:\nText: So i got the code to work. I will post it here, and hopefully other can make use of it :-) \nText: import pandas as pd import pyplot as plt import numpy as np \nCode: # Prefix for the scenario\nscen_name = \"scenario1\"\n\n# Importing the Data\ndata = pd.read_csv(scen_name+'_frequencies50limcleaned.csv', sep=',')\ndata.columns= ['Replication', 'State', 'Machine', 'Average Time','Std. Percent']\n\n# Conforming the data\ndata.State = data.State.replace('State Bend Blocked', 'Blocked')\ndata.State = data.State.replace('State Punch Blocked', 'Blocked')\ndata.State = data.State.replace('State Shear Blocked', 'Blocked')\ndata.State = data.State.replace('State Form Blocked', 'Blocked')\n\ndata.State = data.State.replace('State Bend real failur', 'Real Failure')\ndata.State = data.State.replace('State Punch real failur', 'Real Failure')\ndata.State = data.State.replace('State Shear real failur', 'Real Failure')\ndata.State = data.State.replace('State Form real failur', 'Real Failure')\n\ndata.State = data.State.replace('State Bend Die change', 'Die Change')\ndata.State = data.State.replace('State Punch Die change', 'Die Change')\ndata.State = data.State.replace('State Shear Die change', 'Die Change')\ndata.State = data.State.replace('State Form Die change', 'Die Change')\n\n# Splitting data on machine\nbend = data[data['Machine'] == 'bend']\npunch = data[data['Machine'] == 'punch']\nform = data[data['Machine'] == 'form']\nshear = data[data['Machine'] == 'shear']\n\nreport_vals = ['Average Time', 'Std. Percent']\nstates = ['IDLE', 'BUSY', 'Blocked']\npost_fixes = ['avg_time', 'percent']\ntitles = ['Average time in state: ','Frequencies of in percent: ']\n\ni = 0\nfor rep in report_vals:\n    for state in states:\n        bend = bend.sort_values('Replication')\n        x = bend[bend['State'] == state]['Replication']\n        punch = punch.sort_values('Replication')\n        y = punch[punch['State'] == state]['Replication']\n        form = form.sort_values('Replication')\n        z = form[form['State'] == state]['Replication']\n        shear = shear.sort_values('Replication')\n        w = shear[shear['State'] == state]['Replication']\n\n        b = bend[bend['State'] == state][rep]\n        p = punch[punch['State'] == state][rep]\n        f = form[form['State'] == state][rep]\n        s = shear[shear['State'] == state][rep]\n\n        fig, ax = plt.subplots()\n\n        rects1 = plt.plot(x,b,'b',\n                         label='Bend')\n\n        rects2 = plt.plot(y,p,'g',\n                         label='Punch')\n\n        rects3 = plt.plot(z,f,'r',\n                         label='Form')\n\n        rects4 = plt.plot(w,s,'y',\n                         label='Shear')\n\n        plt.xlabel('Replication')\n        plt.ylabel(rep)\n        plt.title(titles[i]+state)\n        plt.legend()\n\n        plt.tight_layout()\n        plt.savefig('\/home\/misterwhite\/Dropbox\/Aarhus Universitet\/8. Semester\/Modellering, simulation og analyse\/Arena Project\/Report\/images\/'+scen_name+'_'+state+'_'+post_fixes[i]+'.png')\n    i +=1\n\nAPI:\nmatplotlib.pyplot\n","label":[[149,155,"Mention"],[3010,3027,"API"]],"Comments":[]}
{"id":59379,"text":"ID:44057990\nPost:\nText: The label you can set or get with .get_label and .set_label on the spines is not the xlabel or ylabel you can set or get for the axes. \nText: It is rather the label that every artist in matplotlib has. \nText: Spine subclasses matplotlib.patches.Patch. Patch subclasses matplotlib.artist.Artist. \nText: So a Spine is an Artist and every Artist has a label and a getter and setter for it. This label is not used in many cases, but can be used for some artists, e.g. to create a legend entry or label it for other purposes. In that sense it's just an attribute of the object which may or may not be used. \nAPI:\nmatplotlib.spines.Spine\nmatplotlib.patches.Patch\nmatplotlib.artist.Artist\n","label":[[233,238,"Mention"],[276,281,"Mention"],[360,366,"Mention"],[632,655,"API"],[656,680,"API"],[681,705,"API"]],"Comments":[]}
{"id":59380,"text":"ID:44064470\nPost:\nText: Every plot in matplotlib lives in an axes. To get a handle to the axes, use ax = plt.subplot(..). You can then use a ConnectionPatch to connect the axes. \nText: Complete code: \nText: import matplotlib as mpl import pyplot as plt import matplotlib.gridspec as gridspec import numpy as np labels = 'Good', 'Poor', 'Mediocre' sizes = [124, 205, 133] gs = gridspec.GridSpec(2,5) def make_autopct(sizes): def my_autopct(pct): total = sum(sizes) val = int(round(pct*total\/100.0)) return '{p:.1f}%({v:d})'.format(p=pct,v=val) return my_autopct plt.figure(figsize=(15,8)) ax = plt.subplot(gs[:,2]) plt.axis('equal') piechart = plt.pie(sizes, radius=3, labels=labels, labeldistance=1.1, textprops={'backgroundcolor':'w', 'fontsize':12}, autopct=make_autopct(sizes), pctdistance=0.5, shadow=False, colors='w')[0] piechart[0].set_hatch('\\\\\\\\\\\\\\\\\\\\') piechart[1].set_hatch('xxxx') piechart[2].set_hatch('+++') mpl.rcParams['font.size'] = 20 poor = [201, 25] mediocre = [97, 32] good = [83, 30] label_small = \"Private\",\"Government\" ax1 = plt.subplot(gs[0,0]) plt.axis('equal') pie_poor = plt.pie(poor, labels=label_small, autopct='%1.1f%%', pctdistance=1.5, textprops={'fontsize':8}, shadow=False, colors='w', startangle=0)[0] pie_poor[0].set_hatch('\/\/\/\/\/\/') pie_poor[1].set_hatch('....') ax2 = plt.subplot(gs[0,4]) plt.axis('equal') pie_good = plt.pie(good, labels=label_small, autopct='%1.1f%%', pctdistance=1.4, textprops={'fontsize':8}, shadow=False, colors='w', startangle=270)[0] pie_good[0].set_hatch('\/\/\/\/\/\/') pie_good[1].set_hatch('....') ax3 = plt.subplot(gs[1,4]) plt.axis('equal') pie_mediocre = plt.pie(mediocre, labels=label_small, autopct='%1.1f%%', pctdistance=1.3, textprops={'fontsize':8}, shadow=False, colors='w', startangle=180)[0] pie_mediocre[0].set_hatch('\/\/\/\/\/\/') pie_mediocre[1].set_hatch('....') from matplotlib.patches import ConnectionPatch xy = (1, 1) con = ConnectionPatch(xyA=(0,0), xyB=xy, coordsA=\"data\", coordsB=\"data\", axesA=ax2, axesB=ax, color=\"crimson\", lw=3) ax2.add_artist(con) xy = (-1, 0) con = ConnectionPatch(xyA=(0,0), xyB=xy, coordsA=\"data\", coordsB=\"data\", axesA=ax1, axesB=ax, color=\"darkblue\", lw=3) ax1.add_artist(con) xy = (1, -1) con = ConnectionPatch(xyA=(0,0), xyB=xy, coordsA=\"data\", coordsB=\"data\", axesA=ax3, axesB=ax, color=\"gold\", lw=3) ax3.add_artist(con) plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[239,245,"Mention"],[2345,2362,"API"]],"Comments":[]}
{"id":59381,"text":"ID:44092076\nPost:\nText: The efficient solution \nText: Since replacing the method for each axis individually is a lot more typing than using a simple function, the most efficient method would be to create a Python file, myhacks.py, with the respective function \nCode: def pcolormesh(ax, *args, **kwargs):\n    p = ax.pcolormesh(*args, **kwargs)\n    p.set_edgecolor('face')\n    p.set_linewidth(0.2)\n    return p\n\nText: And use it whenever the improved version of the pcolormesh is needed: \nCode: import matplotlib.pyplot as plt\nimport myhacks as m\n# ...other imports\n\nfig, ax = plt.subplots()\nm.pcolormesh(ax, other_arguments)\n\nText: This works also well for already created files, where one would simply search an replace \"ax.pcolormesh(\" with \"m.pcolormesh(ax,\" (if necessary using regex for possible other axes names). \nText: The academic solution \nText: It is of course possible to subclass Axes to include the desired function. Since there is no real benefit of this, except knowing how to do it, I would call it \"academic solution\". \nText: So, again we can create a file, myhacks.py, for our custom class, register the custom class as a projection to Matplotlib, \nCode: from matplotlib.axes import Axes\nfrom matplotlib.projections import register_projection\n\nclass MyAxes(Axes):\n    name = 'mycoolnewaxes'\n    def pcolormesh(self,*args, **kwargs):\n        p = Axes.pcolormesh(self,*args, **kwargs)\n        p.set_edgecolor('face')\n        p.set_linewidth(0.2)\n        return p\n\nregister_projection(MyAxes)\n\nText: And use this by importing it and creating the axes using the projection: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport myhacks as m\n\nfig = plt.figure()\ngs = GridSpec(2,1)\nax = fig.add_subplot(gs[0,0], projection='mycoolnewaxes')\n\nz = np.random.rand(10,13)\nax.pcolormesh(z)\n\nplt.show()\n\nAPI:\nmatplotlib.axes.Axes\n","label":[[892,896,"Mention"],[1866,1886,"API"]],"Comments":[]}
{"id":59382,"text":"ID:44177038\nPost:\nText: You can use a FuncAnimation to paint a new Polygon every second. \nCode: import matplotlib.pyplot as plt\nimport matplotlib.animation\nimport numpy as np; np.random.seed(1)\n\ndef get_vertices():\n    p = np.random.rand(4,2)*.8+.1\n    d = p-np.mean(p,axis=0)\n    s = np.arctan2(d[:,0], d[:,1])\n    return p[np.argsort(s),:]\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\npatches = []\nkw = dict(edgecolor='darkorange', lw=3, facecolor=\"peachpuff\")\npatches.append(plt.Polygon(get_vertices(),**kw)) \n# of course  PathPatch(Path(get_vertices())) \n# would also work\nax.add_patch(patches[0])\n\ndef update(i):\n    patches[0].remove()\n    patches[0] = plt.Polygon(get_vertices(),**kw)\n    ax.add_patch(patches[0])\n\nani = matplotlib.animation.FuncAnimation(fig, update, interval=1000)\nplt.show()\n\nAPI:\nmatplotlib.animation.FuncAnimation\n","label":[[38,51,"Mention"],[810,844,"API"]],"Comments":[]}
{"id":59383,"text":"ID:44180609\nPost:\nText: To format a date or time, you may use a DateFormatter as shown e.g. in this example. Calling this dateformatter with the respective time would produce the desired formatted string. \nCode: dateformatter = mdates.DateFormatter('%H:%M:%S')\ndateformatter(xdata)\n\nText: The problem of showing the formatted time is actually independend on the GUI implementation. So I leave this out and just provide a solution that shows the formatted time within the axes. From the question it is clear how to provide the formatted string to the GUI's statusbar. \nCode: u = u\"\"\"Time    CPU MEMEMORY    FLOW\n21:41:45    7   2.065984885 0\n21:41:49    24  2.143804486 207.1622516\n21:41:53    18  2.099176758 254.0634666\n21:41:57    16  2.148127797 546.959479\n21:42:01    25  2.120096005 837.3973892\n21:42:05    14  2.123164162 865.6367548\n21:42:09    19  2.126511241 894.4122738\n21:42:13    16  2.090111751 924.6048394\n21:42:17    16  2.088717134 953.8266646\n21:42:21    18  2.090948521 985.3669382\"\"\"\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport io\n\ndf = pd.read_csv(io.StringIO(u), delim_whitespace=True)\n\ndf[\"Time\"] = pd.to_datetime(df[\"Time\"])\n\nfig, ax = plt.subplots()\n\nax.plot(df[\"Time\"], df[\"CPU\"])\nax.plot(df[\"Time\"], df[\"MEMEMORY\"])\n\ntext = ax.text(0.02,0.98, \"\", transform=ax.transAxes, va=\"top\")\ndateformatter = mdates.DateFormatter('%H:%M:%S')\n\ndef onmousemove(evt):\n    if evt.inaxes:\n        xdata = evt.xdata\n        xtime =  dateformatter(xdata)\n        string = \"%s\" % (xtime)\n        text.set_text(string)\n        fig.canvas.draw_idle()\n\ncid = fig.canvas.mpl_connect(\"motion_notify_event\",onmousemove)\n\nplt.show()\n\nAPI:\nmatplotlib.dates.DateFormatter\n","label":[[64,77,"Mention"],[1688,1718,"API"]],"Comments":[]}
{"id":59384,"text":"ID:44214830\nPost:\nText: In general the datetime utilities of pandas and matplotlib are incompatible. So trying to use a mdates object on a date axis created with pandas will in most cases fail. \nText: One reason is e.g. seen from the documentation \nText: datetime objects are converted to floating point numbers which represent time in days since 0001-01-01 UTC, plus 1. For example, 0001-01-01, 06:00 is 1.25, not 0.25. \nText: However, this is not the only difference and it is thus advisable not to mix pandas and matplotlib when it comes to datetime objects. \nText: There is however the option to tell pandas not to use its own datetime format. In that case using the mdates tickers is possible. This can be steered via. \nCode: df.plot(x_compat=True)\n\nText: Since pandas does not provide sophisticated formatting capabilities for dates, one can use matplotlib for plotting and formatting. \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\n\ndf = pd.DataFrame({'date':['20170527','20170526','20170525'],'ratio1':[1,0.98,0.97]})\ndf['date'] = pd.to_datetime(df['date'])\n\nusePandas=True\n#Either use pandas\nif usePandas:\n    df = df.set_index('date')\n    df.plot(x_compat=True)\n    plt.gca().xaxis.set_major_locator(dates.DayLocator())\n    plt.gca().xaxis.set_major_formatter(dates.DateFormatter('%d\\n\\n%a'))\n    plt.gca().invert_xaxis()\n    plt.gcf().autofmt_xdate(rotation=0, ha=\"center\")\n# or use matplotlib\nelse:\n    plt.plot(df[\"date\"], df[\"ratio1\"])\n    plt.gca().xaxis.set_major_locator(dates.DayLocator())\n    plt.gca().xaxis.set_major_formatter(dates.DateFormatter('%d\\n\\n%a'))\n    plt.gca().invert_xaxis()\n\nplt.show()\n\nText: Updated using the matplotlib object oriented API \nCode: usePandas=True\n#Either use pandas\nif usePandas:\n    df = df.set_index('date')\n    ax = df.plot(x_compat=True, figsize=(6, 4))\n    ax.xaxis.set_major_locator(dates.DayLocator())\n    ax.xaxis.set_major_formatter(dates.DateFormatter('%d\\n\\n%a'))\n    ax.invert_xaxis()\n    ax.get_figure().autofmt_xdate(rotation=0, ha=\"center\")\n    \n# or use matplotlib\nelse:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.plot('date', 'ratio1', data=df)\n    ax.xaxis.set_major_locator(dates.DayLocator())\n    ax.xaxis.set_major_formatter(dates.DateFormatter('%d\\n\\n%a'))\n    fig.invert_xaxis()\n\nplt.show()\n\nAPI:\nmatplotlib.dates\nmatplotlib.dates\n","label":[[120,126,"Mention"],[671,677,"Mention"],[2325,2341,"API"],[2342,2358,"API"]],"Comments":[]}
{"id":59385,"text":"ID:44239744\nPost:\nText: It seems that blitting is performed per axes. So it might be that the procedure is \nCode: for ax in all axes:\n    get axes background\n    draw line\n\nText: This means that the first line is part of the background from the second axes and as such will be part of every successive frame. \nText: The only solution I can think of at the moment is to make the lines invisible until the backgrounds of all axes have been stored for blitting. \nCode: line = Line2D(tdata, ydata, color=color, visible=False)\n\nText: Only after the first call to updateLines turn them visible again. \nCode: n = [0]\ndef updateLines(lines, arrays):\n    \"\"\"Update individual lines and return a sequence of artists to the animator.\"\"\"\n    artists = []\n    for iline in range(len(lines)):\n        artists.append(updateLine(lines[iline], arrays[iline]))\n        if n[0] > 0:\n            lines[iline].set_visible(True)\n    n[0] += 1\n    return artists\n\nText: Complete code: \nText: import pyplot as plt import matplotlib.animation as animation from matplotlib.lines import Line2D import math # Initalize script constants ymin = -1.1 ymax = 1.1 linesPerPlot = 3 samplesPerFrame = 1 framesPerSecond = 20 secondsPerPlot = 5 # Calculate dependent constants samplesPerSecond = samplesPerFrame * framesPerSecond samplesPerPlot = samplesPerSecond * secondsPerPlot secondsPerSample = 1.0\/samplesPerSecond millisPerFrame = 1000.0\/framesPerSecond # Define core functions def makeLine(ax, maxt, dt, ymin, ymax, color): \"\"\"Make an empty Line2D for the initial chart.\"\"\" nvalues = int(round(maxt\/dt)) tdata = [dt*tm for tm in range(nvalues)] ydata = [0 for tm in range(nvalues)] line = Line2D(tdata, ydata, color=color, visible=False) ### <- visible false ax.add_line(line) ax.set_ylim(ymin, ymax) return line def makeChart(ax, maxt, dt, linesPerPlot, ymin, ymax): \"\"\"Make a chart and return a list of the lines it contains.\"\"\" colors = [ 'r', 'b', 'g', 'k' ] lines = [] # Add first line, then add subsequent lines sharing the x-axis. lines.append(makeLine(ax, maxt, dt, ymin, ymax, colors[0])) for iline in range(1,linesPerPlot): twin_ax = ax.twinx() lines.append(makeLine(twin_ax, maxt, dt, ymin, ymax, colors[iline % len(colors)])) ax.set_xlim(0, maxt) return lines def initDisplay(lines): \"\"\"Init display.\"\"\" return lines def updateLine(line, ys): \"\"\"Update the data in one line, popping off the last value.\"\"\" tdata, ydata = line.get_data() for y in ys: ydata.append(y) ydata.pop(0) line.set_data(tdata, ydata) return line n = [0] def updateLines(lines, arrays): \"\"\"Update individual lines and return a sequence of artists to the animator.\"\"\" artists = [] for iline in range(len(lines)): artists.append(updateLine(lines[iline], arrays[iline])) if n[0] > 0: lines[iline].set_visible(True) n[0] += 1 return artists def emitData(linesPerPlot, samplesPerFrame): \"\"\"Create the data that will be plotted.\"\"\" nsample = 0 while True: samples = [[] for i in range(linesPerPlot)] for isample in range(samplesPerFrame): nsample = nsample + 1 for iline in range(linesPerPlot): pi_increment = (math.pi\/(10.0 * (iline+1))) samples[iline].append(math.sin(nsample * pi_increment)) yield samples # Make chart. fig, ax = plt.subplots() lines = makeChart(ax, secondsPerPlot, secondsPerSample, linesPerPlot, ymin, ymax) # Start the animator. update = lambda samples: updateLines(lines, samples) emitter = lambda: emitData(linesPerPlot, samplesPerFrame) init = lambda: initDisplay(lines) ani = animation.FuncAnimation(fig, update, emitter, init_func=init, interval=millisPerFrame, blit=True) plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[976,982,"Mention"],[3567,3584,"API"]],"Comments":[]}
{"id":59386,"text":"ID:44261864\nPost:\nText: If shape is the shape from a .shp file, you can supply it to a Polygon and add some hatch with the hatch argument. \nCode: p= Polygon(np.array(shape), fill=False, hatch=\"X\")\nplt.gca().add_artist(p) \n\nText: A complete example: \nCode: from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nimport numpy as np\n\nm = Basemap(llcrnrlon=-10,llcrnrlat=35,urcrnrlon=35,urcrnrlat=60.,\n             resolution='i', projection='tmerc', lat_0 = 48.9, lon_0 = 15.3)\n\nm.drawcoastlines()\n\n# shape file from \n# http:\/\/www.naturalearthdata.com\/downloads\/10m-cultural-vectors\/10m-admin-0-countries\/\nfn = r\"ne_10m_admin_0_countries\\ne_10m_admin_0_countries\"\nm.readshapefile(fn, 'shf', drawbounds = False) \n# here, 'shf' is the name we later use to access the shapes.\n\n#Madrid\nx,y = m([-3.703889],[40.4125])\nm.plot(x,y, marker=\"o\", color=\"k\", label=\"Madrid\", ls=\"\")\n\nhatches = [\"\\\\\\\\\",\"++\", \"XX\"]\ncountries = ['Spain', 'Ireland', \"Belgium\"]\nhatchdic = dict(zip(countries, hatches))\nshapes = {}\nfor info, shape in zip(m.shf_info, m.shf):\n    if info['NAME'] in countries:\n        p= Polygon(np.array(shape), fill=False, hatch=hatchdic[info['NAME']])\n        shapes.update({info['NAME'] : p})\n\nfor country in countries:\n    plt.gca().add_artist(shapes[country]) \n\nhandles, labels = plt.gca().get_legend_handles_labels()\nhandles.extend([shapes[c] for c in countries])  \nlabels.extend(countries)     \nplt.legend(handles=handles, labels=labels, handleheight=3, handlelength=3, framealpha=1. )\n\nplt.show()\n\nAPI:\nmatplotlib.patches.Polygon\n","label":[[87,94,"Mention"],[1565,1591,"API"]],"Comments":[]}
{"id":59387,"text":"ID:44335767\nPost:\nText: If you take a look at the plt.legend documentation you'll notice that you called it with the wrong format. Citing the docs: \nText: For full control of which artists have a legend entry, it is possible to pass an iterable of legend artists followed by an iterable of legend labels respectively: legend((line1, line2, line3), ('label1', 'label2', 'label3')) \nText: So you'll need to pass in two tuples: \nCode: plt.legend((alldata[0], l1[0], l2[0]),                                # plots\n           (\"alldata\",\"d=%i\" % model1.order, \"d=%i\" % model2.order),  # names\n           loc=1)\n\nAPI:\nmatplotlib.pyplot.legend\n","label":[[50,60,"Mention"],[612,636,"API"]],"Comments":[]}
{"id":59388,"text":"ID:44385334\nPost:\nText: pyplot is often called \"statemachine\". This means that the function it provides do certain things depending on the internal state of pyplot. \nText: In your code, you have created a figure and this is stored as an object; pyplot knows it has one figure. \nText: If you then call other commands, it is assumend that they apply to that one figure which has been created previously, like plt.savefig. \nText: plt.show() would work on all previously created figures (all of them would be shown). \nAPI:\nmatplotlib.pyplot\n","label":[[24,30,"Mention"],[519,536,"API"]],"Comments":[]}
{"id":59389,"text":"ID:44438440\nPost:\nText: A colorbar in matplotlib maps number between 0 and 1 to a color. In order to map other numbers to colors you need a normalization to the range [0,1] first. This is usually done automatically from the minimum and maximum data, or by using vmin and vmax arguments to the respective plotting function. Internally a normalization instance mpl.colors.Normalize is used to perform the normalization and by default a linear scale between vmin and vmax is assumed. \nText: Here you want a nonlinear scale, which (a) shifts the middle point to some specified value, and (b) squeezes the colors around that value. \nText: The idea can now be to subclass Normalize and let it return a a mapping which fulfills the criteria (a) and (b). \nText: An option might be the combination of two root functions as shown below. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\n\nclass SqueezedNorm(matplotlib.colors.Normalize):\n    def __init__(self, vmin=None, vmax=None, mid=0, s1=2, s2=2, clip=False):\n        self.vmin = vmin # minimum value\n        self.mid  = mid  # middle value\n        self.vmax = vmax # maximum value\n        self.s1=s1; self.s2=s2\n        f = lambda x, zero,vmax,s: np.abs((x-zero)\/(vmax-zero))**(1.\/s)*0.5\n        self.g = lambda x, zero,vmin,vmax, s1,s2: f(x,zero,vmax,s1)*(x>=zero) - \\\n                                             f(x,zero,vmin,s2)*(x<zero)+0.5\n        matplotlib.colors.Normalize.__init__(self, vmin, vmax, clip)\n\n    def __call__(self, value, clip=None):\n        r = self.g(value, self.mid,self.vmin,self.vmax, self.s1,self.s2)\n        return np.ma.masked_array(r)\n\n\nfig, (ax, ax2, ax3) = plt.subplots(nrows=3, \n                                   gridspec_kw={\"height_ratios\":[3,2,1], \"hspace\":0.25})\n\nx = np.linspace(-13,4, 110)\nnorm=SqueezedNorm(vmin=-13, vmax=4, mid=0, s1=1.7, s2=4)\n\nline, = ax.plot(x, norm(x))\nax.margins(0)\nax.set_ylim(0,1)\n\nim = ax2.imshow(np.atleast_2d(x).T, cmap=\"Spectral_r\", norm=norm, aspect=\"auto\")\ncbar = fig.colorbar(im ,cax=ax3,ax=ax2, orientation=\"horizontal\")\n\nText: The function is chosen such that independent of its parameters it will map any range onto the range [0,1], such that a colormap can be used. The parameter mid determines which value should be mapped to the middle of the colormap. This would be 0 in this case. The parameters s1 and s2 determine how squeezed the colormap is in both directions. \nText: Setting mid = np.mean(vmin, vmax), s1=1, s2=1 would recover the original scaling. \nText: In order to choose good parameters, one may use some Sliders to see the live updated plot. \nCode: from matplotlib.widgets import Slider\n\nmidax = plt.axes([0.1, 0.04, 0.2, 0.03], facecolor=\"lightblue\")\ns1ax = plt.axes([0.4, 0.04, 0.2, 0.03], facecolor=\"lightblue\")\ns2ax = plt.axes([0.7, 0.04, 0.2, 0.03], facecolor=\"lightblue\")\n\nmid = Slider(midax, 'Midpoint', x[0], x[-1], valinit=0)\ns1 = Slider(s1ax, 'S1', 0.5, 6, valinit=1.7)\ns2 = Slider(s2ax, 'S2', 0.5, 6, valinit=4)\n\n\ndef update(val):\n    norm=SqueezedNorm(vmin=-13, vmax=4, mid=mid.val, s1=s1.val, s2=s2.val)\n    im.set_norm(norm)\n    cbar.update_bruteforce(im) \n    line.set_ydata(norm(x))\n    fig.canvas.draw_idle()\n\nmid.on_changed(update)\ns1.on_changed(update)\ns2.on_changed(update)\n\nfig.subplots_adjust(bottom=0.15)\n\nAPI:\nmatplotlib.colors.Normalize\nmatplotlib.colors.Normalize\n","label":[[359,379,"Mention"],[666,675,"Mention"],[3306,3333,"API"],[3334,3361,"API"]],"Comments":[]}
{"id":59390,"text":"ID:44472836\nPost:\nText: Instead of placing the buttonbox in absolute coordinates, you may add it to a layout, just like you did with the canvas and the toolbar. To this end, you may use another widget, which contains the canvas and toolbar. This widget can be placed in a QHBoxLayout(); the QDialogButtonBox would be added second in the layout. \nText: import sys #from PyQt5.QtWidgets import QPushButton, QVBoxLayout, QApplication, QDialog from PyQt4.QtGui import QPushButton, QVBoxLayout, QApplication, QDialog from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas from matplotlib.backends.backend_qt4agg import NavigationToolbar2QT as NavigationToolbar import pyplot as plt import random #from PyQt5 import QtCore, QtGui, QtWidgets from PyQt4 import QtCore, QtGui #, QtWidgets class Ui_Dialog(QDialog): def __init__(self, parent=None): super(Ui_Dialog, self).__init__(parent) self.setupUi(self) self.setupPlot() def setupUi(self, Dialog): Dialog.setObjectName(\"Dialog\") Dialog.resize(400, 300) self.buttonBox = QtGui.QDialogButtonBox(Dialog) self.buttonBox.setOrientation(QtCore.Qt.Vertical) self.buttonBox.setStandardButtons(QtGui.QDialogButtonBox.Cancel|QtGui.QDialogButtonBox.Ok) self.buttonBox.setObjectName(\"buttonBox\") self.buttonBox.accepted.connect(Dialog.accept) self.buttonBox.rejected.connect(Dialog.reject) QtCore.QMetaObject.connectSlotsByName(Dialog) self.setLayout(QtGui.QHBoxLayout()) self.layout().setContentsMargins(0,0,0,0) def setupPlot(self): self.figure = plt.figure() self.figure.set_facecolor(\"none\") self.canvas = FigureCanvas(self.figure) self.canvas.setContentsMargins(0,0,0,0) self.widget = QtGui.QWidget() self.widget.setContentsMargins(0,0,0,0) self.toolbar = NavigationToolbar(self.canvas, self) self.toolbar.setContentsMargins(0,0,0,0) layout = QVBoxLayout() layout.setSpacing(0) layout.setContentsMargins(0,0,0,0) layout.addWidget(self.toolbar) layout.addWidget(self.canvas) self.widget.setLayout(layout) self.layout().addWidget(self.widget) self.layout().addWidget(self.buttonBox) self.plot() def plot(self): data = [random.random() for i in range(10)] self.figure.clear() ax = self.figure.add_subplot(111) ax.plot(data, '*-') self.canvas.draw() if __name__ == '__main__': app = QApplication(sys.argv) main = Ui_Dialog() main.show() sys.exit(app.exec_()) \nText: Alternatively, you can first place the toolbar in a QVBoxLayout and beneath place a widget containing the canvas and the Button box in a QHBoxLayout. \nText: import sys from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas from matplotlib.backends.backend_qt4agg import NavigationToolbar2QT as NavigationToolbar import plt as plt import random from PyQt4 import QtCore, QtGui class Ui_Dialog(QtGui.QDialog): def __init__(self, parent=None): super(Ui_Dialog, self).__init__(parent) self.setupUi(self) def setupUi(self, Dialog): Dialog.setObjectName(\"Dialog\") Dialog.resize(400, 300) self.buttonBox = QtGui.QDialogButtonBox(Dialog) self.buttonBox.setOrientation(QtCore.Qt.Vertical) self.buttonBox.setStandardButtons(QtGui.QDialogButtonBox.Cancel|QtGui.QDialogButtonBox.Ok) self.buttonBox.setObjectName(\"buttonBox\") self.buttonBox.accepted.connect(Dialog.accept) self.buttonBox.rejected.connect(Dialog.reject) QtCore.QMetaObject.connectSlotsByName(Dialog) self.setLayout(QtGui.QVBoxLayout()) self.layout().setContentsMargins(0,0,0,0) self.figure = plt.figure() self.figure.set_facecolor(\"none\") self.canvas = FigureCanvas(self.figure) self.widget = QtGui.QWidget() self.toolbar = NavigationToolbar(self.canvas, self) self.layout().addWidget(self.toolbar) layout = QtGui.QHBoxLayout() layout.addWidget(self.canvas) layout.addWidget(self.buttonBox) self.widget.setLayout(layout) self.layout().addWidget(self.widget) self.plot() def plot(self): data = [random.random() for i in range(10)] self.figure.clear() ax = self.figure.add_subplot(111) ax.plot(data, '*-') self.canvas.draw() if __name__ == '__main__': app = QtGui.QApplication(sys.argv) main = Ui_Dialog() main.show() sys.exit(app.exec_()) \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[689,695,"Mention"],[2670,2673,"Mention"],[4048,4065,"API"],[4066,4083,"API"]],"Comments":[]}
{"id":59391,"text":"ID:44483450\nPost:\nText: A seaborn heatmap plots categorical data. This means that each occuring value would take the same space in the heatmap as any other value, independent on how far they are separated numerically. This is usually undesired for numerical data. Instead one of the following techniques may be chosen. \nText: Scatter \nText: A colored scatter plot may be just as good as a heatmap. The colors of the points would represent the yy value. \nCode: ax.scatter(df.v1, df.v2, c=df.yy,  cmap=\"copper\")\n\nText: u = u\"\"\"v1 v2 yy 15.25 44.34 100.00 83.05 59.78 100.00 96.61 65.09 100.00 100.00 75.47 100.00 100.00 50.00 100.00 100.00 68.87 100.00 100.00 79.35 100.00 100.00 100.00 100.00 100.00 63.21 100.00 100.00 100.00 100.00 100.00 68.87 100.00 0.00 56.52 92.86 10.17 52.83 92.86 23.73 46.23 92.86\"\"\" import pandas as pd import pyplot as plt import io df = pd.read_csv(io.StringIO(u), delim_whitespace=True ) fig, ax = plt.subplots() sc = ax.scatter(df.v1, df.v2, c=df.yy, cmap=\"copper\") fig.colorbar(sc, ax=ax) ax.set_aspect(\"equal\") plt.show() \nText: Hexbin \nText: You may want to look into hexbin. The data would be shown in hexagonal bins and the data is aggregated as the mean inside each bin. The advantage here is that if you choose the gridsize large, it will look like a scatter plot, while if you make it small, it looks like a heatmap, allowing to adjust the plot easily to the desired resolution. \nCode: h1 = ax.hexbin(df.v1, df.v2, C=df.yy, gridsize=100, cmap=\"copper\")\nh2 = ax2.hexbin(df.v1, df.v2, C=df.yy, gridsize=10, cmap=\"copper\")\n\nText: u = u\"\"\"v1 v2 yy 15.25 44.34 100.00 83.05 59.78 100.00 96.61 65.09 100.00 100.00 75.47 100.00 100.00 50.00 100.00 100.00 68.87 100.00 100.00 79.35 100.00 100.00 100.00 100.00 100.00 63.21 100.00 100.00 100.00 100.00 100.00 68.87 100.00 0.00 56.52 92.86 10.17 52.83 92.86 23.73 46.23 92.86\"\"\" import pandas as pd import yplot as plt import io df = pd.read_csv(io.StringIO(u), delim_whitespace=True ) fig, (ax, ax2) = plt.subplots(nrows=2) h1 = ax.hexbin(df.v1, df.v2, C=df.yy, gridsize=100, cmap=\"copper\") h2 = ax2.hexbin(df.v1, df.v2, C=df.yy, gridsize=10, cmap=\"copper\") fig.colorbar(h1, ax=ax) fig.colorbar(h2, ax=ax2) ax.set_aspect(\"equal\") ax2.set_aspect(\"equal\") ax.set_title(\"gridsize=100\") ax2.set_title(\"gridsize=10\") fig.subplots_adjust(hspace=0.3) plt.show() \nText: Tripcolor \nText: A tripcolor plot can be used to obtain colored reagions in the plot according to the datapoints, which are then interpreted as the edges of triangles, colorized according the edgepoints' data. Such a plot would require to have more data available to give a meaningful representation. \nCode: ax.tripcolor(df.v1, df.v2, df.yy,  cmap=\"copper\")\n\nText: u = u\"\"\"v1 v2 yy 15.25 44.34 100.00 83.05 59.78 100.00 96.61 65.09 100.00 100.00 75.47 100.00 100.00 50.00 100.00 100.00 68.87 100.00 100.00 79.35 100.00 100.00 100.00 100.00 100.00 63.21 100.00 100.00 100.00 100.00 100.00 68.87 100.00 0.00 56.52 92.86 10.17 52.83 92.86 23.73 46.23 92.86\"\"\" import pandas as pd import plt as plt import io df = pd.read_csv(io.StringIO(u), delim_whitespace=True ) fig, ax = plt.subplots() tc = ax.tripcolor(df.v1, df.v2, df.yy, cmap=\"copper\") fig.colorbar(tc, ax=ax) ax.set_aspect(\"equal\") ax.set_title(\"tripcolor\") plt.show() \nText: Note that atricontourf plot may equally be suited, if more datapoints throughout the grid are available. \nCode: ax.tricontourf(df.v1, df.v2, df.yy,  cmap=\"copper\")\n\nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[836,842,"Mention"],[1884,1889,"Mention"],[3025,3028,"Mention"],[3443,3460,"API"],[3461,3478,"API"],[3479,3496,"API"]],"Comments":[]}
{"id":59392,"text":"ID:44525929\nPost:\nText: You may decide not to use seaborn and turn the axes spines invisible: \nCode: for d in [\"left\", \"top\", \"bottom\", \"right\"]:\n    plt.gca().spines[d].set_visible(False)\n\nText: import numpy as np import pyplot as plt mapy = np.random.rand(100,100) pf = 2.8 areaX = mapy.shape[0]\/2*pf # half of the area! areaY = mapy.shape[1]\/2*pf # half of the area! fig = plt.imshow(mapy,interpolation='spline16',origin='lower', cmap='Reds',extent=[-areaX*pf,areaX*pf,-areaY*pf,areaY*pf]) plt.grid(False) for d in [\"left\", \"top\", \"bottom\", \"right\"]: plt.gca().spines[d].set_visible(False) plt.show() \nText: The same can be done, using rcParams, \nCode: s = {\"axes.spines.left\"   : False,\n    \"axes.spines.bottom\" : False,\n    \"axes.spines.top\"    : False,\n    \"axes.spines.right\"  : False}\nplt.rcParams.update(s)\n\nText: import numpy as np import pyplot as plt s = {\"axes.spines.left\" : False, \"axes.spines.bottom\" : False, \"axes.spines.top\" : False, \"axes.spines.right\" : False} plt.rcParams.update(s) mapy = np.random.rand(100,100) pf = 2.8 areaX = mapy.shape[0]\/2*pf # half of the area! areaY = mapy.shape[1]\/2*pf # half of the area! fig = plt.imshow(mapy,interpolation='spline16',origin='lower', cmap='Reds',extent=[-areaX*pf,areaX*pf,-areaY*pf,areaY*pf]) plt.grid(False) plt.show() \nText: Alternatively you can set the axes edgecolor to transparent. \nCode: plt.rcParams[\"axes.edgecolor\"]=(1,1,1,0)\n\nText: import numpy as np import pyeplot as plt plt.rcParams[\"axes.edgecolor\"]=(1,1,1,0) mapy = np.random.rand(100,100) pf = 2.8 areaX = mapy.shape[0]\/2*pf # half of the area! areaY = mapy.shape[1]\/2*pf # half of the area! fig = plt.imshow(mapy,interpolation='spline16',origin='lower', cmap='Reds',extent=[-areaX*pf,areaX*pf,-areaY*pf,areaY*pf]) plt.grid(False) plt.show() \nText: Or, if you want to use seaborn (and it's white-style), additionally reactivate the ticks using \nCode: plt.tick_params(axis=\"both\", which=\"major\", length=5)\n\nText: import numpy as np import plt as plt import seaborn as sns sns.set_style(style='white') #turn off grid mapy = np.random.rand(100,100) pf = 2.8 areaX = mapy.shape[0]\/2*pf # half of the area! areaY = mapy.shape[1]\/2*pf # half of the area! fig = plt.imshow(mapy,interpolation='spline16',origin='lower', cmap='Reds',extent=[-areaX*pf,areaX*pf,-areaY*pf,areaY*pf]) plt.grid(False) for d in [\"left\", \"top\", \"bottom\", \"right\"]: plt.gca().spines[d].set_visible(False) plt.tick_params(axis=\"both\", which=\"major\", length=5) plt.show() \nText: As @mwaskom points out in the comments, Seaborn also offers sns.despine() to get rid of the spines, which you would then call like \nCode: sns.despine(left=True, top=True, bottom=True, right=True)\n\nText: Note the double negation (despine True means not to have spines). \nText: import numpy as np import mpl.pyplot as plt import seaborn as sns sns.set_style(style='white') #turn off grid mapy = np.random.rand(100,100) pf = 2.8 areaX = mapy.shape[0]\/2*pf # half of the area! areaY = mapy.shape[1]\/2*pf # half of the area! fig = plt.imshow(mapy,interpolation='spline16',origin='lower', cmap='Reds',extent=[-areaX*pf,areaX*pf,-areaY*pf,areaY*pf]) plt.grid(False) sns.despine(left=True, top=True, bottom=True, right=True) plt.tick_params(axis=\"both\", which=\"major\", length=5) plt.show() \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[222,228,"Mention"],[849,855,"Mention"],[1438,1445,"Mention"],[1974,1977,"Mention"],[2782,2792,"Mention"],[3268,3285,"API"],[3286,3303,"API"],[3304,3321,"API"],[3322,3339,"API"],[3340,3357,"API"]],"Comments":[]}
{"id":59393,"text":"ID:44609685\nPost:\nText: The matplotlib default rcParams are defined in the file rcsetup.py. \nText: The default for the savefig.dpi of the current development version is \nText: 'savefig.dpi': ['figure', validate_dpi], # DPI (currently line 1298) \nText: where 'figure' means that the dpi of the figure should be used. The value of the figure dpi defaults to \nText: 'figure.dpi': [100, validate_float], # DPI (currently line 1277) \nText: Note however two things: \nText: Those values are overwritten in case the matplotlib rc file specifies them (i.e. the respective line in the rc file is not commented out). The actual values can be found using e.g. import pyplot as plt print(plt.rcParams) Those may already differ from the default values. As an example, the Jupyter notebook sets the default dpi to 72: \nText: Concerning the first question, even though the values are commented out in the matplotlibrc, they should be the same as in the rcsetup.py. So either the value is commented out, then they still default to the values in the rc file, or they are not commented out, in which the statement is also obviously correct. \nAPI:\nmatplotlib.pyplot\n","label":[[655,661,"Mention"],[1128,1145,"API"]],"Comments":[]}
{"id":59394,"text":"ID:44609935\nPost:\nText: Matplotlib axes have major and minor ticks. You may use the minor ticks to show the marginal locations of the points. You may turn the major ticks off but show the ticklabels for them. \nText: To set ticks at certain positions you can use a FixedLocator. To change the appearance of the ticks or turn them off, the axes has a tick_params method. \nCode: import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nx = [ 0,  1, 1.2, 1.3, 4, 5, 6, 7, 8.2, 9, 10]\ny = [.2, .4, 2, 3, 4, 5, 5.1, 5.2, 4, 3, 8]\n\nxticklabels = [0, 5, 10]\nyticklabels = xticklabels\n\nfig, ax = plt.subplots()\nfor spine in ax.spines.values():\n    spine.set_visible(False)\n\nax.scatter(x, y)\n\nax.xaxis.set_major_locator(ticker.FixedLocator(xticklabels))\nax.yaxis.set_major_locator(ticker.FixedLocator(yticklabels))\n\nax.xaxis.set_minor_locator(ticker.FixedLocator(x))\nax.yaxis.set_minor_locator(ticker.FixedLocator(y))\n\nax.tick_params(axis=\"both\", which=\"major\", bottom=\"off\", left=\"off\")\nax.tick_params(axis=\"both\", which=\"minor\", length=4)\n\nplt.show()\n\nText: Note that I personally find this plot rather difficult to grasp and if I may, I would propose something more like this: \nText: import pyplot as plt import matplotlib.ticker as ticker x = [ 0, 1, 1.2, 1.3, 4, 5, 6, 7, 8.2, 9, 10] y = [.2, .4, 2, 3, 4, 5, 5.1, 5.2, 4, 3, 8] xticklabels = [0, 5, 10] yticklabels = xticklabels fig, ax = plt.subplots() ax.scatter(x, y) ax.xaxis.set_minor_locator(ticker.FixedLocator(x)) ax.yaxis.set_minor_locator(ticker.FixedLocator(y)) c = \"#aaaaaa\" ax.tick_params(axis=\"both\", which=\"major\", direction=\"out\", color=c) ax.tick_params(axis=\"both\", which=\"minor\", length=6, direction=\"in\", color=\"C0\", width=1.5) plt.setp(ax.spines.values(), color=c) plt.setp(ax.get_xticklabels(), color=c) plt.setp(ax.get_yticklabels(), color=c) plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[1195,1201,"Mention"],[1839,1856,"API"]],"Comments":[]}
{"id":59395,"text":"ID:44655020\nPost:\nText: It's not perfectly clear what your desired outcome is. \nText: You may use automatic aspect on the image ax.imshow(z, aspect=\"auto\") Or you may set the aspect of the line plot depending on its axis limits such that it gets the same size as the image (in case the image has equal x and y sizes) asp = np.diff(ax2.get_xlim())[0] \/ np.diff(ax2.get_ylim())[0] ax2.set_aspect(asp) Complete code: import numpy as np import pyplot as plt x = np.linspace(0,10,20) y = np.sin(x) z = np.random.rand(100,100) fig, (ax, ax2) = plt.subplots(ncols=2) ax.imshow(z) ax2.plot(x,y, marker=\".\") asp = np.diff(ax2.get_xlim())[0] \/ np.diff(ax2.get_ylim())[0] ax2.set_aspect(asp) plt.show() If the image does not have equal limits (is not square), one still needs to divide by the aspect of the image: asp = np.diff(ax2.get_xlim())[0] \/ np.diff(ax2.get_ylim())[0] asp \/= np.abs(np.diff(ax1.get_xlim())[0] \/ np.diff(ax1.get_ylim())[0]) ax2.set_aspect(asp) More sophisticated solutions: This answer for using the subplot parameters to achieve a certain aspect. If you want to use mpl_toolkits and make your hands dirty, this answer would be a good read. \nAPI:\nmatplotlib.pyplot\n","label":[[440,446,"Mention"],[1159,1176,"API"]],"Comments":[]}
{"id":59396,"text":"ID:44690862\nPost:\nText: What happened is your axes_class becomes \"a custom (and very experimental) Axes class\" whose ticklabels cannot be processed by fig.autofmt_xdate(). In order to set ticklabels, you have to follow this documentation. As you can imagine, you have two options: \nText: Use axes_class=AA.Axes: you will need to do 3 things to achieve the result of fig.autofmt_xdate() a. Rotate ticklabels by host.axis[\"bottom\"].major_ticklabels.set_rotation(30) b. Set alignment by host.axis[\"bottom\"].major_ticklabels.set_ha(\"right\") c. Move x-axis label by host.axis[\"bottom\"].label.set_pad(30) A complete example and its result: from mpl_toolkits.axes_grid1 import host_subplot import mpl_toolkits.axisartist as AA import pyplot as plt host = host_subplot(111, axes_class=AA.Axes) plt.subplots_adjust(right=0.75) par1 = host.twinx() par2 = host.twinx() offset = 60 new_fixed_axis = par2.get_grid_helper().new_fixed_axis par2.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par2, offset=(offset, 0)) par2.axis[\"right\"].toggle(all=True) host.set_xlim(0, 2000000) host.set_ylim(0, 2) host.set_xlabel(\"Distance\") host.set_ylabel(\"Density\") par1.set_ylabel(\"Temperature\") par2.set_ylabel(\"Velocity\") p1, = host.plot([0, 1000000, 2000000], [0, 1, 2], label=\"Density\") p2, = par1.plot([0, 1000000, 2000000], [0, 3, 2], label=\"Temperature\") p3, = par2.plot([0, 1000000, 2000000], [50, 30, 15], label=\"Velocity\") par1.set_ylim(0, 4) par2.set_ylim(1, 65) host.legend() host.axis[\"left\"].label.set_color(p1.get_color()) par1.axis[\"right\"].label.set_color(p2.get_color()) par2.axis[\"right\"].label.set_color(p3.get_color()) host.axis[\"bottom\"].major_ticklabels.set_rotation(30) host.axis[\"bottom\"].major_ticklabels.set_ha(\"right\") host.axis[\"bottom\"].label.set_pad(30) plt.draw() plt.show() \nText: Do not use axes_class=AA.Axes so that you can use fig.autofmt_xdate(): you will need to remove the , axes_class=AA.Axes part as well as the following 3 lines since they are axisartist.Axes specific. new_fixed_axis = par2.get_grid_helper().new_fixed_axis par2.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=par2, offset=(offset, 0)) par2.axis[\"right\"].toggle(all=True) However, your 3rd axis will overlap your 2nd axis. You need to use the following 2 lines to move it right: par2.spines[\"right\"].set_position(('outward', offset)) par2.spines[\"right\"].set_visible(True) You can use fig.autofmt_xdate() now. A complete example and its result: from mpl_toolkits.axes_grid1 import host_subplot import pyplot as plt host = host_subplot(111) plt.subplots_adjust(right=0.75) par1 = host.twinx() par2 = host.twinx() offset = 60 par2.spines[\"right\"].set_position(('outward', offset)) par2.spines[\"right\"].set_visible(True) host.set_xlim(0, 2000000) host.set_ylim(0, 2) host.set_xlabel(\"Distance\") host.set_ylabel(\"Density\") par1.set_ylabel(\"Temperature\") par2.set_ylabel(\"Velocity\") p1, = host.plot([0, 1000000, 2000000], [0, 1, 2], label=\"Density\") p2, = par1.plot([0, 1000000, 2000000], [0, 3, 2], label=\"Temperature\") p3, = par2.plot([0, 1000000, 2000000], [50, 30, 15], label=\"Velocity\") par1.set_ylim(0, 4) par2.set_ylim(1, 65) host.legend() host.axis[\"left\"].label.set_color(p1.get_color()) par1.axis[\"right\"].label.set_color(p2.get_color()) par2.axis[\"right\"].label.set_color(p3.get_color()) plt.gcf().autofmt_xdate() plt.draw() plt.show() \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[727,733,"Mention"],[2490,2496,"Mention"],[3337,3354,"API"],[3355,3372,"API"]],"Comments":[]}
{"id":59397,"text":"ID:44711187\nPost:\nText: It is really hard to reproduce this problem, so I'll try to give some general advises and try to guess the actual root of the problem. \nText: First of all, it is in your best interest to use virtualenvs, if you aren't using them already. You will have a requirements.txt file in your project and will freeze the requirements from your home computer (the one that works) into requirements.txt, then will create a new virtualenv on the computer at work and finally install the requirements. That way you will be sure that you have the same versions of all packages on both computers. \nText: After that you should try and see if it works. If it doesn't please try these things and provide more details: \nText: Do you see any errors or warnings when you run it on the computer at work? Can you do very basic plots using matplotlib? Like this one: import mpl.pyplot as plt plt.plot([1, 2, 3, 4]) plt.ylabel('some numbers') plt.show() If the example from 2 doesn't work, try to replace plt.show() with plt.savefig('numbers.png') and see if the figure is successfully saved. If that's the case, then you have some problems with matplotlib's interactivity. If you can't see a file named numbers.png, then probably there is something wrong with matplotlib's installation in general, not just the animation part. Or maybe with the installation of some package matplotlib relies on, like Tkinter, for instance. \nText: Instead of going into further hypothetical scenarios, I'll stop here and wait for more details. \nText: p.s. Links you may find useful if there is problem with showing the generated plots\/animations in a window: \nText: How can I set the 'backend' in matplotlib in Python? \nText: http:\/\/matplotlib.org\/faq\/usage_faq.html#what-is-a-backend \nAPI:\nmatplotlib.pyplot\n","label":[[874,884,"Mention"],[1774,1791,"API"]],"Comments":[]}
{"id":59398,"text":"ID:44811541\nPost:\nText: Adapting the candlestick function from the matplotlib finance package (documentation, code): \nCode: def westerncandlestick(ax, quotes, width=0.2, colorup='k', colordown='r', \n                 ochl=True, linewidth=0.5):\n\n    \"\"\"\n    Plot the time, open, high, low, close as a vertical line ranging\n    from low to high.  Use a rectangular bar to represent the\n    open-close span.  If close >= open, use colorup to color the bar,\n    otherwise use colordown\n    Parameters\n    ----------\n    ax : `Axes`\n        an Axes instance to plot to\n    quotes : sequence of quote sequences\n        data to plot.  time must be in float date format - see date2num\n        (time, open, high, low, close, ...) vs\n        (time, open, close, high, low, ...)\n        set by `ochl`\n    width : float\n        fraction of a day for the open and close lines\n    colorup : color\n        the color of the lines close >= open\n    colordown : color\n         the color of the lines where close <  open\n    ochl: bool\n        argument to select between ochl and ohlc ordering of quotes\n    linewidth: float\n        linewidth of lines\n    Returns\n    -------\n    ret : tuple\n        returns (lines, openlines, closelines) where lines is a list of lines\n        added\n    \"\"\"\n\n    OFFSET = width \/ 2.0\n\n    lines = []\n    openlines = []\n    closelines = []\n    for q in quotes:\n        if ochl:\n            t, open, close, high, low = q[:5]\n        else:\n            t, open, high, low, close = q[:5]\n\n        if close >= open:\n            color = colorup\n        else:\n            color = colordown\n\n        vline = Line2D( xdata=(t, t), ydata=(low, high),\n            color=color, linewidth=linewidth, antialiased=True)\n        lines.append(vline)\n\n        openline = Line2D(xdata=(t - OFFSET, t), ydata=(open,open),\n                          color=color, linewidth=linewidth, antialiased=True)\n        openlines.append(openline)\n\n        closeline = Line2D(xdata=(t , t+OFFSET), ydata=(close,close),\n                          color=color, linewidth=linewidth, antialiased=True)\n        closelines.append(closeline)\n\n        ax.add_line(vline)\n        ax.add_line(openline)\n        ax.add_line(closeline)\n\n    ax.autoscale_view()\n\n    return lines, openlines, closelines\n\nText: call it e.g. like this: \nCode: westerncandlestick(ax, quotes, width=0.6, linewidth=1.44, ochl=False)\n\nText: Of course you may adapt the colors using colorup and colordown argument. \nText: Complete code to reproduce the above plot: \nText: import pyplot as plt from matplotlib.finance import quotes_historical_yahoo_ohlc from matplotlib.lines import Line2D def westerncandlestick(ax, quotes, width=0.2, colorup='k', colordown='r', ochl=True, linewidth=0.5): \"\"\" Plot the time, open, high, low, close as a vertical line ranging from low to high. Use a rectangular bar to represent the open-close span. If close >= open, use colorup to color the bar, otherwise use colordown Parameters ---------- ax : `Axes` an Axes instance to plot to quotes : sequence of quote sequences data to plot. time must be in float date format - see date2num (time, open, high, low, close, ...) vs (time, open, close, high, low, ...) set by `ochl` width : float fraction of a day for the open and close lines colorup : color the color of the lines close >= open colordown : color the color of the lines where close < open ochl: bool argument to select between ochl and ohlc ordering of quotes linewidth: float linewidth of lines Returns ------- ret : tuple returns (lines, openlines, closelines) where lines is a list of lines added \"\"\" OFFSET = width \/ 2.0 lines = [] openlines = [] closelines = [] for q in quotes: if ochl: t, open, close, high, low = q[:5] else: t, open, high, low, close = q[:5] if close >= open: color = colorup else: color = colordown vline = Line2D( xdata=(t, t), ydata=(low, high), color=color, linewidth=linewidth, antialiased=True) lines.append(vline) openline = Line2D(xdata=(t - OFFSET, t), ydata=(open,open), color=color, linewidth=linewidth, antialiased=True) openlines.append(openline) closeline = Line2D(xdata=(t , t+OFFSET), ydata=(close,close), color=color, linewidth=linewidth, antialiased=True) closelines.append(closeline) ax.add_line(vline) ax.add_line(openline) ax.add_line(closeline) ax.autoscale_view() return lines, openlines, closelines from mdates import DateFormatter, WeekdayLocator,\\ DayLocator, MONDAY # (Year, month, day) tuples suffice as args for quotes_historical_yahoo date1 = (2004, 2, 1) date2 = (2004, 4, 12) mondays = WeekdayLocator(MONDAY) # major ticks on the mondays alldays = DayLocator() # minor ticks on the days weekFormatter = DateFormatter('%b %d') # e.g., Jan 12 dayFormatter = DateFormatter('%d') # e.g., 12 quotes = quotes_historical_yahoo_ohlc('INTC', date1, date2) if len(quotes) == 0: raise SystemExit fig, ax = plt.subplots() fig.subplots_adjust(bottom=0.2) ax.xaxis.set_major_locator(mondays) ax.xaxis.set_minor_locator(alldays) ax.xaxis.set_major_formatter(weekFormatter) westerncandlestick(ax, quotes, width=0.6, linewidth=1.44, ochl=False) ax.xaxis_date() ax.autoscale_view() plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right') plt.show() \nAPI:\nmatplotlib.pyplot\nmatplotlib.dates\n","label":[[2521,2527,"Mention"],[4336,4342,"Mention"],[5201,5218,"API"],[5219,5235,"API"]],"Comments":[]}
{"id":59399,"text":"ID:44868640\nPost:\nText: One may use a il.inset_axes to place an axes inside another axes. This axes can be used to host the colorbar. Its position is relative the the parent axes, similar to how legends are placed, using a loc argument (e.g. loc=3 means lower left). Its width and height can be specified in absolute numbers (inches) or relative to the parent axes (percentage). \nCode: cbaxes = inset_axes(ax1, width=\"30%\", height=\"3%\", loc=3) \n\nCode: import matplotlib.pyplot as plt \nimport numpy as np\nimport matplotlib.gridspec as gridspec\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\nx = np.random.randn(60) \ny = np.random.randn(60)\nz = [np.random.random() for _ in range(60)]\n\nfig = plt.figure()\ngs = gridspec.GridSpec(1, 2)\n\nax0 = plt.subplot(gs[0, 0])\nplt.scatter(x, y, s=20)\n\nax1 = plt.subplot(gs[0, 1])\ncm = plt.cm.get_cmap('RdYlBu_r')\nplt.scatter(x, y, s=20 ,c=z, cmap=cm)\n\nfig.tight_layout()\n\ncbaxes = inset_axes(ax1, width=\"30%\", height=\"3%\", loc=3) \nplt.colorbar(cax=cbaxes, ticks=[0.,1], orientation='horizontal')\n\n\nplt.show()\n\nText: Note that in order to suppress the warning, one might simply call tight_layout prior to adding the inset axes. \nAPI:\nmpl_toolkits.axes_grid1.inset_locator.inset_axes\n","label":[[38,51,"Mention"],[1182,1230,"API"]],"Comments":[]}
{"id":59400,"text":"ID:44875742\nPost:\nText: You will need more than just pandas to get your desired output. Pandas is a data analysis tool with a limited number of visualization features. In fact, all the visualization features it possesses are built on top of matplotlib. That being the case, I would recommend using a basic scatterplot from the matplotlib toolkit. I have used your starting dataset to draw something similar to what your output looks like. The following should constitute a decent starting point: \nCode: import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport pandas as pd\n\n\nd = {'action': ['login', 'logout', 'login', 'login', 'logout', 'logout'],\n 'ts': [pd.Timestamp('2017-07-01 09:01:10'),\n  pd.Timestamp('2017-07-01 09:01:20'),\n  pd.Timestamp('2017-07-01 09:01:15'),\n  pd.Timestamp('2017-07-01 09:03:15'),\n  pd.Timestamp('2017-07-01 09:04:03'),\n  pd.Timestamp('2017-07-01 09:05:50')],\n 'uid': ['A', 'A', 'B', 'A', 'A', 'B']}\n\n\ndf = pd.DataFrame(d)\nuuids = {k:v for v, k in enumerate(df.uid.unique(), 1)}\n\ndf = df.assign(uid_n = df.uid.apply(lambda v: uuids.get(v)))\n\nfig = plt.figure(figsize=(8,5))\nax = fig.add_subplot(111)\nxfmt = mdates.DateFormatter('%Y-%m-%d %H:%M:%S')\nax.xaxis.set_major_formatter(xfmt)\nax.set_xlabel('time')\nax.set_xticks(df.ts.values)\nax.set_yticks(df.uid_n)\nax.set_yticklabels(df.uid)\nax.plot_date(x = df.ts, y = df.uid_n, marker='*', color='black')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n\nText: The main tools used here are the pyplot and dates modules. The first module allows you to create axes, add labels and ticks to them and add a plot. The second one allows you to format the date values on the x-axis to show date values. \nText: One important thing to mention here is that I created an integer column called uuid_n, which is used in the process of creating a scatterplot. This is because string values are not accepted on the y-axis. So, I created a dictionary in which every unique string value is mapped to a unique integer value. Once this is done, you can then use the uuid column to create ticks and labels. \nText: The snippet above should yield the following: \nText: I hope this helps. \nAPI:\nmatplotlib.pyplot\nmatplotlib.dates\n","label":[[1490,1496,"Mention"],[1501,1506,"Mention"],[2168,2185,"API"],[2186,2202,"API"]],"Comments":[]}
{"id":59401,"text":"ID:45076855\nPost:\nText: sns.plt.show() works fine for me using seaborn 0.7.1. Could be that this is different in other versions. However, if you anyways import plt as plt you may as well simply use plt.show(), as sns.plt.show() is only working because pyplot is available inside the seaborn namespace. \nAPI:\nmatplotlib.pyplot\n","label":[[160,163,"Mention"],[308,325,"API"]],"Comments":[]}
{"id":59402,"text":"ID:45080859\nPost:\nText: You can't pass the words into the abar directly. However you could create an indices array for bar and then replace these indices with the words using matplotlib.pyplot.xticks: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\nindices = np.arange(len(example_list))\nplt.bar(indices, frequency, color='r')\nplt.xticks(indices, word, rotation='vertical')\nplt.tight_layout()\nplt.show()\n\nText: The for-loop to create word and frequency could also be replaced by a simple zip und list unpacking: \nCode: word, frequency = zip(*example_list)\n\nAPI:\nmatplotlib.pyplot.bar\n","label":[[58,62,"Mention"],[573,594,"API"]],"Comments":[]}
{"id":59403,"text":"ID:45090330\nPost:\nText: bar takes as obligatory paramaters two sequence of scalars: the x coordinates of the left sides of the bars and the heights of the bars. So you should use range to get the paramater needed, and then use plt.xticks to set your desired ticks: \nCode: import matplotlib.pyplot as plt\n\ndegree_distri = {'F2': 102, 'EGFR': 23, 'C1R': 20}\nkeys, values = degree_distri.keys(), degree_distri.values()\nplt.bar(range(len(values)), values, color='r')\nplt.xticks(range(len(values)), keys)\nplt.show()\n\nAPI:\nmatplotlib.pyplot.bar\n","label":[[24,27,"Mention"],[517,538,"API"]],"Comments":[]}
{"id":59404,"text":"ID:45220585\nPost:\nText: I assume you have read the linked question and it's answer. It clearly states \nText: Colormaps are always ranged between 0 and 1. \nText: and further explains how to arrive at a suitable colormap. It thus makes no sense to supply values of 10 or -9 to the colormap. \nText: While you could directly copy the code from the answer here and would receive a decent result, you may of course also refine it to match this specific case, where one may go for 80 colors, 30 from the lower part of the colormap, 30 from the upper part and the remaining 20 in the middle to be white. \nCode: n=30\nx = 0.5\nlower = plt.cm.seismic(np.linspace(0, x, n))\nwhite = plt.cm.seismic(np.ones(80-2*n)*x)\nupper = plt.cm.seismic(np.linspace(1-x, 1, n))\ncolors = np.vstack((lower, white, upper))\ntmap = matplotlib.colors.LinearSegmentedColormap.from_list('map_white', colors)\n\nText: In order to get the jet colormap, which does not have white in it, an array of ones may be used in the middle \nCode: n=30\nx = 0.5\ncmap = plt.cm.jet\nlower = cmap(np.linspace(0, x, n))\nwhite = np.ones((80-2*n,4))\nupper = cmap(np.linspace(1-x, 1, n))\ncolors = np.vstack((lower, white, upper))\ntmap = matplotlib.colors.LinearSegmentedColormap.from_list('map_white', colors)\n\nText: Complete code to reproduce the image above: \nText: import numpy as np import plt as plt import matplotlib.colors n=30 x = 0.5 cmap = plt.cm.jet lower = cmap(np.linspace(0, x, n)) white = np.ones((80-2*n,4)) upper = cmap(np.linspace(1-x, 1, n)) colors = np.vstack((lower, white, upper)) tmap = matplotlib.colors.LinearSegmentedColormap.from_list('map_white', colors) x = np.linspace(0,10) X,Y = np.meshgrid(x,x) z = np.sin(X) * np.cos(Y*0.4)*40 fig, ax = plt.subplots() im = ax.contourf(z, cmap=tmap, vmin=-40, vmax=40) plt.colorbar(im) plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[1333,1336,"Mention"],[1809,1826,"API"]],"Comments":[]}
{"id":59405,"text":"ID:45236431\nPost:\nText: The problem is arising because numpy.histogram2d and mpl.pyplot.pcolormesh interpret rows and columns inverted. You can see this with a very simple example, e.g. producing and then plotting a histogram of a single point at (0.75, 0.25) with bins of 0.0-0.5 and 0.5-1.0 in both directions: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.asarray([0.75])\ny = np.asarray([0.25])\nbin_edges = np.asarray([0.0, 0.5, 1.0])  # called gridx\/gridy above\nhist,_,_ = np.histogram2d(x, y, bins=[bin_edges, bin_edges]) \n                                         # called grid above\nprint(hist)\n# array([[ 0.,  0.],\n#        [ 1.,  0.]])\nplt.gca().set_aspect(\"equal\")\nplt.pcolormesh(bin_edges, bin_edges, hist)\nplt.scatter(x, y)\nplt.show()\n\nText: gives: \nText: While histogram2d interprets row-indices as x-direction and column-indices as y-direction (meaning that columns share an x-value and rows share an y-value), the inverse is true for pcolormesh. To get the correct behavior you can change the plot command to: \nCode: plt.gca().set_aspect(\"equal\")\nplt.pcolormesh(bin_edges, bin_edges, hist.T)\nplt.scatter(x, y)\nplt.show()\n\nText: which gives: \nAPI:\nmatplotlib.pyplot.pcolormesh\n","label":[[77,98,"Mention"],[1181,1209,"API"]],"Comments":[]}
{"id":59406,"text":"ID:45264604\nPost:\nText: To show a plot, you need to call \nCode: plt.show()\n\nText: where plt is import plt as plt. \nAPI:\nmatplotlib.pyplot\n","label":[[102,105,"Mention"],[120,137,"API"]],"Comments":[]}
{"id":59407,"text":"ID:45280668\nPost:\nText: You are correct that sqlite3.Cursor.fetchall() returns a (possibly empty) list of sqlite3.Row mappable\/iterable objects, you just reversed the order - you try to parse temperature field as a date. And your values variable is not defined. \nText: Try this: \nCode:     c.execute('SELECT temperature, humidity, feelslike, timenow FROM external')\n    data = c.fetchall()\n    # data[*][0] = temperature\n    # data[*][1] = humidity\n    # data[*][2] = feelslike\n    # data[*][3] = timenow\n\n    temperature = []\n    humidity = []\n    feelslike = []\n    timenow = []\n\n    for row in data:\n        temperature.append(row[0])\n        humidity.append(row[1])\n        feelslike.append(row[2])\n        timenow.append(parser.parse(row[3]))\n\nText: Additionally, as Row in addition to access by index also supports mapping access by column name, you could do: \nCode:     for row in data:\n        temperature.append(row['temperature'])\n        humidity.append(row['humidity'])\n        feelslike.append(row['feelslike'])\n        timenow.append(parser.parse(row['timenow']))\n\nText: However access by index is faster and more efficient. \nText: For plotting, you need to call plot multiple times, each time with the same x and different y data. For test, I generated my data set with: \nCode:     random.seed(0)\n    data = [(random.uniform(10, 25),\n             random.uniform(70, 99),\n             random.uniform(15, 30),\n             \"2017-07-24 12:{:02d}:00.{:06d}\".format(i, i * 1000 + int(random.uniform(0, 50)))\n             ) for i in range(10)]\n\nText: instead of using a database and data = c.fetchall(). \nText: Now you have a set of float data points and associated datetime.datetime objects. However, mpl.axes.Axes.plot_date function accepts: \nText: x and\/or y can be a sequence of dates represented as float days since 0001-01-01 UTC. \nText: To convert the date from datetime.datetime object to float days for matplotlib, function mdates.date2num is used. \nCode:    for row in data:\n       temperature.append(row[0])\n       humidity.append(row[1])\n       feelslike.append(row[2])\n       timenow.append(parser.parse(row[3]))\n\n   # Convert datetime.datetime to float days since 0001-01-01 UTC.\n   dates = [date2num(t) for t in timenow]\n\n   fig = plt.figure()\n   ax1 = fig.add_subplot(111)\n   ax1.set_title(\"My environmental data\")\n\n   # Configure x-ticks\n   ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d.%m.%Y %H:%M'))\n\n   # Plot temperature data on left Y axis\n   ax1.set_ylabel(\"Temperature [C]\")\n   ax1.plot_date(dates, temperature, '-', label=\"Temperature\", color='r')\n   ax1.plot_date(dates, feelslike, '-', label=\"Feels like\", color='b')\n\n   # Plot humidity data on right Y axis\n   ax2 = ax1.twinx()\n   ax2.set_ylabel(\"Humidity [% RH]\")\n   ax2.plot_date(dates, humidity, '-', label=\"Humidity\", color='g')\n\n   # Format the x-axis for dates (label formatting, rotation)\n   fig.autofmt_xdate(rotation=60)\n   fig.tight_layout()\n\n   # Show grids and legends\n   ax1.grid(True)\n   ax1.legend(loc='best', framealpha=0.5)\n   ax2.legend(loc='best', framealpha=0.5)\n\n   plt.savefig(\"figure.png\")\n\nText: This code produces the following image: \nAPI:\nmatplotlib.axes.Axes.plot_date\nmatplotlib.dates.date2num\n","label":[[1711,1734,"Mention"],[1942,1957,"Mention"],[3163,3193,"API"],[3194,3219,"API"]],"Comments":[]}
{"id":59408,"text":"ID:45349235\nPost:\nText: Seaborn heatmap is a categorical plot. It scales from 0 to number of columns - 1, in this case from 0 to 366. The datetime locators and formatters expect values as dates (or more precisely, numbers that correspond to dates). For the year in question that would be numbers between 730120 (= 01-01-2000) and 730486 (= 01-01-2001). \nText: So in order to be able to use dates formatters and locators, you would need to convert your dataframe index to datetime objects first. You can then not use a heatmap, but a plot that allows for numerical axes, e.g. an imshow plot. You may then set the extent of that imshow plot to correspond to the date range you want to show. \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndf = pd.DataFrame(np.random.randn(367, 5), \n                 index = pd.DatetimeIndex(start='01-01-2000', end='01-01-2001', freq='1D'))\n\ndates = df.index.to_pydatetime()\ndnum = mdates.date2num(dates)\nstart = dnum[0] - (dnum[1]-dnum[0])\/2.\nstop = dnum[-1] + (dnum[1]-dnum[0])\/2.\nextent = [start, stop, -0.5, len(df.columns)-0.5]\n\nfig, ax = plt.subplots()\nim = ax.imshow(df.T.values, extent=extent, aspect=\"auto\")\n\nax.xaxis.set_major_locator(mdates.MonthLocator())\nax.xaxis.set_minor_locator(mdates.DayLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n\nfig.colorbar(im)\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[390,395,"Mention"],[1405,1421,"API"]],"Comments":[]}
{"id":59409,"text":"ID:45355570\nPost:\nText: From the mpl.pyplot api documentation of boxplot. boxplot has a whis parameter that specifies the range for the whiskers. With a dedault value of 1.5. \nText: whis : float, sequence, or string (default = 1.5) As a float, determines the reach of the whiskers to the beyond the first and third quartiles. In other words, where IQR is the interquartile range (Q3-Q1), the upper whisker will extend to last datum less than Q3 + whisIQR). Similarly, the lower whisker will extend to the first datum greater than Q1 - whisIQR. Beyond the whiskers, data are considered outliers and are plotted as individual points. Set this to an unreasonably high value to force the whiskers to show the min and max values. Alternatively, set this to an ascending sequence of percentile (e.g., [5, 95]) to set the whiskers at specific percentiles of the data. Finally, whis can be the string 'range' to force the whiskers to the min and max of the data. \nText: The default of the range of the whiskers thus is 1.5* the interquartile range. In practice that means that any value lower then Q1 - 1.5* the interquartile range and any value higher then Q3 + 1.5* the interquartile range will be considered an outlier when using the default value. \nText: Given a non default value the output will be adjusted for that value. \nAPI:\nmatplotlib.pyplot\n","label":[[33,43,"Mention"],[1327,1344,"API"]],"Comments":[]}
{"id":59410,"text":"ID:45379451\nPost:\nText: The approach you took is in principle correct. However, just like when placing a legend with bbox_to_anchor, the location is determined as an interplay between bbox_to_anchor and loc. Most of the explanation in the above linked answer applies here as well. \nText: The default loc for inset_axes is loc=1 (\"upper right\"). This means that if you you specify bbox_to_anchor=(0.4,0.1), those will be the coordinates of the upper right corner, not the lower left one. You would therefore need to specify loc=3 to have the lower left corner of the inset positionned at (0.4,0.1). \nText: However, specifying a bounding as a 2-tuple only makes sense if not specifying the width and height in relative units (\"30%\"). Or in other words, in order to use relative units you need to use a 4-tuple notation for the bbox_to_anchor. \nText: In case of specifying the bbox_to_anchor in axes units one needs to use the bbox_transform argument, again, just as with legends explained here, and set it to ax.transAxes. \nCode: plt.figure(figsize=(6,3))\nax = plt.subplot(221)\nax.set_title(\"100%, (0.5,1-0.3,.3,.3)\")\nax.plot(xdata, ydata)\naxins = inset_axes(ax, width=\"100%\", height=\"100%\", loc='upper left',\n                   bbox_to_anchor=(0.5,1-0.3,.3,.3), bbox_transform=ax.transAxes)\n\n\nax = plt.subplot(222)\nax.set_title(\"30%, (0.5,0,1,1)\")\nax.plot(xdata, ydata)\naxins = inset_axes(ax, width=\"30%\", height=\"30%\", loc='upper left',\n                   bbox_to_anchor=(0.5,0,1,1), bbox_transform=ax.transAxes)\n\nText: Find a complete example on the matplotlib page: Inset Locator Demo \nText: Another option is to use InsetPosition instead of inset_axes and to give an existing axes a new position. InsetPosition takes the x and y coordinates of the lower left corner of the axes in normalized axes coordinates, as well as the width and height as input. \nCode: import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n\nfig, ax= plt.subplots()\n\niax = plt.axes([0, 0, 1, 1])\nip = InsetPosition(ax, [0.4, 0.1, 0.3, 0.7]) #posx, posy, width, height\niax.set_axes_locator(ip)\n\niax.plot([1,2,4])\nplt.show()\n\nText: Finally one should mention that from matplotlib 3.0 on, you can use ax.Axes.inset_axes \nCode: import matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,3))\nax = plt.subplot(221)\nax.set_title(\"ax.inset_axes, (0.5,1-0.3,.3,.3)\")\nax.plot([0,4], [0,10])\naxins = ax.inset_axes((0.5,1-0.3,.3,.3))\n\nplt.show()\n\nText: The result is roughly the same, except that il.inset_axes allows for a padding around the axes (and applies it by default), while Axes.inset_axes does not have this kind of padding. \nAPI:\nmatplotlib.axes.Axes.inset_axes\nmpl_toolkits.axes_grid1.inset_locator.inset_axes\n","label":[[2215,2233,"Mention"],[2498,2511,"Mention"],[2642,2673,"API"],[2674,2722,"API"]],"Comments":[]}
{"id":59411,"text":"ID:45390874\nPost:\nText: Starting with datetime objects you may use matplotlib's date2num and num2date functions to convert to and from numerical values. The advantage is that the numerical data is then understood by mdates locators and formatters. \nCode: import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates\n\nidx_1 = datetime.datetime(2017,06,07,0,0,0)\nidx_2 = datetime.datetime(2017,07,27,0,0,0)\n\nidx = [idx_1, idx_2]\n\ny1 = 155.98\ny2 = 147.07\n\nx = matplotlib.dates.date2num(idx)\ny = [y1, y2]\nDifference = x[1] - x[0] #this helps to end the plotted line at specific point\ncoefficients = np.polyfit(x, y, 1)\npolynomial = np.poly1d(coefficients)\n# the np.linspace lets you set number of data points, line length.\nx_axis = np.linspace(x[0], x[1] + Difference, 3)  # linspace(start, end, num)\ny_axis = polynomial(x_axis)\nplt.plot(x_axis, y_axis)\nplt.plot(x[0], y[0], 'go')\nplt.plot(x[1], y[1], 'go')\n\nloc= matplotlib.dates.AutoDateLocator()\nplt.gca().xaxis.set_major_locator(loc)\nplt.gca().xaxis.set_major_formatter(matplotlib.dates.AutoDateFormatter(loc))\nplt.gcf().autofmt_xdate()\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[216,222,"Mention"],[1134,1150,"API"]],"Comments":[]}
{"id":59412,"text":"ID:45422749\nPost:\nText: You need to weigh everything by the length of the concatenated array. Also, you should keep a consistent bin size and histogram range. \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\n\nnp.random.seed(0)\ndf = DataFrame(np.random.normal(300, 100, 2000))  # Two normal distributions\ndf2 = DataFrame(np.random.normal(700, 100, 1500))\ndf_merged = pd.concat([df, df2], ignore_index=True)\n\n# weights\ndf_weights = np.ones_like(df.values) \/ len(df_merged)\ndf2_weights = np.ones_like(df2.values) \/ len(df_merged)\ndf_merged_weights = np.ones_like(df_merged.values) \/ len(df_merged)\n\nplt_range = (df_merged.values.min(), df_merged.values.max())\nfig, ax = plt.subplots()\nax.hist(df.values, bins=100, weights=df_weights, color='black', histtype='step', label='df', range=plt_range)\nax.hist(df2.values, bins=100, weights=df2_weights, color='green', histtype='step', label='df2', range=plt_range)\nax.hist(df_merged.values, bins=100, weights=df_merged_weights, color='red', histtype='step', label='Combined', range=plt_range)\n\nax.margins(0.05)\nax.set_ylim(bottom=0)\nax.set_xlim([0, 1000])\nplt.legend(loc='upper right')\n# plt.savefig('output.png')\n\nText: See hist \nText: weights : (n, ) array_like or None, optional An array of weights, of the same shape as x. Each value in x only contributes its associated weight towards the bin count (instead of 1). \nAPI:\nmatplotlib.axes.Axes.hist\n","label":[[1223,1227,"Mention"],[1424,1449,"API"]],"Comments":[]}
{"id":59413,"text":"ID:45433424\nPost:\nText: The problem is, that the numpy ufunc isfinite is not defined for the numpy.datetime64 dtype. There is an effort to change this, though. This issue on numpy's github is being worked on in this pull-request, but as long as this is not finished up and merged, you will not be able to use isfinite on that dtype. This is a problem as fill_between is using this function implicitly, when calling numpy.ma.masked_invalid to mask all invalid entries of your input array. \nText: There is a work-around though. As pointed out in this answer to a similar question concerning fill_between plotting of a pandas Series of datetime64 type, pandas registers a custom converter for (among others) numpy arrays of datetime64 dtype with matplotlib. To make use of that, you simply have to import pandas: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\n# import pandas for its converter that is then used in pyplot!\nimport pandas\n\nx1 = np.arange(0.0, 2, 0.01)\nnow = np.datetime64(datetime.datetime.now())\nx = np.array([now - np.timedelta64(datetime.timedelta(seconds=i))\n              for i in range(200)])\ny1 = np.sin(2 * np.pi * x1)\ny2 = 1.2 * np.sin(4 * np.pi * x1)\n\nfig, ax1 = plt.subplots(1, 1, sharex=True)\n\n# Test support for masked arrays.\nax1.fill_between(x, 0, y1)\nax1.set_ylabel('between y1 and 0')\ny2 = np.ma.masked_greater(y2, 1.0)\nax1.plot(x, y1, x, y2, color='black')\nax1.fill_between(\n    x, y1, y2, where=y2 >= y1,\n    facecolor='green',\n    interpolate=True)\nax1.fill_between(x, y1, y2, where=y2 <= y1, facecolor='red', interpolate=True)\nax1.set_title('Now regions with y2>1 are masked')\n\n# Show the plot.\nplt.show()\n\nText: will work and give you your desired output: \nAPI:\nmatplotlib.pyplot.fill_between\n","label":[[354,366,"Mention"],[1723,1753,"API"]],"Comments":[]}
{"id":59414,"text":"ID:45549150\nPost:\nText: If you do not need interactive backend, you need to do this in your import statements. The order is very important. First matplotlib, then use and finally import pyplot. \nCode: import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nText: This changes the backend to a non-interactive one. If you do need an interactive backend, then ignore this and use ssh -X when logging into the remote host. It allows X11 forwarding. \nAPI:\nmatplotlib.use\n","label":[[163,166,"Mention"],[469,483,"API"]],"Comments":[]}
{"id":59415,"text":"ID:45704659\nPost:\nText: First of all you have to convert pandas date objects to python date objects. This conversion is needed because of matplotlib internal date conversion functions. Then use functions from mpl.dates to set desired formatter and tick positions like here: \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport matplotlib.dates as mdates\n\n# convert date objects from pandas format to python datetime\nindex = pd.date_range(start = \"2015-07-01\", end = \"2017-01-01\", freq = \"D\")\nindex = [pd.to_datetime(date, format='%Y-%m-%d').date() for date in index]\ndata = np.random.randint(1,100, size=len(index))\ndf = pd.DataFrame(data=data,index=index, columns=['data'])\nprint (df.head())\n\nax = df.plot()\n# set monthly locator\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n# set formatter\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))\n# set font and rotation for date tick labels\nplt.gcf().autofmt_xdate()\n\nplt.show()\n\nText: For season labels you have to construct it by yourself and then set it with plt.setp function (for month 02 set label winter, 04 - spring etc.): plt.setp(new_labels, rotation=90, fontsize=9). \nText: head of df: \nCode:             data\n2015-07-01    26\n2015-07-02    33\n2015-07-03    46\n2015-07-04    69\n2015-07-05    17\n\nAPI:\nmatplotlib.dates\n","label":[[209,218,"Mention"],[1317,1333,"API"]],"Comments":[]}
{"id":59416,"text":"ID:45717773\nPost:\nText: To fix the tick locations to exactly the dates that the data points are located at, you need to use a Locator as well as a Formatter. \nText: The locator should be a mticker.FixedLocator which sets the locations according to the input data. The formatter can be any dates formatter. \nCode: data = [[u'2017-07-03', 427],\n        [u'2017-07-10', 201],\n        [u'2017-07-17', 594],\n        [u'2017-07-24', 525],\n        [u'2017-07-31', 239],\n        [u'2017-08-07', 437]]\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates\nimport matplotlib.ticker\nimport datetime\n\nx,y = zip(*data)\nf = lambda s: datetime.datetime.strptime(s, '%Y-%m-%d')\nx = list(map(f, x))\n\nfig, ax = plt.subplots()\nax.plot(x,y)\n\nloc = matplotlib.ticker.FixedLocator(matplotlib.dates.date2num(x) )\nfmt = matplotlib.dates.DateFormatter('%Y-%m-%d')\nax.xaxis.set_major_locator(loc)\nax.xaxis.set_major_formatter(fmt)\n\nfig.autofmt_xdate()\nplt.show()\n\nAPI:\nmatplotlib.ticker.FixedLocator\nmatplotlib.dates\n","label":[[189,209,"Mention"],[289,294,"Mention"],[946,976,"API"],[977,993,"API"]],"Comments":[]}
{"id":59417,"text":"ID:45725032\nPost:\nText: That's definitely possible but I'm not sure if it's easy to do with hist because that doesn't return the histogram data. You would have to do another matplotlib.pyplot.hist (or numpy.hist) to get the actual bins and heights. \nText: However if you use matplotlib directly this would work: \nCode: import matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\n\nimport numpy as np\n\ndata = np.random.normal(550, 20, 100000)\n\nfig, ax = plt.subplots(1, 1)\nplot = ax.hist(data, bins=30, label='Cycle time', color='darkgrey')\n\nps = np.percentile(data, [5, 95])\n_, ymax = ax.get_ybound()\n\n# Search for the heights of the bins in which the percentiles are\nheights = plot[0][np.searchsorted(plot[1], ps, side='left')-1]\n\n# The height should be the bin-height divided by the y_bound (at least if y_min is zero)\nax.axvline(ps[0], label='5%', color='red', linestyle='dashed', linewidth=2, ymax=heights[0] \/ ymax)\nax.axvline(ps[1], label='95%', color='blue', linestyle='dashed', linewidth=2, ymax=heights[1] \/ ymax)\nplt.legend()\n\nText: In case you don't want to bother with calculating the relative height, you could also use Lines2D from matplotlib.lines \nCode: import matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\n\nplt.style.use('ggplot')\n\nimport numpy as np\n\ndata = np.random.normal(550, 20, 100000)\n\nfig, ax = plt.subplots(1, 1)\nplot = ax.hist(data, bins=30, label='Cycle time', color='darkgrey')\n\nps = np.percentile(data, [5, 95])\n\n# Search for the heights of the bins in which the percentiles are\nheights = plot[0][np.searchsorted(plot[1], ps, side='left')-1]\n\n# The height should be the bin-height divided by the y_bound (at least if y_min is zero)\nl1 = mlines.Line2D([ps[0], ps[0]], [0, heights[0]], label='5%', color='red', linestyle='dashed', linewidth=2)\nl2 = mlines.Line2D([ps[1], ps[1]], [0, heights[1]], label='95%', color='blue', linestyle='dashed', linewidth=2)\nax.add_line(l1)\nax.add_line(l2)\nplt.legend()\n\nAPI:\npandas.DataFrame.hist\n","label":[[92,96,"Mention"],[1945,1966,"API"]],"Comments":[]}
{"id":59418,"text":"ID:45734500\nPost:\nText: Changing the backend \nText: The issue seems only present using the Tk backend. Using the Qt backend, the window would stay where it was while updating with plt.pause. \nText: To change the backend use those lines at the beginning of your script. \nCode: import matplotlib\nmatplotlib.use(\"Qt4agg\") # or \"Qt5agg\" depending on you version of Qt\n\nText: Modifying plt.pause \nText: If changing the backend is not an option, the following might help. The cause of the window constantly popping up to the front comes from plt.pause calling plt.show() internally. You therefore implement you own pause function, without calling show. This requires to be in interactive mode plt.ion() first and then at least once call plt.show(). Afterwards you may update the plot with the custom mypause function as shown below. \nCode: import matplotlib\nmatplotlib.use(\"TkAgg\")\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom random import random\n\nplt.ion()\n# set up the figure\nfig = plt.figure()\nplt.xlabel('Time')\nplt.ylabel('Value')\n\nplt.show(block=False)\n\ndef mypause(interval):\n    backend = plt.rcParams['backend']\n    if backend in matplotlib.rcsetup.interactive_bk:\n        figManager = matplotlib._pylab_helpers.Gcf.get_active()\n        if figManager is not None:\n            canvas = figManager.canvas\n            if canvas.figure.stale:\n                canvas.draw()\n            canvas.start_event_loop(interval)\n            return\n\n\nt0 = time()\nt = []\ny = []\nwhile True:\n    t.append( time()-t0 )\n    y.append( random() )\n    plt.gca().clear()\n    plt.plot( t , y )\n    mypause(1)\n\nText: Using an animation. \nText: Finally, using a matplotlib.animation class would render all of the above obsolete. An example for FuncAnimation is shown on the matplotlib page. \nAPI:\nmatplotlib.animation.FuncAnimation\n","label":[[1735,1748,"Mention"],[1788,1822,"API"]],"Comments":[]}
{"id":59419,"text":"ID:45773726\nPost:\nText: The way you create the white middle part in the above code is by obfuscating the center of the pie by a circle. This can of course not procude a transparent interior. \nText: A solution to this would also be found in the more sophisticated question Double donut chart in matplotlib. Let me go into detail: \nText: In order to produce a true donut chart with a hole in the middle, one would need to cut the wedges such that they become partial rings. Fortunately, matplotlib provides the tools to do so. A pie chart consists of several wedges. From the Wedge documentation we learn \nText: class matplotlib.patches.Wedge(center, r, theta1, theta2, width=None, **kwargs) Wedge shaped patch. [...] If width is given, then a partial wedge is drawn from inner radius r - width to outer radius r. \nText: In order to give set the width to all wedges, an easy method is to use plt.setp \nCode: wedges, _ = ax.pie([20,80], ...)\nplt.setp( wedges, width=0.25)\n\nText: Complete example: \nCode: import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nfig.set_facecolor(\"#fff9c9\") # set yellow background color to see effect\n\nwedges, text, autotext = ax.pie([25, 40], colors=['limegreen','crimson'],\n                                labels=['Correct', 'Wrong'], autopct='%1.1f%%')\nplt.setp( wedges, width=0.25)\n\nax.set_aspect(\"equal\")\n# the produced png will have a transparent background\nplt.savefig(__file__+\".png\", transparent=True)\nplt.show()\n\nText: The following would be a way to tackle the problem if the Wedge did not have a width argument. Since the pie chart is centered at (0,0), copying the outer path coordinates, reverting them and multiplying by some number smaller 1 (called r for radius in below code), gives the coordinates of the inner ring. Joining those two list of coordinates and taking care of the proper path codes allows to create a ring shape as desired. \nCode: import matplotlib.pyplot as plt\nimport matplotlib.path as mpath\nimport matplotlib.patches as mpatches\nimport numpy as np\n\ndef cutwedge(wedge, r=0.8):\n    path = wedge.get_path()\n    verts = path.vertices[:-3]\n    codes = path.codes[:-3]\n    new_verts = np.vstack((verts , verts[::-1]*r, verts[0,:]))\n    new_codes =  np.concatenate((codes , codes[::-1], np.array([79])) )\n    new_codes[len(codes)] = 2\n    new_path = mpath.Path(new_verts, new_codes)\n    new_patch = mpatches.PathPatch(new_path)\n    new_patch.update_from(wedge)\n    wedge.set_visible(False)\n    wedge.axes.add_patch(new_patch)\n    return new_patch\n\nfig, ax = plt.subplots()\nfig.set_facecolor(\"#fff9c9\") # set yellow background color to see effect\n\n\nwedges, text, autotext = ax.pie([25, 75], colors=['limegreen','indigo'], \n                                labels=['Correct', 'Wrong'], autopct='%1.1f%%')\n\nfor w in wedges:\n    cutwedge(w)\n    # or try cutwedge(w, r=0.4)\n\nax.set_aspect(\"equal\")\n\n# the produced png will have a transparent background\nplt.savefig(__file__+\".png\", transparent=True)\nplt.show()\n\nAPI:\nmatplotlib.patches.Wedge\n","label":[[574,579,"Mention"],[2973,2997,"API"]],"Comments":[]}
{"id":59420,"text":"ID:45783189\nPost:\nText: There are just so many ways to tackle this. All of the following will give more or less the same image \nText: A. Reduce the available space \nText: You may reduce the available space such that both plots are constrained to the same vertical margins. This can be done by \nText: reducing figure height fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,2.3), ...) using subplots_adjust to limit the margins fig.subplots_adjust(top=0.7, bottom=0.3) \nText: B. Use InsetPosition \nText: You may use vInsetPosition to adjust the coordinates of the second axes to match those of the first one. \nCode: import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n\ndef visualize(arr):\n    fig, (ax1, ax2) = plt.subplots(1, 2,\n                               gridspec_kw = {'width_ratios': [1, 3]})\n\n    ax1.imshow(arr)\n\n    flat = arr.flatten()\n    x = flat[~np.isnan(flat)]\n    sns.distplot(x, ax=ax2)\n\n    ip = InsetPosition(ax1, [1.5,0,3,1]) \n    ax2.set_axes_locator(ip)\n\n    plt.show()\n\narr = np.random.randn(200,120)\nvisualize(arr)\n\nText: C. Use an axes divider \nText: You may create only the axes for the image and then use mpl_toolkits.axes_grid1.make_axes_locatable to create a new axes next to it. \nCode: import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef visualize(arr):\n    fig, ax = plt.subplots()\n    divider = make_axes_locatable(ax)\n    ax2 = divider.new_horizontal(size=\"300%\", pad=0.5)\n    fig.add_axes(ax2)\n\n    ax.imshow(arr)\n\n    flat = arr.flatten()\n    x = flat[~np.isnan(flat)]\n    sns.distplot(x, ax=ax2)\n\n    plt.show()\n\narr = np.random.randn(200,120)\nvisualize(arr)\n\nText: D. calculate the desired aspect ratio \nCode: import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize(arr):\n    gkw = {'width_ratios':[1, 3] }\n    fig, (ax1, ax2) = plt.subplots(1, 2,  gridspec_kw = gkw )\n\n    ax1.imshow(arr)\n\n    flat = arr.flatten()\n    x = flat[~np.isnan(flat)]\n    sns.distplot(x, ax=ax2)\n\n    ya = np.diff(np.array(ax2.get_ylim()))[0]\n    xa = np.diff(np.array(ax2.get_xlim()))[0]\n    wa = gkw['width_ratios'][0]\/float(gkw['width_ratios'][1])\n    ia = arr.shape[0]\/float(arr.shape[1])\n    ax2.set_aspect(float(wa*ia\/(ya\/xa)))\n\n    plt.show()\n\narr = np.random.randn(200,120)\nvisualize(arr)\n\nText: E. Dynamically copy positions \nText: You may get the position of the left plot and copy its y-coordinates to the right subplot's position. This is a nice add-on to existing code. The drawback is necessary because subsequent changes to the figure size require to recalculate the positions. \nCode: import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize(arr):\n    gkw = {'width_ratios':[1, 3] }\n    fig, (ax1, ax2) = plt.subplots(1, 2,  gridspec_kw = gkw )\n\n    ax1.imshow(arr)\n\n    flat = arr.flatten()\n    x = flat[~np.isnan(flat)]\n    sns.distplot(x, ax=ax2)\n\n    def on_resize(evt=None):\n        ax1.apply_aspect()\n        bb1 = ax1.get_position()\n        bb2 = ax2.get_position()\n        bb2.y0 = bb1.y0; bb2.y1 = bb1.y1\n        ax2.set_position(bb2)\n\n    fig.canvas.mpl_connect(\"resize_event\", on_resize)\n    on_resize()\n\n    plt.show()\n\narr = np.random.randn(200,120)\nvisualize(arr)\n\nAPI:\nmpl_toolkits.axes_grid1.inset_locator.InsetPosition\n","label":[[513,527,"Mention"],[3343,3394,"API"]],"Comments":[]}
{"id":59421,"text":"ID:45812071\nPost:\nText: Copying the axes \nText: The inital answer here does not work, we keep it for future reference and also to see why a more sophisticated approach is needed. \nText: #There are some pitfalls on the way with the initial approach. #Adding an `axes` to a figure can be done via `fig.add_axes(axes)`. However, at this point, #the axes' figure needs to be the figure the axes should be added to. #This may sound a bit like running in circles but we can actually set the axes' #figure as `axes.figure = fig2` and hence break out of this. #One might then also position the axes in the new figure to take the usual dimensions. #For this a dummy axes can be added first, the axes can change its position to the position #of the dummy axes and then the dummy axes is removed again. In total, this would look as follows. import plt as plt import numpy as np num_rows = 10 num_cols = 1 fig, axs = plt.subplots(num_rows, num_cols, sharex=True) for i in xrange(num_rows): ax = axs[i] ax.plot(np.arange(10), np.arange(10)**i) def on_click(event): axes = event.inaxes if not axes: return fig2 = plt.figure() axes.figure=fig2 fig2.axes.append(axes) fig2.add_axes(axes) dummy = fig2.add_subplot(111) axes.set_position(dummy.get_position()) dummy.remove() fig2.show() fig.canvas.mpl_connect('button_press_event', on_click) plt.show() #So far so good, however, be aware that now after a click the axes is somehow #residing in both figures, which can cause all sorts of problems, e.g. if you # want to resize or save the initial figure. \nText: Instead, the following will work: \nText: Pickling the figure \nText: The problem is that axes cannot be copied (even deepcopy will fail). Hence to obtain a true copy of an axes, you may need to use pickle. The following will work. It pickles the complete figure and removes all but the one axes to show. \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport io\n\nnum_rows = 10\nnum_cols = 1\nfig, axs = plt.subplots(num_rows, num_cols, sharex=True)\nfor i in range(num_rows):\n     ax = axs[i]\n     ax.plot(np.arange(10), np.arange(10)**i)\n\ndef on_click(event):\n\n    if not event.inaxes: return\n    inx = list(fig.axes).index(event.inaxes)\n    buf = io.BytesIO()\n    pickle.dump(fig, buf)\n    buf.seek(0)\n    fig2 = pickle.load(buf) \n\n    for i, ax in enumerate(fig2.axes):\n        if i != inx:\n            fig2.delaxes(ax)\n        else:\n            axes=ax\n\n    axes.change_geometry(1,1,1)\n    fig2.show()\n\nfig.canvas.mpl_connect('button_press_event', on_click)\n\nplt.show()\n\nText: Recreate plots \nText: The alternative to the above is of course to recreate the plot in a new figure each time the axes is clicked. To this end one may use a function that creates a plot on a specified axes and with a specified index as input. Using this function during figure creation as well as later for replicating the plot in another figure ensures to have the same plot in all cases. \nCode: import matplotlib.pyplot as plt\nimport numpy as np\n\nnum_rows = 10\nnum_cols = 1\ncolors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nlabels = [\"Label {}\".format(i+1) for i in range(num_rows)]\n\ndef myplot(i, ax):\n    ax.plot(np.arange(10), np.arange(10)**i, color=colors[i])\n    ax.set_ylabel(labels[i])\n\n\nfig, axs = plt.subplots(num_rows, num_cols, sharex=True)\nfor i in xrange(num_rows):\n     myplot(i, axs[i])\n\n\ndef on_click(event):\n    axes = event.inaxes\n    if not axes: return\n    inx = list(fig.axes).index(axes)\n    fig2 = plt.figure()\n    ax = fig2.add_subplot(111)\n    myplot(inx, ax)\n    fig2.show()\n\nfig.canvas.mpl_connect('button_press_event', on_click)\n\nplt.show()\n\nAPI:\nmatplotlib.pyplot\n","label":[[837,840,"Mention"],[3632,3649,"API"]],"Comments":[]}
{"id":59422,"text":"ID:45838382\nPost:\nText: To make it short, plotly's mpl_to_plotly is currently (as of August 2017) not able to convert pie charts. \nText: The longer version: A matplotlib pie chart consists of Wedge objects. Those are matplotlib.patches.Patches. If mpl_to_plotly finds a patch, it will convert it to a path and assume that it is part of a bar chart. Since it isn't in this case, it will give up, throwing a warning \"I found a path object that I don't think is part of a bar chart. Ignoring.\". The produced figure will therefore not have any data and the error as in the question is thrown. \nAPI:\nmatplotlib.patches.Wedge\n","label":[[192,197,"Mention"],[595,619,"API"]],"Comments":[]}
{"id":59423,"text":"ID:45858469\nPost:\nText: The problem is that the colors are chosen from the colormap by dividing the range between the minimum and maximum values into equal parts. Since most levels lie very close to each other, they fall into the same range and thus have the same color. \nText: The easiest solution is not to use a colormap, but a plot where each of the levels gets its color from the colorlist. In this case you may provide the list of colors directly to the contourf plot. \nCode: plt.contourf(x,y,data,contour_levels,colors=diffmap_17)\n\nText: Note, that because you have 19 levels your list would then need 18 colors (I therefore added one). \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.colors\n\nx, y= np.meshgrid(np.linspace(-3,3), np.linspace(-3,3))\nr = np.sqrt(x**2+y**2)\ndata = np.tan((r*0.7-1.5))*1.3\n\ndiffmap_17 = [\"#FF0000\", \"#F81318\", \"#F12731\", \"#EB3B4A\", \"#EB5C66\", \"#EB7D82\", \n              \"#EB9E9E\", \"#F1BEBE\", \"#F8DEDE\", \"#FFFFFF\", \"#DDDCFD\", \"#BCB9FB\", \n              \"#9B96FA\", \"#6A6CFA\", \"#3A43FA\", \"#1D21FC\", \"#0000FF\", \"#0000ce\"]\n\ncontour_levels = [-20, -10, -5, -2, -1, -0.75, -0.5, -0.25, -0.1, 0.0, \n                  0.1, 0.25, 0.5, 0.75, 1, 2, 5, 10, 20]\ncs = plt.contourf(x,y,data,contour_levels,colors=diffmap_17)\n\nplt.colorbar(cs)\n\nplt.show()\n\nText: If you want to use a colormap instead, you would need to provide a normalization instance together with the colormap. A goundaryNorm would chose the colors according to the list of boundaries supplied to it, which would be the list of levels for the contour plot. \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.colors\n\nx, y= np.meshgrid(np.linspace(-3,3), np.linspace(-3,3))\nr = np.sqrt(x**2+y**2)\ndata = np.tan((r*0.7-1.5))*1.3\n\ndiffmap_17 = [\"#FF0000\", \"#F81318\", \"#F12731\", \"#EB3B4A\", \"#EB5C66\", \"#EB7D82\", \n              \"#EB9E9E\", \"#F1BEBE\", \"#F8DEDE\", \"#FFFFFF\", \"#DDDCFD\", \"#BCB9FB\", \n              \"#9B96FA\", \"#6A6CFA\", \"#3A43FA\", \"#1D21FC\", \"#0000FF\", \"#0000ce\"]\ndiffmap_17_colormap = matplotlib.colors.ListedColormap(diffmap_17)\n\ncontour_levels = [-20, -10, -5, -2, -1, -0.75, -0.5, -0.25, -0.1, 0.0, \n                  0.1, 0.25, 0.5, 0.75, 1, 2, 5, 10, 20]\nnorm = matplotlib.colors.BoundaryNorm(contour_levels, diffmap_17_colormap.N)\ncs = plt.contourf(x,y,data,contour_levels,cmap=diffmap_17_colormap, norm=norm)\n\nplt.colorbar(cs)\n\nplt.show()\n\nText: The output plot is the same as above. \nAPI:\nmatplotlib.colors.BoundaryNorm\n","label":[[1429,1441,"Mention"],[2444,2474,"API"]],"Comments":[]}
{"id":59424,"text":"ID:45861666\nPost:\nText: You could use a mcolors.BoundaryNorm to specify the levels to use for the colormapping. \nCode: import matplotlib.pyplot as plt\nimport matplotlib.colors \nimport numpy as np\n\nxvec = np.linspace(-10.,10.,100)\nx,y = np.meshgrid(xvec, xvec)\nz = -(x**2 + y**2)**2\n\nfig, (ax0, ax1) = plt.subplots(2)\np0 = ax0.contourf(x, y, z, 100)\nfig.colorbar(p0, ax=ax0)\n\nlevels = np.percentile(z, np.linspace(0,100,101))\nnorm = matplotlib.colors.BoundaryNorm(levels,256)\np1 = ax1.contourf(x, y, z, 100, levels=levels, norm=norm)\nfig.colorbar(p1, ax=ax1)\n\nplt.show()\n\nAPI:\nmatplotlib.colors.BoundaryNorm\n","label":[[40,60,"Mention"],[576,606,"API"]],"Comments":[]}
{"id":59425,"text":"ID:45884249\nPost:\nText: This is not the most flexible workaround but will work for your question specifically. \nCode: def sephist(col):\n    yes = df[df['group'] == 'yes'][col]\n    no = df[df['group'] == 'no'][col]\n    return yes, no\n\nfor num, alpha in enumerate('abcd'):\n    plt.subplot(2, 2, num)\n    plt.hist(sephist(alpha)[0], bins=25, alpha=0.5, label='yes', color='b')\n    plt.hist(sephist(alpha)[1], bins=25, alpha=0.5, label='no', color='r')\n    plt.legend(loc='upper right')\n    plt.title(alpha)\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n\nText: You could make this more generic by: \nText: adding a df and by parameter to sephist: def sephist(df, by, col) making the subplots loop more flexible: for num, alpha in enumerate(df.columns) \nText: Because the first argument to hist can take \nText: either a single array or a sequency of arrays which are not required to be of the same length \nText: ...an alternattive would be: \nCode: for num, alpha in enumerate('abcd'):\n    plt.subplot(2, 2, num)\n    plt.hist((sephist(alpha)[0], sephist(alpha)[1]), bins=25, alpha=0.5, label=['yes', 'no'], color=['r', 'b'])\n    plt.legend(loc='upper right')\n    plt.title(alpha)\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n\nAPI:\nmatplotlib.pyplot.hist\n","label":[[786,790,"Mention"],[1229,1251,"API"]],"Comments":[]}
{"id":59426,"text":"ID:45939045\nPost:\nText: I think you were on the right track with your second attempt, except that your data is not normalized correctly for the colormap. When you try to get a color value from a colormap, you need to provide a value in the range [0-1]. To make things easier, I often use ScalarMappable (link to documentation) which handles this transformation automatically. \nText: To solve your problem I modified the function plotFeatures() like so: \nCode: def plotFeatures( patches, colours, legends, str_title, colour_scale ):\n\n    fig = plt.figure(); ax = plt.gca()\n\n    p = PatchCollection(patches, cmap=plt.get_cmap('Spectral_r'), alpha=0.9)\n    p.set_array(np.array(colours))\n    ax.add_collection(p)\n    p.set_clim(colour_scale)\n    fig.colorbar(p, ax=ax, fraction=0.015)\n    plt.xlabel(str_title)\n\n    # generate legend\n    # create a `ScalarMappable` object with the colormap used, and the right scaling\n    cm = matplotlib.cm.ScalarMappable(cmap=p.get_cmap())\n    cm.set_clim(colour_scale)\n    # create a list of Patches for the legend\n    l = [Circle((None,None), facecolor=cm.to_rgba(mean_value)) for mean_value in colours]\n    # add legend to plot\n    plt.legend(handles=l, labels=legends, bbox_to_anchor=(0., 1.02, 1., .2), mode='expand', ncol=3, loc=\"lower left\")\n\n\n    # ax.set_xticks([]); ax.set_yticks([])\n    ax.set_xlim([0,100])\n    ax.set_ylim([0,100])\n\nAPI:\nmatplotlib.cm.ScalarMappable\n","label":[[288,302,"Mention"],[1383,1411,"API"]],"Comments":[]}
{"id":59427,"text":"ID:45982707\nPost:\nText: \"What is causing this behavior?\" \nText: The formatter of an axes of a pandas dates plot is a FixedFormatter (see e.g. print plt.gca().xaxis.get_major_formatter()). \"Fixed\" means that it formats the ith tick (if shown) with some constant string. \nText: When zooming or panning, you shift the tick locations, but not the format strings. In short: A pandas date plot may not be the best choice for interactive plots. \nText: Solution \nText: A solution is usually to use matplotlib formatters directly. This requires the dates to be datetime objects (which can be ensured using df.index.to_pydatetime()). \nCode: import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates\n\ndtnow = datetime.datetime.now()\ndindex = pd.date_range(dtnow , dtnow  + datetime.timedelta(7), freq='110T')\ndata = np.linspace(1,100, num=len(dindex))\ndf = pd.DataFrame({'ds': dindex, 'y': data})\ndf = df.set_index('ds')\ndf.index.to_pydatetime()\ndf.plot(marker=\"o\")\n\n\nplt.gca().xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%w - %H:%M'))\nplt.show()\n\nAPI:\nmatplotlib.ticker.FixedFormatter\n","label":[[117,131,"Mention"],[1109,1141,"API"]],"Comments":[]}
{"id":59428,"text":"ID:46027320\nPost:\nText: As the documentation for plot() explains, plot() returns a list of Line2D objects, not an Axes, which is why your second code does not work. \nText: In essence, there are 2 ways to use matplotlib: \nText: Either you use the pyplot API (import pyplot as plt). Then each command starts with plt.xxxxx() and they work on the last created Axes object, which you usually don't need to reference explicitly. \nText: Your code would then be: \nCode: plt.plot(x,y)\nplt.xlabel('x label')\nplt.ylabel('y label')\nplt.xlabel('title')\n\nText: Either you use the object-oriented approach \nText: where your code would be written: \nCode: fig, ax = plt.subplots()\nline, = ax.plot(x,y)\nax.set_xlabel('x label')\nax.set_ylabel('y label')\nax.set_xlabel('title')\n\nText: It is usually not recommended to mix both approaches. The pyplot API is useful for people migrating from MATLAB, but with several subplots, it gets difficult to be sure which Axes one's working on, therefore the OO approach is recommended. \nText: see this part of matplotlib FAQs for more information. \nAPI:\nmatplotlib.pyplot\n","label":[[265,271,"Mention"],[1074,1091,"API"]],"Comments":[]}
{"id":59429,"text":"ID:46059318\nPost:\nText: Essentially you do not want to change the colormap at all. Instaed you want to create your custom normalization. To this end, you can subclass Normaize and let it return the values of your custom function. The function would need to take values between vmin and vmax as input and return values in the range [0,1]. \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.colors as mcolors\n\n\nclass MyNormalize(mcolors.Normalize):\n    def __call__(self, value, clip=None):\n        # function to normalize any input between vmin and vmax linearly to [0,1]\n        n = lambda x: (x-self.vmin)\/(self.vmax-self.vmin)\n        # nonlinear function between [0,1] and [0,1]\n        f = lambda x,a: (2*x)**a*(2*x<1)\/2. +(2-(2*(1-1*x))**a)*(2*x>=1)\/2.\n        return np.ma.masked_array(f(n(value),0.5))\n\n\nfig, (ax,ax2) = plt.subplots(ncols=2)\n\nx = np.linspace(-0.3,1.2, num=101)\nX = (np.sort(np.random.rand(100))*1.5-0.3)\n\nnorm=  MyNormalize(vmin=-0.3, vmax=1.2)\n\nax.plot(x,norm(x))\nim = ax2.imshow(X[::-1,np.newaxis], norm=norm, cmap=\"coolwarm\", aspect=\"auto\")\nfig.colorbar(im)\n\nplt.show()\n\nText: The image of the desired colorbar rather suggests a partially linear function like the following beeing used. \nCode: class MyNormalize2(mcolors.Normalize):\n    def __call__(self, value, clip=None):\n        n = lambda x: self.vmin+(self.vmax-self.vmin)*x\n        x, y = [self.vmin, n(0.2), n(0.8), self.vmax], [0, 0.48,0.52, 1]\n        return np.ma.masked_array(np.interp(value, x, y))\n\nAPI:\nmatplotlib.colors.Normalize\n","label":[[167,175,"Mention"],[1520,1547,"API"]],"Comments":[]}
{"id":59430,"text":"ID:46092601\nPost:\nText: There are three cases where plt.figure is useful: \nText: Obtaining a handle to a figure. In many cases it is useful to have a handle to a figure, i.e. a variable to store the Figure instance in, such that it can be used later on. Example: fig = plt.figure() #... other code fig.autofmt_xdate() Set figure parameters. An option to set some of the parameters for the figure is to supply them as arguments to plt.figure, e.g. plt.figure(figsize=(10,7), dpi=144) Create several figures. In order to create several figures in the same script, plt.figure can be used. Example: plt.figure() # create a figure plt.plot([1,2,3]) plt.figure() # create another figure plt.plot([4,5,6]) # successive commands are plotted to the new figure \nText: In many other cases, there would not actually be any need to use plt.figure. Using the pyplot interface, a call to any plotting command is sufficient to create a figure and you can always obtain a handle to the current figure with plt.gcf(). \nText: From another perspective it is often desired not only to have a handle to the figure but also to an axes to plot to. In such cases, the use of plt.subplots is more favorable, fig, ax = plt.subplots(). \nAPI:\nmatplotlib.figure.Figure\n","label":[[199,205,"Mention"],[1214,1238,"API"]],"Comments":[]}
{"id":59431,"text":"ID:46207710\nPost:\nText: Hey I was able to solve my problem based on the link of @ImportanceOfBeingErnest! \nText: Part of myplot class... Just remove all selfs and import plt as plt and customize to your needs, voil! \nCode: def broken_barh(self):\n    plot_data = []\n    for index,item in enumerate(self.data[\"occupation_starting\"]):\n        plot_data.append(list(zip(self.data[\"occupation_starting\"][index], self.data[\"occupation_durations\"][index])))\n\n    print(\"plot_data:\")\n    #print(plot_data)\n    data_len = len(plot_data)\n    print(data_len)\n\n    self.ax.set_ylim(5, 5*data_len+15)\n    self.ax.set_xlim(0, self.timer*self.repetitions)\n    self.ax.xaxis.grid(self.grid)\n    self.ax.yaxis.grid(self.grid)\n\n    # FIXME: Now ACKS are absolutely required, or data_len\/2 doesn't make sense!\n    self.ax.set_yticks([x*10+15 for x in range(int(data_len\/2))])\n    #self.ax.set_yticklabels([\"measurement \"+str(self.measurement[index]) for index in range(int(data_len\/2))])\n\n    self.setLabels(\n        xlabel=\"time[s]\",\n        title=self.title\n    )\n\n    blue_patch  = mpatches.Patch(color='blue', label=\"Data\")\n    red_patch   = mpatches.Patch(color='red', label='Acks')\n\n    if self.legend_coordinates[2] != \"best\":\n        self.ax.legend( handles=[red_patch, blue_patch],\n                        fancybox=True,\n                        loc=self.legend_coordinates[2],\n                        bbox_to_anchor=(self.legend_coordinates[0],\n                                        self.legend_coordinates[1]))\n    else:\n        self.ax.legend( handles=[red_patch, blue_patch],\n                        fancybox=True,\n                        loc=\"best\")\n\n    for index,item in enumerate(plot_data):\n        print(\"Added index \"+str(index)+\" to plot.\")\n        if index % 2 == 0:\n            self.ax.broken_barh(item,((index+1)*5+5, 9), facecolors='blue')\n        elif (index-1) % 2 == 0:\n            self.ax.broken_barh(item,(index*5+5,9), facecolors='red')\n\nAPI:\nmatplotlib.pyplot\n","label":[[170,173,"Mention"],[1957,1974,"API"]],"Comments":[]}
{"id":59432,"text":"ID:46263911\nPost:\nText: The idea can be to place a lot of small polar axes at the positions of the points. To this end, inset_axes may be used. This would be placed at coordinates x, y (bbox_to_anchor=(x,y)) specified in data coordinates of the main axes (bbox_transform=axis_main.transData). The loc parameter should be set to \"center\" (loc=10), such that the middle of the polar plot sits at position (x,y). \nText: You may then plot whatever you like into the polar axes. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nfrom matplotlib.projections import get_projection_class\n\nd = np.array([[ 1.0, 0.6, 0.8, 0.2, 56890, 98.67],\n              [ 0.8, 0.3, 1.0, 0.5, 94948, 98.00],\n              [ 1.0, 0.8, 0.1, 0.3, 78483, 97.13]])\n\nfig, ax = plt.subplots()\nax.margins(0.15)\n\n\ndef plot_inset(data, x,y, axis_main, width ):\n    ax_sub= inset_axes(axis_main, width=width, height=width, loc=10, \n                       bbox_to_anchor=(x,y),\n                       bbox_transform=axis_main.transData, \n                       borderpad=0.0, axes_class=get_projection_class(\"polar\"))\n\n    theta = np.linspace(0.0, 2 * np.pi, 4, endpoint=False)\n    radii = [90, 90, 90, 90]\n    width = np.pi \/ 4 * data\n    bars = ax_sub.bar(theta, radii, width=width, bottom=0.0)\n    ax_sub.set_thetagrids(theta*180\/np.pi, frac=1.4)\n    ax_sub.set_xticklabels([\"v{}\".format(i) for i in range(1,5)])\n    ax_sub.set_yticks([])\n\n\nfor da in d:    \n    plot_inset(da[:4], da[4],da[5], ax, 0.5 )\n\n#plot invisible scatter plot for the axes to autoscale\nax.scatter(d[:,4], d[:,5], s=1, alpha=0.0)\n\nplt.show()\n\nAPI:\nmpl_toolkits.axes_grid1.inset_locator.inset_axes\n","label":[[120,130,"Mention"],[1656,1704,"API"]],"Comments":[]}
{"id":59433,"text":"ID:46365537\nPost:\nText: The problem of wrong labels appears because by default, the x axes of the subplots are shared, hence all plots will have the same x-axis as the last plot. \nText: You can use the sharex=False argument in order to prevent sharing of the axes: \nCode: grid = sns.FacetGrid(df, col='class', sharex=False)\n\nText: import pandas as pd import numpy as np; np.random.seed(42) import pyplot as plt import seaborn as sns codes = [200, 201, 202, 204, 302, 304, 400, 404, 500, 502] p = np.random.rand(len(codes)) p = p\/p.sum() df = pd.DataFrame({ 'code': np.random.choice(codes, size=300, p=p) }) def determine_response_class(row): response_code = row['code'] if response_code >= 200 and response_code < 300: return 'success' elif response_code >= 300 and response_code < 400: return 'warning' elif response_code >= 400 and response_code < 500: return 'client_error' elif response_code >= 500 and response_code < 600: return 'server_error' else: return 'unknown' df['class'] = df.apply(determine_response_class, axis='columns') grid = sns.FacetGrid(df, col='class', sharex=False) grid.map(sns.countplot, 'code') plt.show() \nText: The problem of sorting is now a chicken-or-egg problem. In order to set the order of the columns you need to know the counts for each, which are determined as part of the plotting. At this point it is probably wise to stick to a clear separation between data generation, analysis and visualization. The following would show a sorted graph, without the use of FacetGrid, by first counting an sorting the values in the dataframe. \nCode: import pandas as pd\nimport numpy as np; np.random.seed(42)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncodes = [200, 201, 202, 204, 302, 304, 400, 404, 500, 502]\np = np.random.rand(len(codes))\np = p\/p.sum()\ndf = pd.DataFrame({ 'code': np.random.choice(codes, size=300, p=p) })\n\ndef determine_response_class(row):    \n    response_code = row['code']\n\n    if response_code >= 200 and response_code < 300:\n        return 'success'\n    elif response_code >= 300 and response_code < 400:\n        return 'warning'\n    elif response_code >= 400 and response_code < 500:\n        return 'client_error'\n    elif response_code >= 500 and response_code < 600:\n        return 'server_error'\n    else:\n        return 'unknown'\n\ndf['class'] = df.apply(determine_response_class, axis='columns')\n\ndf2 = df.groupby([\"code\",\"class\"]).size().reset_index(name=\"count\") \\\n        .sort_values(by=\"count\", ascending=0).reset_index(drop=True)\n\nfig, axes = plt.subplots(ncols=4, sharey=True, figsize=(8,3))\nfor ax,(n, group) in zip(axes, df2.groupby(\"class\")):\n    sns.barplot(x=\"code\",y=\"count\", data=group, ax=ax, color=\"C0\", order=group[\"code\"])\n    ax.set_title(n)\n\nplt.tight_layout()\nplt.show()\n\nAPI:\nmatplotlib.pyplot\n","label":[[397,403,"Mention"],[2767,2784,"API"]],"Comments":[]}
{"id":59434,"text":"ID:46468734\nPost:\nText: From what you show in the question, the code is correct. You input the string c = b'19970108' and get the output 729032.0. This output signifies the first of august 1997 in the numeric datetime format that matplotlib uses. \nText: You may convert back to datetime via \nCode: print(mdates.num2date(a(c)))\n# this prints 1997-01-08 00:00:00+00:00\n\nText: to see that it's working. \nText: To plot the output you have essentially 3 options. \nText: Just using plot will of course show the numeric numbers (how would matplotlib know that it is supposed to plot a date?). import pyplyot as plt plt.plot( a(c), 1, marker=\"d\") Using plot_date(): plt.plot_date( a(c), 1, marker=\"d\") Convert to datetime with mdates.num2date: plt.plot( mdates.num2date(a(c)), 1, marker=\"d\") Using a Locator and Formatter: plt.plot( a(c), 1, marker=\"d\") loc = mdates.AutoDateLocator() plt.gca().xaxis.set_major_locator(loc) plt.gca().xaxis.set_major_formatter(mdates.AutoDateFormatter(loc)) This last method allows for the greatest flexibility, as you may also use other Locators and Formatters. See the mdates API or the official example. \nAPI:\nmatplotlib.pyplot\nmatplotlib.dates\n","label":[[593,600,"Mention"],[1096,1102,"Mention"],[1138,1155,"API"],[1156,1172,"API"]],"Comments":[]}
{"id":59435,"text":"ID:46507382\nPost:\nText: By default, the same formatter is used for the values shown in the NavigationToolbar as on the axes. I suppose that you want to use the format \"%m-%d\\n%H:%M\" in question just for the ticklabel formatting and are happy to use a single-line format for the values shown when moving the mouse. \nText: This can be achieved by using a different formatter for those two cases. \nCode: # Format tick labels\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\\n%H:%M\"))\n# Format toolbar coordinates\nax.fmt_xdata = mdates.DateFormatter('%m-%d %H:%M')\n\nText: Example picture: \nText: Complete code for reproduction: \nText: import matplotlib matplotlib.use(\"TkAgg\") import pandas as pd import pyplot as plt import dates as mdates import numpy as np dates = pd.date_range(\"2016-06-01 09:00\", \"2016-06-01 16:00\", freq=\"H\" ) y = np.cumsum(np.random.normal(size=len(dates))) df = pd.DataFrame({\"Dates\" : dates, \"y\": y}) fig, ax = plt.subplots() ax.plot_date(df[\"Dates\"], df.y, '-') ax.xaxis.set_major_locator(mdates.HourLocator()) ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\\n%H:%M\")) ax.fmt_xdata = mdates.DateFormatter('%m-%d %H:%M') plt.show() \nAPI:\nmatplotlib.pyplot\nmatplotlib.dates\n","label":[[708,714,"Mention"],[729,734,"Mention"],[1177,1194,"API"],[1195,1211,"API"]],"Comments":[]}
{"id":59436,"text":"ID:46513336\nPost:\nText: Two options come directly to my mind: \nText: Make the swarmplot translucent. This can be done by adding the alpha argument, e.g. alpha=0.5 for half-transparent. Of course the darker the bar in the background, the less visible the points are (hence I made it yellow here). import pyplot as plt import seaborn as sns import pandas as pd fig, axes = plt.subplots(figsize=(6,2)) data = pd.DataFrame({'a':[3,3,3,3,4,5,6,8,11,11,8,7,7,7,7,7,7,3,3, 3,3,3,7,7,7,7,7,7,7,7,7,7,7,8,9,10,11,12,11,11,11]}) sns.boxplot(x='a',data=data, ax = axes, color=\"gold\") sns.swarmplot(x='a', data=data, color = 'grey', ax = axes, alpha=0.5) plt.show() Show the median line on top of the swarmplot points. This can be done by specifying the zorder of the medianline via a dictionary passed through the medianprops keyword argument. sns.boxplot(x='a',data=data, ax = axes, color=\"gold\", medianprops={\"zorder\":3}) In this case, making the median line half-transparent via medianprops={\"zorder\":3, \"alpha\":0.6} is equally possible. \nText: Of course any combination of the two options can help as well. \nAPI:\nmatplotlib.pyplot\n","label":[[303,309,"Mention"],[1106,1123,"API"]],"Comments":[]}
{"id":59437,"text":"ID:46531511\nPost:\nText: I think this behaviour was introduced to matplotlib with version 2.0 (at least I can't remember this being as bad as now in previous versions, but I may be wrong here) and one could consider it to be a bug. Bugs can be reported at the issue tracker. Of course there are cases, where no optimal solutions can exist (i.e. if bars are very thin). \nText: The following are some considerations as to how to cope with the issue. \nText: First, note that a figure exported to pdf does not have the problem of unequal bar widths, due to the vector format in use. So for publications or similar, using pdf is definitely an option to consider. import numpy as np import mpl.pyplot as plt x = np.random.randn(5000) plt.hist(x,bins=50, rwidth=0.9) plt.savefig(__file__+\".pdf\") Because pdf works fine, you may convert the pdf to a png, e.g. through imagemagick > convert -density 300 -trim test.pdf test.png You may create a png image with a higher resolution. I.e. changing the dpi to something above 200 may already give you the desired output plt.savefig(__file__+\".pdf\", dpi=288) When working in a Jupyter notebook you may also set the dpi for the figure to a higher value plt.rcParams[\"figure.dpi\"] = 288 for all figures or plt.figure(dpi=288) for a single figure. The drawback might be that the figure becomes too large. To avoid the above huge image, you may instead of displaying the actual figure, display a figure of heigh dpi but set the width of the output to some smaller value. This is a little tedious, but of course one can copy paste the below function for reuse. import numpy as np import mpl.pyplot as plt from IPython.display import Image, display import io def display_figure(fig=None): if not fig: fig=plt.gcf(); buff = io.BytesIO() fig.savefig(buff, dpi=288) buff.seek(0) display(Image(data=buff.getvalue(), width=480)) x = np.random.randn(5000); plt.hist(x,bins=50, rwidth=0.9); display_figure(plt.gcf()) \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[683,693,"Mention"],[1617,1627,"Mention"],[1945,1962,"API"],[1963,1980,"API"]],"Comments":[]}
{"id":59438,"text":"ID:46531880\nPost:\nText: First off, you will already gain a little by using \nCode: self.fig.canvas.draw_idle()\n\nText: instead of draw(). This redraws the canvas only when it's not currently beeing repainted, saving you a lot of draws. \nText: If this is not enough, you would need to use the technique of blitting. Now since you don't have a minimal example, I will not provide any complete solution for this here, but e.g. the answer to this question, why is plotting with Matplotlib so slow?, has an example of that. The idea is to store the background, and only redraw the part that changes (here the line). \nCode: background = fig.canvas.copy_from_bbox(ax.bbox)\n# then during mouse move\nfig.canvas.restore_region(background)\nline.set_data(...)\nax.draw_artist(line)\nfig.canvas.blit(ax.bbox)\n# only after mouse has stopped moving\nfig.canvas.draw_idle()\n\nText: This technique is also used internally by some matplotlib widgets, e.g. Cursor to let the lines follow the cursor quickly. \nText: This brings me to the last point, which is: You don't need to reinvent the wheel. There is a matplotlib.widgets.RectangleSelector, which by defaut draws a rectangle for selection. But you may use its drawtype='line' argument, to change the selection to a line, together with the argument blit=True this should already give you what you need - you will just need to add the code to finally draw a line once the selection is finished. \nText: Note that in the newest matplotlib version, there is even a matplotlib.widgets.PolygonSelector, which may directly be what you need. \nAPI:\nmatplotlib.widgets.Cursor\n","label":[[932,938,"Mention"],[1569,1594,"API"]],"Comments":[]}
{"id":59439,"text":"ID:46561949\nPost:\nText: You need to \nCode: import matplotlib\nmatplotlib.use('Agg')\n\nText: on a fresh kernel, especially if you are using ipython, before importing mpl.pyplot \nText: I'd be curious, and happy, to know if there are ways to clear\/flush the ipython kernel, without having to restart it; so far, my quest has not been successful. \nAPI:\nmatplotlib.pyplot\n","label":[[163,173,"Mention"],[347,364,"API"]],"Comments":[]}
{"id":59440,"text":"ID:46720189\nPost:\nText: Rotating the ticklabels for a polar plot may be not as easy as for a usual cartesian plot. For a cartesian plot, one can simply do something like \nCode: for label in ax.get_xticklabels():\n    label.set_rotation(...)\n\nText: This does not work for a polar plot, because their rotation is reset at draw time to 0 degrees. \nText: One option that comes to mind is to create new ticklabels as additional text objects which copy the attributes of the ticklabels but can have a persistent rotation. Then remove all original ticklabels. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nax = plt.subplot(111, projection='polar')\nax.plot(theta, r)\nax.set_rmax(2)\nax.set_rticks([]) \n\n\nplt.gcf().canvas.draw()\nangles = np.linspace(0,2*np.pi,len(ax.get_xticklabels())+1)\nangles[np.cos(angles) < 0] = angles[np.cos(angles) < 0] + np.pi\nangles = np.rad2deg(angles)\nlabels = []\nfor label, angle in zip(ax.get_xticklabels(), angles):\n    x,y = label.get_position()\n    lab = ax.text(x,y, label.get_text(), transform=label.get_transform(),\n                  ha=label.get_ha(), va=label.get_va())\n    lab.set_rotation(angle)\n    labels.append(lab)\nax.set_xticklabels([])\n\nplt.show()\n\nText: For longer labels you may play with the y coordinates of the labels: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nax = plt.subplot(111, projection='polar')\nax.plot(theta, r)\nax.set_rmax(2)\nax.set_rticks([])\nticks= np.linspace(0,360,9)[:-1] \nax.set_xticks(np.deg2rad(ticks))\nticklabels = [\"\".join(np.random.choice(list(\"ABCDE\"),size=15)) for _ in range(len(ticks))]\nax.set_xticklabels(ticklabels, fontsize=10)\n\nplt.gcf().canvas.draw()\nangles = np.linspace(0,2*np.pi,len(ax.get_xticklabels())+1)\nangles[np.cos(angles) < 0] = angles[np.cos(angles) < 0] + np.pi\nangles = np.rad2deg(angles)\nlabels = []\nfor label, angle in zip(ax.get_xticklabels(), angles):\n    x,y = label.get_position()\n    lab = ax.text(x,y-.65, label.get_text(), transform=label.get_transform(),\n                  ha=label.get_ha(), va=label.get_va())\n    lab.set_rotation(angle)\n    labels.append(lab)\nax.set_xticklabels([])\n\nplt.subplots_adjust(top=0.68,bottom=0.32,left=0.05,right=0.95)\nplt.show()\n\nText: Corrected version of the edited question's code: \nText: import pandas as pd import plt as plt import numpy as np data = {'0-b__|ce0Ji|aaaiIi9abGc_|ti5l-baa1tcciii|irGi': 0.28774963897009614, '0-b__|ce0Ji|aaaiIi9abGc_|ti6l-baa1tcciii|irGi': 0.18366735937444964, 'allb_e__|tla1a|ali|_auc7en_|e': -0.11720263463773731, 'b__0|lp|..ii80p.e7l_|an4obln.llll0ai|': -0.021168680215561269, 'b__Ass8._ii8.c4on|Ay|mbessoxiAxa': 0.17845443978725653, 'b__Bts4o_rrtiordae|Bei|obe7rattrniBno': 0.32077066676059313, 'b__|aaa|tteiatlim_|e1rblttaaeei|e': -0.27915536613715614, 'b__|as4.|ei2.l7ov_|e0tblaaoxi|xa': 0.43309499489274772, 'b__|as4.|ei2.l7ov_|e9tblaaoxi|xa': 0.47835581698425556, 'b__|cu|ppripcae_|co2tbopnccpei|': -0.20330386390053184, 'b__|eoea|cccimacnuuh_|ra0obarceenbi|ba': 0.062889648127927869, 'b__|oa|ggrigoip_|nr6ybmgvoohii|i': -0.045648268817583035, 'b__|p1|ooiioi4rs_|sr5eba0otsoi|ox': -0.52544820541720971, 'b__|paa|piatgn_|hy1cboippoli|la': 0.27260399422352155, 'b__|triu|mmriumay_|eb4ebcimrttnhi|hc': 0.62680074671550845, 'b__|tru|mmriumad_|eb2obcmittisi|': 0.34780388151174668, 'etob_m__|aol2l|ooeui|_lool7r': 0.4856468599203973, 'etpb_s__|apl2l|lleni|_loll8e': 0.24430277200521291, 'ib__rCalc_hhdiorchubai|CSt|absahodrsiCsaaca': -0.13484907188897891, 'nlab___|oa1i|ssni|_iesa9': 0.13636363636363635, 'nlnb_i__|dn1t|rrnfi|_tera8ig_|e': -0.056954668733049205, 'nrfb_h__|afl3r|ssnti|_resl3yn_': 0.56102285935683849, 'o5b__l|rcoa|eecialaeprh_|as1o5bie0trrnlii|irLa': 0.53377831002782572, 'oelb_a__Aelt3_rrovi__rro|a': 0.32230284245007218, 'oelb_a__Aelt4_rrovi__rro|a': 0.16580958754534889, 'porb_i__Ctrc6c_oopci__cloa|ny|C': 0.38260364199922509, 'porb_i__Ctrc7g_rrpci__glra|ay|C': 0.51829805219964076, 'ptab_a__|hac2b|uupci|_boui3ct_|': 0.50873516255151285, 'reab_a__|aa2a|rrrhi|_axrl4ra_|': -0.47742242259871087, 'sb__o|sSac|ccnibocsctlhd_|a0dbuacmssioai|anCca': 0.42733612764608503, 'teob___|oa1b|iiti|_bnil3': -0.32684653587404461, 'uoib_i__|ia2a|bbuli|_arbi2it': -0.13636363636363635} Se_corr = pd.Series(data, name=\"correlation\") def plot_polar(r): with plt.style.context(\"seaborn-whitegrid\"): fig = plt.figure(figsize=(10,10)) ax = fig.add_subplot(111, polar=True) ax.set_rmax(2) #ax.set_rticks([]) ticks= np.linspace(0, 360, r.index.size + 1)[:-1] ax.set_xticks(np.deg2rad(ticks)) ax.set_xticklabels(r.index, fontsize=15,) angles = np.linspace(0,2*np.pi,len(ax.get_xticklabels())+1) angles[np.cos(angles) < 0] = angles[np.cos(angles) < 0] + np.pi angles = np.rad2deg(angles) for i, theta in enumerate(angles[:-1]): ax.plot([theta,theta], [0,r[i]], color=\"black\") ax.scatter(x=theta,y=r[i], color=\"black\") fig.canvas.draw() labels = [] for label, theta in zip(ax.get_xticklabels(), angles): x,y = label.get_position() lab = ax.text(x, y, label.get_text(), transform=label.get_transform(), ha=label.get_ha(), va=label.get_va()) lab.set_rotation(theta) labels.append(lab) ax.set_xticklabels([]) return fig, ax fig,ax = plot_polar(Se_corr) plt.show() \nText: ; \nText: Image produced by that code \nAPI:\nmatplotlib.pyplot\n","label":[[2373,2376,"Mention"],[5298,5315,"API"]],"Comments":[]}
{"id":59441,"text":"ID:46722325\nPost:\nText: mdates vendors dateutil's parser, so you could use that: \nCode: >>> import matplotlib.dates\n>>> matplotlib.dates.dateutil.parser.parse('11-OCT-2017')\ndatetime.datetime(2017, 10, 11, 0, 0)\n\nText: Or, if you are trying to parse into a matplotlib datenum, then the month format is %b: \nCode: >>> parse = matplotlib.dates.strpdate2num('%d-%b-%Y')\n>>> parse('11-OCT-2017')\n736613.0\n\nAPI:\nmatplotlib.dates\n","label":[[24,30,"Mention"],[407,423,"API"]],"Comments":[]}
{"id":59442,"text":"ID:46748062\nPost:\nText: Assuming that a month has 30.4375 days, you can divide the days by the number of 30.4375 and obtain the number of months. \nCode: plt.plot(maleAge\/30.4375, maleP01, maleAge\/30.4375, maleP1, ...)\n\nText: To define a specific tick interval, you can use a mticker.MultipleLocator with an interval of 12 for the major ticks and an interval of 2 for the minor ticks. \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker\nimport numpy as np\n\n#create some dataset\nage = np.arange(0,1857) # in days\ndf = pd.DataFrame({\"age\" : age})\nfor i in range(10):\n    df[\"maleP{}\".format(i)] = (3.8-0.9*(1+2*i\/10.))*np.sqrt(age)\/4.2\n\n# Plot with data:\nplt.plot(df[\"age\"]\/30.4375, df[[\"maleP{}\".format(i) for i in range(10)]])\n\n# Set up the axes\/labels\nplt.title('Weight-for-Age:  male (WHO)')\nplt.xlabel('Age  (months)')\nplt.ylabel('Weight  (kg)')\nplt.axis([0,1900\/30.4375,0,30])\nplt.grid(True)\n\nplt.gca().xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(12))\nplt.gca().xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(2))\n\nplt.show()\n\nAPI:\nmatplotlib.ticker.MultipleLocator\n","label":[[275,298,"Mention"],[1091,1124,"API"]],"Comments":[]}
{"id":59443,"text":"ID:46796882\nPost:\nText: In order to make sure only integer locations obtain a ticklabel, you may use a mticker.MultipleLocator with an integer number as argument. \nText: To then format the numbers on the axes, you may use a matplotlib.ticker.StrMethodFormatter. \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker\n\ndf = pd.DataFrame({\"FISCAL_YEAR\" : np.arange(2000,2017),\n                   'TOTAL' : np.random.rand(17)})\n\nplt.bar(df['FISCAL_YEAR'],df['TOTAL'],align='center')\n\n\nlocator = matplotlib.ticker.MultipleLocator(2)\nplt.gca().xaxis.set_major_locator(locator)\nformatter = matplotlib.ticker.StrMethodFormatter(\"{x:.0f}\")\nplt.gca().xaxis.set_major_formatter(formatter)\nplt.show()\n\nAPI:\nmatplotlib.ticker.MultipleLocator\n","label":[[103,126,"Mention"],[744,777,"API"]],"Comments":[]}
{"id":59444,"text":"ID:46798484\nPost:\nText: You would usually just scale the data prior to plotting. So instead of plt.plot(x,y), you'd use plt.plot(x,y\/1e6). \nText: To format the values with 3 decimal places, use a mticker.StrMethodFormatter and supply a format with 3 decimals, in this case \"{x:.3f}\". \nCode: import matplotlib.pyplot as plt\nimport matplotlib.ticker\nimport numpy as np; np.random.seed(42)\n\nx = np.arange(5)\ny = np.array([5e5,2e5,0,3e5,4e5])\n\nplt.plot(x,y\/1e6)\n\nplt.gca().yaxis.set_major_formatter(matplotlib.ticker.StrMethodFormatter(\"{x:.3f}\"))\n\nplt.show()\n\nAPI:\nmatplotlib.ticker.StrMethodFormatter\n","label":[[196,222,"Mention"],[562,598,"API"]],"Comments":[]}
{"id":59445,"text":"ID:46875845\nPost:\nText: You seem to be overcomplicating the importing of modules. In the code, you have imported matplotlib and pyplo as plt. In addition, having already imported matplotlib.pyplot, you try and do it again using from matplotlib import pyplot \nText: When you try and save your file you have then done matplotlib.pyplot.savefig, but you have already imported pyhplot as plt. \nText: The specific error you have shown is because, while you have import matplotlib itself, you have imported it as plt, which is why the error says that matplotlib is not defined. \nText: In order to fix this, you need to clean up the imports like so: \nCode: import pandas as pd\nimport matplotlib # if abbreviating this, use \"as mpl\"\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n%matplotlib inline\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('png', 'pdf')\n\ndf = pd.read_excel('test.xlsx', sheetname='IvT')\n\nsns.set_style(\"white\")\nplt.figure(figsize=(12,10))\nplt.xlabel('Test', fontsize=18)\nplt.title ('Test', fontsize=22)\n#sns.boxplot(df[['Short total']])\nsns.boxplot(df[['Short total']])\nplt.show()\n\nText: Then in order to save your figure use: \nCode: plt.savefig(\"test.svg\", format=\"svg\")\n\nText: Remember to call this before plt.show() \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[128,133,"Mention"],[373,380,"Mention"],[1275,1292,"API"],[1293,1310,"API"]],"Comments":[]}
{"id":59446,"text":"ID:46876886\nPost:\nText: The problem you run into is that the text bounding box is expanded to host the complete rotated text, but that box itself is still defined in cartesian coordinates. The picture below shows two texts with horizontalalignment \"left\" and vertical alignment \"bottom\"; the problem is that the rotated text has its bounding box edge much further away from the text. \nText: What you want is rather to have the text rotate about an edge point of its own surrounding as below. \nText: This can be achieved using the rotation_mode=\"anchor\" argument to matplotlib.text.Text, which steers exactly the above functionality. \nCode: ax.text(..., rotation_mode=\"anchor\")\n\nText: In this example: \nCode: from matplotlib import pyplot as plt\nimport numpy as np\n\nlObjectsALLcnts = [1, 1, 1, 2, 2, 3, 5, 14, 15, 20, 32, 33, 51, 1, 1, 2, 2, 3, 3, 3, 3, \n                   3, 4, 6, 7, 7, 10, 10, 14, 14, 14, 17, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, \n                   5, 5, 6, 14, 14, 27, 27, 1, 1, 2, 3, 4, 4, 5]\n\nlObjectsALLlbls = ['DuctPipe', 'Column', 'Protrusion', 'Tree', 'Pole', 'Bar', 'Undefined', \n                   'EarthingConductor', 'Grooves', 'UtilityPipe', 'Cables', 'RainPipe', 'Moulding', \n                   'Intrusion', 'PowerPlug', 'UtilityBox', 'Balcony', 'Lighting', 'Lock', 'Doorbell', \n                   'Alarm', 'LetterBox', 'Grate', 'Undefined', 'CableBox', 'Canopy', 'Vent', 'PowerBox', \n                   'UtilityHole', 'Recess', 'Protrusion', 'Shutter', 'Handrail', 'Lock', 'Mirror', \n                   'SecuritySpike', 'Bench', 'Intrusion', 'Picture', 'Showcase', 'Camera', \n                   'Undefined', 'Stair', 'Protrusion', 'Alarm', 'Graffiti', 'Lighting', 'Ornaments', \n                   'SecurityBar', \n                   'Grate', 'Vent', 'Lighting', 'UtilityHole', 'Intrusion', 'Undefined', 'Protrusion']\n\niN = len(lObjectsALLcnts)\narrCnts = np.array(lObjectsALLcnts)\n\ntheta=np.arange(0,2*np.pi,2*np.pi\/iN)\nwidth = (2*np.pi)\/iN *0.9\nbottom = 50\n\nfig = plt.figure(figsize=(8,8))\nax = fig.add_axes([0.1, 0.1, 0.75, 0.75], polar=True)\nbars = ax.bar(theta, arrCnts, width=width, bottom=bottom)\n\nplt.axis('off')\n\nrotations = np.rad2deg(theta)\nfor x, bar, rotation, label in zip(theta, bars, rotations, lObjectsALLlbls):\n    lab = ax.text(x,bottom+bar.get_height() , label, \n             ha='left', va='center', rotation=rotation, rotation_mode=\"anchor\",)   \nplt.show()\n\nText: Note that this uses the given 50 units of bottom spacing. You may increase this number a bit to have more spacing between bars and text. \nText: The below initial version of this answer is somehow outdated. I will keep it here for reference. \nText: The problem you run into is that the text bounding box is expanded to host the complete rotated text, but that box itself is still defined in cartesian coordinates. The picture below shows two texts with horizontalalignment \"left\" and vertical alignment \"bottom\"; the problem is that the rotated text has its bounding box edge much further away from the text. \nText: An easy solution may be to define the horizontal and vertical alignment as \"center\", such the pivot of the text stays the same independent of its rotation. \nText: The problem would then be to get a good estimate for the distance between the center of the text and the bar's top. \nText: One could take half the number of letters in the text and multiply it with some factor. This would need to be found by trial and error. \nCode: bottom = 50\nrotations = np.rad2deg(theta)\ny0,y1 = ax.get_ylim()\n\nfor x, bar, rotation, label in zip(theta, bars, rotations, lObjectsALLlbls):\n     offset = (bottom+bar.get_height())\/(y1-y0)\n     h =offset + len(label)\/2.*0.032\n     lab = ax.text(x, h, label, transform=ax.get_xaxis_transform(), \n             ha='center', va='center')\n     lab.set_rotation(rotation)\n\nText: You could also try to find out how large the rendered text really is and use this information to find out the coordinates, \nCode: bottom = 50\nrotations = np.rad2deg(theta)\ny0,y1 = ax.get_ylim()\n\nfor x, bar, rotation, label in zip(theta, bars, rotations, lObjectsALLlbls):\n     offset = (bottom+bar.get_height())\/(y1-y0)\n     lab = ax.text(0, 0, label, transform=None, \n             ha='center', va='center')\n     renderer = ax.figure.canvas.get_renderer()\n     bbox = lab.get_window_extent(renderer=renderer)\n     invb = ax.transData.inverted().transform([[0,0],[bbox.width,0] ])\n     lab.set_position((x,offset+(invb[1][0]-invb[0][0])\/2.*2.7 ) )\n     lab.set_transform(ax.get_xaxis_transform())\n     lab.set_rotation(rotation)\n\nText: Complete code for reproduction: \nText: import numpy as np import pyplot as plt lObjectsALLcnts = [1, 1, 1, 2, 2, 3, 5, 14, 15, 20, 32, 33, 51, 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 6, 7, 7, 10, 10, 14, 14, 14, 17, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 5, 5, 6, 14, 14, 27, 27, 1, 1, 2, 3, 4, 4, 5] lObjectsALLlbls = ['DuctPipe', 'Column', 'Protrusion', 'Tree', 'Pole', 'Bar', 'Undefined', 'EarthingConductor', 'Grooves', 'UtilityPipe', 'Cables', 'RainPipe', 'Moulding', 'Intrusion', 'PowerPlug', 'UtilityBox', 'Balcony', 'Lighting', 'Lock', 'Doorbell', 'Alarm', 'LetterBox', 'Grate', 'Undefined', 'CableBox', 'Canopy', 'Vent', 'PowerBox', 'UtilityHole', 'Recess', 'Protrusion', 'Shutter', 'Handrail', 'Lock', 'Mirror', 'SecuritySpike', 'Bench', 'Intrusion', 'Picture', 'Showcase', 'Camera', 'Undefined', 'Stair', 'Protrusion', 'Alarm', 'Graffiti', 'Lighting', 'Ornaments', 'SecurityBar', 'Grate', 'Vent', 'Lighting', 'UtilityHole', 'Intrusion', 'Undefined', 'Protrusion'] iN = len(lObjectsALLcnts) arrCnts = np.array(lObjectsALLcnts) theta=np.arange(0,2*np.pi,2*np.pi\/iN) width = (2*np.pi)\/iN *0.9 bottom = 50 fig = plt.figure(figsize=(8,8)) ax = fig.add_axes([0.1, 0.1, 0.75, 0.75], polar=True) bars = ax.bar(theta, arrCnts, width=width, bottom=bottom) plt.axis('off') rotations = np.rad2deg(theta) y0,y1 = ax.get_ylim() for x, bar, rotation, label in zip(theta, bars, rotations, lObjectsALLlbls): offset = (bottom+bar.get_height())\/(y1-y0) lab = ax.text(0, 0, label, transform=None, ha='center', va='center') renderer = ax.figure.canvas.get_renderer() bbox = lab.get_window_extent(renderer=renderer) invb = ax.transData.inverted().transform([[0,0],[bbox.width,0] ]) lab.set_position((x,offset+(invb[1][0]-invb[0][0])\/2.*2.7 ) ) lab.set_transform(ax.get_xaxis_transform()) lab.set_rotation(rotation) plt.show() \nText: Unfortunately there is again some strange factor 2.7 involved. Even more unfornate is that in this case I have absolutely no idea why it must be there. But the result may still be good enough to work with. \nText: One could also use a solution from ths question: Align arbitrarily rotated text annotations relative to the text, not the bounding box \nAPI:\nmatplotlib.pyplot\n","label":[[4627,4633,"Mention"],[6719,6736,"API"]],"Comments":[]}
{"id":59447,"text":"ID:46905263\nPost:\nText: You are plotting the dates on the x axis as strings. This will lead matplotlib to think it is some catogorical variable (e.g. [\"apple\", \"banana\", \"cherry\"]) and it will show all labels (which would be meaningful for such cases). \nText: Here you don't want to have categories but true dates or numbers. First you need to makes sure that those strings actually represent dates or numbers - remove somthing like '92017\/01\/13' from the list. \nText: Decimal numbers \nText: To use usual decimal numbers, remove the str cast from your function. \nCode: decdate = year + ((month-1)*30+day)\/365.\n\nText: Complete code for reproduction: \nText: import pandas as pd import numpy as np from matplotlib import pyplot as plt import random lst1 = ['2015\/01\/01','2016\/01\/01','2017\/01\/01','2015\/01\/02', '2016\/01\/02','2017\/01\/02','2015\/01\/03','2016\/01\/03','2017\/01\/03', '2015\/01\/04','2015\/01\/05','2017\/01\/04','2016\/01\/04','2016\/01\/05', '2015\/01\/06','2017\/01\/05','2016\/01\/06','2015\/01\/07','2017\/01\/06', '2017\/01\/07','2016\/01\/07','2015\/01\/08','2017\/01\/08','2016\/01\/08', '2015\/01\/09','2016\/01\/09','2017\/01\/09','2016\/01\/10','2017\/01\/10', '2015\/01\/11','2016\/01\/11','2017\/01\/11','2015\/01\/12','2016\/01\/12', '2015\/01\/13','2017\/01\/12','2016\/01\/13','2017\/01\/13','2016\/01\/14', '2015\/01\/14','2017\/01\/14','2015\/01\/15','2016\/01\/15','2017\/01\/15', '2016\/01\/16','2015\/01\/16','2017\/01\/16','2017\/01\/17','2016\/01\/17', '2015\/01\/18','2016\/01\/18','2017\/01\/18','2015\/01\/19','2016\/01\/19', '2017\/01\/19','2015\/01\/20','2016\/01\/20','2017\/01\/20','2015\/01\/21', '2016\/01\/21','2017\/01\/21','2015\/01\/22','2016\/01\/22','2017\/01\/22', '2015\/01\/23','2016\/01\/23','2017\/01\/23','2015\/01\/24','2016\/01\/24', '2017\/01\/24', '2015\/01\/25', '2016\/01\/25'] lst2 = random.sample(range(72), 72) def date2decdate(date): d = date.split('\/') year = float(d[0]) month = float(d[1]) day = float(d[2]) decdate = year + ((month-1)*30+day)\/365. return decdate df = pd.DataFrame( {'Date': lst1, 'Elevation': lst2 }) df['Elevation']*=100 h = float(df['Elevation'].head(1)) df['Elevation']-=h df = df[np.abs(df.Elevation-df.Elevation.median())<=(3*df.Elevation.std())] df['Date'] = df['Date'].apply(date2decdate) #converts Dates to decimal date plt.scatter(df.Date, df.Elevation) plt.xlabel('Dates') plt.ylabel('Displacement(cm)') plt.show() \nText: Dates (pandas) \nText: In many cases, it is advantageous to use real dates. You can convert the column to datetime, \nCode: df['Date'] = pd.to_datetime(df[\"Date\"], format=\"%Y\/%m\/%d\")\n\nText: This can then directly plotted via \nCode: df.plot(x=\"Date\", y=\"Elevation\")\n# or, if you want scatter points\ndf.plot(x=\"Date\", y=\"Elevation\", ls=\"\", marker=\"o\")\n\nText: Complete code for reproduction: \nText: import pandas as pd import numpy as np from matplotlib import pyplot as plt import random lst1 = ['2015\/01\/01','2016\/01\/01','2017\/01\/01','2015\/01\/02', '2016\/01\/02','2017\/01\/02','2015\/01\/03','2016\/01\/03','2017\/01\/03', '2015\/01\/04','2015\/01\/05','2017\/01\/04','2016\/01\/04','2016\/01\/05', '2015\/01\/06','2017\/01\/05','2016\/01\/06','2015\/01\/07','2017\/01\/06', '2017\/01\/07','2016\/01\/07','2015\/01\/08','2017\/01\/08','2016\/01\/08', '2015\/01\/09','2016\/01\/09','2017\/01\/09','2016\/01\/10','2017\/01\/10', '2015\/01\/11','2016\/01\/11','2017\/01\/11','2015\/01\/12','2016\/01\/12', '2015\/01\/13','2017\/01\/12','2016\/01\/13','2017\/01\/13','2016\/01\/14', '2015\/01\/14','2017\/01\/14','2015\/01\/15','2016\/01\/15','2017\/01\/15', '2016\/01\/16','2015\/01\/16','2017\/01\/16','2017\/01\/17','2016\/01\/17', '2015\/01\/18','2016\/01\/18','2017\/01\/18','2015\/01\/19','2016\/01\/19', '2017\/01\/19','2015\/01\/20','2016\/01\/20','2017\/01\/20','2015\/01\/21', '2016\/01\/21','2017\/01\/21','2015\/01\/22','2016\/01\/22','2017\/01\/22', '2015\/01\/23','2016\/01\/23','2017\/01\/23','2015\/01\/24','2016\/01\/24', '2017\/01\/24', '2015\/01\/25', '2016\/01\/25'] lst2 = random.sample(range(72), 72) df = pd.DataFrame( {'Date': lst1, 'Elevation': lst2 }) df['Elevation']*=100 h = float(df['Elevation'].head(1)) df['Elevation']-=h df = df[np.abs(df.Elevation-df.Elevation.median())<=(3*df.Elevation.std())] #Convert to datetime df['Date'] = pd.to_datetime(df[\"Date\"], format=\"%Y\/%m\/%d\") #plot with pandas wrapper df.plot(x=\"Date\", y=\"Elevation\", ls=\"\", marker=\"o\") plt.xlabel('Dates') plt.ylabel('Displacement(cm)') plt.show() \nText: Dates (matplotlib) \nText: To have more control over the appearance of the dates on the axes, you may use matplotlib. E.g. to tick every first of the months january and july, and use the datetime format with slashes, use \nCode: plt.scatter(df['Date'].values,df['Elevation'])\nplt.gca().xaxis.set_major_locator(dates.MonthLocator((1,7)))\nplt.gca().xaxis.set_major_formatter(dates.DateFormatter(\"%Y\/%m\/%d\"))\nplt.gcf().autofmt_xdate()\n\nText: import pandas as pd import numpy as np from matplotlib import pyplot as plt import dates as dates import random lst1 = ['2015\/01\/01','2016\/01\/01','2017\/01\/01','2015\/01\/02', '2016\/01\/02','2017\/01\/02','2015\/01\/03','2016\/01\/03','2017\/01\/03', '2015\/01\/04','2015\/01\/05','2017\/01\/04','2016\/01\/04','2016\/01\/05', '2015\/01\/06','2017\/01\/05','2016\/01\/06','2015\/01\/07','2017\/01\/06', '2017\/01\/07','2016\/01\/07','2015\/01\/08','2017\/01\/08','2016\/01\/08', '2015\/01\/09','2016\/01\/09','2017\/01\/09','2016\/01\/10','2017\/01\/10', '2015\/01\/11','2016\/01\/11','2017\/01\/11','2015\/01\/12','2016\/01\/12', '2015\/01\/13','2017\/01\/12','2016\/01\/13','2017\/01\/13','2016\/01\/14', '2015\/01\/14','2017\/01\/14','2015\/01\/15','2016\/01\/15','2017\/01\/15', '2016\/01\/16','2015\/01\/16','2017\/01\/16','2017\/01\/17','2016\/01\/17', '2015\/01\/18','2016\/01\/18','2017\/01\/18','2015\/01\/19','2016\/01\/19', '2017\/01\/19','2015\/01\/20','2016\/01\/20','2017\/01\/20','2015\/01\/21', '2016\/01\/21','2017\/01\/21','2015\/01\/22','2016\/01\/22','2017\/01\/22', '2015\/01\/23','2016\/01\/23','2017\/01\/23','2015\/01\/24','2016\/01\/24', '2017\/01\/24', '2015\/01\/25', '2016\/01\/25'] lst2 = random.sample(range(72), 72) df = pd.DataFrame( {'Date': lst1, 'Elevation': lst2 }) df['Elevation']*=100 h = float(df['Elevation'].head(1)) df['Elevation']-=h df = df[np.abs(df.Elevation-df.Elevation.median())<=(3*df.Elevation.std())] df['Date'] = pd.to_datetime(df[\"Date\"], format=\"%Y\/%m\/%d\") plt.scatter(df['Date'].values,df['Elevation']) plt.gca().xaxis.set_major_locator(dates.MonthLocator((1,7))) plt.gca().xaxis.set_major_formatter(dates.DateFormatter(\"%Y\/%m\/%d\")) plt.gcf().autofmt_xdate() plt.xlabel('Dates') plt.ylabel('Displacement(cm)') plt.show() \nAPI:\nmatplotlib.dates\n","label":[[4720,4725,"Mention"],[6282,6298,"API"]],"Comments":[]}
{"id":59448,"text":"ID:46953206\nPost:\nText: From https:\/\/matplotlib.org\/devdocs\/api\/_as_gen\/matplotlib.pyplot.suptitle.html: \nText: Add a centered title to the figure. kwargs are Text properties. Using figure coordinates, the defaults are: x : 0.5 The x location of the text in figure coords y : 0.98 The y location of the text in figure coords \nText: You can move the position of the suptitle in figure coordinates by choosing different values for x and y. For example, \nText: plt.suptitle(country, x=0.1, y=.95, horizontalalignment='left', verticalalignment='top', fontsize = 15) \nText: will put the suptitle in the upper left corner. \nText: The keywords horizontalalignment and verticalalignment work a bit different ( see e.g. https:\/\/matplotlib.org\/users\/text_props.html): \nText: horizontalalignment controls whether the x positional argument for the text indicates the left, center or right side of the text bounding box. verticalalignment controls whether the y positional argument for the text indicates the bottom, center or top side of the text bounding box. \nAPI:\nmatplotlib.text.Text\n","label":[[159,163,"Mention"],[1055,1075,"API"]],"Comments":[]}
{"id":59449,"text":"ID:47017318\nPost:\nText: NOTE: Updated to answer OP question more directly. \nText: You are mixing Pandas plotting as well as the matplotlib PyPlot API and Object-oriented API by using axes (ax1 above) methods and plt methods. The latter are two distinctly different APIs and they may not work correctly when mixed. The matplotlib documentation recommends using the object-oriented API. \nText: While it is easy to quickly generate plots with the mpl.pyplot module, we recommend using the object-oriented approach for more control and customization of your plots. See the methods in the matplotlib.axes.Axes() class for many of the same plotting functions. For examples of the OO approach to Matplotlib, see the API Examples. \nText: Here's how you can control the x-axis \"tick\" values\/labels using proper matplotlib date formatting (see matplotlib example) with the object-oriented API. Also, see link from @ImportanceOfBeingErnest answer to another question for incompatibilities between Pandas' and matplotlib's datetime objects. \nCode: # prepare your data\ndf = pd.read_csv('..\/..\/..\/so\/dbo.Access_Stat_all.csv',error_bad_lines=False, usecols=['Range_Start','Format','Resource_ID','Number'])\ndf.head()\ndf1 = df[df['Resource_ID'] == 10021]\ndf1 = df1[['Format','Range_Start','Number']]\ndf1[\"Range_Start\"] = df1[\"Range_Start\"].str[:7]\ndf1 = df1.groupby(['Format','Range_Start'], as_index=True).last()\npd.options.display.float_format = '{:,.0f}'.format\ndf1 = df1.unstack()\ndf1.columns = df1.columns.droplevel()\nif df1.index.contains('entry'):\n    df2 = df1[1:4].sum(axis=0)\nelse:\n    df2 = df1[0:3].sum(axis=0)\ndf2.name = 'sum'\ndf2 = df1.append(df2)\nprint(df2)\ndf2.to_csv('test.csv', sep=\"\\t\", float_format='%.f')\nif df2.index.contains('entry'):\n    # convert your index to use pandas datetime format\n    df3 = df2.T[['entry','sum']].copy()\n    df3.index = pd.to_datetime(df3.index)\n    # for illustration, I changed a couple dates and added some dummy values\n    df3.loc['2014-01-01']['entry'] = 48\n    df3.loc['2014-05-01']['entry'] = 28\n    df3.loc['2015-05-01']['entry'] = 36\n    print(df3)\n\n    # plot your data\n    fig, ax = plt.subplots()\n\n    # use matplotlib date formatters\n    years = mdates.YearLocator()   # every year\n    yearsFmt = mdates.DateFormatter('%Y-%m')\n\n    # format the major ticks\n    ax.xaxis.set_major_locator(years)\n    ax.xaxis.set_major_formatter(yearsFmt)\n\n    ax.plot(df3)\n\n    # add legend\n    ax.legend([\"Seitenzugriffe\", \"Dateiabrufe\"])\n\n    fig.savefig('image.png')\nelse:\n    # left as an exercise...\n    df2.T[['sum']].plot(kind = 'bar')\n\nAPI:\nmatplotlib.pyplot\n","label":[[444,454,"Mention"],[2577,2594,"API"]],"Comments":[]}
{"id":59450,"text":"ID:47064852\nPost:\nText: As a rule of thumb you can say: Everything that you see on the canvas is an artist. \nText: As the artist tutorial puts it \nText: the Artist is the object that knows how to use a renderer to paint onto the canvas. [...] the Artist handles all the high level constructs like representing and laying out the figure, text, and lines. The typical user will spend 95% of their time working with the Artists. There are two types of Artists: primitives and containers. The primitives represent the standard graphical objects we want to paint onto our canvas: Line2D, Rectangle, Text, AxesImage, etc., and the containers are places to put them (Axis, Axes and Figure). The standard use is to create a Figure instance, use the Figure to create one or more Axes or Subplot instances, and use the Axes instance helper methods to create the primitives. \nText: Inverting that, one might say that you need to dig really deep to come across something that is not an artist. You may easily check if some object is an artist, \nCode: import matplotlib \nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nline,  = ax.plot([1,2,3])\nscatter = ax.scatter([1,2,3],[2,3,1])\n\nfor some_object in [fig,ax,line,scatter]:\n    print(isinstance(some_object, matplotlib.artist.Artist))\n\nText: will all print True. \nText: When it comes to animations, it is of course artists that you want to animate. The FuncAnimation itself is not an Artist (handwavingly, because you don't see it on the screen). \nCode: import matplotlib.animation\nf =  lambda i: line.set_ydata(line.get_ydata()-0.02)\nani = matplotlib.animation.FuncAnimation(fig, f, frames=20)\n\nprint(isinstance(ani, matplotlib.artist.Artist)) # prints False\n\nText: But it needs a Figure as input as well as some function which will manipulate some artists properties (otherwise there would not be any animation seen). E.g. in the above, the y coordinates of the line's data are changed in every iteration. \nAPI:\nmatplotlib.artist.Artist\n","label":[[157,163,"Mention"],[1963,1987,"API"]],"Comments":[]}
{"id":59451,"text":"ID:47065339\nPost:\nText: Usually the datetimes utilities of pandas and matplotlib are incompatible. If you use a dates object on a date axis created with pandas then this will in most cases fail. \nText: Here is a solution where pandas is used for plotting and matplotlib for formatting (see comments): \nCode: import matplotlib.pyplot as plt  # version 2.1.0\nimport matplotlib.ticker as ticker\nimport pandas as pd  # version 0.21.0\n\ndf = pd.read_csv('data.csv', delim_whitespace=True, index_col=0, parse_dates=['Created'])\ndate_index = pd.date_range(df.Created.min(), df.Created.max(), freq='D')\n\n_, ax = plt.subplots()\n\ns1 = df.resample('D', on='Created').size().fillna(0).reindex(date_index, fill_value=0)\ns2 = df.groupby('Created')['MachineCount'].first().fillna(0).reindex(date_index, fill_value=0)\ns = (s1 \/ s2).fillna(0).reindex(date_index, fill_value=0)\n\ns1rolling = s1.rolling(window=7, center=False).mean().fillna(0).reindex(date_index, fill_value=0)\ns2rolling = s2.rolling(window=7, center=False).mean().fillna(0).reindex(date_index, fill_value=0)\nsrolling = s.rolling(window=7, center=False).mean().fillna(0).reindex(date_index, fill_value=0)\n\n# Plot with pandas without date axis (i.e. use_index=False).\ns1.plot(kind='bar', color='C0', position=0, label='Sales Total', width=0.25, use_index=False)\ns.plot(kind='bar', color='C1', position=1, label='Adjusted For Machine Count', width=0.25, use_index=False)\n\n# Plot with pandas without date axis (i.e. use_index=False).\ns1rolling.plot(kind='line', color='C0', label='_nolegend_', use_index=False)\nsrolling.plot(kind='line', color='C1',  label='_nolegend_', use_index=False)\n\nplt.ylim(0, s1.max() * 1.1)\nplt.legend(loc='upper left')\nplt.ylabel('Frequency')\nplt.title('Items Deposited Per Day')\n\n# Format date axis with matplotlib.\nticklabels = s1.index.strftime('%Y-%m-%d')\nax.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels))\nplt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.show()\n\nText: I hope this will help you. \nAPI:\nmatplotlib.dates\n","label":[[112,117,"Mention"],[1990,2006,"API"]],"Comments":[]}
{"id":59452,"text":"ID:47074245\nPost:\nText: It's too complicated to use pandas for setting colorbar's label. You can use pyplot directly, this is an example \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nn = 100000\nx = np.random.standard_normal(n)\ny = 2.0 + 3.0 * x + 4.0 * np.random.standard_normal(n)\nxmin = x.min()\nxmax = x.max()\nymin = y.min()\nymax = y.max()\n\nfig, axs = plt.subplots(ncols=2, sharey=True, figsize=(7, 4))\nfig.subplots_adjust(hspace=0.5, left=0.07, right=0.93)\nax = axs[0]\nhb = ax.hexbin(x, y, gridsize=50, cmap='inferno')\nax.axis([xmin, xmax, ymin, ymax])\nax.set_title(\"Hexagon binning\")\ncb = fig.colorbar(hb, ax=ax)\ncb.set_label('counts')\n\nax = axs[1]\nhb = ax.hexbin(x, y, gridsize=50, bins='log', cmap='inferno')\nax.axis([xmin, xmax, ymin, ymax])\nax.set_title(\"With a log color scale\")\ncb = fig.colorbar(hb, ax=ax)\ncb.set_label('log10(N)')\n\nplt.show()\n\nText: ref: http:\/\/matplotlib.org\/api\/pyplot_api.html#matplotlib.pyplot.hexbin \nAPI:\nmatplotlib.pyplot\n","label":[[101,107,"Mention"],[970,987,"API"]],"Comments":[]}
{"id":59453,"text":"ID:47083733\nPost:\nText: Those are probably two different questions. \nText: 1) How to set a label on top of the colorbar? \nText: Use cb.ax.set_title(\"Mylabel\") \nCode: import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.rand(25)\ny = np.random.rand(25)\nc = np.random.rand(25)\n\nplt.scatter(x,y, c=c)\ncb = plt.colorbar()\ncb.ax.set_title(\"Mylabel\")\nplt.show()\n\nText: 2) What keyword arguments are available for colorbar.set_label? \nText: The label for a colorbar is essentially a label for an axis. Hence it accepts the same keyword arguments as set_xlabel \nText: Those are fontdict, labelpad and any mtext.Text properties. \nText: Hence you could also position the label using rotation and position \nCode: cb.set_label(\"Mylabel\", rotation=0, position=(1,1))\n\nAPI:\nmatplotlib.axes.Axes.set_xlabel\nmatplotlib.text.Text\n","label":[[554,564,"Mention"],[609,619,"Mention"],[772,803,"API"],[804,824,"API"]],"Comments":[]}
{"id":59454,"text":"ID:47122717\nPost:\nText: You can annotate each point with a list of strings. Use matplotlib.annotate is the solution. However you call annotate on a PathColelction object (result of matplotlib.scatter) instead of a Axes object. \nText: In your code: \nCode: variable = plt.scatter(test1, test2)\nfor i, txt in enumerate(variablelabel):\n    variable.annotate(txt, (test1[i], test2[i]))\n\nText: variable is a matplotlib.collections.PathCollection. Instead use the following: \nCode: plt.scatter(test1, test2)\nfor i, txt in enumerate(variablelabel):\n    plt.annotate(txt, (test1[i], test2[i]))\n\nText: You should get something like this: \nText: I hope this will help you. \nAPI:\nmatplotlib.collections.PathCollection\nmatplotlib.axes.Axes\n","label":[[148,162,"Mention"],[214,218,"Mention"],[668,705,"API"],[706,726,"API"]],"Comments":[]}
{"id":59455,"text":"ID:47166787\nPost:\nText: Here is a code that uses a scatter and shows an annotation upon hovering over the scatter points. \nCode: import matplotlib.pyplot as plt\nimport numpy as np; np.random.seed(1)\n\nx = np.random.rand(15)\ny = np.random.rand(15)\nnames = np.array(list(\"ABCDEFGHIJKLMNO\"))\nc = np.random.randint(1,5,size=15)\n\nnorm = plt.Normalize(1,4)\ncmap = plt.cm.RdYlGn\n\nfig,ax = plt.subplots()\nsc = plt.scatter(x,y,c=c, s=100, cmap=cmap, norm=norm)\n\nannot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n                    arrowprops=dict(arrowstyle=\"->\"))\nannot.set_visible(False)\n\ndef update_annot(ind):\n    \n    pos = sc.get_offsets()[ind[\"ind\"][0]]\n    annot.xy = pos\n    text = \"{}, {}\".format(\" \".join(list(map(str,ind[\"ind\"]))), \n                           \" \".join([names[n] for n in ind[\"ind\"]]))\n    annot.set_text(text)\n    annot.get_bbox_patch().set_facecolor(cmap(norm(c[ind[\"ind\"][0]])))\n    annot.get_bbox_patch().set_alpha(0.4)\n    \n\ndef hover(event):\n    vis = annot.get_visible()\n    if event.inaxes == ax:\n        cont, ind = sc.contains(event)\n        if cont:\n            update_annot(ind)\n            annot.set_visible(True)\n            fig.canvas.draw_idle()\n        else:\n            if vis:\n                annot.set_visible(False)\n                fig.canvas.draw_idle()\n\nfig.canvas.mpl_connect(\"motion_notify_event\", hover)\n\nplt.show()\n\nText: Because people also want to use this solution for a line plot instead of a scatter, the following would be the same solution for plot (which works slightly differently). \nText: import pyplot as plt import numpy as np; np.random.seed(1) x = np.sort(np.random.rand(15)) y = np.sort(np.random.rand(15)) names = np.array(list(\"ABCDEFGHIJKLMNO\")) norm = plt.Normalize(1,4) cmap = plt.cm.RdYlGn fig,ax = plt.subplots() line, = plt.plot(x,y, marker=\"o\") annot = ax.annotate(\"\", xy=(0,0), xytext=(-20,20),textcoords=\"offset points\", bbox=dict(boxstyle=\"round\", fc=\"w\"), arrowprops=dict(arrowstyle=\"->\")) annot.set_visible(False) def update_annot(ind): x,y = line.get_data() annot.xy = (x[ind[\"ind\"][0]], y[ind[\"ind\"][0]]) text = \"{}, {}\".format(\" \".join(list(map(str,ind[\"ind\"]))), \" \".join([names[n] for n in ind[\"ind\"]])) annot.set_text(text) annot.get_bbox_patch().set_alpha(0.4) def hover(event): vis = annot.get_visible() if event.inaxes == ax: cont, ind = line.contains(event) if cont: update_annot(ind) annot.set_visible(True) fig.canvas.draw_idle() else: if vis: annot.set_visible(False) fig.canvas.draw_idle() fig.canvas.mpl_connect(\"motion_notify_event\", hover) plt.show() \nText: In case someone is looking for a solution for lines in twin axes, refer to How to make labels appear when hovering over a point in multiple axis? \nText: In case someone is looking for a solution for bar plots, please refer to e.g. this answer. \nAPI:\nmatplotlib.pyplot\n","label":[[1636,1642,"Mention"],[2884,2901,"API"]],"Comments":[]}
{"id":59456,"text":"ID:47382270\nPost:\nText: For a solution using TextArea see this answer. You would then need to recreate the fontproperties for the text inside the TextArea. \nText: Since here you want to show exactly the symbol you have as text also in the legend, a simpler way to create a legend handler for some text object would be the following, which maps the text to a TextHandler. The TextHandler subclasses mpl.legend_handler.HandlerBase and its create_artists produces a copy of the text to show in the legend. Some of the text properties then need to be adjusted for the legend. \nCode: import matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerBase\nimport copy\n\nax = plt.gca()\nax.axis([-1, 3,-1, 2])\n\ntx1 = ax.text(x=0, y=0, s=ur'$\\u2660$', color='r',size=30, ha=\"right\")\ntx2 = ax.text(x=2, y=0, s=ur'$\\u2665$', color='g',size=30)\n\n\nclass TextHandler(HandlerBase):\n    def create_artists(self, legend, orig_handle,xdescent, ydescent,\n                        width, height, fontsize,trans):\n        h = copy.copy(orig_handle)\n        h.set_position((width\/2.,height\/2.))\n        h.set_transform(trans)\n        h.set_ha(\"center\");h.set_va(\"center\")\n        fp = orig_handle.get_font_properties().copy()\n        fp.set_size(fontsize)\n        # uncomment the following line, \n        # if legend symbol should have the same size as in the plot\n        h.set_font_properties(fp)\n        return [h]\n\nlabels = [\"label 1\", \"label 2\"]\nhandles = [tx1,tx2]\nhandlermap = {type(tx1) : TextHandler()}\nax.legend(handles, labels, handler_map=handlermap,) \n\nplt.show()\n\nText: Also see this more generic answer \nAPI:\nmatplotlib.legend_handler.HandlerBase\n","label":[[398,428,"Mention"],[1611,1648,"API"]],"Comments":[]}
{"id":59457,"text":"ID:47387799\nPost:\nText: Inside mpl.pyplot there is a line saying \nCode: from matplotlib.lines import Line2D\n\nText: Hence, if you import matplotlib.pyplot, Line2D is available from the pyplot namespace, \nCode: import matplotlib.pyplot as plt\nplt.Line2D(...)\n\nText: Because the availability in the namespace itself is not a documented feature, one should probably not rely on it. Also it is better style to use the functions\/objects imported from their original submodule. On the other hand it is very convenient to just write plt.Rectangle instead of having to import the patches module first. I guess at the end you need to decide for yourself which case applies. \nAPI:\nmatplotlib.pyplot\n","label":[[31,41,"Mention"],[670,687,"API"]],"Comments":[]}
{"id":59458,"text":"ID:47470408\nPost:\nText: For a quick fix, use: \nCode: import matplotlib.dates as dates\nplt_dates = dates.date2num(t.to_pydatetime())\n\nText: or: \nCode: import matplotlib.dates as dates\nplt_dates = dates.date2num(list(t))\n\nText: It seems the latest (matplotlib.__version__ '2.1.0') does not like numpy arrays... Edit: In my case, after checking the source code, the problem seems to be that the latest cbook cannot create an iterable from the numpy array and thinks the array is a number. \nText: For similar but a bit more complex problems, check http:\/\/stackoverflow.com\/questions\/13703720\/converting-between-datetime-timestamp-and-datetime64, possibly Why do I get \"python int too large to convert to C long\" errors when I use matplotlib's DateFormatter to format dates on the x axis?, and maybe matplotlib plot_date AttributeError: 'numpy.datetime64' object has no attribute 'toordinal' (if someone answers) Edit: someone answered, his code using to_pydatetime() seems best, also: pandas 0.21.0 Timestamp compatibility issue with matplotlib, though that did not work in my case (because of python 2???) \nAPI:\nmatplotlib.cbook\n","label":[[399,404,"Mention"],[1109,1125,"API"]],"Comments":[]}
{"id":59459,"text":"ID:47485840\nPost:\nText: The problem lies in the lines \nCode: fig = plt.figure()\nax = fig.add_axes([0,0,30,30])\n\nText: This will create an axes which is 30 times larger than the figure in each dimension. Hence you only see 1.\/(30*30) = 1 per mille of the axes. \nText: What you would probably like to do is add a normal subplot and set its data range to 0..30 in both directions. \nCode: fig, ax = plt.subplots()\nax.axis([0,30,0,30])\n\nText: Complete example: \nText: import mpl.pyplot as plt import matplotlib.patches as patches plt.close('all') pts={} #defining points pts.update({1:(0,0)}) pts.update({2:(10,0)}) pts.update({3:(10,20)}) pts.update({4:(0,20)}) pts.update({5:(30,0)}) pts.update({6:(30,20)}) #defining rectangle connecting to points rect={} rect.update({10:(1,2,3,4)}) rect.update({11:(2,5,6,3)}) #plotting fig, ax = plt.subplots() ax.axis([0,30,0,30]) for i in rect: p2=rect[i] dx,dy = pts[p2[0]] wd = pts[p2[1]][0] - pts[p2[0]][0] ht = pts[p2[3]][1] - pts[p2[0]][1] midpt = [ (pts[p2[0]][0] + pts[p2[2]][0])\/2, (pts[p2[0]][1] + pts[p2[2]][1])\/2 ] #finding midpoint of points zeroth and second #x_mid= (x_0+x_2)\/2 and similar for y_mid p = patches.Rectangle((dx,dy), wd,ht,fill=False, clip_on=False) ax.add_patch(p) plt.text(midpt[0],midpt[1],i,color='k') else: plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[470,480,"Mention"],[1293,1310,"API"]],"Comments":[]}
{"id":59460,"text":"ID:47490220\nPost:\nText: The BT lives in its own axes, which you need to supply via the first argument. So you need to create an axes somewhere. \nText: Depending on what you are trying to achieve, you may simply choose coordinates inside the axes, \nCode: button_ax = plt.axes([0.4, 0.5, 0.2, 0.075])  #posx, posy, width, height\nButton(button_ax, 'Click me')\n\nText: The coordinates here are in units of the figure width and height. Hence the button will be created at 40% of figure width, 50% of figure height and is 20% wide, 7.5% heigh. \nText: Alternatively you may place the button axes relative to the subplot axes using InsetPosition. \nCode: import matplotlib.pyplot as plt\nfrom  matplotlib.widgets import Button\nfrom mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n\nfig, ax= plt.subplots()\n\nbutton_ax = plt.axes([0, 0, 1, 1])\nip = InsetPosition(ax, [0.4, 0.5, 0.2, 0.1]) #posx, posy, width, height\nbutton_ax.set_axes_locator(ip)\nButton(button_ax, 'Click me')\n\nplt.show()\n\nText: Here, the button is positioned at 40% of the axes width and 50% of its height, 20% of the axes width long, and 8% heigh. \nAPI:\nmatplotlib.widgets.Button\n","label":[[28,30,"Mention"],[1120,1145,"API"]],"Comments":[]}
{"id":59461,"text":"ID:47529821\nPost:\nText: The boxplot returns a dictionary of artists \nText: result : dict A dictionary mapping each component of the boxplot to a list of the Line2D instances created. That dictionary has the following keys (assuming vertical boxplots): boxes: the main body of the boxplot showing the quartiles and the medians confidence intervals if enabled. [...] \nText: Using the boxes, you can get the legend artists as \nCode: ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], ['A', 'B'], loc='upper right')\n\nText: Complete example: \nCode: import matplotlib.pyplot as plt\nimport numpy as np; np.random.seed(1)\n\ndata1=np.random.randn(40,2)\ndata2=np.random.randn(30,2)\n\nfig, ax = plt.subplots()\nbp1 = ax.boxplot(data1, positions=[1,4], notch=True, widths=0.35, \n                 patch_artist=True, boxprops=dict(facecolor=\"C0\"))\nbp2 = ax.boxplot(data2, positions=[2,5], notch=True, widths=0.35, \n                 patch_artist=True, boxprops=dict(facecolor=\"C2\"))\n\nax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], ['A', 'B'], loc='upper right')\n\nax.set_xlim(0,6)\nplt.show()\n\nAPI:\nmatplotlib.lines.Line2D\n","label":[[157,163,"Mention"],[1074,1097,"API"]],"Comments":[]}
{"id":59462,"text":"ID:47734907\nPost:\nText: According to the color cycle docs, set_prop_cycle() can be applied to plt.rc() or to a ax.Axes object. \nText: Here's one way to do it with an Axes object: \nCode: from cycler import cycler\n\n# separate the figure and axis elements of plt\nf, ax = plt.subplots()\n\ncy = cycler('color', ['black', 'red'])\nax.set_prop_cycle(cy)\nax.plot(x,y1,x,y2)\n\nText: FWIW for something like this you might find Pandas reads more clearly: \nCode: data = {\"x\":x, \"y1\":y1, \"y2\":y2}\ncolors = [\"black\", \"red\"]\n\npd.DataFrame(data).set_index(\"x\").plot(color=colors)\n\nAPI:\nmatplotlib.axes.Axes\n","label":[[111,118,"Mention"],[568,588,"API"]],"Comments":[]}
{"id":59463,"text":"ID:47790514\nPost:\nText: It is not possible to insert plots into a matplotlib table. However subplot grids allow to create a table-like behaviour. \nCode: import matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.rand(100,4)\ncol1 = [\"WAR\", \"ERA\", \"IP\", \"WHIP\", \"Final\\nScore\"]\ncol2 = [0.23,1.60,0.28,0.02,0.38]\ncol2colors = [\"red\", \"g\", \"r\", \"r\", \"r\"]\nfinalsc = \"D+\"\n\nfig, axes = plt.subplots(ncols=3, nrows=5, figsize=(4,2.6),\n                         gridspec_kw={\"width_ratios\":[1,0.5,2]})\nfig.subplots_adjust(0.05,0.05,0.95,0.95, wspace=0.05, hspace=0)\n\nfor ax in axes.flatten():\n    ax.tick_params(labelbottom=0, labelleft=0, bottom=0, top=0, left=0, right=0)\n    ax.ticklabel_format(useOffset=False, style=\"plain\")\n    for _,s in ax.spines.items():\n        s.set_visible(False)\nborder = fig.add_subplot(111)\nborder.tick_params(labelbottom=0, labelleft=0, bottom=0, top=0, left=0, right=0)\nborder.set_facecolor(\"None\")\n\ntext_kw = dict(ha=\"center\", va=\"bottom\", size=13)\nfor i,ax in enumerate(axes[:,0]):\n    ax.text(0.5, 0.05, col1[i], transform=ax.transAxes, **text_kw)\nfor i,ax in enumerate(axes[:,1]):\n    ax.text(0.5, 0.05, \"{:.2f}\".format(col2[i]),transform=ax.transAxes, **text_kw)\n    ax.set_facecolor(col2colors[i])\n    ax.patch.set_color(col2colors[i])\naxes[-1,-1].text(0.5, 0.05, finalsc,transform=axes[-1,-1].transAxes, **text_kw)\n\nfor i,ax in enumerate(axes[:-1,2]):\n    ax.plot(data[:,i], color=\"green\", linewidth=1)\n\n\nplt.show()\n\nText: To put several such plots into a figure you would approach this a bit differently and create a gridspec with several subgrids. \nText: import pyplot as plt from matplotlib import gridspec import numpy as np def summaryplot2subplot(fig, gs, data, col1, col2, finalsc): col2colors = [\"g\" if col2[i] > 1 else \"r\" for i in range(len(col2)) ] sgs = gridspec.GridSpecFromSubplotSpec(5,3, subplot_spec=gs, wspace=0.05, hspace=0, width_ratios=[0.9,0.7,2]) axes = [] for n in range(5): for m in range(3): axes.append(fig.add_subplot(sgs[n,m])) axes = np.array(axes).reshape(5,3) for ax in axes.flatten(): ax.tick_params(labelbottom=0, labelleft=0, bottom=0, top=0, left=0, right=0) ax.ticklabel_format(useOffset=False, style=\"plain\") for _,s in ax.spines.items(): s.set_visible(False) border = fig.add_subplot(gs) border.tick_params(labelbottom=0, labelleft=0, bottom=0, top=0, left=0, right=0) border.set_facecolor(\"None\") text_kw = dict(ha=\"center\", va=\"bottom\", size=11) for i,ax in enumerate(axes[:,0]): ax.text(0.5, 0.05, col1[i], transform=ax.transAxes, **text_kw) for i,ax in enumerate(axes[:,1]): ax.text(0.5, 0.05, \"{:.2f}\".format(col2[i]),transform=ax.transAxes, **text_kw) ax.set_facecolor(col2colors[i]) ax.patch.set_color(col2colors[i]) axes[-1,-1].text(0.5, 0.05, finalsc,transform=axes[-1,-1].transAxes, **text_kw) for i,ax in enumerate(axes[:-1,2]): ax.plot(data[:,i], color=col2colors[i], linewidth=1) fig = plt.figure(figsize=(8,6)) gs = gridspec.GridSpec(2,2) col1 = [\"WAR\", \"ERA\", \"IP\", \"WHIP\", \"Final\\nScore\"] finalsc = \"D+\" for i in range(4): data = np.random.rand(100,4) col2 = np.random.rand(5)*2 summaryplot2subplot(fig, gs[i], data, col1, col2, finalsc) plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[1605,1611,"Mention"],[3151,3168,"API"]],"Comments":[]}
{"id":59464,"text":"ID:47850208\nPost:\nText: Application to your problem: \nCode: from matplotlib.ticker import FuncFormatter\nfrom matplotlib.pyplot import show\nimport matplotlib.pyplot as plt\nimport numpy as np\n\na=np.random.random((1000,1000))\n\n# create scaled formatters \/ for Y with Atom prefix\nformatterY = FuncFormatter(lambda y, pos: 'Atom {0:g}'.format(y*0.01))\nformatterX = FuncFormatter(lambda x, pos: '{0:g}'.format(x*0.01))\n\n# apply formatters \nfig, ax = plt.subplots()\nax.yaxis.set_major_formatter(formatterY)\nax.xaxis.set_major_formatter(formatterX)\n\nplt.imshow(a, cmap='Reds', interpolation='nearest')\n\n# create labels\nplt.xlabel('nanometer')\nplt.ylabel('measure')\nplt.xticks(list(range(0, 1001,100)))\n\nplt.yticks(list(range(0, 1001,100)))\n\nplt.show()\n\nText: Sources: \nText: A possible solution is to format the ticklabels according to some function as seen in below example code from the matplotlib page. \nText: from matplotlib.ticker import FuncFormatter import plt as plt import numpy as np x = np.arange(4) money = [1.5e5, 2.5e6, 5.5e6, 2.0e7] def millions(x, pos): 'The two args are the value and tick position' return '$%1.1fM' % (x * 1e-6) formatter = FuncFormatter(millions) fig, ax = plt.subplots() ax.yaxis.set_major_formatter(formatter) plt.bar(x, money) plt.xticks(x, ('Bill', 'Fred', 'Mary', 'Sue')) plt.show() matplotlib.org Example \nText: A similar solution is shown in this answer, where you can set a function to label the axis for you and scale it down: \nText: ticks = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x*scale)) ax.xaxis.set_major_formatter(ticks) \nText: Here, you would need to do \/100 instead of *scale \nText: The easier way for yours would probably be: \nText: ticks = plt.xticks()\/100 plt.gca().set_xticklabels(ticks.astype(int)) (adapted from https:\/\/stackoverflow.com\/a\/10171851\/7505395) \nAPI:\nmatplotlib.pyplot\n","label":[[956,959,"Mention"],[1827,1844,"API"]],"Comments":[]}
{"id":59465,"text":"ID:47862356\nPost:\nText: You can create the annotation objects before and then update the text as part of your annotate() function. This can be done by set_text() method of Text Class on the annotate object. (because Annotation class is based on Text class) \nText: Here is how this done: \nCode: from matplotlib import pyplot as plt\n\nfig, ax = plt.subplots()\nx=1\nannotation = ax.annotate('', (0.5,0.5), textcoords='data', size=10) # empty annotate object\nother_annotation = ax.annotate('Other annotation', (0.5,0.4), textcoords='data', size=10) # other annotate\n\ndef annotate():\n    global x\n    if x==1:\n        x=-1\n    else:\n        x=1\n    annotation.set_text(x)\n\n\ndef onclick(event):\n    annotate()\n    fig.canvas.draw()\n\ncid = fig.canvas.mpl_connect('button_press_event',onclick)\nplt.show()\n\nAPI:\nmatplotlib.text.Annotation\nmatplotlib.text.Text\n","label":[[216,226,"Mention"],[245,249,"Mention"],[801,827,"API"],[828,848,"API"]],"Comments":[]}
{"id":59466,"text":"ID:47931659\nPost:\nText: I'll prepare the data the same as you, except to remove the time dimension I'll use iris.util.squeeze, which removes any length-1 dimension. \nCode: import iris\n\nelev = iris.load_cube('elev.0.5-deg.nc')\nelev = iris.util.squeeze(elev)\nmalawi = iris.Constraint(longitude=lambda v: 32.0 <= v <= 36.,\n                         latitude=lambda v: -17. <= v <= -8.)      \nelev = elev.extract(malawi)\n\nText: As @ImportanceOfBeingErnest says, you want a contour plot. When unsure what plotting function to use, I recommend browsing the matplotlib gallery to find something that looks similar to what you want to produce. Click on an image and it shows you the code. \nText: So, to make the contour plot you can use the mpl.pyplot.contourf function, but you have to get the relevant data from the cube in the form of numpy arrays: \nCode: import matplotlib.pyplot as plt\nimport matplotlib.cm as mpl_cm\nimport numpy as np\nimport cartopy\n\ncmap = mpl_cm.get_cmap('YlGn')\nlevels = np.arange(0,2000,150)\nextend = 'max'\n\nax = plt.axes(projection=cartopy.crs.PlateCarree())\nplt.contourf(elev.coord('longitude').points, elev.coord('latitude').points, \n             elev.data, cmap=cmap, levels=levels, extend=extend)\n\nText: However, iris provides a shortcut to the maplotlib.pyplot functions in the form of iris.plot. This automatically sets up an axes instance with the right projection, and passes the data from the cube through to matplotlib.pyplot. So the last two lines can simply become: \nCode: import iris.plot as iplt\niplt.contourf(elev, cmap=cmap, levels=levels, extend=extend)\n\nText: There is also iris.quickplot, which is basically the same as iris.plot, except that it automatically adds a colorbar and labels where appropriate: \nCode: import iris.quickplot as qplt\nqplt.contourf(elev, cmap=cmap, levels=levels, extend=extend)\n\nText: Once plotted, you can get hold of the axes instance and add your other items (for which I simply copied your code): \nCode: from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n\nqplt.contourf(elev, cmap=cmap, levels=levels, extend=extend)\nax = plt.gca()\n\nax.add_feature(cartopy.feature.COASTLINE)   \nax.add_feature(cartopy.feature.BORDERS)\nax.add_feature(cartopy.feature.LAKES, alpha=0.5)\nax.add_feature(cartopy.feature.RIVERS)\n\nax.set_xticks([33, 34, 35]) \nax.set_yticks([-10, -12, -14, -16]) \nlon_formatter = LongitudeFormatter(zero_direction_label=True)\nlat_formatter = LatitudeFormatter()\nax.xaxis.set_major_formatter(lon_formatter)\nax.yaxis.set_major_formatter(lat_formatter)\n\nAPI:\nmatplotlib.pyplot.contourf\n","label":[[732,751,"Mention"],[2551,2577,"API"]],"Comments":[]}
{"id":59467,"text":"ID:48151947\nPost:\nText: The problem appears to be that you do not have a graphical backend installed. The error you are getting about Python Tk is happening because normally Tk comes with any Python distribution, so you should have at least that. You can install any Python bindings for Tk, Pyqt4, Pyqt5, Wx, GTK, (possibly others) to get a working interactive graphical backend. Check the package repository for the actual package names to install. \nText: Keep in mind that functions like imshow are part of the (sub)package matplotlib.pyplot, not matplotlib itself. import matplotlib as plt is simply wrong if you intend to do plt.imshow(...). The correct import is either from matplotlib import pyplot as plt or import pyplrt as plt. \nAPI:\nmatplotlib.pyplot\n","label":[[722,728,"Mention"],[743,760,"API"]],"Comments":[]}
{"id":59468,"text":"ID:48196370\nPost:\nText: You do this by digging slightly deeper into the workings of matplotlib ticks using the object oriented API. \nText: First off, you can get a list of your major ticks using xaxis.get_major_ticks(). This returns a list of XTick objects. These have an attribute label1 which is the label of the tick. This is a Text instance which has a property set_horizontalalignment(align). Which states: \nText: set_horizontalalignment(align) Set the horizontal alignment to one of ACCEPTS: [ center | right | left ] \nText: Then, as you only want to modify the first and last ticks, simply set the alignment of the specific ticks using the first and last entries in the list. \nText: A working example: \nCode: import matplotlib.pyplot as plt\n\nx = [1,2,3]\ny = [4,5,6]\n\nlabels = [\"2004\", \"2006\", \"2008\"]\n\nfig, ax = plt.subplots()\nax.margins(0)\nax.plot(x,y)\nax.set_xticks(x)\nax.set_xticklabels(labels)\n# above code recreates the issue\n\n# get list of x tick objects\nxTick_objects = ax.xaxis.get_major_ticks()\n\nxTick_objects[0].label1.set_horizontalalignment('left')   # left align first tick \nxTick_objects[-1].label1.set_horizontalalignment('right') # right align last tick\n\nplt.show() \n\nText: Which gives: \nAPI:\nmatplotlib.axis.XTick\n","label":[[243,248,"Mention"],[1222,1243,"API"]],"Comments":[]}
{"id":59469,"text":"ID:48241972\nPost:\nText: Currently you are plotting the data against its index. However, if you want to use dates locators and formatters you would need to plot dates on the axes. This is not possible using candlestick2_ohlc. Instead you would need to use candlestick_ohlc function. Actually this is also said in this answer to the question you link to. Using actual dates however, does not allow to merge the sements, other than possibly plotting in different subplots, see broken axes example. \nText: So a solution here might be to stay with plotting the index and setting the ticks to the locations that correspond the desired tick labels. \nCode: xdate = bar_df.index\ndef mydate(x, pos):\n    try:\n        return xdate[int(x)]\n    except IndexError:\n        return ''\n# create date ranges of possible dates to show as major and minor ticklabels\nmajor_dr = pd.date_range('2017-09-13 21:00:00','2017-09-14 15:00:00', freq='60min')\nminor_dr = pd.date_range('2017-09-13 21:00:00','2017-09-14 15:00:00', freq='15min')\n# calculate positions of the above dates inside the dataframe index\nmajor_ticks = np.isin(xdate, major_dr).nonzero()[0] \nminor_ticks = np.isin(xdate, minor_dr).nonzero()[0]\n# use those positions to put ticks at\nax.xaxis.set_major_locator(ticker.FixedLocator(major_ticks))\nax.xaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\nax.minorticks_on()\nax.xaxis.set_major_formatter(ticker.FuncFormatter(mydate))\nfig.autofmt_xdate()\n\nText: The result would look like \nText: This is reading very confusingly, but to the best of my understanding this is what the question asks for. \nAPI:\nmatplotlib.dates\n","label":[[107,112,"Mention"],[1599,1615,"API"]],"Comments":[]}
{"id":59470,"text":"ID:48257654\nPost:\nText: If Pandas is available to you, consider this approach: \nCode: import pandas as pd\ndata = pd.to_datetime(data, yearfirst=True)\nplt.plot(data.date, data.time)\n_=plt.ylim([\"00:00:00\", \"23:59:59\"])\n\nText: Update per comments X-axis date formatting can be adjusted using the Locator and Formatter methods of the dates module. Locator finds the tick positions, and Formatter specifies how you want the labels to appear. \nText: Sometimes Matplotlib\/Pandas just gets it right, other times you need to call out exactly what you want using these extra methods. In this case, I'm not sure why those numbers are showing up, but this code will remove them. \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nf, ax = plt.subplots()\ndata = pd.to_datetime(data, yearfirst=True)\nax.plot(data.date, data.time)\nax.set_ylim([\"00:00:00\", \"23:59:59\"])\n\ndays = mdates.DayLocator()\nd_fmt = mdates.DateFormatter('%m-%d')\nax.xaxis.set_major_locator(days)\nax.xaxis.set_major_formatter(d_fmt) \n\nAPI:\nmatplotlib.dates\n","label":[[331,336,"Mention"],[1039,1055,"API"]],"Comments":[]}
{"id":59471,"text":"ID:48266871\nPost:\nText: You can of course use mpl.pyplot to create a circle quite easily: \nCode: import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\ncircle = plt.Circle((0,0), radius)\nax.add_artist(circle)\n\nText: If we examine the type of circle: \nCode: print (type(circle))\n# <class 'matplotlib.patches.Circle'>\n\nText: Therefore, if you wanted to do this without importing pyplot, simply use mpatches.Circle \nText: For your specific example, your __init__ would look something like: \nCode: def __init__(self, parent=None):\n    super(MainGUI, self).__init__(parent)\n    self.setupUi(self)\n\n    #### Lots of other code goes here ####\n\n    # PlotView\n    self.figure = Figure(tight_layout=True)\n    self.canvas = FigureCanvas(self.figure)\n    self.toolbar = NavigationToolbar(self.canvas, self)\n    self.plotLayout.addWidget(self.toolbar)\n    self.plotLayout.addWidget(self.canvas)\n    self.graph = self.figure.add_subplot(111)\n    self.graph.axis('equal')\n\n    self.graph.plot([-3, -2, -1, 0, 1, 2, 3], [0, -1, 4, 2, 0, -3, -5])\n    self.graph.plot(0, 0, '*')\n    self.circle = matplotlib.patches.Circle((0,0), 0.5, color=\"r\")\n    self.graph.add_artist(self.circle)\n    ### Even more code goes here ####\n\nText: Which gives: \nAPI:\nmatplotlib.pyplot\nmatplotlib.patches.Circle\n","label":[[46,56,"Mention"],[399,414,"Mention"],[1235,1252,"API"],[1253,1278,"API"]],"Comments":[]}
{"id":59472,"text":"ID:48322150\nPost:\nText: Short version: Use mpl.style.use instead of mpl.rc_file. \nText: Long version: You may print out the backend in use to see what is going on. \nCode: import matplotlib as mpl\n\ndef set_default():\n    mpl.rc_file('matplotlibrc.txt') # this is an empty file\n\nimport matplotlib.pyplot as plt\nprint mpl.get_backend()\n# This prints u'TkAgg' (in my case) the default backend in use \n#  due to my rc Params\n\n%matplotlib inline\nprint mpl.get_backend()\n# This prints \"module:\/\/ipykernel.pylab.backend_inline\", because inline has been set\nset_default()\nprint mpl.get_backend()\n# This prints \"agg\", because this is the default backend reset by setting the empty rc file\nplt.plot()\n# Here, no plot is shown because agg (a non interactive backend) is used.\n\nText: Until here no supprise. \nText: Now the second case. \nCode: import matplotlib as mpl\n\ndef set_default():\n    mpl.rc_file('matplotlibrc.txt') # this is an empty file\n\nimport matplotlib.pyplot as plt\nprint mpl.get_backend()\n# This prints u'TkAgg' (in my case) the default backend in use, same as above\n\n%matplotlib inline\nprint mpl.get_backend()\n# This prints \"module:\/\/ipykernel.pylab.backend_inline\", because inline has been set\nplt.plot()\n# This shows the inline plot, because the inline backend is active.\n\nset_default()\nprint mpl.get_backend()\n# This prints \"agg\", because this is the default backend reset by setting the new empty rc file\nplt.plot()\n# Here comes the supprise: Although \"agg\" is the backend, still, an inline plot is shown.\n# This is due to the inline backend being the one registered in pyplot \n#   when doing the first plot. It cannot be changed afterwards.\n\nText: The main point is, you may still change the backend, until the first plot is produced, not after. \nText: The same argument goes for the figure size. The default matplotlib figure size is (6.4,4.8), while the one being set with the inline backend is (6.0,4.0). Also the figure dpi is different, it is 100 in the default rcParams, but 72. in the inline configuration. This makes the plot appear much smaller. \nText: Now to the actual problem. I suppose the use of a stylesheet is meant here to set some styles for plots, not to change the backend. Hence you would rather only set the style from the rc file. This can be done in the usual way,using use \nCode: def set_default():\n    mpl.style.use('matplotlibrc.txt')\n\nText: When this is used, it will not overwrite the backend in use, but only update those parameters, specified in the file itself. \nAPI:\nmatplotlib.style.use\n","label":[[2303,2306,"Mention"],[2509,2529,"API"]],"Comments":[]}
{"id":59473,"text":"ID:48333655\nPost:\nText: According to the Matplotlib documentation, pie() returns two or three lists: \nText: A list of mpatches.Wedge A list of mpl.text.Text labels (conditionally) A list of Txet data labels \nText: Your code needs to manipulate the edge and face colors of the Wedge objects returned by pie(), which are in the first list (zero index) in the return value, pie_wedge_collection. \nAPI:\nmatplotlib.patches.Wedge\nmatplotlib.text.Text\nmatplotlib.text.Text\n","label":[[118,132,"Mention"],[143,156,"Mention"],[190,194,"Mention"],[399,423,"API"],[424,444,"API"],[445,465,"API"]],"Comments":[]}
{"id":59474,"text":"ID:48433250\nPost:\nText: You probably want to use a boundary norm. The problem is that matplotlib would not know which value you want to have corresponding to which color. To give this information, a BoundaryNorm can be used, specifying the bin edges of the colors. As in this example you have the integers 0,1,2,3,4,5 as the values, your bin edges are best chosen to be -0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, such that the values lie in the center of the bins. \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#%matplotlib inline\n\ndf = pd.DataFrame(np.random.randint(0,6,size=(52, 7)))\ncolors = {0:'#05926f', 1:'#99cc33', 2:'#F26419', 3:'#F6AE2D', 4:'#06AED5', 5:'#3baa5d'}\n\ncolorrange = range(len(list(colors.keys())))\ncolorlist = [colors[i] for i in colorrange]\ncmap = mpl.colors.ListedColormap(colorlist)\nbounds = np.array(range(len(list(colors.keys()))+1))-0.5\nnorm = mpl.colors.BoundaryNorm(bounds, len(colorrange))\n\nfig, ax = plt.subplots(figsize=(4.2,8))\nfig.subplots_adjust(right=0.8)\nax = sns.heatmap(df, annot=True, cmap=cmap, norm=norm, \n                 cbar_kws={'format': 'Group-%g'}, linewidths=.05)\nplt.show()\n\nText: Code for when the data range does not start at 0, but is still N successive integers: \nText: import pandas as pd import numpy as np import matplotlib as mpl import pypeot as plt import seaborn as sns #%matplotlib inline df = pd.DataFrame(np.random.randint(1,7,size=(52, 7))) colors = {6:'#05926f', 1:'#99cc33', 2:'#F26419', 3:'#F6AE2D', 4:'#06AED5', 5:'#3baa5d'} colorrange = sorted(list(colors.keys())) colorlist = [colors[i] for i in colorrange] cmap = mpl.colors.ListedColormap(colorlist) bounds = np.array(colorrange+[max(colorrange)+1])-0.5 norm = mpl.colors.BoundaryNorm(bounds, len(colorrange)) fig, ax = plt.subplots(figsize=(4.2,8)) fig.subplots_adjust(right=0.8) ax = sns.heatmap(df, annot=True, cmap=cmap, norm=norm, cbar_kws={'format': 'Group-%g'}, linewidths=.05) plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[1370,1376,"Mention"],[2000,2017,"API"]],"Comments":[]}
{"id":59475,"text":"ID:48452599\nPost:\nText: The reason locator_params(axis='x', nticks=3) is not working as expected is that nticks is not a valid argument to the mticker.AutoLocator in use. \nText: From the documentation: \nText: Typically one might want to reduce the maximum number of ticks and use tight bounds when plotting small subplots, for example:: ax.locator_params(tight=True, nbins=4) \nText: So replace nticks by nbins. \nAPI:\nmatplotlib.ticker.AutoLocator\n","label":[[143,162,"Mention"],[417,446,"API"]],"Comments":[]}
{"id":59476,"text":"ID:48632237\nPost:\nText: There are some questions and answers about defining a midpoint on a colorscale. Especially this one, which is also now part of the matplotlib documentation. \nText: The idea is to subclass mcolors.Normalize and let it take a further argument midpoint. This can then be used to linearly interpolate the two ranges on either side of the midpoint to the ranges [0,0.5] and [0.5,1]. \nText: To have a midpoint on a logarithmic scale, we can in principle do the same thing, just that we subclass LogNorm and take the logarithm of all values, then interpolate this logarithm on the ranges [0,0.5] and [0.5,1]. \nText: In the following example we have data between 0.001 and 10. Using the usual LogNorm this results in the middle of the colormap (white in the case of the RdBu colormap) to be at 0.1. If we want to have white at 1, we specify 1 as the midpoint in the MidPointLogNorm. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom  matplotlib.colors import LogNorm\n\nx,y = np.meshgrid(np.linspace(-3,0,19), np.arange(10))\nf = lambda x,y : 10**x*(1+y)\nz = f(x,y)\n\nfig, (ax,ax2) = plt.subplots(ncols=2, figsize=(12,4.8))\n\nim = ax.pcolormesh(x,y,z, cmap=\"RdBu_r\", norm=LogNorm(vmin=z.min(), vmax=z.max()))\nfig.colorbar(im, ax=ax)\nax.set_title(\"LogNorm\")\n\nclass MidPointLogNorm(LogNorm):\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        LogNorm.__init__(self,vmin=vmin, vmax=vmax, clip=clip)\n        self.midpoint=midpoint\n    def __call__(self, value, clip=None):\n        # I'm ignoring masked values and all kinds of edge cases to make a\n        # simple example...\n        x, y = [np.log(self.vmin), np.log(self.midpoint), np.log(self.vmax)], [0, 0.5, 1]\n        return np.ma.masked_array(np.interp(np.log(value), x, y))\n\n\nim2 = ax2.pcolormesh(x,y,z, cmap=\"RdBu_r\", \n                            norm=MidPointLogNorm(vmin=z.min(), vmax=z.max(), midpoint=1))\nfig.colorbar(im2, ax=ax2)\nax2.set_title(\"MidPointLogNorm\")\nplt.show()\n\nText: Updated solution which works for nan values: You need to replace the nan values by some value (best one outside the range of values from the array) then mask the array by those numbers. Inside the \nText: MidPointLogNorm \nText: we need to take care of nan values, as shown in \nText: this question \nText: . \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom  matplotlib.colors import LogNorm\n\nx,y = np.meshgrid(np.linspace(-3,0,19), np.arange(10))\nf = lambda x,y : 10**x*(1+y)\nz = f(x,y)\nz[1:3,1:3] = np.NaN\n\n#since nan values cannot be used on a log scale, we need to change them to \n# something other than nan, \nreplace = np.nanmax(z)+900\nz = np.where(np.isnan(z), replace, z)\n# now we can mask the array\nz = np.ma.masked_where(z == replace, z)\n\nfig, (ax,ax2) = plt.subplots(ncols=2, figsize=(12,4.8))\n\nim = ax.pcolormesh(x,y,z, cmap=\"RdBu_r\", norm=LogNorm(vmin=z.min(), vmax=z.max()))\nfig.colorbar(im, ax=ax)\nax.set_title(\"LogNorm\")\n\nclass MidPointLogNorm(LogNorm):\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        LogNorm.__init__(self,vmin=vmin, vmax=vmax, clip=clip)\n        self.midpoint=midpoint\n    def __call__(self, value, clip=None):\n        result, is_scalar = self.process_value(value)\n        x, y = [np.log(self.vmin), np.log(self.midpoint), np.log(self.vmax)], [0, 0.5, 1]\n        return np.ma.array(np.interp(np.log(value), x, y), mask=result.mask, copy=False)\n\n\nim2 = ax2.pcolormesh(x,y,z, cmap=\"RdBu_r\", \n                            norm=MidPointLogNorm(vmin=z.min(), vmax=z.max(), midpoint=1))\nfig.colorbar(im2, ax=ax2)\nax2.set_title(\"MidPointLogNorm\")\nplt.show()\n\nAPI:\nmatplotlib.colors.Normalize\nmatplotlib.colors.LogNorm\n","label":[[212,229,"Mention"],[513,520,"Mention"],[3628,3655,"API"],[3656,3681,"API"]],"Comments":[]}
{"id":59477,"text":"ID:48649386\nPost:\nText: Many thanks to @ImportanceOfBeingErnest who deserves credit for basically everything that is correct in this post :) \nText: Here is a slightly modified, runnable version of your code (without Glade) which uses FuncAnimation inside a GTK app. \nText: Three things to note: \nText: A reference to the FuncAnimation object must be kept lest the object (and its timer) be garbage collected. The FigureCanvas should be created before the FuncAnimation, since FuncAnimation creates a timer by calling fig.canvas.new_timer(). If the canvas has not yet been created, fig.canvas is None and you get an AttributeError. If Gtk is not your default backend, use Figur instead of plt.figure here. \nCode: import numpy as np\nfrom gi.repository import Gtk\nfrom gi.repository import GObject\nfrom matplotlib.figure import Figure\nfrom matplotlib.backends.backend_gtk3cairo import FigureCanvasGTK3Cairo as FigureCanvas\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nclass MyApp(object):\n    def __init__(self):\n        window = Gtk.Window()\n        window.connect(\"delete-event\", Gtk.main_quit)\n        window.set_default_size(400, 400)\n        sw = Gtk.ScrolledWindow()\n        window.add(sw)\n\n        self.fig = fig = Figure()\n        self.canvas = FigureCanvas(fig)\n        sw.add_with_viewport(self.canvas)\n\n        ax = fig.add_subplot(111)\n        data = np.zeros(100)\n        self.line, = ax.plot(data, 'r-')\n        ax.set_ylim(-1, 1)\n        self.ani = animation.FuncAnimation(\n            self.fig, self.update_line, interval=100, frames=50, repeat=True)\n\n        window.show_all()\n\n    def update_line(self, *args):\n        data = 2*(np.random.random(100)-0.5)\n        self.line.set_ydata(data)\n        self.canvas.draw()\n        return True\n\nif __name__ == \"__main__\":\n    app = MyApp()\n    Gtk.main()\n\nAPI:\nmatplotlib.figure.Figure\n","label":[[671,676,"Mention"],[1848,1872,"API"]],"Comments":[]}
{"id":59478,"text":"ID:48791644\nPost:\nText: The problem is that while pandas in general directly wraps the matplotlib plotting methods, this is not the case for plots with dates. As soon as dates are involved, pandas uses a totally different numerical representation of dates and hence also uses its own locators for the ticks. \nText: In case you want to use daqtes formatters or locators on plots created with pandas you may use the x_compat=True option in pandas plots. \nCode: df.plot(ax = ax, color = 'black', linewidth = 0.4, x_compat=True)\n\nText: This allows to use the mpl.dates formatters or locators as shown below. Else you may replace df.plot(ax = ax, color = 'black', linewidth = 0.4) by \nCode: ax.plot(df.index, df.values, color = 'black', linewidth = 0.4)\n\nText: Complete example: \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nidx = pd.date_range('2017-01-01 05:03', '2017-01-01 18:03', freq = 'min')\ndf = pd.Series(np.random.randn(len(idx)),  index = idx)\n\nfig, ax = plt.subplots()\nhours = mdates.HourLocator(interval = 1)\nh_fmt = mdates.DateFormatter('%H:%M:%S')\n\nax.plot(df.index, df.values, color = 'black', linewidth = 0.4)\n#or use\ndf.plot(ax = ax, color = 'black', linewidth = 0.4, x_compat=True)\n#Then tick and format with matplotlib:\nax.xaxis.set_major_locator(hours)\nax.xaxis.set_major_formatter(h_fmt)\n\nfig.autofmt_xdate()\nplt.show()\n\nText: If the motivation to use pandas here is (as stated in the comments below) to be able to use \nText: secondary_y \nText: , the equivalent for matplotlib plots would be a twin axes \nText: twinx \nText: . \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nidx = pd.date_range('2017-01-01 05:03', '2017-01-01 18:03', freq = 'min')\n\ndf = pd.DataFrame(np.cumsum(np.random.randn(len(idx), 2),0), \n                  index = idx, columns=list(\"AB\"))\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"A\"], color = 'black')\nax2 = ax.twinx()\nax2.plot(df.index, df[\"B\"], color = 'indigo')\n\nhours = mdates.HourLocator(interval = 1)\nh_fmt = mdates.DateFormatter('%H:%M:%S')\nax.xaxis.set_major_locator(hours)\nax.xaxis.set_major_formatter(h_fmt)\n\nfig.autofmt_xdate()\nplt.show()\n\nAPI:\nmatplotlib.dates\nmatplotlib.dates\n","label":[[339,345,"Mention"],[555,564,"Mention"],[2235,2251,"API"],[2252,2268,"API"]],"Comments":[]}
{"id":59479,"text":"ID:48958334\nPost:\nText: By default the complete axes of a pie plot is \"off\". You can set it on, use the frame argument. \nCode: ax.pie(..., frame=True)\n\nText: This produces ticks and ticklabels on the axes, hence, it might be better to set it on externally, \nCode: ax.pie(..., frame=False)\nax.set_frame_on(True)\n\nText: In addition you probably want to set the spines off, \nCode: for _, spine in ax.spines.items():\n    spine.set_visible(False)\n\nText: or, in a single line \nCode: plt.setp(ax.spines.values(),visible=False)\n\nText: Finally, for the ticklabels not to exceed the axes area, one may fix the axis range, e.g. ax.axis([-1,1,-1,1]) and use a smaller pie radius, e.g. radius=.27. \nText: Complete code for reproduction \nText: import pandas as pd import numpy as np import pyplot as plt n = 3 nums_df = pd.DataFrame([np.random.randint(1, 20, size=5) for _ in xrange(n)]) row_labels = [\"row {:d}\".format(i) for i in xrange(n)] nums_df.index = row_labels fig, axes = plt.subplots(1, n) for i, ax in enumerate(axes): ax.pie(nums_df.loc[row_labels[i]], labels=nums_df.loc[row_labels[i]], frame=False, radius=0.27) ax.set_frame_on(True) ax.axis(\"equal\") ax.axis([-1,1,-1,1]) plt.setp(ax.spines.values(),visible=False) if i%2 == 1: ax.set_facecolor('red') plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[776,782,"Mention"],[1270,1287,"API"]],"Comments":[]}
{"id":59480,"text":"ID:49068690\nPost:\nText: sns.jointplot returns a JointGrid object, which gives you access to the matplotlib axes and you can then manipulate from there. \nCode: import seaborn as sns\nimport numpy as np\n\n# example data\nX = np.random.randn(1000,)\nY = 0.2 * np.random.randn(1000) + 0.5\n\nh = sns.jointplot(X, Y)\n\n# JointGrid has a convenience function\nh.set_axis_labels('x', 'y', fontsize=16)\n\n# or set labels via the axes objects\nh.ax_joint.set_xlabel('new x label', fontweight='bold')\n\n# also possible to manipulate the histogram plots this way, e.g.\nh.ax_marg_y.grid('on') # with ugly consequences...\n\n# labels appear outside of plot area, so auto-adjust\nh.figure.tight_layout() \n\nText: (The problem with your attempt is that functions such as plt.xlabel(\"text\") operate on the current axis, which is not the central one in sns.jointplot; but the object-oriented interface is more specific as to what it will operate on). \nText: Note that the last command uses the figure attribute of the JointGrid. The initial version of this answer used the simpler - but not object-oriented - approach via the pyplot interface. To use the pyplot interface: \nCode: import matplotlib.pyplot as plt\nplt.tight_layout()\n\nAPI:\nmatplotlib.pyplot\n","label":[[1094,1100,"Mention"],[1205,1222,"API"]],"Comments":[]}
{"id":59481,"text":"ID:49090521\nPost:\nText: Matplotlib can as of the current version not handle the categorical pandas datatype. \nText: Options you have: \nText: use strings (as pointed out in the question) This solution will work in matplotlib 2.2 or higher. import numpy as np import pandas as pd import mpl.pyplot as plt df = pd.DataFrame({\"x\" : np.logspace(0,11,12, base=2).astype(int), \"y\" : np.random.randint(900,1200,12)}) plt.plot(df.x.astype(str),df.y) plt.show() plot the data index and set the ticklabels according to the values. import numpy as np import pandas as pd import pyplot as plt df = pd.DataFrame({\"x\" : np.logspace(0,11,12, base=2).astype(int), \"y\" : np.random.randint(900,1200,12)}) plt.plot(df.index,df.y) plt.xticks(df.index, df.x) plt.show() \nText: In both cases the plot will look like \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[285,295,"Mention"],[566,572,"Mention"],[799,816,"API"],[817,834,"API"]],"Comments":[]}
{"id":59482,"text":"ID:49184580\nPost:\nText: You are looking for xticks, Matplotlib automagicly picks the xticks that will be displayed to keep things simple and nice looking. This works fine when you are dealing with time series or numeric x axis's. But not so well in your case. \nText: What you need to do is find out what the co-ordinates at the begining and end of the plot are and use those numbers to manually position your x-ticks. You can get this info by calling plt.xticks(). Which gives you a numpy array of (cordinates, xtick labels) \nCode: >>> plt.xticks()\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n         17, 18, 19, 20, 21]), <a list of 22 Text xticklabel objects>)\n\nText: Here is a link to the matplotlib docs. xticks \nText: Here is the code \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\nplt.ion() # turn on interactive plotting for ipython\n# I stuck your data in a list\ndata = [\n    ['jabber', 'gts', 1], ['jabber', 'aed', 6], ['jabber', 'ame', 2], ['jabber', 'asy', 8],\n    ['jabber', 'fxk', 1], ['jabber', 'jma', 6], ['jabber', 'oaw', 2], ['jabber', 'ejt', 8],\n    ['jabber', 'qat', 1], ['jabber', 'dzj', 6], ['jabber', 'yja', 2], ['jabber', 'ajz', 8],\n    ['jabber', 'jbp', 1], ['jabber', 'bvi', 6], ['jabber', 'pec', 2], ['jabber', 'lre', 8],\n    ['jabber', 'wlx', 1], ['jabber', 'hpw', 6], ['jabber', 'spg', 2], ['jabber', 'bdg', 8],\n    ['jabber', 'fgg', 1], ['jabber', 'fgz', 5], ['soshy', 'gts', 6], ['soshy', 'aed', 2],\n    ['soshy', 'ame', 8], ['soshy', 'asy', 1], ['soshy', 'fxk', 6], ['soshy', 'jma', 2],\n    ['soshy', 'oaw', 8], ['soshy', 'ejt', 1], ['soshy', 'qat', 6], ['soshy', 'dzj', 2],\n    ['soshy', 'yja', 8], ['soshy', 'ajz', 1], ['soshy', 'jbp', 6], ['soshy', 'bvi', 2],\n    ['soshy', 'pec', 8], ['soshy', 'lre', 1], ['soshy', 'wlx', 6], ['soshy', 'hpw', 2],\n    ['soshy', 'spg', 8], ['soshy', 'bdg', 1], ['soshy', 'fgg', 6], ['soshy', 'fgz', 2]]\n\ndf = pd.DataFrame(data, columns=[\"Category\", \"Line\", \"Amount\"])\nfig, ax = plt.subplots(1, 1)\ndf.groupby(\"Category\").plot(x=\"Line\", y=\"Amount\", ax=ax)\nplt.legend([v[0] for v in df.groupby('Category')['Category']], bbox_to_anchor=(1.1, 0.5))\n\n# get the values we want displayed as tick labels\ntick_labels = tuple(df['Line'])\n# get the positions for the maximum xtick label\nx_max = int(max(plt.xticks()[0]))  # int() to convert numpy.int32 => int\n# manually set you xtick labels\nplt.xticks(range(0, x_max + 1), tick_labels, rotation=45) \n\nplt.xlabel('Category')\nplt.ylabel('Amount')\n# change the limits and padding of the figure\nplt.figure(1).subplots_adjust(\n    **dict(left=0.1, right=.8, bottom=.15, top=.9, wspace=.1, hspace=.1))\nfor line in ax.lines:\n    line.set_linewidth(0.5)\n\nplt.plot()    # might need this without ipython\n\nAPI:\nmatplotlib.pyplot.xticks\n","label":[[742,748,"Mention"],[2751,2775,"API"]],"Comments":[]}
{"id":59483,"text":"ID:49233505\nPost:\nText: A pandas bar plot is a categorical plot. It shows one bar for each index at integer positions on the scale. Hence the first bar is at position 0, the next at 1 etc. The labels correspond to the dataframes' index. If you have 100 bars, you'll end up with 100 labels. This makes sense because pandas cannot know if those should be treated as categories or ordinal\/numeric data. \nText: If instead you use a normal matplotlib bar plot, it will treat the dataframe index numerically. This means the bars have their position according to the actual dates and labels are placed according to the automatic ticker. \nCode: import pandas as pd\nimport numpy as np; np.random.seed(42)\nimport matplotlib.pyplot as plt\n\ndatelist = pd.date_range(pd.datetime(2017, 1, 1).strftime('%Y-%m-%d'), periods=42).tolist()\ndf = pd.DataFrame(np.cumsum(np.random.randn(42)), \n                  columns=['error'], index=pd.to_datetime(datelist))\n\nplt.bar(df.index, df[\"error\"].values)\nplt.gcf().autofmt_xdate()\nplt.show()\n\nText: The advantage is then in addition that mdates locators and formatters can be used. E.g. to label each first and fifteenth of a month with a custom format, \nCode: import pandas as pd\nimport numpy as np; np.random.seed(42)\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndatelist = pd.date_range(pd.datetime(2017, 1, 1).strftime('%Y-%m-%d'), periods=93).tolist()\ndf = pd.DataFrame(np.cumsum(np.random.randn(93)), \n                  columns=['error'], index=pd.to_datetime(datelist))\n\nplt.bar(df.index, df[\"error\"].values)\nplt.gca().xaxis.set_major_locator(mdates.DayLocator((1,15)))\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%d %b %Y\"))\nplt.gcf().autofmt_xdate()\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[1063,1069,"Mention"],[1737,1753,"API"]],"Comments":[]}
{"id":59484,"text":"ID:49297173\nPost:\nText: Your x axis data are strings. Hence you will get one tick per data point. This is probably not what you want. Instead use the dates to plot. Because you are using pandas, this is easily converted, \nCode: dates = pd.to_datetime(ts_index, format=\"%Y%m%d\")\n\nText: You may then get rid of your manual xtick locating and formatting, because matplotlib will automatically choose some nice tick locations for you. \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nts_index = pd.period_range(start=\"20060429\", periods=1000).strftime(\"%Y%m%d\")\ndates = pd.to_datetime(ts_index, format=\"%Y%m%d\")\n\nfig, ax = plt.subplots()\n\nfor i in range(5):\n    plt.plot(dates, 1 + i * 0.01 * np.arange(0, 1000), label=\"group %d\"%i)\n\nplt.legend(loc='best')\nplt.title(r'net value curves')\n\nplt.xticks(rotation=\"vertical\")\nplt.xlabel(r'date')\nplt.ylabel('net value')\nplt.grid(True)\nplt.show()\n\nText: However in case you do want to have some manual control over the locations and formats you may use dates locators and formatters. \nCode: # tick every 3 months\nplt.gca().xaxis.set_major_locator(mdates.MonthLocator((1,4,7,10)))\n# format as \"%Y%m%d\"\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y%m%d\"))\n\nAPI:\nmatplotlib.dates\n","label":[[1023,1028,"Mention"],[1245,1261,"API"]],"Comments":[]}
{"id":59485,"text":"ID:49308491\nPost:\nText: To permute over the pairs of variables you can use the itertools.combinations() on the variable names (or indices if you prefer). \nCode: >>>list(itertools.combinations(brute.var_names, 2))\n[('a', 'b'), ('a', 'c'), ('a', 'd'), ('b', 'c'), ('b', 'd'), ('c', 'd')]\n\nText: This will give you the x, y variable pairs for each plot. \nText: You will also need to specify the plt.subplot position index for each plot \nText: i.e. for 4 parameters the lower diagonal of the 3x3 grid with positions. \nCode: (1) (2) (3)\n(4) (5) (6)\n(7) (8) (9)\n\nText: To match the order of the combinations() output above you need the numbers in the order [1, 4, 7, 5, 8, 9] \nText: You can get this with with something like this... \nCode: def get_tril_positions(n):\n    \"\"\"Lower square triangular position\"\"\"\n    tril_pos = np.tril((np.arange(n**2) + 1).reshape(n, -1)).T.ravel()\n    return tril_pos[tril_pos != 0]\n\nText: Where n is the number of parameters minus 1. \nText: Assuming that for the x-y parameters you would like the minimum residual values (letting all other parameters vary) then you can collapse the brute_grid and the brute_Jout along the other axis using np.amin(). \nText: Now with the N-d array collapsed into a 2-d array you can plot it \"normally\" like a 2-d array. \nText: Combing the above together I get something like. from itertools import combinations n = len(brute.var_names) - 1 combos = list(combinations(brute.var_names, 2)) positions = get_tril_positions(n) \nCode: for (xname, yname), pos in zip(combos, positions):\n    # Specify subplot \n    ax = plt.subplot(n, n, pos)\n\n    # Find index for these variables\n    xi = brute.var_names.index(xname)\n    yi = brute.var_names.index(yname)\n\n    # get the meshgrids for x and y\n    X = brute.brute_grid[xi]\n    Y = brute.brute_grid[yi]\n\n    # Find other axis to collapse. \n    axes = tuple([ii for ii in range(brute.brute_Jout.ndim) if ii not in (xi, yi)])\n\n    # Collapse to minimum Jout\n    min_jout = np.amin(brute.brute_Jout, axis=axes)\n    min_xgrid = np.amin(X, axis=axes)\n    min_ygrid = np.amin(Y, axis=axes)\n\n    ax.pcolormesh(min_xgrid, min_ygrid, min_jout)\n\n    # Add colorbar to each plot\n    plt.colorbar()\n\n    # Add labels to edge only \n        if pos >= n**2 - n:\n        plt.xlabel(xname)\n    if pos % n == 1:\n        plt.ylabel(yname)\n\nplt.tight_layout()\nplt.show()\n\nText: Which produces what you want. \nText: Corner plot \nText: Note, I did not multiply brute_Jout by -1 so you may need to us np.amax instead if you are using your value. \nAPI:\nmatplotlib.pyplot.subplot\n","label":[[392,403,"Mention"],[2531,2556,"API"]],"Comments":[]}
{"id":59486,"text":"ID:49311584\nPost:\nText: I think you found the reason yourself: Pandas datetime representation for the matplotlib axes (may) be completely different from the matplotlib date units (this is not always the case and depends on the span of the data). \nText: Since I don't know of any way to convert the rectangle's coordinates to the pandas units, the only option is to plot the pandas plot in matplotlib units. \nText: The problem \nText: But let's start at the beginning. Case 1 and 2 work fine for me. \nText: For the third case, the rectangle is added to the other axes, which does have a different scale. This can be seen by printing the transform. \nCode: def add_rectangle(ax, x, y, width, height, **kwargs):\n    rect = mpatches.Rectangle( (x, y), width, height, **kwargs )\n    ax.add_patch(rect)\n    return rect\n\n# Case 1 - working\nfig, ax = plt.subplots()\nts.plot(ax=ax)\nr = add_rectangle(ax, *args, **kwargs)\nprint r.get_transform()\n\n# This prints\n# BboxTransformTo(\n#        Bbox(x0=17565.0, y0=-1.0, x1=17598.0, y1=1.0)),\n\n# Case 3 - non-working\nfig, axes = plt.subplots(2, sharex=True)\nts.plot(ax=axes[1], x_compat=True)\nr = add_rectangle(axes[0], *args, **kwargs)\nprint r.get_transform()\n\n# BboxTransformTo(\n#        Bbox(x0=736728.0, y0=-1.0, x1=736761.0, y1=1.0)),\n\nText: In the second case, the units are the matplotlib date units, because pandas did not change the transform for the axes in which it did not plot anything. \nText: The solution \nText: The easiest option is probably to tell pandas not to change the scale. This would be done using \nCode: x_compat=True\n\nText: This has essentially the same effect as plotting everything in matplotlib units. \nCode: # Plot 3: 2 subplots with ts plotted first\nfig, axes = plt.subplots(2, sharex=True)\nts.plot(ax=axes[1], x_compat=True)\nr = add_rectangle(axes[0], *args, **kwargs)\n\n# Plot 4: 2 subplots with ts plotted second\nfig, axes = plt.subplots(2, sharex=True)\nadd_rectangle(axes[0], *args, **kwargs)\nts.plot(ax=axes[1], x_compat=True)\n\nText: So indeed the nice pandas formatting is gone. But you may replicate it with the dates formatters. E.g. in this post. an easy solution to add the days is presented. Here, you would maybe rather use a FuncFormatter as follows: \nCode: fig, axes = plt.subplots(2, sharex=True)\nts.plot(ax=axes[1], x_compat=True)\nr = add_rectangle(axes[0], *args, **kwargs)\n\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as mticker\n\ndef f(val, _):\n    d = mdates.num2date(val)\n    if d.month == 1:\n        return d.strftime(\"%b\\n%Y\")\n    else:\n        return d.strftime(\"%b\")\n\naxes[1].xaxis.set_major_locator(mdates.MonthLocator())\naxes[1].xaxis.set_minor_locator(mdates.WeekdayLocator())\naxes[1].xaxis.set_major_formatter(mticker.FuncFormatter(f))\nfig.autofmt_xdate(rotation=0,ha=\"center\")\n\nText: producing \nAPI:\nmatplotlib.dates\n","label":[[2082,2087,"Mention"],[2807,2823,"API"]],"Comments":[]}
{"id":59487,"text":"ID:49474749\nPost:\nText: To get rid of the error you may convert the dates as follows and also set the labels accordingly: \nCode: plt.xticks(dataset['DATE'].tolist(),dataset['DATE'].tolist())\n\nText: or as has been mentionned in the comments \nCode: plt.xticks(dataset['DATE'].dt.to_pydatetime(),dataset['DATE'].dt.to_pydatetime()) \n\nText: But let's look at some more useful options. \nText: Plotting strings \nText: First of all it is possible to plot the data as it is, i.e. as strings. \nCode: import matplotlib.pyplot as plt\nimport pandas as pd\n\ndataset = pd.read_csv('dateunrate.txt')\nplt.plot(dataset['DATE'], dataset['UNRATE'])\n\nplt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\")\nplt.show()\n\nText: This is just like plotting plt.plot([\"apple\", \"banana\", \"cherry\"], [1,2,3]). This means that the successive dates are just placed one-by-one on the axes, independent on whether they are a minute, a day or a year appart. E.g. if your dates were 2018-01-01, 2018-01-03, 2018-01-27 they would still appear equally spaced on the axes. \nText: Plot dates with pandas (automatically) \nText: Pandas can nicely plot dates out of the box if the dates are in the index of the dataframe. To this end you may read the dataframe in a way that the first csv column is parsed as the index. \nCode: import matplotlib.pyplot as plt\nimport pandas as pd\n\ndataset = pd.read_csv('dateunrate.txt', parse_dates=[0], index_col=0)\ndataset.plot()\n\nplt.show() \n\nText: This is equivalent to \nCode: dataset = pd.read_csv('..\/dateunrate.txt', parse_dates=[0])\ndataset = dataset.set_index(\"DATE\")\ndataset.plot()\n\nText: or \nCode: dataset = pd.read_csv('..\/dateunrate.txt')\ndataset[\"DATE\"] = pd.to_datetime(dataset[\"DATE\"])\ndataset = dataset.set_index(\"DATE\")\ndataset.plot()\n\nText: or even \nCode: dataset = pd.read_csv('..\/dateunrate.txt')\ndataset[\"DATE\"] = pd.to_datetime(dataset[\"DATE\"])\ndataset.plot(x=\"DATE\",y=\"UNRATE\")\n\nText: This works nice in this case because you happen to have one date per month and pandas will decide to show all 12 months as ticklabels in this case. For other cases this may result in different tick locations. \nText: Plot dates with matplotlib or pandas (manually) \nText: In the general case, you may use mpl.dates formatters and locators to tweak the tick(label)s in the way you want. Here, we might use a MonthLocator and set the ticklabel format to \"%b %Y\". This works well with matplotlib plot or pandas plot(x_compat=True). \nCode: import matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.dates as mdates\n\ndataset = pd.read_csv('dateunrate.txt', parse_dates=[0], index_col=0)\n\nplt.plot(dataset.index, dataset['UNRATE'])\n## or use \n#dataset.plot(x_compat=True) #note the x_compat argument\n\nplt.gca().xaxis.set_major_locator(mdates.MonthLocator())\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n\nplt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\")\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[2211,2220,"Mention"],[2917,2933,"API"]],"Comments":[]}
{"id":59488,"text":"ID:49498147\nPost:\nText: X is a numpy array, type(X) == numpy.ndarray. The error tells you that numpy arrays do not have a show method; which is expected, since it wouldn't be clear how to show them anyways. \nText: For visualization you use matplotlib.pyplot. \nText: If pyplot is imported like import plt as plt you can show all plots created with pyplot via \nCode: plt.show()\n\nText: Inside interactive sessions you may also show a figure. In this case, \nCode: fig2.show()\n\nAPI:\nmatplotlib.pyplot\n","label":[[300,303,"Mention"],[478,495,"API"]],"Comments":[]}
{"id":59489,"text":"ID:49520642\nPost:\nText: Apparently you cannot call a Normailze with a dataframe any more in matplotlib 2.2. \nText: The solution would be to call it with the dataframe's values instead, changing normed = s.apply(norm) to \nCode:  normed = s.apply(lambda x: norm(x.values))\n\nText: Full code \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\ndef background_gradient(s, m=None, M=None, cmap='PuBu', low=0, high=0):\n    if m is None:\n        m = s.min().min()\n    if M is None:\n        M = s.max().max()\n    rng = M - m\n    norm = colors.Normalize(m ,M)\n    normed = s.apply(lambda x: norm(x.values))\n    cm = plt.cm.get_cmap(cmap)\n    c = normed.applymap(lambda x: colors.rgb2hex(cm(x)))\n    ret = c.applymap(lambda x: 'background-color: %s' % x)\n    return ret\n\ndf = pd.DataFrame([[3,2,10.3,4],[20,1,3.5,2],[5,4,6.9,1]])\ndf.style.apply(background_gradient, axis=None)\n\nText: producing \nAPI:\nmatplotlib.colors.Normalize\n","label":[[53,62,"Mention"],[927,954,"API"]],"Comments":[]}
{"id":59490,"text":"ID:49536325\nPost:\nText: It is unclear what ax.set_aspect('equal') would do here. So I'd remove that. Next, what you need to do is to have the axis coordinate system synchronized to the figure size. \nText: At this point it's unclear from the question what exactly you are trying to do. \nText: Create a 3 inch figure with a 3 inch long line Let the coordinate system go from 0 to 1, remove any margin inside the axes. import pyplot as plt f,ax = plt.subplots(figsize=(3,3)) plt.plot([0,1],[0,0]) ax.axis('off') plt.gca().set_position([0, 0, 1, 1]) ax.margins(0) plt.savefig(\"line.svg\") or let the coordinate system go from 0 to 3, make the line 3 units long. import pyplot as plt f,ax = plt.subplots(figsize=(3,3)) plt.plot([0,3],[0,0]) ax.axis('off') plt.gca().set_position([0, 0, 1, 1]) ax.set_xlim(0,3) plt.savefig(\"line.svg\") Create a 3 inch figure with a 1 inch long line Let the coordinate system go from 0 to 1, create a line of 1\/3 units in length import pyplot as plt f,ax = plt.subplots(figsize=(3,3)) plt.plot([0,1\/.3],[0,0]) ax.axis('off') plt.gca().set_position([0, 0, 1, 1]) ax.set_xlim(0,1) plt.savefig(\"line.svg\") or let the coordinate system go from 0 to 3 and create a line of 1 unit length. import plt as plt f,ax = plt.subplots(figsize=(3,3)) plt.plot([0,1],[0,0]) ax.axis('off') plt.gca().set_position([0, 0, 1, 1]) ax.set_xlim(0,3) plt.savefig(\"line.svg\") This last approach seems to be the most intuitive one, but since it's not too clear from the question how the size in inches of the figure should relate to the axes units, I provided all possible solutions. \nAPI:\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\nmatplotlib.pyplot\n","label":[[423,429,"Mention"],[664,670,"Mention"],[961,967,"Mention"],[1215,1218,"Mention"],[1589,1606,"API"],[1607,1624,"API"],[1625,1642,"API"],[1643,1660,"API"]],"Comments":[]}
{"id":59491,"text":"ID:49542903\nPost:\nText: First of all, if you want to use dates locators and formatters on a plot created via pandas you should use the x_compat=True argument in your plot, otherwise pandas may scale the axis rather arbitrarily. \nText: Then '%D' is not a valid format string. Maybe you mean '%b'? \nText: Now there are two options. \nText: Use sharex=False, set your locators and formatters to both axes, and finally set the limits of the one plot to the limits of the other. In this case since the lower plot comprises a larger range, ax1.set_xlim(ax2.get_xlim()) The other option is to use sharex=True and turn the labels visible again. plt.setp(ax1.get_xticklabels(), visible=True) Unfortunately this option is broken on the newest matplotlib version. I just opened a bug report about it. \nText: Full code for the first option (since the second one is not working): \nCode: from pandas import DataFrame, date_range, Timedelta\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport matplotlib.dates as mdates\n\n#Dataset 1\nrng1 = date_range(start='2015-01-01', periods=5, freq='1M')\ndf1 = DataFrame({'y':np.random.normal(size=len(rng1))}, index=rng1)\ny1 = df1['y']\n\n#Dataset 2\nrng2 = date_range(start='2015-01-01', periods=5, freq='2M')\ndf2 = DataFrame({'y':np.random.normal(size=len(rng2))}, index=rng2)\ny2 = df2['y']\n\n#Figure\nfig,(ax1,ax2) = plt.subplots(2,1,sharex=False)\n\ny1.plot(ax=ax1, x_compat=True)\ny2.plot(ax=ax2, x_compat=True)\n\nplt.xticks(rotation=30)\n\nax1.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nax1.xaxis.set_minor_locator(plt.NullLocator())\nax1.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n\nax2.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nax2.xaxis.set_minor_locator(plt.NullLocator())\nax2.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n\nax1.set_xlim(ax2.get_xlim())\n\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[57,62,"Mention"],[1849,1865,"API"]],"Comments":[]}
{"id":59492,"text":"ID:49696677\nPost:\nText: You can call the function that boxplot uses to calculate the data. \nText: I have always had good luck looking and searching through the Matplotlib source to find how it does stuff. boxplot uses the function boxplot_stats to do the calcs. You can call this function yourself and inspect the returned data: \nCode: from matplotlib import cbook\nprint(help(cbook.boxplot_stats))\ndata = cbook.boxplot_stats(values)\n\nText: From the boxplot_stats docstring: \nCode: Returns\n-------\nbxpstats : list of dict\n    A list of dictionaries containing the results for each column\n    of data. Keys of each dictionary are the following:\n\n    ========   ===================================\n    Key        Value Description\n    ========   ===================================\n    label      tick label for the boxplot\n    mean       arithemetic mean value\n    med        50th percentile\n    q1         first quartile (25th percentile)\n    q3         third quartile (75th percentile)\n    cilo       lower notch around the median\n    cihi       upper notch around the median\n    whislo     end of the lower whisker\n    whishi     end of the upper whisker\n    fliers     outliers\n    ========   ===================================\n\nText: Source links are for the current master on this date. Current Matplotlib version is 2.2.2 \nAPI:\nmatplotlib.cbook.boxplot_stats\n","label":[[231,244,"Mention"],[1334,1364,"API"]],"Comments":[]}
{"id":59493,"text":"ID:49710678\nPost:\nText: You can use mpl.axes.Axes.axhline of matplotlib which adds a horizontal line across the axis. If you need to set any further parameters, refer to the official documentation \nCode: import matplotlib.pyplot as plt    \nplt.axhline(100000, color=\"gray\")\n\nAPI:\nmatplotlib.axes.Axes.axhline\n","label":[[36,57,"Mention"],[280,308,"API"]],"Comments":[]}
{"id":59494,"text":"ID:49712644\nPost:\nText: Just as the comment of @Goyo said, you need to import the mpl.pyplot where you call plot() method, and you can savefig there. It seems like that without return it is still possible, so I guess the info is shared between \"inside\" and \"outside\", i.e., they are \"static\" saying in the Java way. \nText: At last I have to modify the source code, forcing it to return plt when server is True, then I call plot(type=\"roc\", server=True) from outside. I think it has no impact because before the change, by default it returns None. \nCode: if not server: \n    plt.show()\nelse:\n    return plt # return to use plt.savefig\n\nAPI:\nmatplotlib.pyplot\n","label":[[82,92,"Mention"],[640,657,"API"]],"Comments":[]}
{"id":59495,"text":"ID:49777903\nPost:\nText: The Rectangle allows the keyword argument zorder that is by default 1.0. \nText: Choosing a zorder above one should bring your rectangle to the foreground of the image. \nCode: ax.add_patch(\npatches.Rectangle(\n    (776820, 5000),   # (x,y)\n    3000,          # width\n    3500,          # height\n    fill=False,\n    zorder=2\n    )\n)\n\nAPI:\nmatplotlib.patches.Rectangle\n","label":[[28,37,"Mention"],[360,388,"API"]],"Comments":[]}
{"id":59496,"text":"ID:49820552\nPost:\nText: This seems to be a bug in pandas. To get the same as with matplotlib you may always use x_compat=True. This then would also allow to use mdates formatters and locators. \nCode: s = pandas.Series(...)\ns.plot(x_compat=True)\n\nAPI:\nmatplotlib.dates\n","label":[[161,167,"Mention"],[251,267,"API"]],"Comments":[]}
{"id":59497,"text":"ID:49879479\nPost:\nText: A more recent answer to the related question illustrates using the MultipleLocator object. The axis ticks are this type of matplotlib object. Here is an example of it's use. \nCode: ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(5))\n\nText: will place ticks 5 units apart on the x-axis, and \nCode: ax.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(1))\n\nText: will place minor ticks 1 unit apart on the x-axis. \nText: Here is an example from the matplotlib Plotting Cookbook \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nX = np.linspace(-15, 15, 1024)\nY = np.sinc(X)\n\nax = plt.axes()\nax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(5))\nax.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(1))\n\nplt.plot(X, Y, c = 'k')\nplt.show()\n\nAPI:\nmatplotlib.ticker.MultipleLocator\n","label":[[91,106,"Mention"],[833,866,"API"]],"Comments":[]}
{"id":59498,"text":"ID:50004818\nPost:\nText: I eventually arrived at this solution after helpful comments from @apogalacticon. \nCode: import matplotlib.pyplot as plt\nfrom matplotlib.collections import PathCollection\nimport networkx as nx\n\ndef createDiGraph():\n    G = nx.DiGraph()\n\n    # Add nodes:\n    nodes = ['A', 'B', 'C', 'D', 'E']\n    G.add_nodes_from(nodes)\n\n    # Add edges or links between the nodes:\n    edges = [('A','B'), ('B','C'), ('B', 'D'), ('D', 'E')]\n    G.add_edges_from(edges)\n    return G\n\nG = createDiGraph()\n\n# Get a layout for the nodes according to some algorithm.\npos = nx.layout.spring_layout(G, random_state=779)\n\nnode_size = 300\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color=(0,0,0.9),\n                                edgecolors='black')\n# nodes is a matplotlib.collections.PathCollection object\nnodes.set_picker(5)\n\nnx.draw_networkx_edges(G, pos, node_size=node_size, arrowstyle='->',\n                                arrowsize=15, edge_color='black', width=1)\n\nnx.draw_networkx_labels(G, pos, font_color='red', font_family='arial',\n                                font_size=10)\n\ndef onpick(event):\n\n    if isinstance(event.artist, PathCollection):\n        all_nodes = event.artist\n        ind = event.ind[0] # event.ind is a single element array.\n        this_node_name = pos.keys()[ind]\n        print(this_node_name)\n\n        # Set the colours for all the nodes, highlighting the picked node with\n        # a different colour:\n        colors = [(0, 0, 0.9)] * len(pos)\n        colors[ind]=(0, 0.9, 0)\n        all_nodes.set_facecolors(colors)\n\n        # Update the plot to show the change:\n        fig.canvas.draw() # plt.draw() also works.\n        #fig.canvas.flush_events() # Not required? See https:\/\/stackoverflow.com\/a\/4098938\/1843329\n\nfig = plt.gcf()\n\n# Bind our onpick() function to pick events:\nfig.canvas.mpl_connect('pick_event', onpick)\n\n# Hide the axes:\nplt.gca().set_axis_off()\nplt.show()\n\nText: When you click on a node its face colour changes to green whilst the other nodes are set to blue: \nText: Unfortunately it seems the nodes in the network are represented by a single martist.Artist object, which happens to be a collection of paths without any artist children. This means you can't get hold of a single node as such to alter its properties. Instead you're forced to update all the nodes, just making sure the properties for the picked node - colour in this case - are different to the others. \nAPI:\nmatplotlib.artist.Artist\n","label":[[2124,2138,"Mention"],[2456,2480,"API"]],"Comments":[]}
{"id":59499,"text":"ID:50060135\nPost:\nText: I think that you should use the datetime library. You can read your dates using this command date=datetime.strptime('17:21:55','%H:%M:%S') but you have to use the Julian date as a reference by setting a date0=datetime(1970, 1, 1) You can also use the starting point of your time series as a date0 and then set your date as date=datetime.strptime('01-01-2000 17:21:55','%d-%m-%Y %H%H:%M:%S'). Compute the differences between your actual date and the reference date IN SECONDS (there are several functions to do this) for each line in your file using a loop and affect this difference to a list element (We will call this list Diff_list). At the end use T_plot= [dtm.datetime.utcfromtimestamp(i) for i in Diff_List]. Finally a plt.plot(T_plot,values) will allow you to visualize the dates on the x-axis. \nText: You can also use the pandas library \nText: first, define your date parsing depending on the dates type in your file parser=pd.datetime.strptime(date, '%Y-%m-%d %H:%M:%S') \nText: Then you read your file \nText: tmp = pd.read_csv(your_file, parse_dates={'datetime': ['date', 'time']}, date_parser=parser, comment='#',delim_whitespace=True,names=['date', 'time', 'Values']) \nText: data = tmp.set_index(tmp['datetime']).drop('datetime', axis=1) \nText: You can adapt these lines if you need to represent only hours HH:MM:SS not the whole date. \nText: N.B: Indexing will not be from 0 to data.values.shape[0] but the dates will be used as indexes. So if you want to plot you can do a import yplot as plt and then plt.plot(data.index,data.Values) \nAPI:\nmatplotlib.pyplot\n","label":[[1517,1522,"Mention"],[1578,1595,"API"]],"Comments":[]}
{"id":59500,"text":"ID:50287685\nPost:\nText: You can use the DayLocator and DateFormatter from mpl.dates \nCode: import matplotlib.pyplot as plt\n\nfrom datetime import date\nfrom nsepy import get_history\navenue_df=get_history(symbol='DMART',start=date(2018,5,6),end=date(2018,5,10))\n\navenue_df.Open.plot(color='green', label='DMART')\n\nshriram_df = get_history(symbol='SRTRANSFIN',start=date(2018,5,6),end=date(2018,5,10))\nshriram_df.Open.plot(color='red', label='SHRI')\n\ninfy_df = get_history(symbol='INFY',start=date(2018,5,6),end=date(2018,5,10))\nax = infy_df.Open.plot(color='blue', label='INFY')\n\n\n# Add a legend in the top left corner of the plot\nplt.legend(loc='upper left')\n\n# Display the plot\n\n#Format the xaxis date\nfrom matplotlib.dates import DateFormatter, DayLocator\n\nax.xaxis.set_major_locator(DayLocator())\nax.xaxis.set_major_formatter(DateFormatter('%Y\/%m\/%d'))\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[74,83,"Mention"],[871,887,"API"]],"Comments":[]}
{"id":59501,"text":"ID:50302044\nPost:\nText: If the vmin and vmax parameters of imshow are left unspecified, imshow sets them to be \nCode: vmin = array.min()   # in this case, vmin=1\nvmax = array.max()   # in this case, vmax=1\n\nText: It then normalizes the array values to fall between 0 and 1, using mpl.colors.Normalize by default. \nCode: In [99]: norm = mcolors.Normalize(vmin=1, vmax=1)\n\nIn [100]: norm(1)\nOut[100]: 0.0\n\nText: Thus each point in array is mapped to the color associated with 0.0: \nCode: In [101]: plt.cm.binary(0)\nOut[101]: (1.0, 1.0, 1.0, 1.0)  # white\n\nText: Usually array will contain a variety of values and matplotlib's normalization will just \"do the right thing\" for you automatically. However, in these corner cases where array consists of only one value, you may need to set vmin and vmax explicitly: \nCode: import matplotlib.pyplot    as pyplot\nimport numpy                as np\n\narray = np.ones([10, 10])\nfig, ax = pyplot.subplots(figsize=(10, 5))\nax.imshow(array, cmap=pyplot.cm.binary, vmin=0, vmax=1)\npyplot.show()\n\nAPI:\nmatplotlib.colors.Normalize\n","label":[[280,300,"Mention"],[1034,1061,"API"]],"Comments":[]}
{"id":59502,"text":"ID:50303322\nPost:\nText: By default, log scale axis will only label ticks only at integer powers of base, i.e. major ticks. In order to show the labeling of ticks that are not at integer powers of base (potentially minor ticks), you will need to use one of the LogFormatter to setup proper minor_thresholds as documented: \nText: In some cases such as the colorbar, there is no distinction between major and minor ticks; the tick locations might be set manually, or by a locator that puts ticks at integer powers of base and at intermediate locations. For this situation, disable the minor_thresholds logic by using minor_thresholds=(np.inf, np.inf), so that all ticks will be labeled. \nText: You might also want to set minor ticks so that their labels will not overlap. Here is my example: \nCode: import matplotlib.ticker as ticker\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.DataFrame({'width': ['a', 'b', 'c', 'a', 'b', 'c'],\n                   'cycles': [2e5, 3e6, 4e7, 7e5, 8e6, 9e7],\n                   'implementation': ['A', 'A', 'A', 'B', 'B', 'B']})\nax = sns.barplot(x=\"width\", y=\"cycles\", hue=\"implementation\", data=df)\nax.set_yscale('log')\nax.yaxis.set_minor_locator(ticker.LogLocator(subs=[2,3,5,7]))\nax.yaxis.set_minor_formatter(ticker.LogFormatterSciNotation(minor_thresholds=(np.inf, np.inf)))\n\nAPI:\nmatplotlib.ticker.LogFormatter\n","label":[[260,272,"Mention"],[1340,1370,"API"]],"Comments":[]}
{"id":59503,"text":"ID:50331962\nPost:\nText: You should use the label keyword, and the legend() function. \nText: Some comments: \nText: label is a keyword that describes what string will appear when, at the end, you call the legend() function. Dashes are not a very good way to represent what you want, since they are at \"graph units\" (pixels), and you most probably want \"data units\" (time). Try to zoom the graph and you'll see that the dashes don't zoom accordingly. \nText: import pyplot as plt x_axis_data1 = [1,2,3,4,5,6,7,8] x_axis_data2 = [1,2,3,4,5,6,7,8] x_axis_data3 = [1,2,3,4,5,6,7,8] alive_for_time = 6 sleep_for_time = 2 plt.plot(x_axis_data1, [1] * len(x_axis_data1), dashes = [alive_for_time, sleep_for_time], label=\"process1\") plt.plot(x_axis_data2, [2] * len(x_axis_data2), dashes = [alive_for_time, sleep_for_time], label=\"process2\") plt.plot(x_axis_data2, [3] * len(x_axis_data3), dashes = [alive_for_time, sleep_for_time], label=\"process3\") plt.legend() plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[462,468,"Mention"],[970,987,"API"]],"Comments":[]}
{"id":59504,"text":"ID:50351357\nPost:\nText: You can set the minor ticks so that only 1 minor tick appears inbetween your major ticks. This is done using matplotlib.ticker.AutoMinorLocator. Then, set the gridlines to only appear at the minor ticks. You also need to shift your xtick positions by 0.5: \nCode: from matplotlib.ticker import AutoMinorLocator\n\nnp.random.seed(10)\n\nx = range(10)\ny = np.random.random(10)\nplt.plot(x,y)\nplt.xticks(np.arange(0.5,10.5,1), x)\nplt.xlim(0,9.5)\nplt.ylim(0,1)\nminor_locator = AutoMinorLocator(2)\nplt.gca().xaxis.set_minor_locator(minor_locator)\nplt.grid(which='minor')\n\nplt.show()\n\nText: Edit: I'm having trouble getting two AutoMinorLocators to work on the same axis. When trying to add in another one for the y axis, the minor ticks get messed up. A work around I have found is to manually set the locations of the minor ticks using a FixedLocator and passing in the locations of the minor ticks. \nCode: from matplotlib.ticker import AutoMinorLocator\nfrom matplotlib.ticker import FixedLocator\nnp.random.seed(10)\n\nx = range(10)\ny = np.random.random(10)\nplt.plot(x,y)\nplt.xticks(np.arange(0.5,10.5,1), x)\nplt.yticks([0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95,1.05], [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1])\nplt.xlim(0,9.5)\nplt.ylim(0,1.05)\n\nminor_locator1 = AutoMinorLocator(2)\nminor_locator2 = FixedLocator([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1])\nplt.gca().xaxis.set_minor_locator(minor_locator1)\nplt.gca().yaxis.set_minor_locator(minor_locator2)\nplt.grid(which='minor')\n\nplt.show()\n\nAPI:\nmatplotlib.ticker.FixedLocator\n","label":[[852,864,"Mention"],[1519,1549,"API"]],"Comments":[]}
{"id":59505,"text":"ID:50438170\nPost:\nText: To add a point at custom x and y coordinates, add mpl.pyplot.scatter with your coordinates: \nCode: plt.scatter(x=3, y=0.5, color='r')\n\nText: And to color your last point, use the .iloc locator on your data: \nCode: plt.scatter(iris.petal_length.iloc[-1], iris.petal_width.iloc[-1], color='r')\n\nText: Note that the iloc locator is from pandas, and plt.scatter is from matplotlib.pyplot. Both of these are mandatory dependencies of seaborn, so you definitely have them on your machine if you're using seaborn. \nText: For example: \nCode: import seaborn as sns\nimport matplotlib.pyplot as plt\niris = sns.load_dataset(\"iris\")    \ngrid = sns.JointGrid(iris.petal_length, iris.petal_width, space=0, size=6, ratio=50)\ngrid.plot_joint(plt.scatter, color=\"g\")\n# add your point\nplt.scatter(x=3, y=0.5, color='r')\n# or\n# plt.scatter(iris.petal_length.iloc[-1], iris.petal_width.iloc[-1], color='r')\n\nAPI:\nmatplotlib.pyplot.scatter\n","label":[[74,92,"Mention"],[916,941,"API"]],"Comments":[]}
{"id":59506,"text":"ID:50463436\nPost:\nText: You can use mpl.pyplot.text objects in order to achieve the same result. Note that you need to make sure you use the figure coordinate system by using the transform=fig.transFigure argument and set the horizontal alignment using ha = \"center\" \nText: An example: \nCode: import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1,3)\nplt.text(x=0.5, y=0.94, s=\"My title 1\", fontsize=18, ha=\"center\", transform=fig.transFigure)\nplt.text(x=0.5, y=0.88, s= \"My title 2 in different size\", fontsize=12, ha=\"center\", transform=fig.transFigure)\n\nfor i, ax in enumerate(axes.flatten()):\n    ax.set_title(\"D{}\/E\".format(i))\n\nplt.subplots_adjust(top=0.8, wspace=0.3)\n\nplt.show()\n\nAPI:\nmatplotlib.pyplot.text\n","label":[[36,51,"Mention"],[698,720,"API"]],"Comments":[]}
{"id":59507,"text":"ID:50492239\nPost:\nText: In case you want to use the dates locators and formatters on a plot generated by pandas, you need to plot in compatibility mode, \nCode: df.plot(..., x_compat=True)\n\nAPI:\nmatplotlib.dates\n","label":[[52,57,"Mention"],[194,210,"API"]],"Comments":[]}
{"id":59508,"text":"ID:50561194\nPost:\nText: Don't use sns.plt. Instead import plt as plt and use it directly. \nText: Appart the code should run fine. I created the following minimal runnable example \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.DataFrame({\"label\" : np.random.randint(2, size=100).astype(str),\n                    \"data\" : np.random.rayleigh(size=100)})\n\nsns.set(style=\"darkgrid\")\nsns.distplot(df[df['label']=='0']['data'],color='green',label='Benign URLs')\nsns.distplot(df[df['label']=='1']['data'],color='red',label='Phishing URLs')\nplt.title('Url Length Distribution')\nplt.legend(loc='upper right')\nplt.xlabel('Length of URL')\n\nplt.show()\n\nText: which produces the following output \nText: If it does not work for you, consider upgrading your matplotlib and seaborn version. The above is produced with matplotlib 2.2.2 and seaborn 8.1. \nAPI:\nmatplotlib.pyplot\n","label":[[58,61,"Mention"],[903,920,"API"]],"Comments":[]}
{"id":59509,"text":"ID:50680477\nPost:\nText: As for the second question, you may use dates locators and formatters. Those work fine in the case of a hist. \nCode: import matplotlib.pyplot as plt\nplt.rcParams['axes.axisbelow'] = True\nimport matplotlib.dates as dates\nimport numpy as np; np.random.seed(42)\nimport pandas as pd\n\nobjDate = dates.num2date(np.random.normal(735700, 300, 700))\n\nser = pd.Series(objDate)\nax = ser.hist(cumulative=True, density=1, bins=500, histtype='step', linewidth=2)\n\nax.xaxis.set_major_locator(dates.MonthLocator([1,7]))\nax.xaxis.set_major_formatter(dates.DateFormatter(\"%m\/%y\"))\nplt.setp(ax.get_xticklabels(), rotation=60)\n\nplt.show()\n\nText: For the first question, this is not easy, because matplotlib always assumes the complete axis to be ticked. A solution would be to subclass the locator in use and allow it to take restrictive arguments. \nCode: from datetime import datetime\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.axisbelow'] = True\nimport matplotlib.dates as dates\nimport numpy as np; np.random.seed(42)\nimport pandas as pd\n\nobjDate = dates.num2date(np.random.normal(735700, 300, 700))\n\nser = pd.Series(objDate)\nax = ser.hist(cumulative=True, density=1, bins=500, histtype='step', linewidth=2)\n\n\nclass RestrictedLocator(dates.MonthLocator):\n    def __init__(self, dmin=None, dmax=None, **kw):\n        self.dmin = dmin\n        self.dmax = dmax\n        dates.MonthLocator.__init__(self, **kw)\n\n    def __call__(self):\n        try:\n            dmin, dmax = self.viewlim_to_dt()\n        except ValueError:\n            return []\n\n        self.dmin = self.dmin.replace(tzinfo=dmin.tzinfo)\n        self.dmax = self.dmax.replace(tzinfo=dmin.tzinfo)\n        dmin = np.max([dmin, self.dmin])\n        dmax = np.min([dmax, self.dmax])\n        return self.tick_values(dmin, dmax)\n\n\nloc = RestrictedLocator(dmin=datetime(2015,1,1), \n                        dmax = datetime(2017,12,31),\n                        bymonth=[1,7])\n\nax.xaxis.set_major_locator(loc)\nax.xaxis.set_major_formatter(dates.DateFormatter(\"%m\/%y\"))\nplt.setp(ax.get_xticklabels(), rotation=60)\n\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[64,69,"Mention"],[2093,2109,"API"]],"Comments":[]}
{"id":59510,"text":"ID:50773742\nPost:\nText: Not sure why you append 1 to your array in the first place. I guess you mean \nCode: # Creating data variables\nx = []\ny = []\nx.append(datetime.datetime.now())\ny.append(1)\n\nText: Then inside the generator function, there is a lot I don't understand. To me it seems you can leave out most of the back and forth conversion and just use now() as it is. \nCode: def frames1():\n    # Generating time variable\n    target_time = datetime.datetime.now()\n\n    while True:\n        # Add new time + 60 seconds\n        target_time = target_time + datetime.timedelta(seconds=60)\n        x = target_time\n        y = random.randint(250,450)\/10\n        yield (x,y)  \n        time.sleep(random.randint(2,5))\n\nText: You may however format the axis to show times instead of numbers. Inside the init function you may add \nCode: line.axes.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n\nText: where you have imported mpl.dates as mdates. \nText: The line imin = min(max(0, i - win), len(x) - win) does not seem to make much sense, why not use max(0, i - win) alone? \nText: So in total a working version could look like this: \nCode: import random\nimport time\nfrom matplotlib import pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib import animation\nimport datetime\n\n# Plot parameters\nfig, ax = plt.subplots()\nline, = ax.plot([], [], 'k-', label = 'ABNA: Price', color = 'blue')\nlegend = ax.legend(loc='upper right',frameon=False)\nplt.setp(legend.get_texts(), color='grey')\nax.margins(0.05)\nax.grid(True, which='both', color = 'grey')\n\n# Creating data variables\nx = [datetime.datetime.now()]\ny = [1]\n\ndef init():\n    line.set_data(x[:1],y[:1])\n    line.axes.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n    return line,\n\ndef animate(args):\n    # Args are the incoming value that are animated    \n    animate.counter += 1\n    i = animate.counter\n    win = 60\n    imin = max(0, i - win)\n    x.append(args[0])\n    y.append(args[1])\n\n    xdata = x[imin:i]\n    ydata = y[imin:i]\n\n    line.set_data(xdata, ydata)\n    line.set_color(\"red\")\n\n    plt.title('ABNA CALCULATIONS', color = 'grey')\n    plt.ylabel(\"Price\", color ='grey')\n    plt.xlabel(\"Time\", color = 'grey')\n\n    ax.set_facecolor('black')\n    ax.xaxis.label.set_color('grey')\n    ax.tick_params(axis='x', colors='grey')\n    ax.yaxis.label.set_color('grey')\n    ax.tick_params(axis='y', colors='grey')\n\n    ax.relim()\n    ax.autoscale()\n\n    return line,\n\nanimate.counter = 0\n\ndef frames1():\n    # Generating time variable\n    target_time = datetime.datetime.now()\n    while True:\n        # Add new time + 60 seconds\n        target_time = target_time + datetime.timedelta(seconds=60)\n        x = target_time\n        y = random.randint(250,450)\/10\n        yield (x,y)  \n        time.sleep(random.randint(2,5))\n\nanim = animation.FuncAnimation(fig, animate,init_func=init,frames=frames1)\n\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[930,939,"Mention"],[2896,2912,"API"]],"Comments":[]}
{"id":59511,"text":"ID:50800092\nPost:\nText: Wow, so there are three questions here. \nText: Where is the SubplotZero() documentation? \nText: It's currently not documented, since it's an AxesZero, which is augmented to the subplot status via \nCode: SubplotZero = matplotlib.axes.subplot_class_factory(AxesZero)\n\nText: AxesZero is also not documented, but at least appears in the documentation. In general, the complete mpl_toolkits.axisartist is very poorly documented and is missing in the current documentation completely. But one may refer either to an older version or the current devdocs. \nText: It should not be removed in future versions. \nText: What is the reason for the TypeError? \nText: The error TypeError: 'method' object is not subscriptable tells you that a method cannot be indexed. \nText: This happens here as the axis of a SubplotZero, which is a mpl_toolkits.axisartist.axislines.AxesZero, is completely different from the \"usual\" matplotlib.axes.Axes. The former provides its individual axes as part of a dictionary which can be accessed as ax.axis[\"xzero\"]. The latter is a method, which needs to be called and does some modifications on the axes, e.g. ax.axis(\"off\") turns the axes off. It is indeed quite unfortunate that they both have an attribute axis, refering to completely different things in the end. \nText: How to create an axis at zero position? \nText: While the mpl_toolkits.axisartist module provides this functionality as shown in the quoted example, the use use of this module is admittedly (due the missing\/incomplete documentation) rather cumbersome. \nText: An alternative is to use usual matplotlib.axes.Axes. To position a spine at zero position one could refer to the Spine placement demo. \nText: To recreate the linked example with the xaxis spine at zero y position, one would e.g. do \nCode: ax.spines['bottom'].set_position(('data', 0))\n\nText: using an ordinary Axes instance ax. \nText: Complete example: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.set_title('x axis spine at zero data coordinate')\nx = np.arange(0, 2*np.pi, 0.01)\nax.plot(x, np.sin(x))\nax.set_xlabel(\"Axes zero\")\n\nax.spines['bottom'].set_position(('data', 0))\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.show()\n\nAPI:\nmatplotlib.axes.Axes\n","label":[[1884,1888,"Mention"],[2284,2304,"API"]],"Comments":[]}
{"id":59512,"text":"ID:50802773\nPost:\nText: Dataframe.plot() doesn't actually take a color argument. You'd have to drive a matplotlib.pyplot.bar() call directly if you wanted to use a simple sequence of colours (but note that there are better options, listed below). \nText: If you do decide to use matplotlib.pyplot.bar() directly, then take into account that it's color argument then only takes either a single valid color value, so 'r' or 'k', or a sequence of such color values (the documentation for bar() calls it array like). A list of names would work: \nCode: my_colors = ['r', 'g', 'b', 'k', 'y', 'm', 'c']  # red, green, blue, black, etc.\n\nplt.bar(len(train_class), train_class, color=my_colors)\n\nText: The documentation states that the sequence should be equal in length to the number of bars plotted: \nText: The optional arguments color, edgecolor, linewidth, xerr, and yerr can be either scalars or sequences of length equal to the number of bars. \nText: However, it is just easier to pass in a color map to Dataframe.plot() here. Color maps as a handy and fast path towards distinct bar colors. You can pass one in as the colormap keyword argument, this can be a named map (as a string): \nCode: train_class.plot(kind='bar', colormap='Paired')\n\nText: or an actual matplotlib colormap object from the matplotlib.cm module: \nCode: from matplotlib import cm\n\ntrain_class.plot(kind='bar', colormap=cm.Paired)\n\nText: If you wanted to stick with matplotlib.pyplot.bar(), but use a colormap, then create your series of colors from a colormap. Pandas uses np.linspace() for this so here we do too: \nCode: import numpy as np\n\npaired_colors = cm.Paired(np.linspace(0, 1, num=len(train_class))\nplt.bar(len(train_class), train_class, color=paired_colors)\n\nText: For bar plots, I'd pick a qualitative colormap; each name is an attribute of the cm colormap module. In the above, cm.Paired is a one such color map. Calling the color map with a sequence of floats between 0.0 and 1.0 gives you back colours picked at each 'percentage' of the range. You could also pass in a sequence of integers to index individual colours instead. \nText: Circling back to Pandas, you can create a colormap from a hand-picked sequence of colours too, with a ListedColormap instance: \nCode: from matplotlib.colors import ListedColormap\n\nmy_colors = ['r', 'g', 'b', 'k', 'y', 'm', 'c']  # red, green, blue, black, etc.\nmy_colormap = ListedColormap(my_colors)\n\nText: and then pass that to your dataframe .plot() call: \nCode: train_class.plot(kind='bar', colormap=my_colormap)\n\nAPI:\nmatplotlib.colors.ListedColormap\n","label":[[2217,2231,"Mention"],[2538,2570,"API"]],"Comments":[]}
{"id":59513,"text":"ID:50840152\nPost:\nText: As @ImportanceOfBeingErnest suggested, you should use locators to reformat the xtick labels. I have implemented them below: \nCode: import matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.dates as mdates # For formatting dates\n\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize=(15, 4.18))\n\n# Make the original plot\nax[0].plot(df[\"Date\"][:322], \n           df[\"Concentration\"][:322], \n           \"+\", color=\"red\", linewidth=0.5)\nax[0].set_title('Original plot')\n\n# New xticks plot\nmonths = mdates.MonthLocator()          # Add tick every month\ndays = mdates.DayLocator(range(1,32,5)) # Add tick every 5th day in a month\nmonthFmt = mdates.DateFormatter('%b')   # Use abbreviated month name\n\n# Add the locators to the axis\nax[1].xaxis.set_major_locator(months)\nax[1].xaxis.set_major_formatter(monthFmt)\nax[1].xaxis.set_minor_locator(days)\nax[1].plot(df[\"Date\"][:322], \n           df[\"Concentration\"][:322], \n           \"+\", color=\"red\", linewidth=0.5)\nax[1].set_title('Updated xticks')\n\nplt.show()\n\nText: Here are some helpful resources: \nText: The dates api strftime() directives This example, which I drew from heavily \nAPI:\nmatplotlib.dates\n","label":[[1084,1089,"Mention"],[1162,1178,"API"]],"Comments":[]}
{"id":59514,"text":"ID:50879726\nPost:\nText: Well you can extract the edges into a numpy array and then apply the filter. You can use to_numpy_matrix to change edge list to numpy array and apply numpy filter, which are quite performance wise for large graphs as compare to conventional for loops, etc. Then once the modification is done use from_numpy_matrix to get the graph into networkx format. import networkx as nx import numpy as np import mpl.pyplot as plt G = nx.Graph() \nCode: #Add Nodes\nG.add_node(1)\nG.add_node(2)\nG.add_node(3)\nG.add_node(4)\n\n#Add edges\nG.add_edge(1, 2, weight= -1)\nG.add_edge(2, 3, weight= 1)\nG.add_edge(1, 3, weight= 2)\nG.add_edge(4, 3, weight= -1)\n\n#Extract edges into numpy array \nedges =  nx.to_numpy_matrix(G, nodelist= G.nodes())\n\n#Change non-negative values to 0 \nedges[edges<0] = 0\n\n#Save the modified graph\nG2 = nx.from_numpy_matrix(edges)\n\npos=nx.spring_layout(G2)\nnx.draw_networkx_nodes(G2,pos)\nnx.draw_networkx_edges(G2,pos)\nplt.axis('off')\nplt.show() \n\nAPI:\nmatplotlib.pyplot\n","label":[[425,435,"Mention"],[979,996,"API"]],"Comments":[]}
{"id":59515,"text":"ID:50911049\nPost:\nText: Using \nCode: super(matplotlib.patches.RegularPolygon, self).__init__()\n\nText: you calling the init function of the parent of matplotlib.patches.RegularPolygon. However you would really need to call the init of RegularPolygon itself. \nText: I would also suggest not to use the same name for the subclassed artist, as this might add to confusion here. \nText: Options you have \nText: Old style import matplotlib class MyRegularPolygon(matplotlib.patches.RegularPolygon): def __init__(self, *args, **kwargs): matplotlib.patches.RegularPolygon.__init__(self, *args, **kwargs) r = MyRegularPolygon((0.,0.), 5, radius=5, orientation=0) New Style (py2 & 3) import matplotlib class MyRegularPolygon(matplotlib.patches.RegularPolygon): def __init__(self, *args, **kwargs): super(MyRegularPolygon, self).__init__(*args, **kwargs) r = MyRegularPolygon((0.,0.), 5, radius=5, orientation=0) New Style (only py3) import matplotlib class MyRegularPolygon(matplotlib.patches.RegularPolygon): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) r = MyRegularPolygon((0.,0.), 5, radius=5, orientation=0) \nText: I would suggest reading What does 'super' do in Python? for some explanation of super. \nAPI:\nmatplotlib.patches.RegularPolygon\n","label":[[234,248,"Mention"],[1228,1261,"API"]],"Comments":[]}
{"id":59516,"text":"ID:50988953\nPost:\nText: Pandas uses different units to represents dates and times on the axes, depending on the range of dates\/times in use. This means that different locators are in use. \nText: In the first case, \nCode: print(ax.xaxis.get_major_locator())\n# Out: pandas.plotting._converter.PandasAutoDateLocator\n\nText: in the second case \nCode: print(ax.xaxis.get_major_locator())\n# pandas.plotting._converter.TimeSeries_DateLocator\n\nText: You may force pandas to always use the PandasAutoDateLocator using the x_compat argument, \nCode: df.plot(x_compat=True)\n\nText: This would ensure to always get the same datetime definition, consistent with the dates convention. \nText: The drawback is that this removes the nice quarterly ticking \nText: and replaces it with the standard ticking \nText: On the other hand it would then allow to use the very customizable dates tickers and formatters. For example to get quarterly ticks\/labels \nCode: import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as mticker\nimport pandas as pd\n\nindex = pd.to_datetime(['2016-05-01', '2016-11-01', '2017-05-01'])\ndata = pd.DataFrame({'a': [1, 2, 3],\n                     'b': [4, 5, 6]}, index=index)\nax = data.plot(x_compat=True)\n\n# Quarterly ticks\nax.xaxis.set_major_locator(mdates.MonthLocator((1,4,7,10)))\n\n# Formatting:\ndef func(x,pos):\n    q = (mdates.num2date(x).month-1)\/\/3+1\n    tx = \"Q{}\".format(q)\n    if q == 1:\n        tx += \"\\n{}\".format(mdates.num2date(x).year)\n    return tx\nax.xaxis.set_major_formatter(mticker.FuncFormatter(func))\nplt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n\nplt.show()\n\nAPI:\nmatplotlib.dates\nmatplotlib.dates\n","label":[[650,655,"Mention"],[859,864,"Mention"],[1638,1654,"API"],[1655,1671,"API"]],"Comments":[]}
{"id":59517,"text":"ID:51011921\nPost:\nText: From the official documentation, as shown below, the recommendation is to use matplotlib.pyplot. This is not an opinion. \nText: The documentation at Matplotlib, pyplot and pylab: how are they related?, which also describes the difference between pyplot and pylab, states: \"Although many examples use pylab, it is no longer recommended.\". \nText: 2021-05-06 Edit: \nText: From The pylab API (disapproved) \nText: Since heavily importing into the global namespace may result in unexpected behavior, the use of pylab is strongly discouraged. Use pyplot instead. \nAPI:\nmatplotlib.pyplot\n","label":[[564,570,"Mention"],[586,603,"API"]],"Comments":[]}
{"id":59518,"text":"ID:51186545\nPost:\nText: From @MartinEvans suggestion to use AutoDateLocator() I looked up more of the matplotlib documentation and found mpl.dates.MonthLocator along with the WeekdayLocator. This allowed tuning the major and minor xticks to change the format and appearance as required. \nText: I then used this answer to set their rotation. \nCode: fig, ax = plt.subplots(figsize=(2, 4))\n# ax = plt.gca()\nline = ax.plot_date(a.Date, a.Frequency, '.', label='a', alpha=0.5, linewidth=1)\nax.tick_params('y', colors='k')\n# ax.xticks(rotation=70)\nax.set_xlabel('Date')\n# ax.xlabel('Date')\nax.set_ylabel('Frequency')\nax.set_title('Daily Games')\nax.tick_params('y', colors='k')\nax.grid(b=True, which='major', color='w', linewidth=1.0)\nax.grid(b=True, which='minor', color='w', linewidth=0.5)\nax.yaxis.grid(True)\n\nxtick_locator = mpl.dates.MonthLocator(interval=1)\nxtick_formatter = mpl.dates.AutoDateFormatter(xtick_locator)\nax.xaxis.set_major_locator(xtick_locator)\nax.xaxis.set_major_formatter(xtick_formatter)\n\nxtick_locator = mpl.dates.WeekdayLocator(byweekday=3)\nxtick_formatter = mpl.dates.AutoDateFormatter(xtick_locator)\nax.xaxis.set_minor_locator(xtick_locator)\nax.xaxis.set_minor_formatter(xtick_formatter)\n\nplt.setp(ax.xaxis.get_minorticklabels(), rotation=90, size=10)\nplt.setp(ax.xaxis.get_majorticklabels(), rotation=90, size=7)\n\nfig.subplots_adjust(bottom=0.24)\n\n\nplt.show()\n\nAPI:\nmatplotlib.dates.MonthLocator\n","label":[[137,159,"Mention"],[1389,1418,"API"]],"Comments":[]}
{"id":59519,"text":"ID:51197356\nPost:\nText: Seen from the question, you want to set the ticks manually anyways, so you just need to format the ticklabels, \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\ncm = np.sort(np.random.rand(64)).reshape((8,8))\ntick_marks=np.arange(8)\n\nplt.imshow(cm)\n\nx_sum = cm.sum(axis=0)*100\ny_sum = cm.sum(axis=1)*100\n\nfmt = lambda x: \"{:.2f}%\".format(x)\n\nplt.xticks(tick_marks, [fmt(i) for i in x_sum], rotation=90)\nplt.yticks(tick_marks, [fmt(i) for i in y_sum])\n\nplt.tight_layout()\nplt.show()\n\nText: For automatic percentage labeling, the PercentFormatter makes sense: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\n\ncm = np.sort(np.random.rand(64)).reshape((8,8))\n\nplt.imshow(cm, extent=(0,80,0,80))\n\nplt.gca().xaxis.set_major_formatter(PercentFormatter(decimals=2))\nplt.gca().yaxis.set_major_formatter(PercentFormatter(decimals=2))\n\nplt.gca().tick_params(axis=\"x\",rotation=90)\nplt.tight_layout()\nplt.show()\n\nAPI:\nmatplotlib.ticker.PercentFormatter\n","label":[[561,577,"Mention"],[995,1029,"API"]],"Comments":[]}
{"id":59520,"text":"ID:51198715\nPost:\nText: A possible way to specify many rc parameters at once is to use a dictionary and update the mpl.rcParams with it. \nCode: import matplotlib.pyplot as plt\nfrom cycler import cycler\n\nmyparams = {'axes.edgecolor': \"red\",   # BOX colors\n            'axes.linewidth': 1.2,   # BOX width\n            'axes.xmargin': 0,    \n            'axes.ymargin': 0,     \n            'axes.labelcolor': \"crimson\",     \n            'axes.axisbelow': True,   \n            'xtick.color': \"blue\",   # doesn't affect the text\n            'ytick.color': \"gold\",   # doesn't affect the text \n            'axes.prop_cycle': cycler('color', ['#8DA0CB', '#E78AC3', '#A6D854', '#FFD92F', '#E5C494', '#B3B3B3', '#66C2A5', '#FC8D62']), \n            'grid.linestyle': '--', \n            'grid.alpha': '1',\n            'grid.color': '#E5E5E5'}\nplt.rcParams.update(myparams)\n\nText: If instead you want to use a context, you may do so \nCode: with plt.rc_context(myparams):\n    plt.plot([1,2,3])\n\nText: Using plt.rc_context outside of a context (as in the question) may not make too much sense anyways. \nAPI:\nmatplotlib.rcParams\n","label":[[115,127,"Mention"],[1094,1113,"API"]],"Comments":[]}
{"id":59521,"text":"ID:51221041\nPost:\nText: Using subplots_adjust goes in the right direction. Don't use tight_layout afterwards as this would overwrite any settings done via subplots_adjust. \nText: You may decide to opt for something like \nCode: fig.subplots_adjust(left=0.2, bottom=0.2)\n\nText: to make some space for the inset in the lower left corner of the figure. \nText: Then you need to position the inset. Since here you're working in the lower left corner, this is relatively easy. The loc parameter needs to be set to the lower left corner and you may stick to the bbox_to_anchor=(0,0) position. Then just add some padding via borderpad=3 (in units of font size), such that the inset axes' labels are still visible, \nCode: zoomed_inset_axes(ax, 2, loc='lower left', bbox_to_anchor=(0,0), borderpad=3)\n\nText: Complete code: \nText: import numpy as np import pyplot as plt from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes, mark_inset X = np.random.normal(.5,10,1000) Y = np.random.normal(.5,10,1000) fig, ax = plt.subplots(1, figsize=(10,6)) fig.subplots_adjust(left=0.2, bottom=0.2) ax.scatter(X,Y) # # Setup zoom window axins = zoomed_inset_axes(ax, 2, loc='lower left', bbox_to_anchor=(0,0), borderpad=3) mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\") axins.set_xlim([-15,0]) axins.set_ylim([-12,-3]) # # Plot zoom window axins.scatter(X,Y) #fig.savefig('test.png', dpi=70) plt.show() \nText: In general, you have a lot of options to position and size the inset. I recently created a new example on the matplotlib page: Inset Locator Demo, which is currently only available in the devdocs, to show the interplay between the different parameters (in that case for inset_axes - but it totally applies to zoomed_inset_axes as well). \nAPI:\nmatplotlib.pyplot\n","label":[[845,851,"Mention"],[1755,1772,"API"]],"Comments":[]}
{"id":59522,"text":"ID:51309296\nPost:\nText: alpha does afaik not work for tables, but you can change the zorder: \nCode: the_table = plt.table(cellText=table_vals,\n                  colWidths = [0.1]*3,\n                  rowLabels=row_labels,\n                  colLabels=col_labels,\n                  loc='center right', zorder=3)\n\nText: In table the keyword argument alpha is mentioned. But it seems to have no effect. Changing the zorder (the vertical order of the elements) makes the table appear on top of your plots. This does not allow semi-opaque tables, but is at least a work-around to make the grid-lines disappear. \nAPI:\nmatplotlib.axes.Axes.table\n","label":[[320,325,"Mention"],[611,637,"API"]],"Comments":[]}
{"id":59523,"text":"ID:51440735\nPost:\nText: The question can be translated into how to position an axes in figure coordinates with a fixed width and height in absolute (pixel) coordinates. This can be done via setting the axes locator to a il.AnchoredSizeLocator via ax.set_axes_locator. \nCode: import matplotlib.pyplot as plt\nimport matplotlib.transforms as mtrans\nfrom mpl_toolkits.axes_grid1.inset_locator import AnchoredSizeLocator\n\nfig, ax = plt.subplots()\n\n# Create axes, which is positionned in figure coordinates,\n# with width and height fixed in inches.\n\n# axes extent in figure coordinates (width & height ignored)\naxes_extent = [0.03, 0.5, 0, 0]\n# add axes to figure\nrax = fig.add_axes(axes_extent)\n# create locator: Position at (0.03, 0.5) in figure coordinates,\n# 0.7 inches wide and tall, pinned at left center of bbox.\naxes_locator = AnchoredSizeLocator(mtrans.Bbox.from_bounds(*axes_extent),\n                                   .7, .7, loc=\"center left\",\n                                   bbox_transform=fig.transFigure,\n                                   borderpad=0)\nrax.set_axes_locator(axes_locator)\n\nText: Now, when the figure size changes, the axes will stay at the same relative position without changing its width and height. \nAPI:\nmpl_toolkits.axes_grid1.inset_locator.AnchoredSizeLocator\n","label":[[220,242,"Mention"],[1236,1293,"API"]],"Comments":[]}
{"id":59524,"text":"ID:51454883\nPost:\nText: So the issue really is after the except adjustment, is that you are calling the function in a thread without passing any arguments as thread require both arguments and function to work otherwise it throws error that need two only given one arg so in order to fix that we can pass an empty tuple as arg if our functions take no arguments. \nText: so what was \nCode: _thread.start_new_thread(firstfunc,)\n\nText: is now this \nCode: _thread.start_new_thread(firstfunc,())\n\nText: This should work \nText: import _thread import pyplot as plt import matplotlib.animation as animation from matplotlib import style import csv import time \nCode: def firstfunc():\n    count=0\n    with open('airtabletest.txt','r') as dataset:\n        line=csv.reader(dataset)\n        arr=[]\n        for row in line:\n            if(len(arr)>=9):\n                arr.clear()\n            for i in range(1,10):\n                arr.append(int(row[i]))\n            t=time.time()+3\n            while(t>time.time()):\n                    pass\n            if(count>=8):\n                with open('live_graph1','r') as file:\n                    lines=file.readlines()\n                with open('live_graph1','w') as csvfile:\n                    csvfile.writelines(lines[1:])\n            with open('live_graph1','a+') as file:\n                arr2=[]\n                writer=csv.writer(file)\n                arr2.append(row[0][11:])\n                arr2.append(sum(arr)\/10)\n                writer.writerow(arr2)\n                count+=1\n\ndef secondfunc():\n    style.use('fivethirtyeight')\n\n    fig = plt.figure()\n    ax1 = fig.add_subplot(1,1,1)\n    def animate(i):\n        xs = []\n        ys = []\n        count=0\n        label=[]\n        with open('live_graph1','r') as file:\n            reader=csv.reader(file)\n            for row in reader:\n                if(len(row) == 2):\n                    x = float(row[1])\n                    xs.append(count)\n                    ys.append(x)\n                    label.append(row[0])\n                    count+=1\n            ax1.clear()\n            ax1.set_xticks(xs)\n            ax1.set_xticklabels(label)\n            ax1.plot(xs,ys)\n            fig.autofmt_xdate()\n    ani = animation.FuncAnimation(fig, animate, interval=1000)\n    plt.show()\n\ndef main():\n    try:\n        _thread.start_new_thread(secondfunc,())\n        _thread.start_new_thread(firstfunc,())\n    except RuntimeError as e:\n        print(e)\n    while 1:\n        pass\n\nif __name__=='__main__':main()\n\nText: why your code is giving a syntax error is because you are required to specify what to do if try fail. i.e. must specify except: \nText: also I believe in your code you have while 1 that will always return to be true and it seems to be not doing anything at all \nAPI:\nmatplotlib.pyplot\n","label":[[543,549,"Mention"],[2764,2781,"API"]],"Comments":[]}
{"id":59525,"text":"ID:51475384\nPost:\nText: You need to convert your datetime object to a number. For this you may use the inbuild mechanism date2num \nCode: import matplotlib.dates as mdates\n\nax.text(mdates.date2num(datetime.datetime(2016,1,1)), 0, \"Voil\")\n\nAPI:\nmatplotlib.dates.date2num\n","label":[[121,129,"Mention"],[244,269,"API"]],"Comments":[]}
{"id":59526,"text":"ID:51673542\nPost:\nText: Unfortunately, the confidence interval is not provided by the statsmodels cross-correlation function (ccf). In R the ccf() would also print the confidence interval. \nText: Here, we need to calculate the confidence interval by ourself and plot it out afterwards. The confidence interval is here computed as 2 \/ np.sqrt(lags). For basic info on confidence intervals for cross-correlation refer to: \nText: Stats StackExchange answer by Rob Hyndman: https:\/\/stats.stackexchange.com\/a\/3128\/43304 \nText: import numpy as np import plt as plt import statsmodels.tsa.stattools as stattools def create(n): x = np.zeros(n) for i in range(1, n): if np.random.rand() < 0.9: if np.random.rand() < 0.5: x[i] = x[i-1] + 1 else: x[i] = np.random.randint(0,100) return x x = create(4000) y = create(4000) lags= 4000 sl = 2 \/ np.sqrt(lags) plt.plot(x, list(np.ones(lags) * sl), color='r') plt.plot(x, list(np.ones(lags) * -sl), color='r') plt.plot(stattools.ccf(x, y)[:100]) \nText: This leads to the following plot with the additional red lines: \nAPI:\nmatplotlib.pyplot\n","label":[[548,551,"Mention"],[1057,1074,"API"]],"Comments":[]}
{"id":59527,"text":"ID:51716781\nPost:\nText: Indeed, the coordinates obtained via get_offsets are the projected coordinates. The original coordinates are hidden inside the Path3DCollection which is returned by the scatter in three dimensional axes. You would obtain the original coordinates from the ._offsets3d attribute. (This is a \"private\" attribute, but unfortunately the only way to retrieve this information.) \nCode: import numpy as np\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = Axes3D(fig)\nx = [1,2,3,4]\ny = [1,3,3,5]\nz = [10,20,30,40]\nc= [1,2,3,1]\nscatter = ax.scatter(x,y,z,c=c,s=15,cmap='hot',vmin=0,vmax=1)\ndata = np.array(scatter._offsets3d).T\nprint(scatter)  # prints mpl_toolkits.mplot3d.art3d.Path3DCollection\nprint(data)\n\n# prints\n# \n# [[  1.   1.  10.]\n#  [  2.   3.  20.]\n#  [  3.   3.  30.]\n#  [  4.   5.  40.]]\n\nAPI:\nmpl_toolkits.mplot3d.art3d.Path3DCollection\n","label":[[151,167,"Mention"],[876,919,"API"]],"Comments":[]}
{"id":59528,"text":"ID:51747523\nPost:\nText: With the hint of ImportanceOfBeingErnest I used matplotlib.pyplot.bar instead of bar to get what I wanted: \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({'Group 1': {-60.0:0, -20.0:0, 12.5:0, 62.0:0, 123.8:0, 181.0: 5.013532366071429e-06, 225.2: 0.00010224713604266826, 248.0: 0.0002520240051269531, 274.9: 0.0006304542296807856, 304.2: 0.0009587457616051962, 331.0: 0.0021422429744175505}, 'Group 2': {-60.0: 0.0003144776457026891, -20.0: 5.43150903588747e-05, 12.5: 0.00012757662141348495, 62.0: 6.852403753623154e-05, 123.8: 5.980538377849872e-05, 181.0: 5.000001780657088e-05, 225.2: 0.00010152032391840468, 248.0: 0.0005436288535458056, 274.9: 0.00038244130009346957, 304.2: 0.00023423789360943164, 331.0: 9.508221455006986e-05}, 'Group 3': {-60.0: 0.00021804919790451726, -20.0: 0.0002884471518114942, 12.5: 0.00024001954291413006, 62.0: 0.00020780311751064946, 123.8:0, 181.0: 0.0003548555407567293, 225.2: 0.0011448858440205976, 248.0: 0.0031436022397010425, 274.9: 0.001858462242669843, 304.2: 0.0019485330483867962, 331.0: 0.0017062062250634059}})\n\nwidth = 15\nbottom = 0\n\nfor i in df.columns:\n    plt.bar(df.index, df[i], width=width, bottom=bottom)\n    bottom += df[i]\n\nplt.ylabel('TM [mg\/l]')\nplt.xlabel('km')\nplt.legend(df.columns)\nplt.tight_layout()\n\nAPI:\npandas.DataFrame.plot.bar\n","label":[[105,108,"Mention"],[1326,1351,"API"]],"Comments":[]}
{"id":59529,"text":"ID:51998635\nPost:\nText: You can import mpl.pyplot and use the show function: \nCode: from IPython.core.display import display\nimport matplotlib.pyplot as plt\n...\nfor df in list_df:\n    df.plot()\n    plt.show()\n    display(df)\n\nAPI:\nmatplotlib.pyplot\n","label":[[39,49,"Mention"],[231,248,"API"]],"Comments":[]}
{"id":59530,"text":"ID:52009912\nPost:\nText: In terms of what happens in the code, imread delegates to Pillow to read the file and then calls np.asarray on the resulting PIL.Image.Image object. When converting a PIL.Image.Image to a NumPy array, the Image object builds a bytestring to use as the array's buffer. Bytestrings are immutable, so the resulting array is unwriteable. \nText: In terms of why anyone on the Matplotlib or Pillow dev team chose an implementation that results in an unwriteable array, I don't know. It's not clear whether this was deliberate at all. \nText: If you want a writeable array, call the array's copy method: \nCode: mutable_array = matplotlib.pyplot.imread('download.jpeg').copy()\n\nAPI:\nmatplotlib.pyplot.imread\n","label":[[62,68,"Mention"],[698,722,"API"]],"Comments":[]}
{"id":59531,"text":"ID:52040748\nPost:\nText: I guess an easy option is to use a HandlerTuple and supply a tuple of a red and blue rectangle to the legend handles. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.legend_handler\n\nx = np.linspace(0, 5, 5)\ny = np.exp(x)\nw = x[1] - x[0]\n\ncolors = ['blue' if idx % 2 == 0 else 'red' for idx in range(len(x))]\n\nfig, ax = plt.subplots()\nbars = ax.bar(x, y, width=w, color=colors, label='sample plot')\n\nax.legend(handles = [tuple(bars[:2])], labels=['sample plot'], loc='upper left', \n          handler_map = {tuple: matplotlib.legend_handler.HandlerTuple(None)})\n\nplt.show()\n\nText: Else, you can of course use any custom handler you like as described in the legend guide. \nAPI:\nmatplotlib.legend_handler.HandlerTuple\n","label":[[59,71,"Mention"],[727,765,"API"]],"Comments":[]}
{"id":59532,"text":"ID:52055979\nPost:\nText: Apparently the order of importing pyplot and setting the notebook backend matters. \nText: When putting the %matplotlib notebook line on top, it works fine for me \nCode: %matplotlib notebook\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nAPI:\nmatplotlib.pyplot\n","label":[[58,64,"Mention"],[312,329,"API"]],"Comments":[]}
{"id":59533,"text":"ID:52121237\nPost:\nText: The following is an explanation why setting a new style will not update the existing figure. \nText: The rcParams is essentially a dictionary which stores certain default parameters that are meant to be used when creating objects in matplotlib. rcParams.update will update this dictionary; update is a method of the python dict object. mplstyle.use is a shortcut to updating the rcParams, it will load the respective parameters from a file or dictionary. \nText: Those parameters are then used when a new object is created. In a simplified fashion this would look like \nCode: def create_object(arg1, parameter1=None):\n    if not parameter1:\n        # use the default from the rcParams\n        parameter1 = rcParams[\"parameter1\"]\n    obj = MatplotlibObject(arg1, parameter1=parameter1)\n    return obj\n\nText: Such function is used when creating objects. However, once such object is created, it will not be run again when something in rcParams changes. \nCode: obj = create_object(1)\nrcParams.update({\"parameter1\" : \"New Value\"})\n# at this point, obj would not know about the new parameter1 value\n\nText: As commented already, there are two options. \nText: Change the parameter manually. You may change the parameter manually after the object has been created. obj = create_object(1) obj.set_parameter1(\"New Value\") Recreate the object. You may remove the existing object, update the rcParams and recreate the object such that it'll use the new value. obj = create_object(1) del obj rcParams.update({\"parameter1\" : \"New Value\"}) obj = create_object(1) \nAPI:\nmatplotlib.rcParams\nmatplotlib.style.use\n","label":[[128,136,"Mention"],[359,371,"Mention"],[1576,1595,"API"],[1596,1616,"API"]],"Comments":[]}
{"id":59534,"text":"ID:52205831\nPost:\nText: Unfortunately cbar_kws doesn't accept the labelpad argument. Therefore one way to add some padding to the labels will be to access the colorbar after it has been drawn. \nText: You can use ax.collections[0].colorbar to get access to the Colorbar object. This then lets you use the colobar as you normally would with matplotlib. So, you can use set_label() to set your colorbars label, and you can use the labelpad= argument: \nCode: import seaborn as sns\n\nuniform_data = np.random.rand(10, 12) # random data\nax = sns.heatmap(uniform_data)\n\ncbar = ax.collections[0].colorbar\ncbar.set_label('Label for colour bar axis', labelpad=40)\n\nplt.show()\n\nAPI:\nmatplotlib.colorbar.Colorbar\n","label":[[260,268,"Mention"],[671,699,"API"]],"Comments":[]}
{"id":59535,"text":"ID:52234951\nPost:\nText: Yes, you can access the axes object like this: \nCode: import seaborn as sns\nlm = sns.lmplot(...)  # draw a grid of plots\nax = lm.axes  # access a grid of 'axes' objects\n\nText: Here, ax is an array containing all axes objects in the subplot. You can access each one like this: \nCode: ax.shape  # see the shape of the array containing the 'axes' objects\nax[0, 0]  # the top-left (first) subplot \nax[i, j]  # the subplot on the i-th row of the j-th column\n\nText: If there is only one subplot you can either access it as I showed above (with ax[0, 0]) or as you said in your question through (plt.gca()) \nAPI:\nmatplotlib.pyplot.axes\n","label":[[48,52,"Mention"],[630,652,"API"]],"Comments":[]}
{"id":59536,"text":"ID:52246215\nPost:\nText: First of all the line fig = figure(**fig_kw) calls the pyplot.figure function. This registers the figure inside the pyplot state machine. \nText: fig is a mpl.figure.Figure instance. Next, it's subplots method is called. This will essentially create an array and fill it will Axes instances. Those are created with self.add_subplot. \nText: add_subplot will initialize the axes and store it as part of the figure's fig.axes array. \nText: So in total you have pyplot which stores the figures and each figure stores the axes in it. When calling plt.show() it will basically loop through all figures and show them. For each figure, all axes inside fig.axes will be drawn. If you have previously manipulated any of the axes by calling any of their methods, those will of course be taken into account because you manipulated exactly the axes object that is later also drawn. \nAPI:\nmatplotlib.figure.Figure\nmatplotlib.axes.Axes\n","label":[[178,195,"Mention"],[299,303,"Mention"],[898,922,"API"],[923,943,"API"]],"Comments":[]}
{"id":59537,"text":"ID:52317485\nPost:\nText: matplotlib \nText: You are looking for fill_between \nCode: ax.fill_between(x, min, max)\n\nText: see this link for a full example \nText: https:\/\/matplotlib.org\/2.0.1\/examples\/pylab_examples\/fill_between_demo.html \nText: Link to seaborn \nText: In the specific example of seaborn you need to first call the facetgrid function to populate the facetgrid with a blank plot. \nText: Only then you can add lines in a matplotlib way whilst still using pretty seaborn formatting \nText: See right at the bottom of the seaborn.FacetGrid docs and you will find an Attribute ax documented. In some versions this might also be axes \nAPI:\nmatplotlib.pyplot.fill_between\n","label":[[62,74,"Mention"],[644,674,"API"]],"Comments":[]}
{"id":59538,"text":"ID:52470421\nPost:\nText: The problem you are running into is that the y scales are not necessarily going to match. You need to overlay 2 axes that share the same x axis. You can use twinx to accomplish this. \nCode: import matplotlib.pyplot as plt\nimport seaborn as sns\nrisk_heatmap = [\n    [1.2, 0.7, 0.7, 0.3, 0, 0, 0],\n    [1.2, 0.7, 0.7, 0.7, 0.5, 0, 0],\n    [1.2, 1.2, 1.2, 0.7, 0.7, 0, 0],\n    [2.0, 1.2, 1.2, 1.2, 0.7, 0.7, 0],\n    [2.0, 2.0, 2.0, 1.2, 1.2, 0.7, 0.7]\n ]\n\nax1 = sns.heatmap(risk_heatmap, linewidth=0.5, cbar=False, cmap=\"RdYlGn\")\n\nax2 = ax1.twinx()\nax2.plot(np.random.random(7) * .3)\nax2.set_ylim(0, 0.4)\n\nplt.show()\n\nText: Here is another example. \nAPI:\nmatplotlib.axes.Axes.twinx\n","label":[[181,186,"Mention"],[676,702,"API"]],"Comments":[]}
{"id":59539,"text":"ID:52500781\nPost:\nText: there was another post on this here, with a great answer from ImportanceofBeingErnest, suggesting to to use import matplotlib; matplotlib.use(\"TkAgg\") before import mpl.pyplot as plt. You find more on backends on the matplotlib site. You still may have to install the appropriate python-to-toolkit packages, eg. python-tkinter and python3-tk, or try an already installed backend. For example, qt5agg, provides an option to configure some graph parameters interactively. \nAPI:\nmatplotlib.pyplot\n","label":[[189,199,"Mention"],[500,517,"API"]],"Comments":[]}
{"id":59540,"text":"ID:52551475\nPost:\nText: the problem you are reporting is with: mpl.colors.to_rgba_array which expects an array of Matplotlib Color values, not floating point numbers. \nText: Take a look here: https:\/\/matplotlib.org\/api\/colors_api.html#module-matplotlib.colors \nText: And also look at the following stack overflow post: How to map number to color using matplotlib's colormap? \nText: you could fix it by changing it with something like the following (please take note of the arbitrary vmin and vmax values I chose): \nCode: norm = mpl.colors.Normalize(vmin=0, vmax=60)\ncmap = cm.hot\nm = cm.ScalarMappable(norm=norm, cmap=cmap)\nmap_to_color = np.vectorize(m.to_rgba)\n\n# parse good sensor data from imported data\ncolor = map_to_color(data_file[:, 0])\n\nText: There are further issues with the sample you've posted regarding the dimensionalities of what you pass to ax.scatter but I'm sure you can quickly fix that yourself. \nText: Edit: \nText: After looking at the scatter some more (https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.pyplot.scatter.html), here is the quickest way to get your example working. \nText: remove the faulty line: \nCode: color = mpl.colors.to_rgba_array(data_file[:,0], alpha=None)\n\nText: and replace the block: \nCode: c = np.abs(color)\ncmhot = plt.get_cmap(\"hot\")\nax.scatter(X, Y, Z, color, s=50, c=c, cmap=cmhot)\n\nText: with: \nCode: values = data_file[:4, 0]\nax.scatter(X, Y, Z, c=values, cmap=\"hot\")\n\nText: the data_file[:4, 0] instead of data_file[:, 0] is there to make sure the dimensionality of your input x and y (n=4) matches that of the colors\/values passed to c (which should also be of lenght n) \nAPI:\nmatplotlib.pyplot.scatter\n","label":[[959,966,"Mention"],[1631,1656,"API"]],"Comments":[]}
{"id":59541,"text":"ID:52591359\nPost:\nText: The \"numbers\" shown on the x axis for such boxplots are determined via a FsxedFormakter (find out via print(ax.xaxis.get_major_formatter())). This fixed formatter just puts labels on ticks one by one from a list of labels. This makes sense because your boxes are positionned at 0 and 1, yet you want them to be labeled as 0.3, 0.7. I suppose this concept becomes clearer when thinking about what should happen for a dataframe with df.columns=[\"apple\",\"banana\"]. \nText: So the FixedFormatter ignores the locale, because it just takes the labels as they are. The solution I would propose here (although some of those in the comments are equally valid) would be to format the labels yourself. \nCode: ax.set_xticklabels([\"{:n}\".format(l) for l in df.columns]) \n\nText: The n format here is just the same as the usual g, but takes into account the locale. (See python format mini language). Of course using any other format of choice is equally possible. Also note that setting the labels here via ax.set_xticklabels only works because of the fixed locations used by boxplot. For other types of plots with continuous axes, this would not be recommended, and instead the concepts from the linked answers should be used. \nText: Complete code: \nCode: import locale\n# Set to German locale to get comma decimal separator\nlocale.setlocale(locale.LC_NUMERIC, 'german')\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame([[1,2,3],[4,5,6]]).T\ndf.columns = [0.3,0.7]\n\nax = sns.boxplot(data=df)\nax.set_xticklabels([\"{:n}\".format(l) for l in df.columns])\n\nplt.show()\n\nAPI:\nmatplotlib.ticker.FixedFormatter\n","label":[[97,111,"Mention"],[1622,1654,"API"]],"Comments":[]}
{"id":59542,"text":"ID:52641064\nPost:\nText: You can add this part to your code: \nCode: ax = pyplot.gca()\nmajor_ticks = np.arange(0.5, 20, 1)\npyplolt.xticks(rotation=90)\nax.set_xticks(major_ticks)\nax.set_yticks(major_ticks)\nax.grid(which='both')\npyplot.grid(True)\n\nText: Output: \nText: Ps. usually pyplot is imported in this way: import pyplot as plt \nAPI:\nmatplotlib.pyplot\n","label":[[316,322,"Mention"],[336,353,"API"]],"Comments":[]}
{"id":59543,"text":"ID:52679866\nPost:\nText: I would suggest to use datetimes directly without messing with the ticklabels. Using a mpl.dates.MinuteLocator in addition can give you nice positions of the ticks. \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nd = ({\n    'A' : ['08:00:00','08:10:00','08:12:00','08:26:00','08:29:00','08:31:00',\n           '10:10:00','10:25:00','10:29:00','10:31:00'],\n    'B' : ['1','1','1','2','2','2','7','7','7','7'],     \n    'C' : ['X','Y','Z','X','Y','Z','A','X','Y','Z'],\n    })\n\ndf = pd.DataFrame(data=d)\ndf['A'] = pd.to_datetime(df['A'])\n\nfig,ax = plt.subplots()\n\nax.scatter(df[\"A\"].values, df[\"B\"].values)\nax.set_xlim(df[\"A\"].min(), df[\"A\"].max())\n\nax.xaxis.set_major_locator(mdates.MinuteLocator((0,30)))\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\nplt.show()\n\nAPI:\nmatplotlib.dates.MinuteLocator\n","label":[[111,134,"Mention"],[850,880,"API"]],"Comments":[]}
{"id":59544,"text":"ID:52788179\nPost:\nText: A mpatches.PathPatch has a single facecolor. It cannot be used to colorize parts of it differently. This is mentioned in the example \nText: can draw collections of regularly shaped objects with homogeneous properties more efficiently with a PathCollection. \nText: So the motivation to use this strategy instead of creating a usual histogram and colorize its bars is efficiency. A slightly less efficient way of creating a bar plot, but still faster than the usual bars is to use a PolyCollection. \nText: So let's look at three different solutions below. \nCode: import numpy as np; np.random.seed(19680801)\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.path as path\nimport matplotlib.collections\nimport matplotlib.cm\n\ndata = np.random.randn(1000)\n\n\ndef compoundpathhist(data, nbins=50):\n    n, bins = np.histogram(data, nbins)\n    # get the corners of the rectangles for the histogram\n    left = np.array(bins[:-1])\n    right = np.array(bins[1:])\n    bottom = np.zeros(len(left))\n    top = bottom + n\n    \n    \n    # we need a (numrects x numsides x 2) numpy array for the path helper\n    # function to build a compound path\n    XY = np.array([[left, left, right, right], [bottom, top, top, bottom]]).T\n    \n    # get the Path object\n    barpath = path.Path.make_compound_path_from_polys(XY)\n    \n    # make a patch out of it\n    patch = patches.PathPatch(barpath)\n    fig, ax = plt.subplots()\n    ax.add_patch(patch)\n    # update the view limits\n    ax.set_xlim(left[0], right[-1])\n    ax.set_ylim(bottom.min(), top.max())\n\n    fig.savefig(\"bartest.png\")\n        \n\ndef polyhist(data, nbins=50, colors=True):\n    n, bins = np.histogram(data, nbins)\n    # get the corners of the rectangles for the histogram\n    left = np.array(bins[:-1])\n    right = np.array(bins[1:])\n    bottom = np.zeros(len(left))\n    top = bottom + n\n    # we need a (numrects x numsides x 2) numpy array to be used as \n    # vertices for the PolyCollection\n    XY = np.array([[left, left, right, right], [bottom, top, top, bottom]]).T\n    \n    c=None\n    if colors:\n        c = matplotlib.cm.RdYlBu(n\/n.max())\n    pc = matplotlib.collections.PolyCollection(XY, facecolors=c)\n    \n    fig, ax = plt.subplots()\n    ax.add_collection(pc)\n    # update the view limits\n    ax.set_xlim(left[0], right[-1])\n    ax.set_ylim(bottom.min(), top.max())\n    \n    fig.savefig(\"bartest.png\")\n        \n\n\ndef hist_c(data, nbins=50, colors=True):\n\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(data, nbins)\n\n    if colors:\n        cols = matplotlib.cm.RdYlBu(n\/n.max())\n        for p,c in zip(patches, cols):\n            p.set_facecolor(c)\n\n    # update the view limits\n    ax.set_xlim(bins.min(), bins.max())\n    ax.set_ylim(n.min(), n.max())\n\n    fig.savefig(\"bartest.png\")\n\n        \ncompoundpathhist(data, nbins=50)\npolyhist(data, nbins=50)\nhist_c(data, nbins=50, colors=True)\n\nplt.show()\n\nText: The first (compoundpathhist) is the one from the linked example. It is fast, but cannot show color. The second (polyhist) doesn't use a single patch, but instead a PolyCollection. The PolyCollection's facecolors can be set via a colormap. The third (hist_c) is the adapted usual solution to colorize individual bars. \nText: Now we can time the three functions. I'm using 90000 data points and 50, 500 and 5000 bins. \nText: We observe that for a usual number of ~50 bins, there is essentially no difference between those methods. However for larger number of bins, the hist method takes significantly longer. Even for 5000 bins, there is almost no difference between the compoundpathhist method (which cannot use color) and the polyhist, which can use color. This is hence a useful alternative without sacrificing efficiency. There is by the way almost no difference between using color or not using color in the respective functions. \nAPI:\nmatplotlib.patches.PathPatch\n","label":[[26,44,"Mention"],[3872,3900,"API"]],"Comments":[]}
{"id":59545,"text":"ID:52902087\nPost:\nText: FontAwesome is available from here. It provides its icons as vector graphics and as well as as otf-font. \nText: Use FontAwesome otf font \nText: Matplotlib cannot natively read vector graphics, but it can load otf-fonts. After downloading the FontAwesome package you can access the font via a FontProperties object, e.g. \nCode: fp = FontProperties(fname=r\"C:\\Windows\\Fonts\\Font Awesome 5 Free-Solid-900.otf\") \n\nText: Create texts \nText: The FontProperties can be the input for matplotlib text objects \nCode: plt.text(.6, .4, \"\\uf16c\", fontproperties=fp)\n\nText: Unfortunately, using the FontAwesome ligatures is not possible. Hence the individual symbols need to be accessed via their UTF8 key. This is a little cumbersome, but the cheatsheet can come handy here. Storing those needed symbols in a dictionary with a meaningful name may make sense. \nText: Example: \nCode: from matplotlib.font_manager import FontProperties\nimport matplotlib.pyplot as plt\n\nfp1 = FontProperties(fname=r\"C:\\Windows\\Fonts\\Font Awesome 5 Brands-Regular-400.otf\")\nfp2 = FontProperties(fname=r\"C:\\Windows\\Fonts\\Font Awesome 5 Free-Solid-900.otf\")\n\nsymbols = dict(cloud = \"\\uf6c4\", campground = \"\\uf6bb\", hiking = \"\\uf6ec\",\n               mountain = \"\\uf6fc\", tree = \"\\uf1bb\", fish = \"\\uf578\",\n               stackoverflow = \"\\uf16c\")\n\nfig, (ax, ax2) = plt.subplots(ncols=2, figsize=(6.2, 2.2), sharey=True)\nax.text(.5, .5, symbols[\"stackoverflow\"], fontproperties=fp1, size=100, \n         color=\"orange\", ha=\"center\", va=\"center\")\n\n\nax2.stackplot([0,.3,.55,.6,.65,1],[.1,.2,.2,.2,.2,.15],[.3,.2,.2,.3,.2,.2],\n              colors=[\"paleturquoise\", \"palegreen\"])\nax2.axis([0,1,0,1])\nax2.text(.6, .4, symbols[\"mountain\"], fontproperties=fp2, size=16, ha=\"center\")\nax2.text(.09, .23, symbols[\"campground\"], fontproperties=fp2, size=13)\nax2.text(.22, .27, symbols[\"hiking\"], fontproperties=fp2, size=14)\nax2.text(.7, .24, symbols[\"tree\"], fontproperties=fp2, size=14,color=\"forestgreen\")\nax2.text(.8, .33, symbols[\"tree\"], fontproperties=fp2, size=14,color=\"forestgreen\")\nax2.text(.88, .28, symbols[\"tree\"], fontproperties=fp2, size=14,color=\"forestgreen\")\nax2.text(.35, .03, symbols[\"fish\"], fontproperties=fp2, size=14,)\nax2.text(.2, .7, symbols[\"cloud\"], fontproperties=fp2, size=28,)\n\nplt.show()\n\nText: Create markers \nText: Creating a lot of texts like above is not really handy. To have the icons as markers would be nicer for certain applications. Matplotlib does have the ability to use utf symbols as markers, however, only through the mathtext functionality. Getting an otf font to be used as mathfont in matplotlib was unsuccessful in my trials. \nText: An alternative is to create a mpl.path.Path from the symbol. This can be done via a matplotlib.textpath.TextToPath instance, which is unfortunately undocumented. The TextToPath has a method get_text_path taking a fontproperty and a string as input and returning the vertices and codes from which to create a Path. A Path can be used as a marker, e.g. for a scatter plot. \nCode: v, codes = TextToPath().get_text_path(fp, \\uf6fc)\npath = Path(v, codes, closed=False)\nplt.scatter(..., marker=path)\n\nText: Some example: \nCode: import numpy as np; np.random.seed(32)\nfrom matplotlib.path import Path\nfrom matplotlib.textpath import TextToPath\nfrom matplotlib.font_manager import FontProperties\nimport matplotlib.pyplot as plt\n\nfp = FontProperties(fname=r\"C:\\Windows\\Fonts\\Font Awesome 5 Free-Solid-900.otf\")\n\nsymbols = dict(cloud = \"\\uf6c4\", campground = \"\\uf6bb\", hiking = \"\\uf6ec\",\n               mountain = \"\\uf6fc\", tree = \"\\uf1bb\", fish = \"\\uf578\",\n               stackoverflow = \"\\uf16c\")\n\nfig, ax = plt.subplots()\n\ndef get_marker(symbol):\n    v, codes = TextToPath().get_text_path(fp, symbol)\n    v = np.array(v)\n    mean = np.mean([np.max(v,axis=0), np.min(v, axis=0)], axis=0)\n    return Path(v-mean, codes, closed=False)\n\nx = np.random.randn(4,10)\nc = np.random.rand(10)\ns = np.random.randint(120,500, size=10)\nplt.scatter(*x[:2], s=s, c=c, marker=get_marker(symbols[\"cloud\"]), \n            edgecolors=\"none\", linewidth=2)\nplt.scatter(*x[2:], s=s, c=c, marker=get_marker(symbols[\"fish\"]), \n            edgecolors=\"none\", linewidth=2)   \n\nplt.show()\n\nAPI:\nmatplotlib.font_manager.FontProperties\nmatplotlib.path.Path\n","label":[[316,330,"Mention"],[2688,2701,"Mention"],[4217,4255,"API"],[4256,4276,"API"]],"Comments":[]}
{"id":59546,"text":"ID:52969244\nPost:\nText: Sam, \nText: First, your barcode won't scan, as is. The string requires a start character, a checksum and a stop character to be added for Code128B. So, there's that. \nText: I recommend changing to Code 39 font (which, doesn't require checksum, and start and stop characters are the same: \"*\") or writing the code to produce the checksum and learning a little more about Code 128 at Code 128 Wiki. \nText: Second, I suspect there are issues with the bounding box for the graphic during the conversion to PDF. That small section of barcode being converted looks more like a piece of the number nine in the string. I suspect there is some image clipping going on. \nText: Try substituting a regular text font to make sure the barcode image isn't being lost in the conversion. \nText: Edited answer to include suggestion to use PNG instead of PDF. \nText: I managed to get the software to work if you output to PNG format. I know, now the problem becomes how to convert PNG to PDF. You can start by investigating some of the libraries mentioned here: Create PDF from a list of images \nText: In short I recommend you create graphics files and then embed them in document files. \nText: I also added the code you need to build the barcode with the start, checksum and stop characters: \nText: import os import pyplot as plt from matplotlib import font_manager as fm def draw_label(label, label_dimensions_x=3.8189, label_dimensions_y=1.41732): # import barcode code128 font fpath = os.path.join(\".\/\", \"code128.ttf\") prop = fm.FontProperties(fname=fpath, size=32) fig, ax = plt.subplots(1, figsize=(label_dimensions_x, label_dimensions_y)) plt.axis('off') plt.xticks([], []) plt.yticks([], []) plt.tight_layout() plt.xlim(0, label_dimensions_x) plt.ylim(0, label_dimensions_y) # calc checksum THEN plot barcode weight = 1 chksum = 104 for x in label: chksum = chksum + weight*(ord(x)-32) weight = weight + 1 chksum = chksum % 103 chkchar = chr(chksum+32) label128 = \"%s%s%s%s\" % ('', label, chkchar, '') plt.text(label_dimensions_x \/ 2, label_dimensions_y \/ 2, label128, ha='center', va='bottom', fontproperties=prop) try: plt.savefig(os.path.join(\".\/\", label + '.png')) except PermissionError: logging.warning(\"Close the current label pdf's before running this script.\") return draw_label('123456789') draw_label('987654321') draw_label('Test&Show') \nAPI:\nmatplotlib.pyplot\n","label":[[1322,1328,"Mention"],[2370,2387,"API"]],"Comments":[]}
{"id":59547,"text":"ID:52981408\nPost:\nText: Not sure whether matplotlib can do it automatically, but there are several things you try: \nText: Use a log-scale y-axis, i.e. ax.set_yscale('log'). This will to some extent decrease the difference between regular points and outlier points, but you will still see all regular data lying at the bottom of the graph. Manually change the y-value of the outliers to some not-that-extreme value. You can later show those points are of some value by manually setting display-content of y-axis using plt.yticks(actual_data_array, what_to_display_array) Generate a regular (high enough) plot, then do image processing later to cut the middle part of the image. There might be a better way, but one way to do that would be plt.savefig to save the plot, mpl.image.imread to read the plot, and finally process it using plt.imshow(np.concatenate((image_data[:100], image_data[-100:]), 0)) \nText: Here is an example of the second method I was talking about: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\na = np.array(((1, 10), (2, 33), (3, 100000), (4, 17), (5, 45), (6, 8), (7, 950000)), 'f')\n\n# scale the array so that matplotlib can plot it \"uniformly\"\na[a[:,1]>99999,1] = a[a[:,1]>99999,1] \/ 20000 + 55\n\nplt.plot(*a.T)\n\n# do the displaying trick \nplt.yticks(np.r_[np.linspace(0, 50, 5),\n                 np.linspace(100000\/20000+55, 950000\/20000+55, 5)],\n           np.r_[np.linspace(0, 50, 5, dtype='i'),\n                 np.linspace(100000, 950000, 5, dtype='i')])\n\nplt.grid()\n\nText: It looks like this: \nAPI:\nmatplotlib.image.imread\n","label":[[768,784,"Mention"],[1540,1563,"API"]],"Comments":[]}
{"id":59548,"text":"ID:53085020\nPost:\nText: You may use dates formatters and locators. \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import MonthLocator, DateFormatter\n\ny = np.random.rand(72)\nx = pd.date_range('2011-03-01', periods= 72, freq= 'd')\n\nfig, ax = plt.subplots()\nax.plot(x,y)\n\nax.xaxis.set_major_locator(MonthLocator())\nax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m-%d\"))\n\nplt.show()\n\nAPI:\nmatplotlib.dates\n","label":[[36,41,"Mention"],[439,455,"API"]],"Comments":[]}
{"id":59549,"text":"ID:53140227\nPost:\nText: The error is correct, the pyplot library has no .subtitle function, only a .suptitle function. \nText: So you should fix this with: \nText: import ppylot as plt plt.figure(figsize = (15, 80)) for i, audio, rate, name in zip(range(len(audios)), audios, rates, names): plt.subplot(len(audios), 1, i+1) plt.plot(rate, audio) plt.xlabel('Time (s)') plt.ylabel('Amplitude') plt.title(name) plt.suptitle('Figure 1: Plot amplitude of signal') plt.show() \nAPI:\nmatplotlib.pyplot\n","label":[[169,175,"Mention"],[475,492,"API"]],"Comments":[]}
{"id":59550,"text":"ID:53142990\nPost:\nText: I solved this by running the following imports in the following order: \nText: import plt as plt \nText: import mpl_toolkits \nText: from mpl_toolkits.mplot3d import Axes3D \nText: Note: This only worked for me on python3. So first, I had to install python3 and pip3. Then I did \"pip3 install matplotlib\" in Terminal. If you already have matplotlib then try \"pip3 install --upgrade matplotlib\" \nAPI:\nmatplotlib.pyplot\n","label":[[109,112,"Mention"],[420,437,"API"]],"Comments":[]}
{"id":59551,"text":"ID:53233960\nPost:\nText: pylab is part of matplotlib. It is essentially a small file that makes numpy, pyplot and some cbook and mpl.dates functions available under the same namespace. \nText: You cannot install pylab on its own. And there is no pylab-version. \nText: You might notice though that the list you got states numpy 1.92 and pylab 1.9.2. Since (a) There hasn't ever been a version 1.92 of numpy, and (b) there hasn't ever been a version 1.9.2 of matplotlib, what this probably means is \nCode: numpy 1.9.2\n\nText: There are no details given as to why the installation fails. Therefore just note that I was able to install all versions, except tifffile, perfectly fine via conda(-forge) \nCode: >conda create -n sometest python=2.7 numpy=1.9.2 scipy=0.15.1 scikit-image=0.11.3 scikit-learn=0.16.1 pandas=0.16.2 matplotlib=1.4.3\n\nAPI:\nmatplotlib.pyplot\nmatplotlib.cbook\nmatplotlib.dates\n","label":[[102,108,"Mention"],[118,123,"Mention"],[128,137,"Mention"],[839,856,"API"],[857,873,"API"],[874,890,"API"]],"Comments":[]}
{"id":59552,"text":"ID:53235408\nPost:\nText: The problem is that the Path that is taken as input for the marker is normalized to the box (-0.5, 0.5)x(-0.5, 0.5). This is often useful because it allows to plug in arbitrarily scaled paths and still get the same sized markers out. However in this case it will lead to the tips of the triangles to share the same y coodinate (both times y=0.5 in the pre-transformed system). \nText: The only solution I can come up with is to subclass MarkerStyle and replace the rescaling method with a method that does not change the path. \nText: Now the problem is that plot does not currently take MarkerStyle instances as input for its marker argument. Hence the below solution only works for scatter. \nCode: import matplotlib.pyplot as plt\nfrom matplotlib.path import Path\nfrom matplotlib import transforms\nfrom matplotlib.markers import MarkerStyle\n\nclass UnsizedMarker(MarkerStyle):\n    def _set_custom_marker(self, path):\n        self._transform = transforms.IdentityTransform()\n        self._path = path\n\n\nfig = plt.figure(figsize=(10, 5))\nax = fig.add_subplot(1, 1, 1)\n\nax.set_xlim(-10, 10)\nax.set_ylim(-4, 4)\n\ncolors = ax.set_prop_cycle(color = ['#993F00', '#0075DC'])\n\ntriangle1 = Path([[-1.,-1.], [1., -1.], [0., 2.], [0.,0.],], \n                [Path.MOVETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY,])\n\n\nm1 = UnsizedMarker(triangle1)\nline1 = ax.scatter(0, 0, marker=m1, s=50**2, alpha=0.5)\n\nR = transforms.Affine2D().rotate_deg(45)\ntriangle2 = triangle1.transformed(R)\nm2 = UnsizedMarker(triangle2)\nline2 = ax.scatter(0, 0, marker=m2, s=50**2, alpha=0.5)\n\n\nplt.show()\n\nText: Unrelated to that, the question also asks for what a Patch is vs. a Path. A Path is an object that stores vertices and their respective properties. It's main purpose, which makes this more useful than a simple container of two lists or coordinates, is that it has methods to calculate Bezier curves between points. A path by itself can however not be added to a figure. \nText: A Patch is a matplotlib artist, i.e. an object which can be drawn in a matplotlib figure. Matplotlib provides some useful patches like arrows, rectangles, circles etc. A PathPatch is such a patch which creates a visualization of a Path. It's hence the most obvious way to draw a path in a figure. \nAPI:\nmatplotlib.markers.MarkerStyle\n","label":[[460,471,"Mention"],[2279,2309,"API"]],"Comments":[]}
{"id":59553,"text":"ID:53238322\nPost:\nText: It seems that the line is being clipped. \nText: plot can be passed an option to configure clipping. \nCode: ax.plot([1.4, 0], [0, 0], linewidth=2, color='k', clip_on=False)\n\nAPI:\nmatplotlib.axes.Axes.plot\n","label":[[72,76,"Mention"],[202,227,"API"]],"Comments":[]}
{"id":59554,"text":"ID:53239732\nPost:\nText: As shown in the OP, import pyplot as plt is present, but it didn't get executed. You only executed the selected line (9) with plt.show(), not the whole file. You can see the problem by carefully reading the traceback. Line 9 in the script is line 1 in the traceback: ----> 1 plt.show() The solution is to run the whole file, not one line. Click Run All not Run. \nAPI:\nmatplotlib.pyplot\n","label":[[51,57,"Mention"],[392,409,"API"]],"Comments":[]}
{"id":59555,"text":"ID:53255809\nPost:\nText: Since wordcloud produces an image, and plotly's conversion function cannot currently handle images, you would need to somehow regenerate the wordcloud from the positions, sizes and orientations of the wordcloud.wordcloud.WordCloud object. \nText: Those information are stored in the .layout_ attribute \nCode: wc = Wordcloud(...)\nwc.generate(text)\nprint(wc.layout_)\n\nText: prints a list of tuples of the form \nCode: [(word, freq), fontsize, position, orientation, color]\n\nText: e.g. in this case \nCode: [(('Wikipedia', 1.0), 100, (8, 7), None, 'rgb(56, 89, 140)'), \n (('articles', 0.4444444444444444), 72, (269, 310), None, 'rgb(58, 186, 118)'), ...]\n\nText: So in principle this allows to regenerate the wordcloud as text. However care must be taken for the little details. I.e. the font and fontsize need to be the same. \nText: Here is a pure matplotlib example, which reproduces the wordcloud with Text objects. \nCode: import numpy as np\nfrom wordcloud import WordCloud, STOPWORDS \nfrom wordcloud.wordcloud import FONT_PATH\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\nword_list = \"Wikipedia was launched on January 15, 2001, by Jimmy Wales and Larry Sanger.[10] Sanger coined its name,[11][12] as a portmanteau of wiki[notes 3] and 'encyclopedia'. Initially an English-language encyclopedia, versions in other languages were quickly developed. With 5,748,461 articles,[notes 4] the English Wikipedia is the largest of the more than 290 Wikipedia encyclopedias. Overall, Wikipedia comprises more than 40 million articles in 301 different languages[14] and by February 2014 it had reached 18 billion page views and nearly 500 million unique visitors per month.[15] In 2005, Nature published a peer review comparing 42 science articles from Encyclopdia Britannica and Wikipedia and found that Wikipedia's level of accuracy approached that of Britannica.[16] Time magazine stated that the open-door policy of allowing anyone to edit had made Wikipedia the biggest and possibly the best encyclopedia in the world and it was testament to the vision of Jimmy Wales.[17] Wikipedia has been criticized for exhibiting systemic bias, for presenting a mixture of 'truths, half truths, and some falsehoods',[18] and for being subject to manipulation and spin in controversial topics.[19] In 2017, Facebook announced that it would help readers detect fake news by suitable links to Wikipedia articles. YouTube announced a similar plan in 2018.\"\n\ndef get_wordcloud(width, height):\n    wc = WordCloud(background_color='black',\n                    stopwords = set(STOPWORDS),\n                    max_words = 200,\n                    max_font_size = 100, \n                    random_state = 42,\n                    width=int(width), \n                    height=int(height),\n                    mask = None)\n    wc.generate(word_list)\n    return wc\n\n\nfig, (ax, ax2) = plt.subplots(nrows=2, sharex=True, sharey=True)\n\nfp=FontProperties(fname=FONT_PATH)\nbbox = ax.get_position().transformed(fig.transFigure)\nwc = get_wordcloud(bbox.width, bbox.height)\n\nax.imshow(wc)\n\nax2.set_facecolor(\"black\")\nfor (word, freq), fontsize, position, orientation, color in wc.layout_:\n    color = np.array(color[4:-1].split(\", \")).astype(float)\/255.\n    x,y = position\n    rot = {None : 0, 2: 90}[orientation]\n    fp.set_size(fontsize*72.\/fig.dpi)\n    ax2.text(y,x, word, va=\"top\", ha=\"left\", color=color, rotation=rot, \n             fontproperties=fp)\n\nprint(wc.layout_)\nplt.show()\n\nText: The upper plot is the wordcloud image shown via imshow, the lower plot is the regenerated wordcloud. \nText: Now you might want to do the same in plotly instead of matplotlib, but I'm not profilient enough with plotly to directly give a solution here. \nAPI:\nmatplotlib.text.Text\n","label":[[922,926,"Mention"],[3773,3793,"API"]],"Comments":[]}
{"id":59556,"text":"ID:53255882\nPost:\nText: Personally, I tend to use usbplots for these kinds of situations. If your images are really heterogenous it might be a better choice than the image concatenation based approach in the answer you linked to. \nCode: import matplotlib.pyplot as plt\nfrom scipy.misc import face\n\nx = 4\ny = 4\n\nfig,axarr = plt.subplots(x,y)\nims = [face() for i in range(x*y)]\n\nfor ax,im in zip(axarr.ravel(), ims):\n    ax.imshow(im)\n\nfig.savefig('faces.png')\n\nText: My big complaint about subplots is the quantity of whitespace in the resulting figure. As well, for your application you may not want the axes ticks\/frames. Here's a wrapper function that deals with those issues: \nCode: import matplotlib.pyplot as plt\n\ndef savegrid(ims, rows=None, cols=None, fill=True, showax=False):\n    if rows is None != cols is None:\n        raise ValueError(\"Set either both rows and cols or neither.\")\n\n    if rows is None:\n        rows = len(ims)\n        cols = 1\n\n    gridspec_kw = {'wspace': 0, 'hspace': 0} if fill else {}\n    fig,axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw)\n\n    if fill:\n        bleed = 0\n        fig.subplots_adjust(left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed))\n\n    for ax,im in zip(axarr.ravel(), ims):\n        ax.imshow(im)\n        if not showax:\n            ax.set_axis_off()\n\n    kwargs = {'pad_inches': .01} if fill else {}\n    fig.savefig('faces.png', **kwargs)\n\nText: Running savegrid(ims, 4, 4) on the same set of images as used earlier yields: \nText: If you use savegrid, if you want each individual image to take up less space, pass the fill=False keyword arg. If you want to show the axes ticks\/frames, pass showax=True. \nAPI:\nmatplotlib.pyplot.subplots\n","label":[[50,58,"Mention"],[1682,1708,"API"]],"Comments":[]}
{"id":59557,"text":"ID:53312151\nPost:\nText: There are several things called \"labels\" in matplotlib. There are e.g. axis labels, set via set_xlabel, there are tick labels, set via set_ticklabels, and there are artist labels, set via set_label. \nText: Every artist in matplotlib has a label attribute. You usually encounter it when setting a label for it to be shown in the legend, \nCode: plt.plot(..., label=\"my label\")\n\nText: but it may be helpful for other cases as well. The above is equivalent to \nCode: line, = plt.plot(...)\nline.set_label(\"mylabel\")\n\nText: So not only the Line2D object created by plot has a label, but equally the XAxis has a label. This is the one you set with ax.yaxis.set_label. However, this is not by default taken into account when producing a legend, so its usefulness is questionable for the end user. \nText: For the labels of the axes, set_ylabel is really the method to use for setting the ylabel. Internally, this would call yaxis.set_label_text, so as correctly pointed out by @DavidG, you can replace ax.set_ylabel(..) by ax.yaxis.set_label_text(..) (except for an additional argument labelpad that is taken by set_ylabel, but not by set_label_text. \nAPI:\nmatplotlib.axis.XAxis\n","label":[[617,622,"Mention"],[1172,1193,"API"]],"Comments":[]}
{"id":59558,"text":"ID:53335131\nPost:\nText: This is more pseudo-code than actual code since you do not provide much information or the code you have written. Still, this can give you some hints on how to combine pandas.DataFrame.plot and bar with different y axis. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nwidth = 1\nfig, ax1 = plt.subplots()\nvalue_x = np.arange(cir['value'])\nax1.bar(x, cir['value'], width = 3*width)\nax2 = ax1.twinx()\ndf.unstack('Size').plot(kind='bar', stacked=True, ax=ax2, secondary_y=True, width=width)\n\nplt.show()\n\nText: Note that the trick is to use ax2 = ax1.twinx() and plot your series on a different axis. \nText: About width, I am not sure if this will work since I do not know what is your x axis. But this is just to give you an idea on how to do it. \nAPI:\nmatplotlib.pyplot.bar\n","label":[[218,221,"Mention"],[805,826,"API"]],"Comments":[]}
{"id":59559,"text":"ID:53354287\nPost:\nText: If you plot a dataframe with the x_compat=True option you can be certain that the units are mdates units. Else, pandas may decide some units for you. \nText: Here, pandas uses matplotlib dates units. They are defined as: \nText: Matplotlib represents dates using floating point numbers specifying the number of days since 0001-01-01 UTC, plus 1. For example, 0001-01-01, 06:00 is 1.25, not 0.25. Values < 1, i.e. dates before 0001-01-01 UTC are not supported. \nText: You are however not expected to calculate those units yourself. Matplotlib provides helper functions \nText: mpl.dates.date2num mdates.num2date mpl.dates.datestr2num \nText: which you can use to calculate back and forth between the units. \nText: As an example, matplotlib.dates.datestr2num(\"2018-11-15 19:27:48\") gives you 737013.8109722222. \nAPI:\nmatplotlib.dates\nmatplotlib.dates.date2num\nmatplotlib.dates.num2date\nmatplotlib.dates.datestr2num\n","label":[[116,122,"Mention"],[597,615,"Mention"],[616,631,"Mention"],[632,653,"Mention"],[835,851,"API"],[852,877,"API"],[878,903,"API"],[904,932,"API"]],"Comments":[]}
{"id":59560,"text":"ID:53385398\nPost:\nText: The reason you don't want to repeatedly call the plt.specgram function inside the animation is indeed performance. After N cycles you have N images in your figure, which makes drawing more and more expensive. \nText: Of course a possible solution is to remove the previous image in each iteration, e.g. by having a handle to it, (im.remove()) or via the list of images (ax.images[0].remove()). \nText: However, you are right that the more desireable solution is to not recreate any image at all, but instead only change the image data. \nText: In that case you will want to call mpl.mlab.specgram to obtain the spectrum as numpy array and use the set_array() method of the image to update the image in the animation. \nText: Note however, that this might require you to update the color limits of the image as well, if different spectra have different minimum or maximum amplitudes. \nText: Because the image shown by plt.specgram is not directly the spectrogram returned by mlab.specgram, you may then need to set some parameters manually. Especially, by default, the image is shown on a dB scale. \nText: I think the equivalent to \nCode: Fs = rate\nNFFT = 256\nnoverlap= 128\nspec, freqs, t, im = plt.specgram(sound, Fs=Fs, NFFT=NFFT, noverlap=noverlap)\n\nText: would be \nCode: spec, freqs, t = plt.mlab.specgram(sound, Fs=rate, NFFT=NFFT, noverlap=noverlap)\npad_xextent = (NFFT-noverlap) \/ Fs \/ 2\nxmin, xmax = np.min(t) - pad_xextent, np.max(t) + pad_xextent\nextent = xmin, xmax, freqs[0], freqs[-1]\nim = plt.imshow(np.flipud(10. * np.log10(spec)), extent=extent, aspect=\"auto\")\n\nAPI:\nmatplotlib.mlab.specgram\n","label":[[600,617,"Mention"],[1602,1626,"API"]],"Comments":[]}
{"id":59561,"text":"ID:53581838\nPost:\nText: Lets go through your requirements step-by-step: \nText: You need to read the CSV file. This can be done in several ways. I've majorly used both pandas and the csv libraries. You can use any of them or some other method if you find any. You need to plot all of them in the same picture. For this you can use a directory by the name matplotlib. You'll have to import a specific section of it as the library is huge to be imported while running. I suggest you use the following code: import pplot as plt. In the imported section you can use a function called plt.plot() which can take in any number of arrays and plot them. Coming to your last requirement, you might want to refer to tkinter or any other such python GUI libraries. \nAPI:\nmatplotlib.pyplot\n","label":[[511,516,"Mention"],[758,775,"API"]],"Comments":[]}
{"id":59562,"text":"ID:53583476\nPost:\nText: I'm not certain, but I think the clue is in the MaxNLocator object that gridliner defaults to using: \nCode: print(gl.xlocator.tick_values(*ax.get_xlim()))\nprint(gl.ylocator.tick_values(*ax.get_ylim()))\n\nText: gives \nCode: [-180. -120.  -60.    0.   60.  120.  180.]\n[-100.  -80.  -60.  -40.  -20.    0.   20.   40.   60.   80.  100.]\n\nAPI:\nmatplotlib.ticker.MaxNLocator\n","label":[[72,83,"Mention"],[364,393,"API"]],"Comments":[]}
{"id":59563,"text":"ID:12302566\nPost:\nText: You are mixing different concepts, I'm afraid. \nText: Your arrtup array is not an array of tuples, it's a structured ndarray, that is, an array of elements that look like tuples but in fact are records (numpy.void objects, to be exact). In your case, you defined these records to consist in 2 integers. Internally, NumPy creates your array as a 2x2 array of blocks, each block taking a given space defined by your dtype: here, a block consists of 2 consecutive blocks of size int (that is, each sub-block takes the space an int takes on your machine). \nText: When you retrieve an element with arrtup[0,1], you get the corresponding block. Because this block is structured as two-subblocks, NumPy returns a np.void (the generic object representing structured blocks), which has the same dtype as your array. \nText: Because you set the size of those blocks at the creation of the array, you're no longer able to modify it. That means that you cannot transform your 2-int records into 4-int ones as you want. \nText: However, you can transform you structured array into an array of objects: \nCode: new_arr = arrtup.astype(object)\n\nText: Lo and behold, your elements are no longer np.void but tuples, that you can modify as you want: \nCode: new_arr[0,1] = (3,4) # That's a tuple\nnew_arr[0,1] += (4,4) # Adding another tuple to the element\n\nText: Your new_arr is a different beast from your arrtup: it has the same size, true, but it's no longer a structured array, it's an array of objects, as illustrated by \nCode: >>> new_arr.dtype\ndtype(\"object\")\n\nText: In practice, the memory layout is quite different between arrtup and newarr. newarr doesn't have the same constraints as arrtup, as the individual elements can have different sizes, but object arrays are not as efficient as structured arrays. \nAPI:\nnumpy.void\n","label":[[730,737,"Mention"],[1825,1835,"API"]],"Comments":[]}
{"id":59564,"text":"ID:12393965\nPost:\nText: One workaround I can think of would be converting the Python function to ufunc with numpy.frompyfunc: \nCode: numpy.frompyfunc((lambda x: x[0:2]), 1, 1)\n\nText: and use this in apply: \nCode: In [50]: dfrm_test\nOut[50]:\n     A\n0  the\n1  the\n2  the\n3  the\n4  the\n5  the\n6  the\n7  the\n8  the\n9  the\n\nIn [51]: dfrm_test[\"A\"].apply(np.frompyfunc((lambda x: x[0:2]), 1, 1))\nOut[51]:\n0    th\n1    th\n2    th\n3    th\n4    th\n5    th\n6    th\n7    th\n8    th\n9    th\nName: A\n\nIn [52]: pandas.version.version\nOut[52]: '0.7.3'\n\nIn [53]: dfrm_test[\"A\"].apply(lambda x: x[0:2])\nOut[53]:\n0    the\n1    the\nName: A\n\nAPI:\nnumpy.ufunc\n","label":[[97,102,"Mention"],[627,638,"API"]],"Comments":[]}
{"id":59565,"text":"ID:12404419\nPost:\nText: np.meshgrid is modelled after Matlab's meshgrid command. It is used to vectorise functions of two variables, so that you can write \nCode: x = numpy.array([1, 2, 3])\ny = numpy.array([10, 20, 30]) \nXX, YY = numpy.meshgrid(x, y)\nZZ = XX + YY\n\nZZ => array([[11, 12, 13],\n             [21, 22, 23],\n             [31, 32, 33]])\n\nText: So ZZ contains all the combinations of x and y put into the function. When you think about it, meshgrid is a bit superfluous for numpy arrays, as they broadcast. This means you can do \nCode: XX, YY = numpy.atleast_2d(x, y)\nYY = YY.T # transpose to allow broadcasting\nZZ = XX + YY\n\nText: and get the same result. \nText: mgrid and ogrid are helper classes which use index notation so that you can create XX and YY in the previous examples directly, without having to use something like linspace. The order in which the output are generated is reversed. \nCode: YY, XX = numpy.mgrid[10:40:10, 1:4]\nZZ = XX + YY # These are equivalent to the output of meshgrid\n\nYY, XX = numpy.ogrid[10:40:10, 1:4]\nZZ = XX + YY # These are equivalent to the atleast_2d example\n\nText: I am not familiar with the scitools stuff, but ndgrid seems equivalent to meshgrid, while BoxGrid is actually a whole class to help with this kind of generation. \nAPI:\nnumpy.meshgrid\n","label":[[24,35,"Mention"],[1283,1297,"API"]],"Comments":[]}
{"id":59566,"text":"ID:12443736\nPost:\nText: Use the numpy.min and np.max builtins: \nCode: def bounding_box(iterable):\n    min_x, min_y = numpy.min(iterable[0], axis=0)\n    max_x, max_y = numpy.max(iterable[0], axis=0)\n    return numpy.array([(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)])\n\nAPI:\nnumpy.max\n","label":[[46,52,"Mention"],[293,302,"API"]],"Comments":[]}
{"id":59567,"text":"ID:12496595\nPost:\nText: lexsort will work here: \nCode: A[np.lexsort(A.T)]\n\nText: You need to transpose A before passing it to lexsort because when passed a 2d array it expects to sort by rows (last row, second last row, etc). \nText: The alternative possibly slightly clearer way is to pass the columns explicitly: \nCode: A[np.lexsort((A[:, 0], A[:, 1]))]\n\nText: You still need to remember that lexsort sorts by the last key first (there's probably some good reason for this; it's the same as performing a stable sort on successive keys). \nAPI:\nnumpy.lexsort\n","label":[[24,31,"Mention"],[544,557,"API"]],"Comments":[]}
{"id":59568,"text":"ID:12539503\nPost:\nText: You can use the asarray function to convert any sequence-like input to an array: \nCode: >>> import numpy\n>>> numpy.asarray([1,2,3])\narray([1, 2, 3])\n>>> numpy.asarray(numpy.array([2,3]))\narray([2, 3])\n>>> numpy.asarray(1)\narray(1)\n>>> numpy.asarray((2,3))\narray([2, 3])\n>>> numpy.asarray({1:3,2:4})\narray({1: 3, 2: 4}, dtype=object)\n\nText: It's important to note that as the documentation says No copy is performed if the input is already an ndarray. This is really nice since you can pass an existing array in and it just returns the same array. \nText: Once you convert it to a numpy array, just check the length if that's a requirement. Something like: \nCode: >>> def f(x):\n...    x = numpy.asarray(x)\n...    if len(x) != 2:\n...       raise Exception(\"invalid argument\")\n... \n>>> f([1,2])\n>>> f([1,2,3])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 4, in f\nException: invalid argument\n\nText: Update: Since you asked, here's a \"magic\" function that will except *args as an array also: \nCode: >>> def f(*args):\n...    args = numpy.asarray(args[0]) if len(args) == 1 else numpy.asarray(args)\n...    return args\n... \n>>> f(7,3,5)\narray([7, 3, 5])\n>>> f([1,2,3])\narray([1, 2, 3])\n>>> f((2,3,4))\narray([2, 3, 4])\n>>> f(numpy.array([1,2,3]))\narray([1, 2, 3])\n\nAPI:\nnumpy.asarray\n","label":[[40,47,"Mention"],[1335,1348,"API"]],"Comments":[]}
{"id":59569,"text":"ID:12550220\nPost:\nText: I had a similar problem. If you just want to read my solution skip some lines :) I had to: \nText: share a np.array between threads operating on different part of it and... pass Pool.map a function with more then one argument. \nText: I noticed that: \nText: the data of the qarray was correctly read but... changes on the arras where not made permanent Pool.map had problems handling lambda functions, or so it appeared to me (if this point is not clear to you, just ignore it) \nText: My solution was to: \nText: make the target function only argument a list make the target function return the modified data instead of directly trying to write on the ar \nText: I understand that your do_work function already return the computed data, so you would just have to modify to_work to accept a list (containing X,param_1,param_2 and arg) as argument and to pack the input to the target function in this format before passing it to Pool.map. \nText: Here is a sample implementation: \nCode: def do_work2(args):\n    X,param_1,param_2,arg = args\n    return heavy_computation(X, param_1, param_2, arg)\n\nText: Now you have to pack the input to the do_work function before calling it. Your main become: \nCode: if __name__=='__main__':\n   filename = raw_input(\"Filename> \")\n   param_1 = float(raw_input(\"Parameter 1: \"))\n   param_2 = float(raw_input(\"Parameter 2: \"))\n   X = parse_numpy_array(filename)\n   # now you pack the input arguments\n   arglist = [[X,param1,param2,n] for n in linspace(0.0,1.0,100)]\n   # consider that you're not making 100 copies of X here. You're just passing a reference to it\n   results = Pool.map(do_work2,arglist)\n   #save results in a .npy file for analysis\n   save(\"Results\", [X,results])\n\nAPI:\nnumpy.array\nnumpy.array\nnumpy.array\nnumpy.array\n","label":[[130,138,"Mention"],[296,302,"Mention"],[344,349,"Mention"],[673,675,"Mention"],[1734,1745,"API"],[1746,1757,"API"],[1758,1769,"API"],[1770,1781,"API"]],"Comments":[]}
{"id":59570,"text":"ID:12564813\nPost:\nText: You should be able to use np.interp for this. \nText: e.g.: \nCode: numpy.interp(3.,y,x)  #1.5\n\nText: Note that this only works since your y values are monotonic. If your y-values aren't monotonic, there is no guarantee that your mapping of y->x is unique. \nAPI:\nnumpy.interp\n","label":[[50,59,"Mention"],[285,297,"API"]],"Comments":[]}
{"id":59571,"text":"ID:12584701\nPost:\nText: np.linalg.lstsq solves this for you. Object-oriented wrappers for that function, as well as more advanced regression models, are available in both scikit-learn and StatsModels. \nText: (Disclaimer: I'm a scikit-learn developer, so this is not the most unbiased advice ever.) \nAPI:\nnumpy.linalg.lstsq\n","label":[[24,39,"Mention"],[304,322,"API"]],"Comments":[]}
{"id":59572,"text":"ID:12598831\nPost:\nText: What you probabply want to use is the whexe function. Use it like this: \nCode:     >>>unicorns=np.array([[1, \"black\", 0.0, 'Pinky', 1] ,\n                       [2, \"black\", 0.0, 'Winky', 1],\n                       [3, \"white\", 0.0, 'Lala', 1],\n                       [4, \"white\", 0.0, 'Merlin', 1],\n                       [5, \"black\", 0.0, 'Meriva', 1],\n                       [6, \"white\", 0.0, 'Panda', 1]])\n    >>> np.where(unicorns[:,1] == \"black\")\n    (array([0, 1, 4]),)\n    >>> unicorns[np.where(unicorns[:,1] == \"black\")]\n    array([['1', 'black', '0.0', 'Pinky', '1'],\n    ['2', 'black', '0.0', 'Winky', '1'],\n    ['5', 'black', '0.0', 'Meriva', '1']], \n    dtype='|S8')\n\nAPI:\nnumpy.where\n","label":[[62,67,"Mention"],[709,720,"API"]],"Comments":[]}
{"id":59573,"text":"ID:12654144\nPost:\nText: Instead of storing your strings as variable length data in the numpy array, you could try storing them as Python objects instead. Numpy will treat these as references to the original Python string objects, and you can then treat them like you might expect: \nCode: t = np.array([['one','two','three'],['four','five','six']], dtype=object)\nnp.min(t)\n# gives 'five'\nnp.max(t)\n# gives 'two'\n\nText: Keep in mind that here, the np.min and np.max calls are ordering the strings lexicographically - so \"two\" does indeed come after \"five\". To change the comparison operator to look at the length of each string, you could try creating a new numpy array identical in form, but containing each string's length instead of its reference. You could then do a np.argmin call on that array (which returns the index of the minimum) and look up the value of the string in the original array. \nText: Example code: \nCode: # Vectorize takes a Python function and converts it into a Numpy\n# vector function that operates on arrays\nnp_len = np.vectorize(lambda x: len(x))\n\nnp_len(t)\n# gives array([[3, 3, 5], [4, 4, 3]])\n\nidx = np_len(t).argmin(0) # get the index along the 0th axis\n# gives array([0, 0, 1])\n\nresult = t\nfor i in idx[1:]:\n    result = result[i]\nprint result\n# gives \"two\", the string with the smallest length\n\nAPI:\nnumpy.argmin\n","label":[[769,778,"Mention"],[1332,1344,"API"]],"Comments":[]}
{"id":59574,"text":"ID:12716271\nPost:\nText: No, you can't, at least with current version of NumPy. A nan is a special value for float arrays only. \nText: There are talks about introducing a special bit that would allow non-float arrays to store what in practice would correspond to a nan, but so far (2012\/10), it's only talks. \nText: In the meantime, you may want to consider the numpy.ma package: instead of picking an invalid integer like -99999, you could use the special masked value to represent an invalid value. \nCode: a = np.ma.array([1,2,3,4,5], dtype=int)\na[1] = np.ma.masked\nmasked_array(data = [1 -- 3 4 5],\n             mask = [False  True False False False],\n       fill_value = 999999)\n\nAPI:\nnumpy.ma.masked\n","label":[[456,462,"Mention"],[688,703,"API"]],"Comments":[]}
{"id":59575,"text":"ID:12900982\nPost:\nText: Pandas is a good idea, so \"thumbs up\" to the answer by reptilicus. \nText: If you don't want the dependency on Pandas, you can just as easily use the function genfromtxt to read the data directly into a numpy structured array. A structure array acts like both a numpy 1-d array and a dictionary. \nText: For example, here's a sample data file, \"data.csv\": \nCode: alpha, beta, gamma\n100, 0.5, 19.9\n210, 0.25, 21.0\n240, 0.45, 15.0\n290, 0.75, 5.5\n\nText: You can read this into a structured array as follows: \nCode: >>> data = genfromtxt('data.csv', delimiter=',', names=True, dtype=None)\n\nText: The option names=True tells genfromtxt to use the columns headings as the field names in the structured array. Setting dtype=None tells genfromtxt to figure out the data type of the columns automatically (the default is to convert all values to double precision floating point values). \nText: data looks like this. \nCode: >>> data\narray([(100, 0.5, 19.9), (210, 0.25, 21.0), (240, 0.45, 15.0),\n       (290, 0.75, 5.5)], \n      dtype=[('alpha', '<i4'), ('beta', '<f8'), ('gamma', '<f8')])\n\nText: You can access individual elements (each is a structure containing three fields): \nCode: >>> data[0]\n(100, 0.5, 19.9)\n\nText: Or you can access columns using the dictionary-like interface: \nCode: >>> data['beta']\narray([ 0.5 ,  0.25,  0.45,  0.75])\n\nText: And you can combine those: \nCode: >>> data['beta'][1]\n0.25\n>>> data[1]['beta']\n0.25\n\nAPI:\nnumpy.genfromtxt\n","label":[[182,192,"Mention"],[1454,1470,"API"]],"Comments":[]}
{"id":59576,"text":"ID:12969793\nPost:\nText: You cannot vectorize that diffusion calculation in the time dimension of the problem, that still requires a loop. The only obvious optimization here is to replace the Laplacian calculation with a call to the dif function (which is precompiled C), so your heat equation solver becomes: \nCode: def heat(D,u0,q,tdim): \n    xdim = np.size(u0) \n    Z = np.zeros([xdim,tdim]) \n    Z[:,0]=u0; \n\n    for i in range(1,tdim): \n        Z[1:-1,i]=Z[1:-1,i-1] + D*q*np.diff(Z[:,i-1], 2)\n\n    return Z\n\nText: For non-trivial spatial sizes you should see considerable speed up. \nAPI:\nnumpy.diff\n","label":[[232,235,"Mention"],[593,603,"API"]],"Comments":[]}
{"id":59577,"text":"ID:12984112\nPost:\nText: If the shape of your array was (t, n)--so the data for each n-vector was contiguous in memory--you could create a view of the 2-d array as a 1-d structured array, and then use np.unique on this view. \nText: If you can change the storage convention of your array, or if you don't mind making a copy of the transposed array, this could work for you. \nText: Here's an example: \nCode: import numpy as np\n\n# Demo data.\nx = np.array([[1,2,3],\n              [2,0,0],\n              [1,2,3],\n              [3,2,2],\n              [2,0,0],\n              [2,1,2],\n              [3,2,1],\n              [2,0,0]])\n\n# View each row as a structure, with field names 'a', 'b' and 'c'.\ndt = np.dtype([('a', x.dtype), ('b', x.dtype), ('c', x.dtype)])\ny = x.view(dtype=dt).squeeze()\n\n# Now np.unique can be used.  See the `unique` docstring for\n# a description of the options.  You might not need `idx` or `inv`.\nu, idx, inv = np.unique(y, return_index=True, return_inverse=True)\n\nprint(\"Unique vectors\")\nprint(u)\n\nAPI:\nnumpy.unique\n","label":[[200,209,"Mention"],[1023,1035,"API"]],"Comments":[]}
{"id":59578,"text":"ID:12996094\nPost:\nText: The where function is your friend. Because it's implemented to take full advantage of the array datatype, for large images you should notice a speed improvement over the pure python solution you provide. \nText: Using ohere directly will yield a boolean mask indicating whether certain values match your conditions: \nCode: >>> data\narray([[1, 8],\n       [3, 4]])\n>>> numpy.where( data > 3 )\n(array([0, 1]), array([1, 1]))\n\nText: And the mask can be used to index the array directly to get the actual values: \nCode: >>> data[ numpy.where( data > 3 ) ]\narray([8, 4])\n\nText: Exactly where you take it from there will depend on what form you'd like the results in. \nAPI:\nnumpy.where\nnumpy.where\n","label":[[28,33,"Mention"],[241,246,"Mention"],[690,701,"API"],[702,713,"API"]],"Comments":[]}
{"id":59579,"text":"ID:13021436\nPost:\nText: I think the problem is that your numpy arrays have type np.int64 or something similar, which PIL does not understand as an int that it can use to index into the image. \nText: Try this, which converts all the numpy.int64s to Python ints: \nCode: # round to int and convert to int    \nxx = map(int, np.rint(x).astype(int)) \nyy = map(int, np.rint(y).astype(int))\n\nText: In case you're wondering how I figured this out, I used the type function on a value from a numpy array: \nCode: >>> a = np.array([[1.3, 403.2], [1.0, 0.3]])\n>>> b = np.rint(a).astype(int)\n>>> b.dtype\n dtype('int64')\n>>> type(b[0, 0])\n numpy.int64\n>>> type(int(b[0, 0]))\n int\n\nAPI:\nnumpy.int64\n","label":[[80,88,"Mention"],[671,682,"API"]],"Comments":[]}
{"id":59580,"text":"ID:13042687\nPost:\nText: The Basic Idea \nText: Here's what I came up with: \nCode: >>> import numpy as np\n>>> l = [['hotel','good','bad'],['hilton',1,2],['ramada',3,4]]\n>>> a = np.array(l) # convert to a numpy array to make multi-dimensional slicing possible\n>>> a\narray([['hotel', 'good', 'bad'],\n       ['hilton', '1', '2'],\n       ['ramada', '3', '4']], \n      dtype='|S4')\n>>> a[1:,1:] # exclude the first row and the first column\narray([['1', '2'],\n       ['3', '4']], \n      dtype='|S4')\n>>> a[1:,1:].astype(np.float32) # convert to float\narray([[ 1.,  2.],\n       [ 3.,  4.]], dtype=float32)\n\nText: You can pass your 2d list to the numpy array constructor, slice the 2d array to get rid of the first row and column and then use the astype method to convert everything to a float. \nText: All on one line, that'd be: \nCode: >>> l = [['hotel','good','bad'],['hilton',1,2],['ramada',3,4]]\n>>> np.array(l)[1:,1:].astype(np.float32)\narray([[ 1.,  2.],\n       [ 3.,  4.]], dtype=float32)\n\nText: The ValueError \nText: You're getting a ValueError because you actually have a jagged array. Using the variable new_list from the code in your question you can prove this to yourself: \nCode: >>> [len(x) for x in new_list]\n[9, 9, 9, 9, 9, 9, 9, 9, 9, 8]\n\nText: The last row is only of length 8, instead of 9, like all the others. Given a 2d jagged list, the np.array constructor will create a 1d numpy array with a dtype of object. The entries in that array are Python lists. The astype call is attempting to convert Python lists to float32, which is failing. I'm guessing this was just a case of human error. If you fix the missing entry, you should be good to go. \nAPI:\nnumpy.array\n","label":[[1349,1357,"Mention"],[1663,1674,"API"]],"Comments":[]}
{"id":59581,"text":"ID:13049870\nPost:\nText: In your particular example, you could use np.vstack : \nCode: import numpy as np\n\n\na = np.array([[[1,2,3],4],[[4,5,6],5]])\nb = a[:,0]\n\nc = np.vstack(b)\nprint c.shape # (2,3)\n\nText: EDIT : Since your array a is not a real matrix but a collection of arrays (as pointed by wim ), you can also do the following : \nCode:    b = np.array([ line for line in a[:,0]])\n   print b.shape #(2,3)\n\nAPI:\nnumpy.vstack\n","label":[[66,75,"Mention"],[413,425,"API"]],"Comments":[]}
{"id":59582,"text":"ID:13050450\nPost:\nText: Make your data, matrix, a nda object, instead of a list of lists, and then just do matrix.sum(axis=1). \nCode: >>> matrix = np.asarray([[ 47,  43,  51,  81,  54,  81,  52,  54,  31,  46],\n  [ 35,  21,  30,  16,  37,  11,  35,  30,  39,  37],\n  [  8,  17,  11,   2,   5,   4,  11,   9,  17,  10],\n  [  5,   9,   4,   0,   1,   1,   0,   3,   9,   3],\n  [  2,   7,   2,   0,   0,   0,   0,   1,   2,   1],\n  [215, 149, 299, 199, 159, 325, 179, 249, 249, 199],\n  [ 27,  49,  24,   4,  21,   8,  35,  15,  45,  25],\n  [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]])\n\n>>> print matrix.sum(axis=1)\n[ 540  291   94   35   15 2222  253 1000]\n\nText: To get the first five rows from the result, you can just do: \nCode: >>> row_sums = matrix.sum(axis=1)\n>>> rows_0_through_4_sums = row_sums[:5]\n>>> print rows_0_through_4_sums\n[540 291  94  35  15]\n\nText: Or, you can alternatively sub-select only those rows to begin with and only apply the summation to them: \nCode: >>> rows_0_through_4 = matrix[:5,:]\n>>> print rows_0_through_4.sum(axis=1)\n[540 291  94  35  15]\n\nText: Some helpful links will be: \nText: NumPy for Matlab Users, if you are familiar with these things in Matlab\/Octave \nText: Slicing\/Indexing in NumPy \nAPI:\nnumpy.ndarray\n","label":[[50,53,"Mention"],[1242,1255,"API"]],"Comments":[]}
{"id":59583,"text":"ID:13149675\nPost:\nText: the size of X is 100e6 x 10 the size of Y is 100e6 x 1 \nText: so the final size of (X^T*X)^-1 * X^T * Y is 10 x 1 \nText: you can calculate it by following step: \nText: calculate a = X^T*X -> 10 x 10 calculate b = X^T*Y -> 10 x 1 calculate a^-1 * b \nText: matrixs in step 3 is very small, so you just need to do some intermediate steps to calculate 1 & 2. \nText: For example you can read column 0 of X and Y, and calculate it by numpy.dot(X0, Y). \nText: for float64 dtype, the size of X0 and Y is about 1600M, if it cann't fit the memory, you can call np.dot twice for the first half and second half of X0 & Y separately. \nText: So to calculate X^T*Y you need call np.dot 20 times, to calculate X^T*X you need call dot 200 times. \nAPI:\nnumpy.dot\nnumpy.dot\nnumpy.dot\n","label":[[575,581,"Mention"],[688,694,"Mention"],[738,741,"Mention"],[759,768,"API"],[769,778,"API"],[779,788,"API"]],"Comments":[]}
{"id":59584,"text":"ID:13220992\nPost:\nText: 1) Writing: \nCode: In [1]: f = open('ints','wb')\nIn [2]: x = numpy.int16(array([1,2,3]))\nOut[2]: array([1, 2, 3], dtype=int16)\nIn [3]: f.write(x)\nIn [4]: f.close()\n\nText: 2) Reading: \nCode: In [5]: f = open('ints','wb')\nIn [6]: x = f.read()\nIn [7]: x\nOut[7]: '\\x01\\x00\\x02\\x00\\x03\\x00'\nIn [8]: numpy.fromstring(x, dtype=np.uint16, count=3)\nOut[8]: array([1, 2, 3], dtype=uint16)\n\nText: Update: \nText: As J.F.Sebastian suggested there are better ways to do this, like using: \nText: save np.load \nText: or as Janne Karila suggested using: \nText: np.ndarray.tofile fromfile \nAPI:\nnumpy.save\nnumpy.load\nnumpy.ndarray.tofile\nnumpy.fromfile\n","label":[[505,509,"Mention"],[510,517,"Mention"],[568,585,"Mention"],[586,594,"Mention"],[601,611,"API"],[612,622,"API"],[623,643,"API"],[644,658,"API"]],"Comments":[]}
{"id":59585,"text":"ID:13270760\nPost:\nText: So based on the inputs here, I'm marking my original code block with the explicit test as the solution: \nCode: if linalg.cond(x) < 1\/sys.float_info.epsilon:\n    i = linalg.inv(x)\nelse:\n    #handle it\n\nText: Surprisingly, the inv function doesn't perform this test. I checked the code and found it goes through all it's machinations, then just calls the lapack routine - seems quite inefficient. Also, I would 2nd a point made by DaveP: that the inverse of a matrix should not be computed unless it's explicitly needed. \nAPI:\nnumpy.linalg.inv\n","label":[[249,252,"Mention"],[549,565,"API"]],"Comments":[]}
{"id":59586,"text":"ID:13299279\nPost:\nText: If you're just looking for a library function that does this, just use eigenValuesAndVectors and look for the eigenvector with eigenvalue equal to 1. If you need to implement an iterative solver, probably the power method is the best idea for this. It should simply work because the vector with eigenvalue of 1 is also the one with largest eigenvalue. Krylov subspace method (Rayleigh iteration) should work well on this too. \nAPI:\nnumpy.linalg.eig\n","label":[[95,116,"Mention"],[456,472,"API"]],"Comments":[]}
{"id":59587,"text":"ID:13353563\nPost:\nText: Theres a couple of ways of doing this, each has their pros\/cons, the following four where just from the top of my head ... \nText: pythons own random.sample, is simple and built in, though it may not be the fastest... permutation again simple but it creates a copy of which we have to slice, ouch! np.random.shuffle is faster since it shuffles in place, but we still have to slice. sample is the fastest but it only works on the interval 0 to 1 so we have to normalize it, and convert it to ints to get the random indices, at the end we still have to slice, note normalizing to the size we want does not generate a uniform random distribution. \nText: Here are some benchmarks. \nCode: import timeit\nfrom matplotlib import pyplot as plt\n\nsetup = \\\n\"\"\"\nimport numpy\nimport random\n\nnumber_of_members = 20\nvalues = range(50)\n\"\"\"\n\nnumber_of_repetitions = 20\narray_sizes = (10, 200)\n\npython_random_times = [timeit.timeit(stmt = \"[random.sample(values, number_of_members) for index in xrange({0})]\".format(array_size),\n                                     setup = setup,                      \n                                     number = number_of_repetitions)\n                                        for array_size in xrange(*array_sizes)]\n\nnumpy_permutation_times = [timeit.timeit(stmt = \"[numpy.random.permutation(values)[:number_of_members] for index in xrange({0})]\".format(array_size),\n                               setup = setup,\n                               number = number_of_repetitions)\n                                    for array_size in xrange(*array_sizes)]\n\nnumpy_shuffle_times = [timeit.timeit(stmt = \\\n                                \"\"\"\n                                random_arrays = []\n                                for index in xrange({0}):\n                                    numpy.random.shuffle(values)\n                                    random_arrays.append(values[:number_of_members])\n                                \"\"\".format(array_size),\n                                setup = setup,\n                                number = number_of_repetitions)\n                                     for array_size in xrange(*array_sizes)]                                                                    \n\nnumpy_sample_times = [timeit.timeit(stmt = \\\n                                    \"\"\"\n                                    values = numpy.asarray(values)\n                                    random_arrays = [values[indices][:number_of_members] \n                                                for indices in (numpy.random.sample(({0}, len(values))) * len(values)).astype(int)]\n                                    \"\"\".format(array_size),\n                                    setup = setup,\n                                    number = number_of_repetitions)\n                                         for array_size in xrange(*array_sizes)]                                                                                                                                            \n\nline_0 = plt.plot(xrange(*array_sizes),\n                             python_random_times,\n                             color = 'black',\n                             label = 'random.sample')\n\nline_1 = plt.plot(xrange(*array_sizes),\n         numpy_permutation_times,\n         color = 'red',\n         label = 'numpy.random.permutations'\n         )\n\nline_2 = plt.plot(xrange(*array_sizes),\n                    numpy_shuffle_times,\n                    color = 'yellow',\n                    label = 'numpy.shuffle')\n\nline_3 = plt.plot(xrange(*array_sizes),\n                    numpy_sample_times,\n                    color = 'green',\n                    label = 'numpy.random.sample')\n\nplt.xlabel('Number of Arrays')\nplt.ylabel('Time in (s) for %i rep' % number_of_repetitions)\nplt.title('Different ways to sample.')\nplt.legend()\n\nplt.show()\n\nText: and the result: \nText: So it looks like rnd.permutation is the worst, not surprising, pythons own random.sample is holding it own, so it looks like its a close race between np.random.shuffle and rnd.sample with sample edging out, so either should suffice, even though rnd.sample has a higher memory footprint I still prefer it since I really don't need to build the arrays I just need the random indices ... \nCode: $ uname -a\nDarwin Kernel Version 10.8.0: Tue Jun  7 16:33:36 PDT 2011; root:xnu-1504.15.3~1\/RELEASE_I386 i386\n\n$ python --version\nPython 2.6.1\n\n$ python -c \"import numpy; print numpy.__version__\"\n1.6.1\n\nText: UPDATE \nText: Unfortunately np.random.sample doesn't draw unique elements from a population so you'll get repitation, so just stick with shuffle is just as fast. \nText: UPDATE 2 \nText: If you want to remain within numpy to leverage some of its built in functionality just convert the values into numpy arrays. \nCode: import numpy as np\nvalues = ['cat', 'popcorn', 'mescaline']\nnumber_of_members = 2\nN = 1000000\nrandom_arrays = np.asarray([values] * N)\n_ = [np.random.shuffle(array) for array in random_arrays]\nsubset = random_arrays[:, :number_of_members]\n\nText: Note that N here is quite large as such you are going to get repeated number of permutations, by permutations I mean order of values not repeated values within a permutation, since fundamentally theres a finite number of permutations on any giving finite set, if just calculating the whole set then its n!, if only selecting k elements its n!\/(n - k)! and even if this wasn't the case, meaning our set was much larger, we might still get repetitions depending on the random functions implementation, since shuffle\/permutation\/... and so on only work with the current set and have no idea of the population, this may or may not be acceptable, depends on what you are trying to achieve, if you want a set of unique permutations, then you are going to generate that set and subsample it. \nAPI:\nnumpy.random.permutation\nnumpy.random.shuffle\nnumpy.random.sample\nnumpy.random.permutation\nnumpy.random.shuffle\nnumpy.random.sample\nnumpy.random.sample\nnumpy.random.sample\nnumpy.random.sample\n","label":[[241,252,"Mention"],[321,338,"Mention"],[405,411,"Mention"],[3906,3921,"Mention"],[4039,4056,"Mention"],[4061,4071,"Mention"],[4077,4083,"Mention"],[4134,4144,"Mention"],[4518,4534,"Mention"],[5844,5868,"API"],[5869,5889,"API"],[5890,5909,"API"],[5910,5934,"API"],[5935,5955,"API"],[5956,5975,"API"],[5976,5995,"API"],[5996,6015,"API"],[6016,6035,"API"]],"Comments":[]}
{"id":59588,"text":"ID:13381904\nPost:\nText: In numpy version function G(array([pi\/4])) returns an empty array: \nCode: >> G(array([pi\/4]))  \narray([], dtype=float64)\n\nText: The problem is in line: \nCode: return diff(F(theta,phi,phi0,H0),phi)\n\nText: diff calculates differences between consecutive element of the arrays, whereas sympy.diff calculates a derivative. You can modify your own F_phi function to return derivative calculated analytically (if you know the solution) or numerically. For numerical solution you can use: \nCode: def F_phi(theta,phi,phi0,H0, eps=1e-12):\n    return (F(theta,phi+eps,phi0,H0) - F(theta,phi,phi0,H0))\/eps\n\nText: and analytical solution (calculated with sympy): \nCode: def F_phi(theta, phi, phi0, H0):\n    return -H0*a*t*(-sin(phi)*sin(phi0)*sin(theta) - sin(phi)*sin(theta)*cos(phi0)) + 4*t*sin(2*phi)*sin(theta)**4*cos(2*phi)\n\nText: Please remember that numerical solution won't be as precise as analytical. Therefore, there might be still differences between sympy (analytical) and numpy (numerical) approaches. \nAPI:\nnumpy.diff\n","label":[[228,232,"Mention"],[1034,1044,"API"]],"Comments":[]}
{"id":59589,"text":"ID:13442866\nPost:\nText: The short answer to your question is use numpy.delete. E.g. \nCode: import numpy as np\ndata = np.arange(1000).reshape((10,10,10))\n\n# Delete the third slice along the first axis \n# (note that you can delete multiple slices at once)\ndata = np.delete(data, [2], axis=0)\n\nprint data.shape\n\nText: However, this is a poor approach if you're going to be removing individual slices many times. \nText: The longer answer is to avoid doing this each time you want to delete a slice. \nText: Numpy arrays have to be contiguous in memory. Therefore, this will make a new copy (and delete the old) each time. This will be relatively slow, and requires you to have twice the free memory space required to store the array. \nText: In your case, why not store a python list of 2D arrays? That way you can pop the slices you don't want out without any problems. If you need it as a 3D array afterwards, just use dstack to create it. \nText: Of course, if you need to do 3D processing, you'll need the 3D array. Therefore, another approach would be to store a list of \"bad\" indicies and remove them at the end using delete (note that the items to be deleted is a list, so you can just pass in your list of \"bad\" indicies). \nText: On a side note, the way you're updating the image will be very slow. \nText: You're creating lots of images, so each one will be redrawn each time and the update will become very slow as you go on. \nText: You're better off setting the data of the image (im.set_data(next_slice)) instead of creating a new image each time. \nText: Better yet, use blitting, but with image data in matplotlib, it's not as advantageous as it is for other types of plots due to matplotlib's slow-ish rescaling of images. \nText: As a quick example: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider\n\ndef main():\n    # Set up 3D coordinates from -10 to 10 over a 200x100x100 \"open\" grid\n    x, y, z = np.ogrid[-10:10:200j, -10:10:100j, -10:10:100j]\n\n    # Generate a cube of interesting data\n    data= np.sin(x*y*z) \/ (x*y*z)\n\n    # Visualize it\n    viewer = VolumeViewer(data)\n    viewer.show()\n\nclass VolumeViewer(object):\n    def __init__(self, data):\n        self.data = data\n        self.nframes = self.data.shape[0]\n\n        # Setup the axes.\n        self.fig, self.ax = plt.subplots()\n        self.slider_ax = self.fig.add_axes([0.2, 0.03, 0.65, 0.03])\n\n        # Make the slider\n        self.slider = Slider(self.slider_ax, 'Frame', 1, self.nframes, \n                            valinit=1, valfmt='%1d\/{}'.format(self.nframes))\n        self.slider.on_changed(self.update)\n\n        # Plot the first slice of the image\n        self.im = self.ax.imshow(data[0,:,:])\n\n    def update(self, value):\n        frame = int(np.round(value - 1))\n\n        # Update the image data\n        dat = self.data[frame,:,:]\n        self.im.set_data(dat)\n\n        # Reset the image scaling bounds (this may not be necessary for you)\n        self.im.set_clim([dat.min(), dat.max()])\n\n        # Redraw the plot\n        self.fig.canvas.draw()\n\n    def show(self):\n        plt.show()\n\nif __name__ == '__main__':\n    main()\n\nAPI:\nnumpy.dstack\nnumpy.delete\n","label":[[915,921,"Mention"],[1117,1123,"Mention"],[3162,3174,"API"],[3175,3187,"API"]],"Comments":[]}
{"id":59590,"text":"ID:13550615\nPost:\nText: According to the docs np.loadtxt is \nText: a fast reader for simply formatted files. The genfromtxt function provides more sophisticated handling of, e.g., lines with missing values. \nText: so there are only a few options to handle more complicated files. As mentioned genfromtxt has more options. So as an example you could use \nCode: import numpy as np\ndata = np.genfromtxt('e:\\dir1\\datafile.csv', delimiter=',', skip_header=10,\n                     skip_footer=10, names=['x', 'y', 'z'])\n\nText: to read the data and assign names to the columns (or read a header line from the file with names=True) and than plot it with \nCode: ax1.plot(data['x'], data['y'], color='r', label='the data')\n\nText: I think numpy is quite well documented now. You can easily inspect the docstrings from within ipython or by using an IDE like spider if you prefer to read them rendered as HTML. \nAPI:\nnumpy.loadtxt\nnumpy.genfromtxt\n","label":[[46,56,"Mention"],[293,303,"Mention"],[905,918,"API"],[919,935,"API"]],"Comments":[]}
{"id":59591,"text":"ID:13556834\nPost:\nText: One \"easier way\" is to create a NumPy-aware function using numpy.vectorize. A \"ufunc\" is NumPy terminology for an elementwise function (see documentation here). Using vectorize lets you use your element-by-element function to create your own ufunc, which works the same way as other NumPy ufuncs (like standard addition, etc.): the ufunc will accept arrays and it will apply your function to each pair of elements, it will do array shape broadcasting just like standard NumPy functions, etc. The documentation page has some usage examples that might be helpful. \nCode: In [1]: import numpy as np\n   ...: def myfunc(a, b):\n   ...:     \"Return 1 if a>b, otherwise return 0\"\n   ...:     if a > b:\n   ...:         return 1\n   ...:     else:\n   ...:         return 0\n   ...: vfunc = np.vectorize(myfunc)\n   ...: \n\nIn [2]: vfunc([1, 2, 3, 4], [4, 3, 2, 1])\n   ...: \nOut[2]: array([0, 0, 1, 1])\nIn [3]: vfunc([1, 2, 3, 4], 2)\n   ...: \nOut[3]: array([0, 0, 1, 1])\n\nAPI:\nnumpy.vectorize\n","label":[[191,200,"Mention"],[986,1001,"API"]],"Comments":[]}
{"id":59592,"text":"ID:13567388\nPost:\nText: Check out the documentation for numpy.sum, paying particular attention to the axis parameter. To sum over columns: \nCode: >>> import numpy as np\n>>> a = np.arange(12).reshape(4,3)\n>>> a.sum(axis=0)\narray([18, 22, 26])\n\nText: Or, to sum over rows: \nCode: >>> a.sum(axis=1)\narray([ 3, 12, 21, 30])\n\nText: Other aggregate functions, like numpy.mean, cumusum and numpy.std, e.g., also take the axis parameter. \nText: From the Tentative Numpy Tutorial: \nText: Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the ndarray class. By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the axis parameter you can apply an operation along the specified axis of an array: \nAPI:\nnumpy.cumsum\n","label":[[371,378,"Mention"],[834,846,"API"]],"Comments":[]}
{"id":59593,"text":"ID:13663832\nPost:\nText: np.genfromtxt accepts iterators as well as files. That means it will accept the output of itertools.islice. Here, test.txt is a five-line file: \nCode: >>> import itertools, numpy\n>>> with open('test.txt') as t_in:\n...     numpy.genfromtxt(itertools.islice(t_in, 3))\n... \narray([[  1.,   2.,   3.,   4.,   5.],\n       [  6.,   7.,   8.,   9.,  10.],\n       [ 11.,  12.,  13.,  14.,  15.]])\n\nText: One might think this would be slower than letting numpy handle the file IO, but a quick test suggests otherwise. genfromtxt provides a skip_footer keyword argument that you can use if you know how long the file is... \nCode: >>> numpy.genfromtxt('test.txt', skip_footer=2)\narray([[  1.,   2.,   3.,   4.,   5.],\n       [  6.,   7.,   8.,   9.,  10.],\n       [ 11.,  12.,  13.,  14.,  15.]])\n\nText: ...but a few informal tests on a 1000-line file suggest that using islice is faster even if you skip only a few lines: \nCode: >>> def get(nlines, islice=itertools.islice):\n...     with open('test.txt') as t_in:\n...         numpy.genfromtxt(islice(t_in, nlines))\n...         \n>>> %timeit get(3)\n1000 loops, best of 3: 338 us per loop\n>>> %timeit numpy.genfromtxt('test.txt', skip_footer=997)\n100 loops, best of 3: 4.92 ms per loop\n>>> %timeit get(300)\n100 loops, best of 3: 5.04 ms per loop\n>>> %timeit numpy.genfromtxt('test.txt', skip_footer=700)\n100 loops, best of 3: 8.48 ms per loop\n>>> %timeit get(999)\n100 loops, best of 3: 16.2 ms per loop\n>>> %timeit numpy.genfromtxt('test.txt', skip_footer=1)\n100 loops, best of 3: 16.7 ms per loop\n\nAPI:\nnumpy.genfromtxt\n","label":[[24,37,"Mention"],[1565,1581,"API"]],"Comments":[]}
{"id":59594,"text":"ID:13704307\nPost:\nText: To convert djtetime64 to datetime object that represents time in UTC on numpy-1.8: \nCode: >>> from datetime import datetime\n>>> import numpy as np\n>>> dt = datetime.utcnow()\n>>> dt\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> dt64 = np.datetime64(dt)\n>>> ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) \/ np.timedelta64(1, 's')\n>>> ts\n1354650685.3624549\n>>> datetime.utcfromtimestamp(ts)\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n>>> np.__version__\n'1.8.0.dev-7b75899'\n\nText: The above example assumes that a naive datetime object is interpreted by np.datetime64 as time in UTC. \nText: To convert datetime to np.datetime64 and back (numpy-1.6): \nCode: >>> np.datetime64(datetime.utcnow()).astype(datetime)\ndatetime.datetime(2012, 12, 4, 13, 34, 52, 827542)\n\nText: It works both on a single np.datetime64 object and a numpy array of np.datetime64. \nText: Think of np.datetime64 the same way you would about np.int8, np.int16, etc and apply the same methods to convert between Python objects such as int, datetime and corresponding numpy objects. \nText: Your \"nasty example\" works correctly: \nCode: >>> from datetime import datetime\n>>> import numpy \n>>> numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\ndatetime.datetime(2002, 6, 28, 0, 0)\n>>> numpy.__version__\n'1.6.2' # current version available via pip install numpy\n\nText: I can reproduce the long value on numpy-1.8.0 installed as: \nCode: pip install git+https:\/\/github.com\/numpy\/numpy.git#egg=numpy-dev\n\nText: The same example: \nCode: >>> from datetime import datetime\n>>> import numpy\n>>> numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\n1025222400000000000L\n>>> numpy.__version__\n'1.8.0.dev-7b75899'\n\nText: It returns long because for dt64 type .astype(datetime) is equivalent to .astype(object) that returns Python integer (long) on numpy-1.8. \nText: To get datetime object you could: \nCode: >>> dt64.dtype\ndtype('<M8[ns]')\n>>> ns = 1e-9 # number of seconds in a nanosecond\n>>> datetime.utcfromtimestamp(dt64.astype(int) * ns)\ndatetime.datetime(2002, 6, 28, 0, 0)\n\nText: To get datetime64 that uses seconds directly: \nCode: >>> dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100', 's')\n>>> dt64.dtype\ndtype('<M8[s]')\n>>> datetime.utcfromtimestamp(dt64.astype(int))\ndatetime.datetime(2002, 6, 28, 0, 0)\n\nText: The numpy docs say that the datetime API is experimental and may change in future numpy versions. \nAPI:\nnumpy.datetime64\nnumpy.datetime64\n","label":[[35,45,"Mention"],[1785,1789,"Mention"],[2473,2489,"API"],[2490,2506,"API"]],"Comments":[]}
{"id":59595,"text":"ID:13773842\nPost:\nText: the dtype=\"S3\" you don't want to \"go away\". When you print a numpy array it gives you the type of the data in it. it isn't part of the data, it is information about how the data is stored and understood by the program. \nText: In your particular example, you read number, so you probably want to use them afterwards in computation or whatever, in which case you'll want the data to be understood as number (floats in you case). \nText: For the moment they are stored as strings, and that's why you see dtype=\"S3\" which essentially means string type of size 3 or less. (IIRC) \nText: I suggest to you an alternative to your function : genfromtxt is a function to load data from a txt file into a numpy array. \nText: The documentation is quite good and you'll find it very useful if you spend the 20 minutes to understand the parameters. \nCode: array1 = numpy.genfromtxt('path_to_my_file.txt', usecols=0)\narray2 = numpy.genfromtxt('path_to_my_file.txt', usecols=1)\n\nText: This should get you started. \nAPI:\nnumpy.genfromtxt\n","label":[[655,665,"Mention"],[1026,1042,"API"]],"Comments":[]}
{"id":59596,"text":"ID:13999155\nPost:\nText: If you need to micro-optimize like this, the only way to know what's fastest is to test. \nText: The short version is: append is faster than extend, and Joran Beasley's suggestion itertools.chain.from_iterable is slightly faster than eitherbut only if you replace the map with a list comprehension. \nText: So: \nCode: import itertools\nimport timeit\n\ndef makestuff(count):\n    for i in range(count):\n        yield (i, i)\n\ndef f_extend(mystuff):\n    mylist = []\n    for x, y in mystuff:\n        mylist.extend([int(x), int(y)])\n    return mylist\n\ndef f_append(mystuff):\n    mylist = []\n    for x, y in mystuff:\n        mylist.append(int(x))\n        mylist.append(int(y))\n    return mylist\n\ndef f_chainmap(mystuff):\n    return list(map(int, itertools.chain(*mystuff)))\n\ndef f_chaincomp(mystuff):\n    return [int(x) for x in itertools.chain(*mystuff)]\n\ndef f_chainfrommap(mystuff):\n    return list(map(int, itertools.chain.from_iterable(mystuff)))\n\ndef f_chainfromcomp(mystuff):\n    return [int(x) for x in itertools.chain.from_iterable(mystuff)]\n\ndef f_reducecompcomp(mystuff):\n    return [int(x) for x in reduce(operator.iadd, (list(y) for y in mystuff), [])]\n\ndef f_reducecompmap(mystuff):\n    return [int(x) for x in reduce(operator.iadd, map(list, mystuff), [])]\n\n\ntry:\n    import numpy\n    def f_numpy(mystuff):\n        return numpy.array(mystuff).flatten().tolist()\n    def f_numpy2(mystuff):\n        return numpy.array(list(mystuff)).flatten().tolist()\nexcept:\n    pass\n\nif __name__ == '__main__':\n  import sys\n  main = sys.modules['__main__']\n  count = int(sys.argv[1]) if len(sys.argv) > 1 else 10000\n  for f in dir(main):\n    if f.startswith('f_'):\n      func = getattr(main, f)\n      mystuff = makestuff(count)\n      testfunc = lambda: func(mystuff)\n      print('{}: {}'.format(f, timeit.timeit(testfunc, number=count)))\n\nText: For Python 2, I tried the map versions without the extra list, and it was slightly faster, but still not nearly competitive. For Python 3, of course, the list is necessary. \nText: Here are my timings: \nCode: $ python testlister.py 1000000\nf_append: 1.34638285637\nf_chaincomp: 2.12710499763\nf_chainfromcomp: 1.20806899071\nf_chainfrommap: 2.77231812477\nf_chainmap: 3.67478609085\nf_extend: 1.38338398933\nf_numpy: 5.52979397774\nf_numpy2: 7.5826470852\nf_reducecompcomp: 2.17834687233\nf_reducecompmap: 3.16517782211\n\n$ python3 .\/testlister.py 1000000\nf_append: 0.9949617639649659\nf_chaincomp: 2.0521950440015644\nf_chainfromcomp: 0.9724521590862423\nf_chainfrommap: 2.5558998831082135\nf_chainmap: 3.5766013460233808\nf_extend: 1.149905970087275\nf_reducecompcomp: 2.2112889911513776\nf_reducecompmap: 1.9317334480583668\n\nText: My python is Apple's stock Python 2.7.2, while python3 is the python.org 3.3.0, both 64-bit, both on OS X 10.8.2, on a mid-2012 MacBook Pro with a 2.2GHz i7 and 4GB. \nText: If you're using 32-bit Python on a POSIX platform, I've noticed in the past that somewhere in the not-too-distant past, iterators got an optimization that seems to have sped up many things in itertools in 64-bit builds, but slowed them down in 32-bit. So, you may find that append wins in that case. (As always, test on the platform(s) you actually care about optimizing.) \nText: Ashwini Chaudhary linked to Flattening a shallow list in Python, which further linked to finding elements in python association lists efficiently. I suspect part of the difference between my results and theirs was improvements in iterators between 2.6.0 and 2.7.2\/3.3.0, but the fact that we're explicitly using 2-element elements instead of larger ones is probably even more importantly. \nText: Also, at least one of the answers claimed that reduce was the fastest. The reduce implementations in the original post are all terribly slow, but I was able to come up with faster versions. They still aren't competitive with append or chain.from_iterable, but they're in the right ballpark. \nText: The f_numpy function is heltonbiker's implementation. Since mystuff is a 2D iterator, this actually just generates a 0D array wrapping the iterator, so all numpy can do is add overhead. I was able to come up with an implementation that generates a 1D array of iterators, but that was even slower, because now all numpy can do is add overhead N times as often. The only way I could get a 2D array of integers was by calling list first, as in f_numpy2, which made things even slower. (To be fair, throwing an extra list into the other functions slowed them down too, but not nearly as bad as with numpy.) \nText: However, it's quite possible that I'm blanking here, and there is a reasonable way to use numpy here. Of course if you can be sure either the top level mystuff or each element in mystuff is a list or a tuple, you can write something betterand if you can redesign your app so you have a 2D array in the first place, instead of a general sequence of sequences, that'll be a whole different story. But if you just have a general 2D iteration of sequences, it doesn't seem very good for this use case. \nAPI:\nnumpy.array\n","label":[[4821,4826,"Mention"],[5036,5047,"API"]],"Comments":[]}
{"id":59597,"text":"ID:14130246\nPost:\nText: This was an error in my code after all... As @seberg points out, this code works normally: \nCode: >>> import numpy\n>>> import warnings\n>>> numpy.log(0.)\n__main__:1: RuntimeWarning: divide by zero encountered in log\n-inf\n>>> warnings.simplefilter(\"error\", RuntimeWarning)    # not \"RuntimeWarning\"\n>>> try:\n...     numpy.log(0.)\n... except RuntimeWarning:\n...     print \"caught\"\n...\ncaught\n\nText: seterr provides an alternative to handling RuntimeWarning this way, though: \nCode: >>> import numpy\n>>> numpy.seterr(all='raise')\n{'over': 'warn', 'divide': 'warn', 'invalid': 'warn', 'under': 'ignore'}\n>>> try:\n...     numpy.log(0.)\n... except FloatingPointError:\n...     print \"caught\"\n... \ncaught\n\nText: Either way, it works, though Python really should throw some kind of exception for passing a string instead of a class to warnings.simplefilter. \nAPI:\nnumpy.seterr\n","label":[[420,426,"Mention"],[878,890,"API"]],"Comments":[]}
{"id":59598,"text":"ID:14250153\nPost:\nText: A.I and LA.inv are not the same. \nText: A.I is a property which calls matrix.getI: \nCode: def getI(self):\n    M,N = self.shape\n    if M == N:\n        from numpy.dual import inv as func\n    else:\n        from numpy.dual import pinv as func\n    return asmatrix(func(self))\n\nText: So getI either calls numpy.dual.inv (the multiplicative inverse of a square matrix) or numpy.dual.pinv (the Moore-Penrose psuedo-inverse) depending on the shape of the matrix. \nText: If you trace through the definitions (in dual.py), you'll find that numpy.dual.inv is np.linalg.inv and numpy.dual.pinv is numpy.linalg.pinv. \nCode: In [69]: s = np.random.random((3,4))\n\nIn [70]: t = np.matrix(s)\n\nIn [71]: t.I\nOut[71]: \nmatrix([[ 1.09509751, -0.56685735,  0.51704085],\n        [-1.59777153,  0.2777383 ,  1.25579378],\n        [ 0.81899054,  0.7594223 , -0.82760378],\n        [ 0.02845906,  0.50418885, -0.2091376 ]])\n\nIn [72]: np.linalg.inv(t)\n...\nLinAlgError: Array must be square\n\nText: Moreover, np.linalg.inv can be applied to numpy arrays (and return a numpy array) as well be applied to numpy matrices. The matrix.I property is specific to numpy matrices, and returns another numpy matrix. \nCode: In [60]: x = np.random.random((3,3))\n\nIn [62]: y = np.matrix(x)    \n\nIn [64]: type(y.I)\nOut[64]: <class 'numpy.matrixlib.defmatrix.matrix'>\n\nIn [65]: type(np.linalg.inv(x))\nOut[65]: <type 'numpy.ndarray'>\n\nText: A property, like A.I looks syntactically like an attribute, but it actually calls a function (in this case, A.getI). So the value of the inverse is not being stored. Every time Python evaluates A.I, the function A.getI() is called, and the result of the function is returned. \nText: See Properties: attributes managed by get\/set methods for more info on properties. \nAPI:\nnumpy.linalg.inv\nnumpy.linalg.inv\n","label":[[32,38,"Mention"],[571,584,"Mention"],[1789,1805,"API"],[1806,1822,"API"]],"Comments":[]}
{"id":59599,"text":"ID:14276901\nPost:\nText: So the idea illustrated by jterrace seems to work for me with a slight modification: \nCode: class SaneEqualityArray(np.ndarray):\n    def __eq__(self, other):\n        return (isinstance(other, np.ndarray) and self.shape == other.shape and \n            np.allclose(self, other))\n\nText: Like I said, the container with these objects should be on the left side of the equality check. I create SaneEqualityArray objects from existing numpy.ndarrays like this: \nCode: SaneEqualityArray(my_array.shape, my_array.dtype, my_array)\n\nText: in accordance with ndarray constructor signature: \nCode: ndarray(shape, dtype=float, buffer=None, offset=0,\n        strides=None, order=None)\n\nText: This class is defined within the test suite and serves for testing purposes only. The RHS of the equality check is an actual object returned by the tested function and contains real ndarray objects. \nText: P.S. Thanks to the authors of both answers posted so far, they were both very helpful. If anyone sees any problems with this approach, I'd appreciate your feedback. \nAPI:\nnumpy.ndarray\n","label":[[884,891,"Mention"],[1079,1092,"API"]],"Comments":[]}
{"id":59600,"text":"ID:14390487\nPost:\nText: You can call reshape on the values array of the Series: \nCode: In [4]: a.values.reshape(2,2)\nOut[4]: \narray([[1, 2],\n       [3, 4]], dtype=int64)\n\nText: I actually think it won't always make sense to apply reshape to a Series (do you ignore the index?), and that you're correct in thinking it's just numpy's reshape: \nText: a.reshape? Docstring: See reshape \nText: that said, I agree the fact that it let's you try to do this looks like a bug. \nAPI:\nnumpy.ndarray.reshape\n","label":[[374,381,"Mention"],[474,495,"API"]],"Comments":[]}
{"id":59601,"text":"ID:14392453\nPost:\nText: Numpy.all does not understands generator expressions. \nText: From the documentation \nCode:  numpy.all(a, axis=None, out=None)\n\n    Test whether all array elements along a given axis evaluate to True.\n    Parameters :    \n\n    a : array_like\n\n        Input array or object that can be converted to an array.\n\nText: Ok, not very explicit, so lets look at the code \nCode: def all(a,axis=None, out=None):\n    try:\n        all = a.all\n    except AttributeError:\n        return _wrapit(a, 'all', axis, out)\n    return all(axis, out)\n\ndef _wrapit(obj, method, *args, **kwds):\n    try:\n        wrap = obj.__array_wrap__\n    except AttributeError:\n        wrap = None\n    result = getattr(asarray(obj),method)(*args, **kwds)\n    if wrap:\n        if not isinstance(result, mu.ndarray):\n            result = asarray(result)\n        result = wrap(result)\n    return result\n\nText: As generator expression doesn't have all method, it ends up calling _wrapit In _wrapit, it first checks for __array_wrap__ method which generates AttributeError finally ending up calling asarray on the generator expression \nText: From the documentation of NPasarray \nCode:  numpy.asarray(a, dtype=None, order=None)\n\n    Convert the input to an array.\n    Parameters :    \n\n    a : array_like\n\n        Input data, in any form that can be converted to an array. This includes lists, lists of tuples, tuples, tuples of tuples, tuples of lists and ndarrays.\n\nText: It is well documented about the various types of Input data thats accepted which is definitely not generator expression \nText: Finally, trying \nCode: >>> np.asarray(0 for i in range(10))\narray(<generator object <genexpr> at 0x42740828>, dtype=object)\n\nAPI:\nnumpy.asarray\n","label":[[1148,1157,"Mention"],[1710,1723,"API"]],"Comments":[]}
{"id":59602,"text":"ID:14462709\nPost:\nText: plot_surface expects all the inputs arrays to be 2D. \nText: You can use np.meshgrid to generate the grid points \nCode: In [78]: X = np.arange(500)                 \nIn [79]: Y = np.arange(700)\n\nIn [83]: xx, yy = np.meshgrid(X, Y)\n\nIn [84]: xx.shape\nOut[84]: (700, 500)\n\nIn [85]: yy.shape\nOut[85]: (700, 500)\n\nText: You can then call plot_surface(xx, yy, Z.T) \nAPI:\nnumpy.meshgrid\n","label":[[96,107,"Mention"],[388,402,"API"]],"Comments":[]}
{"id":59603,"text":"ID:14475960\nPost:\nText: The two expressions give results that are identical in value but have different types: \nCode: In [17]: numpy.int32(1) * 0.2 == 1 * 0.2\nOut[17]: True\n\nIn [18]: type(numpy.int32(1) * 0.2)\nOut[18]: numpy.float64\n\nIn [19]: type(1 * 0.2)\nOut[19]: float\n\nText: The different output is purely due to the difference in default formatting between np.float64 and float. \nText: If we reverse the types, the output also reverses: \nCode: In [12]: float(numpy.int32(1) * 0.2)\nOut[12]: 0.2\n\nIn [13]: numpy.float64(1 * 0.2)\nOut[13]: 0.20000000000000001\n\nText: It's purely a display issue. There is no numerical difference here. \nAPI:\nnumpy.float64\n","label":[[362,372,"Mention"],[642,655,"API"]],"Comments":[]}
{"id":59604,"text":"ID:14493167\nPost:\nText: Boy, have I got a treat for you. genfromtxt has a converters parameter, which allows you to specify a function for each column as the file is parsed. The function is fed the CSV string value. Its return value becomes the corresponding value in the numpy array. \nText: Morever, the dtype = None parameter tells genfromtxt to make an intelligent guess as to the type of each column. In particular, numeric columns are automatically cast to an appropriate dtype. \nText: For example, suppose your data file contains \nCode: 2011-06-19 17:29:00.000,72,44,56\n\nText: Then \nCode: import numpy as np\nimport datetime as DT\n\ndef make_date(datestr):\n    return DT.datetime.strptime(datestr, '%Y-%m-%d %H:%M:%S.%f')\n\narr = np.genfromtxt(filename, delimiter = ',',\n                    converters = {'Date':make_date},\n                    names =  ('Date', 'Stock', 'Action', 'Amount'),\n                    dtype = None)\nprint(arr)\nprint(arr.dtype)\n\nText: yields \nCode: (datetime.datetime(2011, 6, 19, 17, 29), 72, 44, 56)\n[('Date', '|O4'), ('Stock', '<i4'), ('Action', '<i4'), ('Amount', '<i4')]\n\nText: Your real csv file has more columns, so you'd want to add more items to names, but otherwise, the example should still stand. \nText: If you don't really care about the extra columns, you can assign a fluff-name like this: \nCode: arr = np.genfromtxt(filename, delimiter=',',\n                    converters={'Date': make_date},\n                    names=('Date', 'Stock', 'Action', 'Amount') +\n                    tuple('col{i}'.format(i=i) for i in range(22)),\n                    dtype = None)\n\nText: yields \nCode: (datetime.datetime(2011, 6, 19, 17, 29), 72, 44, 56, 0.4772, 0.3286, 0.8497, 31.3587, 0.3235, 0.9147, 28.5751, 0.3872, 0.2803, 0, 0.2601, 0.2073, 0.1172, 0, 0.0, 0, 5.8922, 1, 0, 0, 0, 1.2759)\n\nText: You might also be interested in checking out the pandas module which is built on top of numpy, and which takes parsing CSV to an even higher level of luxury: It has a pandas.read_csv function whose parse_dates = True parameter will automatically parse date strings (using dateutil). \nText: Using pandas, your csv could be parsed with \nCode: df = pd.read_csv(filename, parse_dates = [0,1], header = None,\n                    names=('Date', 'Stock', 'Action', 'Amount') +\n                    tuple('col{i}'.format(i=i) for i in range(22)))\n\nText: Note there is no need to specify the make_date function. Just to be clear --pands.read_csvreturns aDataFrame, not a numpy array. The DataFrame may actually be more useful for your purpose, but you should be aware it is a different object with a whole new world of methods to exploit and explore. \nAPI:\nnumpy.genfromtxt\n","label":[[57,67,"Mention"],[2674,2690,"API"]],"Comments":[]}
{"id":59605,"text":"ID:14510892\nPost:\nText: ushort is 2 bytes, and uint8 is 1 byte. \nAPI:\nnumpy.uint8\n","label":[[47,52,"Mention"],[70,81,"API"]],"Comments":[]}
{"id":59606,"text":"ID:14540675\nPost:\nText: It would be nice to use np.average directly. However, to do so, d and the weights e would have to have the same shape, and broadcasting is not done implicitly for you here. \nText: Explicitly broadcasting e (using np.broadcast_arrays) so it has same shape as d is possible, but a waste of memory. So instead of doing that, would could a peek at the source code defining np.average and try to reproduce the calculation: \nCode: In [121]: d = np.random.random((16,3,90,144))\n\nIn [122]: e = np.random.random((16,3))\n\nIn [123]: f = e[:,:,None,None]\n\nIn [124]: scl = f.sum(axis = 1)\n\nIn [125]: avg = np.multiply(d,f).sum(axis = 1)\/scl\n\nText: Here is a check that the calculation returns the same result as the list comprehension: \nCode: In [126]: avg_lc = np.array([np.average(d[n], weights=e[n], axis=0) for n in range(d.shape[0])])\n\nIn [127]: np.allclose(avg, avg_lc)\nOut[127]: True\n\nAPI:\nnumpy.average\n","label":[[393,403,"Mention"],[908,921,"API"]],"Comments":[]}
{"id":59607,"text":"ID:14561052\nPost:\nText: Only integers can be used as array or matrix indices. The default type for a matrix initialised like that is float. \nText: You can use a arrpy not a numpy.matrix: \nCode: In [2]: import numpy as np\nIn [3]: x = np.array([1, 0, 2, 4, 3, 6, 5])\nIn [4]: x[x]\nOut[4]: array([0, 1, 2, 3, 4, 5, 6])\n\nText: Or you can explicitly change your matrix to an integer type: \nCode: In [5]: x = np.matrix(x).astype(int)\nIn [6]: x[0, x]\nOut[7]: matrix([[0, 1, 2, 3, 4, 5, 6]])\n\nText: A matrix is a specialised class designed for 2D matrices. In particular, you can't index a 2D matrix with a single integer, because -- well -- it's two dimensional and you need to specify two integers, hence the need for the extra 0 index in the second example. \nAPI:\nnumpy.array\nnumpy.matrix\n","label":[[161,166,"Mention"],[492,498,"Mention"],[758,769,"API"],[770,782,"API"]],"Comments":[]}
{"id":59608,"text":"ID:14577330\nPost:\nText: You want to use the mode 'L' instead of 'LA' as the parameter to the convert() method. 'LA' leaves an alpha channel and then the np.asarray doesn't work as you intended. If you need the alpha channel, then you will need a different method to convert to a numpy array. Otherwise, use mode 'L'. \nAPI:\nnumpy.asarray\n","label":[[153,163,"Mention"],[323,336,"API"]],"Comments":[]}
{"id":59609,"text":"ID:14753067\nPost:\nText: I believe np.column_stack should do what you want. Example: \nCode: >>> a = np.array((0, 1))\n>>> b = np.array((2, 1))\n>>> c = np.array((-1, -1))\n>>> numpy.column_stack((a,b,c))\narray([[ 0,  2, -1],\n       [ 1,  1, -1]])\n\nText: It is essentially equal to \nCode: >>> numpy.vstack((a,b,c)).T\n\nText: though. As it says in the documentation. \nAPI:\nnumpy.column_stack\n","label":[[34,49,"Mention"],[366,384,"API"]],"Comments":[]}
{"id":59610,"text":"ID:14791245\nPost:\nText: I can think of two approaches that provide some of the functionality you are asking for: \nText: To read a file either in chunks \/ or in strides of n-lines \/ etc.: You can pass a generator to genfromtxt as well as to numpy.loadtxt. This way you can load a large dataset from a textfile memory-efficiently while retaining all the convenient parsing features of the two functions. To read data only from lines that match a criterion that can be expressed as a regex: You can use fromregex and use a regular expression to precisely define which tokens from a given line in the input file should be loaded. Lines not matching the pattern will be ignored. \nText: To illustrate the two approaches, I'm going to use an example from my research context. I often need to load files with the following structure: \nCode: 6\n generated by VMD\n  CM         5.420501        3.880814        6.988216\n  HM1        5.645992        2.839786        7.044024\n  HM2        5.707437        4.336298        7.926170\n  HM3        4.279596        4.059821        7.029471\n  OD1        3.587806        6.069084        8.018103\n  OD2        4.504519        4.977242        9.709150\n6\n generated by VMD\n  CM         5.421396        3.878586        6.989128\n  HM1        5.639769        2.841884        7.045364\n  HM2        5.707584        4.343513        7.928119\n  HM3        4.277448        4.057222        7.022429\n  OD1        3.588119        6.069086        8.017814\n\nText: These files can be huge (GBs) and I'm only interested in the numerical data. All data blocks have the same size -- 6 in this example -- and they are always separated by two lines. So the stride of the blocks is 8. \nText: Using the first approach: \nText: First I'm going to define a generator that filters out the undesired lines: \nCode: def filter_lines(f, stride):\n    for i, line in enumerate(f):\n        if i%stride and (i-1)%stride:\n            yield line\n\nText: Then I open the file, create a filter_lines-generator (here I need to know the stride), and pass that generator to genfromtxt: \nCode: with open(fname) as f:\n    data = np.genfromtxt(filter_lines(f, 8),\n                         dtype='f',\n                         usecols=(1, 2, 3))\n\nText: This works like a breeze. Note that I'm able to use usecols to get rid of the first column of the data. In the same way, you could use all the other features of genfromtxt -- detecting the types, varying types from column to column, missing values, converters, etc. \nText: In this example data.shape was (204000, 3) while the original file consisted of 272000 lines. \nText: Here the generator is used to filter homogenously strided lines but one can likewise imagine it filtering out inhomogenous blocks of lines based on (simple) criteria. \nText: Using the second approach: \nText: Here's the regexp I'm going to use: \nCode: regexp = r'\\s+\\w+' + r'\\s+([-.0-9]+)' * 3 + r'\\s*\\n'\n\nText: Groups -- i.e. inside () -- define the tokens to be extracted from a given line. Next, fromregex does the job and ignores lines not matching the pattern: \nCode: data = np.fromregex(fname, regexp, dtype='f')\n\nText: The result is exactly the same as in the first approach. \nAPI:\nnumpy.genfromtxt\nnumpy.fromregex\n","label":[[215,225,"Mention"],[500,509,"Mention"],[3192,3208,"API"],[3209,3224,"API"]],"Comments":[]}
{"id":59611,"text":"ID:14823383\nPost:\nText: Not for vectorize, but most numpy functions take an out argument that does exactly what you want. \nText: What function are you trying to use np.vectorize with? vectorize is almost always the wrong solution when you're trying to \"vectorize\" a calculation. \nText: In your example above, if you wanted to do the operation in-place, you could accomplish it with: \nCode: a = numpy.zeros((1, 10))\na += 1\n\nText: Or, if you wanted to be a bit verbose, but do exactly what your example would do: \nCode: a = numpy.zeros((1, 10))\nbuf = numpy.empty_like(a)\nnumpy.add(a, 1, out=buf)\n\nText: vectorize has to call a python function for every element in the array. Therefore, it has additional overhead when compared to numpy functions that operate on the entire array. Usually, when people refer to \"vectorizing\" an expression to get a speedup, they're referring to building the expression out of building-blocks of basic numpy functions, rather than using vectorize (which is certainly confusing...). \nText: Edit: Based on your comment, vectorize really does fit your use case! (Writing a \"raster calculator\" is a pretty perfect use case for it, beyond security\/sandboxing issues.) \nText: On the other hand, numexpr is probably an even better fit if you don't mind an additional dependency. \nText: It's faster and takes an out parameter. \nAPI:\nnumpy.vectorize\nnumpy.vectorize\n","label":[[165,177,"Mention"],[601,610,"Mention"],[1354,1369,"API"],[1370,1385,"API"]],"Comments":[]}
{"id":59612,"text":"ID:14909654\nPost:\nText: Except from saving some code and time by using built in functions like numpy.diag, your problem seems to be the * operator. In numpy you have to use np.dot for matrix multiplication. See the code below for a working example... \nCode: In [16]: import numpy as np\n\nIn [17]: A = np.arange(15).reshape(5,3)\n\nIn [18]: A\nOut[18]: \narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\nIn [19]: u, s, v = np.linalg.svd(A)\n\nIn [20]: S = np.diag(s)\n\nIn [21]: S = np.vstack([S, np.zeros((2,3)) ])\n\nIn [22]: #fill in zeros to get the right shape\n\nIn [23]: np.allclose(A, np.dot(u, np.dot(S,v)))\nOut[23]: True\n\nText: np.allclose checks whether two arrays are numerically close... \nAPI:\nnumpy.dot\nnumpy.allclose\n","label":[[173,179,"Mention"],[683,694,"Mention"],[752,761,"API"],[762,776,"API"]],"Comments":[]}
{"id":59613,"text":"ID:14970625\nPost:\nText: The problem arises because you do not specify the right mode. Read it up in the documentation: np.convolve \nText: The default for convolve is mode='full'. \nText: This returns the convolution at each point of overlap, with an output shape of (N+M-1,). \nText: N is the size of the input array, M is the size of the filter. So the output is larger than the input. \nText: Instead you want to use np.convolve(filter,img[...],mode='same'). \nText: Also have a look at scipy.convolve which allows 2D convolution using the FFT. \nAPI:\nnumpy.convolve\nnumpy.convolve\n","label":[[119,130,"Mention"],[154,162,"Mention"],[549,563,"API"],[564,578,"API"]],"Comments":[]}
{"id":59614,"text":"ID:14986768\nPost:\nText: You can probably at least get more information on the type of error by seterrcall (in conjunction with np.seterr as demonstrated in the link). As far as figuring out which array is giving the problems, that probably isn't possible as numpy necessarily creates temporary arrays: \nCode: a = b + (c*d)\n#       ^This creates a temporary array before adding it to `a`\n\nText: Of course, you could use seterr to raise exceptions instead of warnings if you want to know what line number is the offending one in your code. \nAPI:\nnumpy.seterrcall\nnumpy.seterr\n","label":[[95,105,"Mention"],[127,136,"Mention"],[544,560,"API"],[561,573,"API"]],"Comments":[]}
{"id":59615,"text":"ID:15052064\nPost:\nText: You're correct in thinking that append or concatenate are going to be expensive if repeated many times (this is to do with numpy declaring a new array for the two previous arrays). \nText: The best suggestion (If you know how much space you're going to need in total) would be to declare that before you run your routine, and then just put the results in place as they become available. \nText: If you're going to run this nrows times, then \nCode: results = np.zeros([nrows, 5])\n\nText: and then add your results \nCode: def function(x, i, results):\n    <.. snip ..>\n    results[i,0] = a\n    results[i,1] = b\n    results[i,2] = c[-1,0]\n    results[i,3] = c[-1,1]\n    results[0,4] = c[-1,2]\n\nText: Of course, if you don't know how many times you're going to be running function this won't work. In that case, I'd suggest a less elegant approach; \nText: Declare a possibly large results array and add to results[i, x] as above (keeping track of i and the size of results. When you reach the size of results, then do the apped (or concatenate) on a new array. This is less bad than appending repetitively and shouldn't destroy performance - but you will have to write some wrapper code. \nText: There are other ideas you could pursue. Off the top of my head you could \nText: Write the results to disk, depending on the speed of OtherFunctionThatReturnsAThreeColumnArray and the size of your data this may not be too daft an idea. Save your results in a list comprehension (forgetting numpy until after the run). If function returned (a, b, c) not results; \nText: results = [function(x) for x in my_data] \nText: and now do some shuffling to get results into the form you need. \nAPI:\nnumpy.append\nnumpy.concatenate\nnumpy.append\n","label":[[56,62,"Mention"],[66,77,"Mention"],[1038,1043,"Mention"],[1698,1710,"API"],[1711,1728,"API"],[1729,1741,"API"]],"Comments":[]}
{"id":59616,"text":"ID:15102415\nPost:\nText: I guess you want vectorize which is basically like the builtin map for ndarrays. \nCode: >>> a = np.array([[np.array([1,2,3]), np.array([2])], [np.array([0,1,2,3,4]), np.array([0,4])]])\n>>> vmean = np.vectorize(np.mean)\n>>> vmean(a)\narray([[ 2.,  2.],\n       [ 2.,  2.]])\n\nAPI:\nnumpy.vectorize\n","label":[[41,50,"Mention"],[301,316,"API"]],"Comments":[]}
{"id":59617,"text":"ID:15114952\nPost:\nText: Here's a bit of code that might help. \nCode: from __future__ import division\n\nimport numpy as np\n\n\ndef find_transition_times(t, y, threshold):\n    \"\"\"\n    Given the input signal `y` with samples at times `t`,\n    find the times where `y` increases through the value `threshold`.\n\n    `t` and `y` must be 1-D numpy arrays.\n\n    Linear interpolation is used to estimate the time `t` between\n    samples at which the transitions occur.\n    \"\"\"\n    # Find where y crosses the threshold (increasing).\n    lower = y < threshold\n    higher = y >= threshold\n    transition_indices = np.where(lower[:-1] & higher[1:])[0]\n\n    # Linearly interpolate the time values where the transition occurs.\n    t0 = t[transition_indices]\n    t1 = t[transition_indices + 1]\n    y0 = y[transition_indices]\n    y1 = y[transition_indices + 1]\n    slope = (y1 - y0) \/ (t1 - t0)\n    transition_times = t0 + (threshold - y0) \/ slope\n\n    return transition_times\n\n\ndef periods(t, y, threshold):\n    \"\"\"\n    Given the input signal `y` with samples at times `t`,\n    find the time periods between the times at which the\n    signal `y` increases through the value `threshold`.\n\n    `t` and `y` must be 1-D numpy arrays.\n    \"\"\"\n    transition_times = find_transition_times(t, y, threshold)\n    deltas = np.diff(transition_times)\n    return deltas\n\n\nif __name__ == \"__main__\":\n    import matplotlib.pyplot as plt\n\n    # Time samples\n    t = np.linspace(0, 50, 501)\n    # Use a noisy time to generate a noisy y.\n    tn = t + 0.05 * np.random.rand(t.size)\n    y = 0.6 * ( 1 + np.sin(tn) + (1.\/3) * np.sin(3*tn) + (1.\/5) * np.sin(5*tn) +\n               (1.\/7) * np.sin(7*tn) + (1.\/9) * np.sin(9*tn))\n\n    threshold = 0.5\n    deltas = periods(t, y, threshold)\n    print(\"Measured periods at threshold %g:\" % threshold)\n    print(deltas)\n    print(\"Min:  %.5g\" % deltas.min())\n    print(\"Max:  %.5g\" % deltas.max())\n    print(\"Mean: %.5g\" % deltas.mean())\n    print(\"Std dev: %.5g\" % deltas.std())\n\n    trans_times = find_transition_times(t, y, threshold)\n\n    plt.plot(t, y)\n    plt.plot(trans_times, threshold * np.ones_like(trans_times), 'ro-')\n    plt.show()\n\nText: The output: \nCode: Measured periods at threshold 0.5:\n[ 6.29283207  6.29118893  6.27425846  6.29580066  6.28310224  6.30335003]\nMin:  6.2743\nMax:  6.3034\nMean: 6.2901\nStd dev: 0.0092793\n\nText: You could use np.histogram and\/or matplotlib.pyplot.hist to further analyze the array returned by periods(t, y, threshold). \nAPI:\nnumpy.histogram\n","label":[[2362,2374,"Mention"],[2478,2493,"API"]],"Comments":[]}
{"id":59618,"text":"ID:15180553\nPost:\nText: This has to do with how cov interprets its first argument. You have each observation in a row, while np.cov expects each observation in a column. \nText: To fix, take the transpose of data in np.cov(data.T) to get the X x X covariance matrix: \nCode: In [58]: N, X = 100, 3\n\nIn [59]: data = np.random.random((N,X))\n\nIn [60]: mean = np.mean(data, axis = 0)\n\nIn [61]: mean\nOut[61]: array([ 0.4913433 ,  0.49484566,  0.52463666])\n\nIn [62]: np.cov(data.T).shape\nOut[62]: (3, 3)\n\nIn [63]: cov = np.cov(data.T)\n\nIn [64]: np.random.multivariate_normal(mean, cov)\nOut[64]: array([ 0.27194062,  0.65995531,  0.67367201])\n\nText: Alternatively, use the rowval=False parameter: \nCode: In [68]: cov = np.cov(data, rowvar=False)\n\nIn [69]: cov.shape\nOut[69]: (3, 3)\n\nAPI:\nnumpy.cov\nnumpy.cov\n","label":[[48,51,"Mention"],[125,131,"Mention"],[779,788,"API"],[789,798,"API"]],"Comments":[]}
{"id":59619,"text":"ID:15183594\nPost:\nText: I guess you can use np.inf to identify those that are infinity and treat them separately. \nText: Ref: github.com\/pydata\/pandas \nAPI:\nnumpy.inf\n","label":[[44,50,"Mention"],[157,166,"API"]],"Comments":[]}
{"id":59620,"text":"ID:15198527\nPost:\nText: I think your solution is ok except that instead of using append you would better use np.diff, like np.diff(np.unique(a)): \nCode: In [1]: import numpy as np\n\nIn [2]: a = np.random.randint(0,100,size=50)\n\nIn [4]: np.unique(a)\nOut[4]: \narray([ 0,  2,  3,  5,  7,  8, 15, 18, 20, 22, 23, 27, 30, 31, 32, 33, 37,\n       38, 42, 43, 45, 48, 49, 57, 59, 62, 65, 70, 74, 75, 76, 78, 79, 80,\n       83, 84, 88, 91, 93, 94, 96, 98])\n\nIn [5]: np.diff(np.unique(a))\nOut[5]: \narray([2, 1, 2, 2, 1, 7, 3, 2, 2, 1, 4, 3, 1, 1, 1, 4, 1, 4, 1, 2, 3, 1, 8,\n       2, 3, 3, 5, 4, 1, 1, 2, 1, 1, 3, 1, 4, 3, 2, 1, 2, 2])\n\nIn [6]: np.diff(np.unique(a)).min()\nOut[6]: 1\n\nAPI:\nnumpy.append\n","label":[[81,87,"Mention"],[678,690,"API"]],"Comments":[]}
{"id":59621,"text":"ID:15225010\nPost:\nText: I think that np.interp is exactly what you want. e.g.: \nCode: numpy.interp(value_x,array_x,array_y)\n\nText: Note that here value_x can be a scalar or another array-like value. If it is an array-like value, you will be returned an array of corresponding interpolated values. \nAPI:\nnumpy.interp\n","label":[[37,46,"Mention"],[303,315,"API"]],"Comments":[]}
{"id":59622,"text":"ID:15294155\nPost:\nText: Here is the full code: \nCode: import numpy\nfrom scipy import ndimage\n\narray = numpy.zeros((100, 100), dtype=np.uint8)\nx = np.random.randint(0, 100, 2000)\ny = np.random.randint(0, 100, 2000)\narray[x, y] = 1\n\npl.imshow(array, cmap=\"gray\", interpolation=\"nearest\")\n\ns = ndimage.generate_binary_structure(2,2) # iterate structure\nlabeled_array, numpatches = ndimage.label(array,s) # labeling\n\nsizes = ndimage.sum(array,labeled_array,range(1,numpatches+1)) \n# To get the indices of all the min\/max patches. Is this the correct label id?\nmap = numpy.where(sizes==sizes.max())[0] + 1 \nmip = numpy.where(sizes==sizes.min())[0] + 1\n\n# inside the largest, respecitively the smallest labeled patches with values\nmax_index = np.zeros(numpatches + 1, np.uint8)\nmax_index[map] = 1\nmax_feature = max_index[labeled_array]\n\nmin_index = np.zeros(numpatches + 1, np.uint8)\nmin_index[mip] = 1\nmin_feature = min_index[labeled_array]\n\nText: Notes: \nText: np.where returns a tuple the size of label 1 is sizes[0], so you need to add 1 to the result of where To get a mask array with multiple labels, you can use labeled_array as the index of a label mask array. \nText: The results: \nAPI:\nnumpy.where\nnumpy.where\n","label":[[957,965,"Mention"],[1053,1058,"Mention"],[1189,1200,"API"],[1201,1212,"API"]],"Comments":[]}
{"id":59623,"text":"ID:15299366\nPost:\nText: Just for reference the 'answer' to this question is related to how I was doing my profiling, not in the actual calls to loadtxt being slower or faster within a function. \nText: Be careful of the arguments to %timeit: -n: execute the given statement times in a loop. If this value is not given, a fitting value is chosen. -r: repeat the loop iteration times and take the best result. Default: 3 \nText: Notice that I was specifying -n 1 to force %timeit to only run the loadtxt code 1 time. However, -n 1 is not sufficient for this. You must also specify -r 1 to force the evaluation of the code to only happen exactly once. \nText: So, my call to %timeit was effectively evaluating the call to np.loadtxt 3 times. The first call would actually read all of the file and take the majority of the total runtime. The next two calls would have no data to read because the file handle passed to np.loadtxt didn't have anymore data to read. Thus, two out of three calls didn't have any real work to do and took almost no time at all. \nText: Be careful what the time reported from %timeit means. \nText: Notice what the call to %timeit reports as part of it's output, 1 loops, best of 3: 30 us per loop. Since two of my three calls effectively did no work one of these two calls would be the best of 3. \nText: So by comparing my original call to %timeit and %lprun I was effectively comparing the time loadtxe takes to look at an empty\/finished file handle and the time np.loadtxt takes to truly open and read a full 208k of data. \nText: The real timings, when using correct arguments to %timeit make much more sense: \nCode: >>> f = open('test.out', 'r');f.readline()\n'a b c d e f g h i j k l m n o p q r s t u v w x y z\\n'\n>>> %timeit -n 1 -r 1 np.loadtxt(f, unpack=True)\n1 loops, best of 1: 31.1 ms per loop\n\n\nFunction: file_to_numpy_ordered_dict at line 88\nTotal time: 0.083706 s\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    88                                           def file_to_numpy_ordered_dict(filename):\n    89                                               \"\"\"\n    90                                               Read a space-separated-value file as a dict of columns, keyed by\n    91                                               column header where each column (dict value) is a numpy array.\n    92                                               \"\"\"\n    93                                           \n    94         1          583    583.0      0.7      with open(filename, 'r') as file_obj:\n    95         1          313    313.0      0.4          headers = file_obj.readline().split()\n    96                                           \n    97                                                   # unpack=True b\/c want data organized as column based arrays, not rows\n    98         1        82417  82417.0     98.5          arrs = np.loadtxt(file_obj, unpack=True)\n    99                                           \n   100         1          226    226.0      0.3      ret_dict = collections.OrderedDict()\n   101        27           35      1.3      0.0      for ii, colname in enumerate(headers):\n   102        26          131      5.0      0.2          ret_dict[colname] = arrs[ii]\n   103                                           \n   104         1            1      1.0      0.0      return ret_dict\n\nText: 31ms vs 83 ms makes a bit more sense. These numbers are close enough that I'm assuming the differences are simply because I'm only running this relatively fast operation one time. To effectively compare these it would be best to take an average of a bunch of runs. \nAPI:\nnumpy.loadtxt\nnumpy.loadtxt\nnumpy.loadtxt\nnumpy.loadtxt\nnumpy.loadtxt\nnumpy.loadtxt\n","label":[[144,151,"Mention"],[492,499,"Mention"],[716,726,"Mention"],[911,921,"Mention"],[1415,1422,"Mention"],[1483,1493,"Mention"],[3689,3702,"API"],[3703,3716,"API"],[3717,3730,"API"],[3731,3744,"API"],[3745,3758,"API"],[3759,3772,"API"]],"Comments":[]}
{"id":59624,"text":"ID:15375714\nPost:\nText: Here is another method that use split or numpy.array_split: \nCode: df = pd.DataFrame({\"A\":np.arange(9), \"B\":np.arange(10, 19)}, \n                  index=np.arange(100, 109))\nfor tmp in np.split(df, 3):\n    print tmp\n\nText: the output is: \nCode:      A   B\n100  0  10\n101  1  11\n102  2  12\n     A   B\n103  3  13\n104  4  14\n105  5  15\n     A   B\n106  6  16\n107  7  17\n108  8  18\n\nAPI:\nnumpy.split\n","label":[[56,61,"Mention"],[407,418,"API"]],"Comments":[]}
{"id":59625,"text":"ID:15390958\nPost:\nText: The final number is a long (Python's name for an arbitrary precision integer), which NumPy apparently can't deal with: \nCode: >>> type(100000000000000000000)\n<type 'long'>\n>>> type(np.int(100000000000000000000))\n<type 'long'>\n>>> np.int64(100000000000000000000)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nOverflowError: Python int too large to convert to C long\n\nText: The AttributeError occurs because NumPy, seeing a type that it doesn't know how to handle, defaults to calling the sqrt method on the object; but that doesn't exist. So it's not sqrt that's missing, but long.sqrt. \nText: By contrast, math.sqrt knows about long. If you're going to deal with very large numbers in NumPy, use floats whenever feasible. \nText: EDIT: Alright, you're using Python 3. While the distinction between int and long has disappeared in that version, NumPy is still sensitive to the difference between a PyLongObject that can be successfully converted to a C long using PyLong_AsLong and one that can't. \nAPI:\nnumpy.sqrt\n","label":[[601,605,"Mention"],[1053,1063,"API"]],"Comments":[]}
{"id":59626,"text":"ID:15514883\nPost:\nText: You can use numpy.argsort, np.searchsorted to get the positions: \nCode: import numpy as np\nA = np.unique(np.random.randint(0, 100, 100))\nB = np.random.choice(A, 10)\n\nidxA = np.argsort(A)\nsortedA = A[idxA]\nidxB = np.searchsorted(sortedA, B)\npos = idxA[idxB]\nprint A[pos]\nprint B\n\nText: If you want faster method, consider using pandas. \nCode: import pandas as pd\ns = pd.Index(A)\npos = s.get_indexer(B)\nprint A[pos]\nprint B\n\nAPI:\nnumpy.searchsorted\n","label":[[51,66,"Mention"],[452,470,"API"]],"Comments":[]}
{"id":59627,"text":"ID:15622926\nPost:\nText: Check out einsum for another method: \nCode: In [52]: a\nOut[52]: \narray([[1, 2, 3],\n       [3, 4, 5]])\n\nIn [53]: b\nOut[53]: \narray([[1, 2, 3],\n       [1, 2, 3]])\n\nIn [54]: einsum('ij,ij->i', a, b)\nOut[54]: array([14, 26])\n\nText: Looks like einsum is a bit faster than inner1d: \nCode: In [94]: %timeit inner1d(a,b)\n1000000 loops, best of 3: 1.8 us per loop\n\nIn [95]: %timeit einsum('ij,ij->i', a, b)\n1000000 loops, best of 3: 1.6 us per loop\n\nIn [96]: a = random.randn(10, 100)\n\nIn [97]: b = random.randn(10, 100)\n\nIn [98]: %timeit inner1d(a,b)\n100000 loops, best of 3: 2.89 us per loop\n\nIn [99]: %timeit einsum('ij,ij->i', a, b)\n100000 loops, best of 3: 2.03 us per loop\n\nText: Note: NumPy is constantly evolving and improving; the relative performance of the functions shown above has probably changed over the years. If performance is important to you, run your own tests with the version of NumPy that you will be using. \nAPI:\nnumpy.einsum\n","label":[[34,40,"Mention"],[953,965,"API"]],"Comments":[]}
{"id":59628,"text":"ID:15665305\nPost:\nText: IIUC, simply pass the size parameter: \nCode: >>> m = np.random.normal(0, 1, size=(500, 2))\n>>> m.shape\n(500, 2)\n>>> m.mean(axis=0)\narray([-0.02394296,  0.0684164 ])\n>>> m.std(axis=0)\narray([ 1.04018539,  0.95281971])\n\nText: [I'm assuming that your rand is the random module.] \nText: See also the random.multivariate_normal function if you want to specify a particular covariance. \nAPI:\nnumpy.random\n","label":[[284,290,"Mention"],[410,422,"API"]],"Comments":[]}
{"id":59629,"text":"ID:15724465\nPost:\nCode: number = 1\nelements = 1000\n\nthelist = [number] * elements\n\nCode: >>> [1] * 10\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\nText: NB: Don't try to duplicate mutable objects (notably lists of lists) like that, or this will happen: \nCode: In [23]: a = [[0]] * 10\n\nIn [24]: a\nOut[24]: [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n\nIn [25]: a[0][0] = 1\n\nIn [26]: a\nOut[26]: [[1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]\n\nText: If you are using numpy, for multidimensional lists repeat is your best bet. It can repeat arrays of all shapes over separate axes. \nAPI:\nnumpy.repeat\n","label":[[495,501,"Mention"],[581,593,"API"]],"Comments":[]}
{"id":59630,"text":"ID:15789284\nPost:\nText: matrox can take string as an argument. \nCode: Docstring:\nmatrix(data, dtype=None, copy=True)\n\n[...]\n\nParameters\n----------\ndata : array_like or string\n   If `data` is a string, it is interpreted as a matrix with commas\n   or spaces separating columns, and semicolons separating rows.\n\nCode: In [1]: import numpy as np\n\nIn [2]: s = '[2 3; 4 5]'    \n\nIn [3]: def mag_func(s):\n   ...:     return np.array(np.matrix(s.strip('[]')))\n\nIn [4]: mag_func(s)\nOut[4]: \narray([[2, 3],\n       [4, 5]])\n\nAPI:\nnumpy.matrix\n","label":[[24,30,"Mention"],[519,531,"API"]],"Comments":[]}
{"id":59631,"text":"ID:15879527\nPost:\nText: ar is just a convenience function to create an ndarray; it is not a class itself. \nText: You can also create an array using numpy.ndarray, but it is not the recommended way. From the docstring of numpy.ndarray: \nText: Arrays should be constructed using array, zeros or empty ... The parameters given here refer to a low-level method (ndarray(...)) for instantiating an array. \nText: Most of the meat of the implementation is in C code, here in multiarray, but you can start looking at the ndarray interfaces here: \nText: https:\/\/github.com\/numpy\/numpy\/blob\/master\/numpy\/core\/numeric.py \nAPI:\nnumpy.array\n","label":[[24,26,"Mention"],[616,627,"API"]],"Comments":[]}
{"id":59632,"text":"ID:15884574\nPost:\nText: I found what I was looking for here tofile \nAPI:\nnumpy.ndarray.tofile\n","label":[[60,66,"Mention"],[73,93,"API"]],"Comments":[]}
{"id":59633,"text":"ID:15907592\nPost:\nText: I always thought x\/=5. was equivalent to x=x\/5 \nText: It is, unless the class overrides the __idiv__ operator, like ndarray does. nda overrides it to modify the array in-place, which is good because it avoids creating a new copy of the array, when copying is not required. As you can guess, it also overrides the rest of the __i*__ operators. \nAPI:\nnumpy.ndarray\nnumpy.ndarray\n","label":[[140,147,"Mention"],[154,157,"Mention"],[373,386,"API"],[387,400,"API"]],"Comments":[]}
{"id":59634,"text":"ID:15923228\nPost:\nText: The Y array in your screenshot is not a 1D array, it's a 2D array with 300 rows and 1 column, as indicated by its shape being (300, 1). \nText: To remove the extra dimension, you can slice the array as Y[:, 0]. To generally convert an n-dimensional array to 1D, you can use np.reshape(a, a.size). \nText: Another option for converting a 2D array into 1D is flatten() function from np.ndarray module, with the difference that it makes a copy of the array. \nAPI:\nnumpy.ndarray\n","label":[[403,413,"Mention"],[483,496,"API"]],"Comments":[]}
{"id":59635,"text":"ID:15970173\nPost:\nText: You should call matrix with parentheses: \nCode: >>> numpy.matrix([[1,2],[1,2]])\nmatrix([[1, 2],\n        [1, 2]])\n\nText: Try numpy tutorial. \nAPI:\nnumpy.matrix\n","label":[[40,46,"Mention"],[170,182,"API"]],"Comments":[]}
{"id":59636,"text":"ID:15970188\nPost:\nText: Just use the np.roll function: \nCode: a = np.array([0,1,2,3,4])\nb = np.roll(a,1)\nprint(b)\n>>> [4 0 1 2 3]\n\nText: See also this question. \nAPI:\nnumpy.roll\n","label":[[37,44,"Mention"],[167,177,"API"]],"Comments":[]}
{"id":59637,"text":"ID:15976131\nPost:\nText: You can do this with np.lexsort \nCode: In [1]: import numpy as np\nIn [2]: a = np.array([(4,0), (1,9), (1,0), (4,9)],\n                     dtype=[('x',int),('y',float)])\n\nIn [3]: a\nOut[3]: \narray([(4, 0.0), (1, 9.0), (1, 0.0), (4, 9.0)], \n      dtype=[('x', '<i8'), ('y', '<f8')])\n\nIn [4]: a['x']\nOut[4]: array([4, 1, 1, 4])\n\nIn [5]: a['y']\nOut[5]: array([ 0.,  9.,  0.,  9.])\n\nText: The order priority of the arguments to lexsort are opposite that of np.sort(..., order=...). So, to sort first by descending 'x' and then by ascending 'y': \nCode: In [6]: a[np.lexsort((a['y'], -a['x']))]\nOut[6]: \narray([(4, 0.0), (4, 9.0), (1, 0.0), (1, 9.0)], \n      dtype=[('x', '<i8'), ('y', '<f8')])\n\nText: Notes: \nText: This works assuming all your values are numerical (since the negative won't reverse string sorting). I've seen somewhere the use of a['x'][::-1] as a key instead of -a['x'] but that's not working for me right now. \nAPI:\nnumpy.lexsort\n","label":[[45,55,"Mention"],[952,965,"API"]],"Comments":[]}
{"id":59638,"text":"ID:15982404\nPost:\nText: @Bakuriu is probably correct that this is probably a micro-optimization. Your bottleneck is almost definitely IO, and after that, decompression. Allocating the memory twice probably isn't significant. \nText: However, if you wanted to avoid the extra memory allocation, you could use frombuffer to view the string as a numpy array. \nText: This avoids duplicating memory (the string and the array use the same memory buffer), but the array will be read-only, by default. You can then change it to allow writing, if you need to. \nText: In your case, it would be as simple as replacing fromstring with frombuffer: \nCode: f = gzip.GzipFile(filename)\nf.read(10000) # fixed length ascii header\nevent_dtype = np.dtype([\n        ('Id', '>u4'),                # simplified\n        ('UnixTimeUTC', '>u4', 2), \n        ('Data', '>i2', (1600,1024) ) \n        ])\ns = f.read( event_dtype.itemsize )\nevent = np.frombuffer(s, dtype=event_dtype, count=1)\n\nText: Just to prove that memory is not duplicated using this approach: \nCode: import numpy as np\n\nx = \"hello\"\ny = np.frombuffer(x, dtype=np.uint8)\n\n# Make \"y\" writeable...\ny.flags.writeable = True\n\n# Prove that we're using the same memory\ny[0] = 121\nprint x # <-- Notice that we're outputting changing y and printing x...\n\nText: This yields: yello instead of hello. \nText: Regardless of whether or not it's a significant optimization in this particular case, it's a useful approach to be aware of. \nAPI:\nnumpy.frombuffer\n","label":[[307,317,"Mention"],[1466,1482,"API"]],"Comments":[]}
{"id":59639,"text":"ID:15988852\nPost:\nText: Is this what you have in mind? \nText: Edit: As Patrick points out, one has to be careful with translating A(:) to Python. \nText: Of course if you just want to flatten out a matrix or 2-D array of zeros it does not matter. \nText: So here is a way to get behavior like matlab's. \nCode: >>> a = np.array([[1,2,3], [4,5,6]])\n>>> a\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> # one way to get Matlab behaivor\n... (a.T).ravel()\narray([1, 4, 2, 5, 3, 6])\n\nText: ravel does flatten 2D array, but does not do it the same way matlab's (:) does. \nCode: >>> import numpy as np\n>>> a = np.array([[1,2,3], [4,5,6]])\n>>> a\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> a.ravel()\narray([1, 2, 3, 4, 5, 6])\n\nAPI:\nnumpy.ravel\n","label":[[476,481,"Mention"],[712,723,"API"]],"Comments":[]}
{"id":59640,"text":"ID:16026231\nPost:\nText: The method multivariate_normal of the Generator class in random is the function that you want. \nText: Example: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\n\nnum_samples = 400\n\n# The desired mean values of the sample.\nmu = np.array([5.0, 0.0, 10.0])\n\n# The desired covariance matrix.\nr = np.array([\n        [  3.40, -2.75, -2.00],\n        [ -2.75,  5.50,  1.50],\n        [ -2.00,  1.50,  1.25]\n    ])\n\n# Generate the random samples.\nrng = np.random.default_rng()\ny = rng.multivariate_normal(mu, r, size=num_samples)\n\n\n# Plot various projections of the samples.\nplt.subplot(2,2,1)\nplt.plot(y[:,0], y[:,1], 'b.', alpha=0.25)\nplt.plot(mu[0], mu[1], 'ro', ms=3.5)\nplt.ylabel('y[1]')\nplt.axis('equal')\nplt.grid(True)\n\nplt.subplot(2,2,3)\nplt.plot(y[:,0], y[:,2], 'b.', alpha=0.25)\nplt.plot(mu[0], mu[2], 'ro', ms=3.5)\nplt.xlabel('y[0]')\nplt.ylabel('y[2]')\nplt.axis('equal')\nplt.grid(True)\n\nplt.subplot(2,2,4)\nplt.plot(y[:,1], y[:,2], 'b.', alpha=0.25)\nplt.plot(mu[1], mu[2], 'ro', ms=3.5)\nplt.xlabel('y[1]')\nplt.axis('equal')\nplt.grid(True)\n\nplt.show()\n\nText: Result: \nText: See also CorrelatedRandomSamples in the SciPy Cookbook. \nAPI:\nnumpy.random\n","label":[[81,87,"Mention"],[1168,1180,"API"]],"Comments":[]}
{"id":59641,"text":"ID:16082040\nPost:\nText: The problem is, that numpy can't give you the derivatives directly and you have two options: \nText: With NUMPY \nText: What you essentially have to do, is to define a grid in three dimension and to evaluate the function on this grid. Afterwards you feed this table of function values to gradient to get an array with the numerical derivative for every dimension (variable). \nText: Example from here: \nCode: from numpy import *\n\nx,y,z = mgrid[-100:101:25., -100:101:25., -100:101:25.]\n\nV = 2*x**2 + 3*y**2 - 4*z # just a random function for the potential\n\nEx,Ey,Ez = gradient(V)\n\nText: Without NUMPY \nText: You could also calculate the derivative yourself by using the centered difference quotient. \nText: This is essentially, what np.gradient is doing for every point of your predefined grid. \nAPI:\nnumpy.gradient\nnumpy.gradient\n","label":[[310,318,"Mention"],[754,765,"Mention"],[822,836,"API"],[837,851,"API"]],"Comments":[]}
{"id":59642,"text":"ID:16112798\nPost:\nText: There are a few different things going on here. \nText: First, Python has two mechanisms for turning an object into a string, called repr and str. repr is supposed to give 'faithful' output that would (ideally) make it easy to recreate exactly that object, while str aims for more human-readable output. For floats in Python versions up to and including Python 3.1, repr gives enough digits to determine the value of the float completely (so that evaluating the returned string gives back exactly that float), while str rounds to 12 decimal places; this has the effect of hiding inaccuracies, but means that two distinct floats that are very close together can end up with the same str value - something that can't happen with repr. When you print an object, you get the str of that object. In contrast, when you just evaluate an expression at the interpreter prompt, you get the repr. \nText: For example (here using Python 2.7): \nCode: >>> x = 1.0 \/ 7.0\n>>> str(x)\n'0.142857142857'\n>>> repr(x)\n'0.14285714285714285'\n>>> print x  # print uses 'str'\n0.142857142857\n>>> x  # the interpreter read-eval-print loop uses 'repr'\n0.14285714285714285\n\nText: But also, a little bit confusingly from your point of view, we get: \nCode: >>> x = 0.4\n>>> str(x)\n'0.4'\n>>> repr(x)\n'0.4'\n\nText: That doesn't seem to tie in too well with what you were seeing above, but we'll come back to this below. \nText: The second thing to bear in mind is that in your first example, you're printing two separate items, while in your second example (with the j removed), you're printing a single item: a tuple of length 2. Somewhat surprisingly, when converting a tuple for printing with str, Python nevertheless uses repr to compute the string representation of the elements of that tuple: \nCode: >>> x = 1.0 \/ 7.0\n>>> print x, x  # print x twice;  uses str(x)\n0.142857142857 0.142857142857\n>>> print(x, x)  # print a single tuple; uses repr(x)\n(0.14285714285714285, 0.14285714285714285)\n\nText: That explains why you're seeing different results in the two cases, even though the underlying floats are the same. \nText: But there's one last piece to the puzzle. In Python >= 2.7, we saw above that for the particular float 0.4, the str and repr of that float were the same. So where does the 0.40000000000000002 come from? Well, you don't have Python floats here: because you're getting these values from a NumPy array, they're actually of type numpy.float64: \nCode: >>> from numpy import zeros\n>>> A = zeros((2, 2))\n>>> A[:] = [[0.6, 0.4], [0.4, 0.6]]\n>>> A\narray([[ 0.6,  0.4],\n       [ 0.4,  0.6]])\n>>> type(A[0, 0])\n<type 'numpy.float64'>\n\nText: That type still stores a double-precision float, just like Python's float, but it's got some extra goodies that make it interact nicely with the rest of NumPy. And it turns out that NumPy uses a slightly different algorithm for computing the repr of a folat64 than Python uses for computing the repr of a float. Python (in versions >= 2.7) aims to give the shortest string that still gives an accurate representation of the float, while NumPy simply outputs a string based on rounding the underlying value to 17 significant digits. Going back to that 0.4 example above, here's what NumPy does: \nCode: >>> from numpy import float64\n>>> x = float64(1.0 \/ 7.0)\n>>> str(x)\n'0.142857142857'\n>>> repr(x)\n'0.14285714285714285'\n>>> x = float64(0.4)\n>>> str(x)\n'0.4'\n>>> repr(x)\n'0.40000000000000002'\n\nText: So these three things together should explain the results you're seeing. Rest assured that this is all completely cosmetic: the underlying floating-point value is not being changed in any way; it's just being displayed differently by the four different possible combinations of str and repr for the two types: float and numpy.float64. \nText: The Python tutorial give more details of how Python floats are stored and displayed, together with some of the potential pitfalls. The answers to this SO question have more information on the difference between str and repr. \nAPI:\nnumpy.float64\n","label":[[2894,2901,"Mention"],[4014,4027,"API"]],"Comments":[]}
{"id":59643,"text":"ID:16127536\nPost:\nText: np.where always returns a tuple of arrays, even when the argument is 1-D. It returns one array for each dimension. \nText: For example: \nCode: In [2]: a = np.array([10, 5, 3, 9, 1])\n\nIn [3]: np.where(a > 5)\nOut[3]: (array([0, 3]),)\n\nText: Note that Out[3] shows a tuple of length 1. The single object in the tuple is the numpy array of indices. To get the array, just pull it out of the tuple: \nCode: In [4]: np.where(a > 5)[0]\nOut[4]: array([0, 3])\n\nText: For your code, change your calcuation of missingValue to \nCode: missingValue = np.where(checkValue == False)[0]\n\nAPI:\nnumpy.where\n","label":[[24,32,"Mention"],[598,609,"API"]],"Comments":[]}
{"id":59644,"text":"ID:16229848\nPost:\nText: You need to convert array b to a (2, 1) shape array, use None or np.newaxis in the index tuple: \nCode: import numpy\na = numpy.array([[2,3,2],[5,6,1]])\nb = numpy.array([3,5])\nc = a * b[:, None]\n\nText: Here is the document. \nAPI:\nnumpy.newaxis\n","label":[[89,99,"Mention"],[252,265,"API"]],"Comments":[]}
{"id":59645,"text":"ID:16240957\nPost:\nText: Your question is difficult to understand because it contains extraneous information and contains typos. If I understand correctly, you simply want an efficient way to perform a set operation on the rows of a 2D array (in this case the intersection of the rows of K and f(K)). \nText: You can do this with np.in1d if you create structured array view. \nText: Code: \nText: if this is K: \nCode: In [50]: k\nOut[50]:\narray([[6, 6],\n       [3, 7],\n       [7, 5],\n       [7, 3],\n       [1, 3],\n       [1, 5],\n       [7, 6],\n       [3, 8],\n       [6, 1],\n       [6, 0]])\n\nText: and this is f(K) (for this example I subtract 1 from the first col and add 1 to the second): \nCode: In [51]: k2\nOut[51]:\narray([[5, 7],\n       [2, 8],\n       [6, 6],\n       [6, 4],\n       [0, 4],\n       [0, 6],\n       [6, 7],\n       [2, 9],\n       [5, 2],\n       [5, 1]])\n\nText: then you can find all rows in K also found in f(K) by doing something this: \nCode: In [55]: k[np.in1d(k.view(dtype='i,i').reshape(k.shape[0]),k2.view(dtype='i,i').\nreshape(k2.shape[0]))]\nOut[55]: array([[6, 6]])\n\nText: view and reshape create flat structured views so that each row appears as a single element to in1d. in1d creates a boolean index of k of matched items which is used to fancy index k and return the filtered array. \nAPI:\nnumpy.in1d\n","label":[[328,335,"Mention"],[1309,1319,"API"]],"Comments":[]}
{"id":59646,"text":"ID:16287044\nPost:\nText: You are computing the outer product of two vectors. You can use the function np.outer for this: \nCode: In [18]: a \nOut[18]: array([1, 2, 3])\n\nIn [19]: b\nOut[19]: array([10, 20, 30, 40])\n\nIn [20]: numpy.outer(b, a)\nOut[20]: \narray([[ 10,  20,  30],\n       [ 20,  40,  60],\n       [ 30,  60,  90],\n       [ 40,  80, 120]])\n\nAPI:\nnumpy.outer\n","label":[[101,109,"Mention"],[351,362,"API"]],"Comments":[]}
{"id":59647,"text":"ID:16406968\nPost:\nText: Taking the transpose of an array does not make a copy: \nCode: >>> a = np.arange(9).reshape(3,3)\n>>> b = np.transpose(a)\n>>> a\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> b\narray([[0, 3, 6],\n       [1, 4, 7],\n       [2, 5, 8]])\n>>> b[0,1] = 100\n>>> b\narray([[  0, 100,   6],\n       [  1,   4,   7],\n       [  2,   5,   8]])\n>>> a\narray([[  0,   1,   2],\n       [100,   4,   5],\n       [  6,   7,   8]])\n\nText: The same applies to a mmtrix object. \nText: This can be beneficial when you want to avoid unnecessarily consuming a lot of memory by copying very large arrays. But you also have to be careful to avoid unintentionally modifying the original array (if you still need it) when you modify the transpose. \nText: A number of numpy functions accept an optional \"out\" keyword (e.g., numpy.dot) to write the output to an existing array. For example, to take the matrix product of a with itself and write the output an existing array c: \nCode: numpy.dot(a, a, out=c)\n\nAPI:\nnumpy.matrix\n","label":[[470,476,"Mention"],[1011,1023,"API"]],"Comments":[]}
{"id":59648,"text":"ID:16415595\nPost:\nText: You want to use np.argsort \nCode: argsortx = np.argsort(x)\nx, y, z = x[argsortx], y[argsortx], z[argsortx]\n\nAPI:\nnumpy.argsort\n","label":[[40,50,"Mention"],[137,150,"API"]],"Comments":[]}
{"id":59649,"text":"ID:16437103\nPost:\nText: From the help of interp \nText: Does not check that the x-coordinate sequence xp is increasing. If xp is not increasing, the results are nonsense. A simple check for increasingness is:: np.all(np.diff(xp) > 0) \nText: It looks like you have mixed up abscissae, i.e. x-values, and ordinates, i.e. f(x) or y-values. \nText: So, if you want to find the x-value, where f(x)=-38, in an automated fashion, you need something more than just interpolation. For example, you may fit a polynomial p(x) to your data and then look for the roots of p(x)-(-38). \nAPI:\nnumpy.interp\n","label":[[41,47,"Mention"],[575,587,"API"]],"Comments":[]}
{"id":59650,"text":"ID:16444514\nPost:\nText: Using map(numpy.all, range(-2,3)) is actually creating a list with: \nCode: [numpy.all(-2), numpy.all(-1), numpy.all(0), numpy.all(1), numpy.all(2)]\n\nText: giving \nCode: [-2, -1, 0, 1, 2]\n\nText: If you did map(lambda x: numpy.all([x]), range(-2,3)), it would do: \nCode: [numpy.all([-2]), numpy.all([-1]), numpy.all([0]), numpy.all([1]), numpy.all([2])]\n\nText: giving \nCode: [True, True, False, True, True]\n\nText: As posted by @Mark Dickinson, there is a known issue with all in which it returns the input value instead of True or False for some inputs. In your second example map(numpy.all, [False, True]) does exactly as before, returning the input value. \nAPI:\nnumpy.all\n","label":[[494,497,"Mention"],[686,695,"API"]],"Comments":[]}
{"id":59651,"text":"ID:16491409\nPost:\nText: First, use the \"values\" component of your data.ix(whatever) object to get the raw array of the time series. Then use np.correlate to calculate the autocorrelation, using the method described in the answers to this question. \nAPI:\nnumpy.correlate\n","label":[[141,153,"Mention"],[254,269,"API"]],"Comments":[]}
{"id":59652,"text":"ID:16506778\nPost:\nText: Given that h is a recarray you can achieve the basic plotting by using \nCode: plt.plot_date(h.date, h.pres, linestyle='-', marker=None)\nplt.gcf().autofmt_xdate() # Optional; included to format x-axis for dates\n\nText: Further customization might be needed for the plot to look the way you want it. \nAPI:\nnumpy.recarray\n","label":[[42,50,"Mention"],[327,341,"API"]],"Comments":[]}
{"id":59653,"text":"ID:16507803\nPost:\nText: Using loadtxt it works, and you can use the converters parameter to define your functions. Having a tmp.txt file with: \nCode: 11,12,13,14,15,16,17,18,19\n21,22,23,24,25,26,27,28,29\n31,32,33,34,35,36,37,38,39\n41,42,43,44,45,46,47,48,49\n51,52,53,54,55,56,57,58,59\n\nText: You can load the selected columns with (also chosing the order which you want them to be stacked): \nCode: import numpy as np\nprint np.loadtxt('tmp.txt',delimiter=',',usecols=(-2,-1))\n#[[ 18.  19.]\n# [ 28.  29.]\n# [ 38.  39.]\n# [ 48.  49.]\n# [ 58.  59.]]\nprint np.loadtxt('tmp.txt',delimiter=',',usecols=(-1,-2),converters={-1: lambda x: float(x)+100})\n#[[ 119.   18.]\n# [ 129.   28.]\n# [ 139.   38.]\n# [ 149.   48.]\n# [ 159.   58.]]\n\nAPI:\nnumpy.loadtxt\n","label":[[30,37,"Mention"],[731,744,"API"]],"Comments":[]}
{"id":59654,"text":"ID:16597695\nPost:\nText: I posted another answer because for the example given here np.memmap worked: \nCode: offset = 0\ndata1 = np.memmap('tmp', dtype='i', mode='r+', order='F',\n                  offset=0, shape=(size1))\noffset += size1*byte_size\ndata2 = np.memmap('tmp', dtype='i', mode='r+', order='F',\n                  offset=offset, shape=(size2))\noffset += size1*byte_size\ndata3 = np.memmap('tmp', dtype='i', mode='r+', order='F',\n                  offset=offset, shape=(size3))\n\nText: for int32 byte_size=32\/8, for int16 byte_size=16\/8 and so forth... \nText: If the sizes are constant, you can load the data in a 2D array like: \nCode: shape = (total_length\/size,size)\ndata = np.memmap('tmp', dtype='i', mode='r+', order='F', shape=shape)\n\nText: You can change the memmap object as long as you want. It is even possible to make arrays sharing the same elements. In that case the changes made in one are automatically updated in the other. \nText: Other references: \nText: Working with big data in python and numpy, not enough ram, how to save partial results on disc? memmap documentation here. \nAPI:\nnumpy.memmap\nnumpy.memmap\n","label":[[83,92,"Mention"],[1072,1078,"Mention"],[1105,1117,"API"],[1118,1130,"API"]],"Comments":[]}
{"id":59655,"text":"ID:16598397\nPost:\nText: This is because b is a ndarray of ndarray, but not a 2-dim ndarray. \nText: You can use argsort to do this quickly: \nCode: import numpy as np\na = np.random.randint(0, 100, 5)\nb = np.random.randint(0, 5, (5, 5))\nprint a\nprint b\nidx = np.argsort(a)[::-1]\nprint a[idx]\nprint b[idx]\n\nText: output is: \nCode: [27 65  8 19 32]\n\n[[4 4 1 4 4]\n [1 3 4 3 3]\n [3 4 2 1 0]\n [1 0 1 0 4]\n [1 4 1 1 4]]\n\n[65 32 27 19  8]\n\n[[1 3 4 3 3]\n [1 4 1 1 4]\n [4 4 1 4 4]\n [1 0 1 0 4]\n [3 4 2 1 0]]\n\nText: if you want to use sorted you can use np.vstack to convert a list of array to a 2-dim ndarray: \nCode: ziped_and_sorted = sorted(zip(a,b), key=operator.itemgetter(0), reverse=True)\nnp.vstack([row[1] for row in ziped_and_sorted])\n\nAPI:\nnumpy.argsort\nnumpy.vstack\n","label":[[111,118,"Mention"],[541,550,"Mention"],[737,750,"API"],[751,763,"API"]],"Comments":[]}
{"id":59656,"text":"ID:16618280\nPost:\nText: I've been looking for ways to easily multithread some of my simple analysis code since I had noticed numpy it was only using one core, despite the fact that it is supposed to be multithreaded. \nText: Who says it's supposed to be multithreaded? \nText: numpy is primarily designed to be as fast as possible on a single core, and to be as parallelizable as possible if you need to do so. But you still have to parallelize it. \nText: In particular, you can operate on independent sub-objects at the same time, and slow operations release the GIL when possiblealthough \"when possible\" may not be nearly enough. Also, numpy objects are designed to be shared or passed between processes as easily as possible, to facilitate using multiprocessing. \nText: There are some specialized methods that are automatically parallelized, but most of the core methods are not. In particular, dot is implemented on top of BLAS when possible, and BLAS is automatically parallelized on most platforms, but mean is implemented in plain C code. \nText: See Parallel Programming with numpy and scipy for details. \nText: So, how do you know which methods are parallelized and which aren't? And, of those which aren't, how do you know which ones can be nicely manually-threaded and which need multiprocessing? \nText: There's no good answer to that. You can make educated guesses (X seems like it's probably implemented on top of ATLAS, and my copy of ATLAS is implicitly threaded), or you can read the source. \nText: But usually, the best thing to do is try it and test. If the code is using 100% of one core and 0% of the others, add manual threading. If it's now using 100% of one core and 10% of the others and barely running faster, change the multithreading to multiprocessing. (Fortunately, Python makes this pretty easy, especially if you use the Executor classes from concurrent.futures or the Pool classes from multiprocessing. But you still often need to put some thought into it, and test the relative costs of sharing vs. passing if you have large arrays.) \nText: Also, as kwatford points out, just because some method doesn't seem to be implicitly parallel doesn't mean it won't be parallel in the next version of numpy, or the next version of BLAS, or on a different platform, or even on a machine with slightly different stuff installed on it. So, be prepared to re-test. And do something like my_mean = man and then use my_mean everywhere, so you can just change one line to my_mean = pool_threaded_mean. \nAPI:\nnumpy.mean\n","label":[[2415,2418,"Mention"],[2523,2533,"API"]],"Comments":[]}
{"id":59657,"text":"ID:16633274\nPost:\nText: Using np.memmap you create arrays directly mapped into a file: \nCode: import numpy\na = numpy.memmap('test.mymemmap', dtype='float32', mode='w+', shape=(200000,1000))\n# here you will see a 762MB file created in your working directory    \n\nText: You can treat it as a conventional array: a += 1000. \nText: It is possible even to assign more arrays to the same file, controlling it from mutually sources if needed. But I've experiences some tricky things here. To open the full array you have to \"close\" the previous one first, using del: \nCode: del a    \nb = numpy.memmap('test.mymemmap', dtype='float32', mode='r+', shape=(200000,1000))\n\nText: But openning only some part of the array makes it possible to achieve the simultaneous control: \nCode: b = numpy.memmap('test.mymemmap', dtype='float32', mode='r+', shape=(2,1000))\nb[1,5] = 123456.\nprint a[1,5]\n#123456.0\n\nText: Great! a was changed together with b. And the changes are already written on disk. \nText: The other important thing worth commenting is the offset. Suppose you want to take not the first 2 lines in b, but lines 150000 and 150001. \nCode: b = numpy.memmap('test.mymemmap', dtype='float32', mode='r+', shape=(2,1000),\n                 offset=150000*1000*32\/8)\nb[1,2] = 999999.\nprint a[150001,2]\n#999999.0\n\nText: Now you can access and update any part of the array in simultaneous operations. Note the byte-size going in the offset calculation. So for a 'float64' this example would be 150000*1000*64\/8. \nText: Other references: \nText: Is it possible to map a discontiuous data on disk to an array with python? memmap documentation here. \nAPI:\nnumpy.memmap\nnumpy.memmap\n","label":[[30,39,"Mention"],[1602,1608,"Mention"],[1635,1647,"API"],[1648,1660,"API"]],"Comments":[]}
{"id":59658,"text":"ID:16699067\nPost:\nText: It appears that there exists a corrconef which computes the correlation coefficients, as desired. However, its interface is different from the Octave\/Matlab corr. \nText: First of all, by default, the function treats rows as variables, with the columns being observations. To mimic the behavior of Octave\/Matlab, you can pass a flag which reverses this. \nText: Also, according to this answer, the cov function (which corrcoef uses internally, I assume) returns a 2x2 matrix, each of which contain a specific covariance: \nCode: cov(a,a)  cov(a,b)\n\ncov(a,b)  cov(b,b)\n\nText: As he points out, the [0][1] element is what you'd want for cov(a,b). Thus, perhaps something like this will work: \nCode: for i in range(25):\n    c2[i] = numpy.corrcoef(a[:,i], b, rowvar=0)[0][1]\n\nText: For reference, here are some excerpts of the two functions that you had tried. It seems to be that they perform completely different things. \nText: Octave: \nText:  Function File: corr (x, y) Compute matrix of correlation coefficients. If each row of x and y is an observation and each column is a variable, then the (i, j)-th entry of corr (x, y) is the correlation between the i-th variable in x and the j-th variable in y. corr (x,y) = cov (x,y) \/ (std (x) * std (y)) If called with one argument, compute corr (x, x), the correlation between the columns of x. \nText: And Numpy: \nText: numpy.correlate(a, v, mode='valid', old_behavior=False)[source] Cross-correlation of two 1-dimensional sequences. This function computes the correlation as generally defined in signal processing texts: z[k] = sum_n a[n] * conj(v[n+k]) with a and v sequences being zero-padded where necessary and conj being the conjugate. \nAPI:\nnumpy.corrcoef\nnumpy.cov\n","label":[[55,64,"Mention"],[420,423,"Mention"],[1715,1729,"API"],[1730,1739,"API"]],"Comments":[]}
{"id":59659,"text":"ID:16732470\nPost:\nText: Use np.in1d : \nCode: In [6]: array1 = np.array([0, 1, 2, 5, 0])\n\nIn [7]: array2 = np.array([0, 10, 20, 1, 2, 30, 5])\n\nIn [8]: np.in1d(array1, array2)\nOut[8]: array([ True,  True,  True,  True,  True], dtype=bool)\n\nIn [9]: np.all(np.in1d(array1, array2))\nOut[9]: True\n\nAPI:\nnumpy.in1d\n","label":[[28,35,"Mention"],[297,307,"API"]],"Comments":[]}
{"id":59660,"text":"ID:16781247\nPost:\nText: It's not overwritten, you are just always appending the same array. pos += vel*dt adds to pos array in-place, it doesn't create a new one. So you end up with a list consisting of a number of references to this same array. \nText: You'll have to np.copy it each time. \nAPI:\nnumpy.copy\n","label":[[268,275,"Mention"],[296,306,"API"]],"Comments":[]}
{"id":59661,"text":"ID:16867494\nPost:\nText: Using np.concatenate apparently load the arrays into memory. To avoid this you can easily create a thrid memmap array in a new file and read the values from the arrays you wish to concatenate. In a more efficient way, you can also append new arrays to an already existing file on disk. \nText: For any case you must choose the right order for the array (row-major or column-major). \nText: The following examples illustrate how to concatenate along axis 0 and axis 1. \nText: 1) concatenate along axis=0 \nCode: a = np.memmap('a.array', dtype='float64', mode='w+', shape=( 5000,1000)) # 38.1MB\na[:,:] = 111\nb = np.memmap('b.array', dtype='float64', mode='w+', shape=(15000,1000)) # 114 MB\nb[:,:] = 222\n\nText: You can define a third array reading the same file as the first array to be concatenated (here a) in mode r+ (read and append), but with the shape of the final array you want to achieve after concatenation, like: \nCode: c = np.memmap('a.array', dtype='float64', mode='r+', shape=(20000,1000), order='C')\nc[5000:,:] = b\n\nText: Concatenating along axis=0 does not require to pass order='C' because this is already the default order. \nText: 2) concatenate along axis=1 \nCode: a = np.memmap('a.array', dtype='float64', mode='w+', shape=(5000,3000)) # 114 MB\na[:,:] = 111\nb = np.memmap('b.array', dtype='float64', mode='w+', shape=(5000,1000)) # 38.1MB\nb[:,:] = 222\n\nText: The arrays saved on disk are actually flattened, so if you create c with mode=r+ and shape=(5000,4000) without changing the array order, the 1000 first elements from the second line in a will go to the first in line in c. But you can easily avoid this passing order='F' (column-major) to memmap: \nCode: c = np.memmap('a.array', dtype='float64', mode='r+',shape=(5000,4000), order='F')\nc[:, 3000:] = b\n\nText: Here you have an updated file 'a.array' with the concatenation result. You may repeat this process to concatenate in pairs of two. \nText: Related questions: \nText: Working with big data in python and numpy, not enough ram, how to save partial results on disc? \nAPI:\nnumpy.concatenate\n","label":[[30,44,"Mention"],[2071,2088,"API"]],"Comments":[]}
{"id":59662,"text":"ID:17039049\nPost:\nText: Use sparse.kron for the Kronecker product of sparse matrices. \nText: kron does not handle sparse matrices. When given sparse matrices, it might not generate an error, but the value it returns will not be correct. \nAPI:\nnumpy.kron\n","label":[[93,97,"Mention"],[243,253,"API"]],"Comments":[]}
{"id":59663,"text":"ID:17050966\nPost:\nText: The problem happened because your code is somehow creating a arrvay of objects. See this question with a similar issue. When it happens you get something like: \nCode: a = numpyp.array([list1, list2, list3, ... , listn], dtype=object)\n\nText: It is a 1D array, but when you ask to print it will call the __str__ of each list inside, giving: \nCode: [[ 1, 2, 3, 4],\n [ 5, 6, 7, 8]]\n\nText: which seems like a 2D array. \nText: You can simulate it doing: \nCode: a = ['aaa' for i in range(10)]\nb = numpy.empty((5),dtype=object)\nb.fill(a) \n\nText: lets check b: \nCode: b.shape # (5,)\nb.ndim  # 1\n\nText: but print b gives: \nCode: [['aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa']\n ['aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa']\n ['aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa']\n ['aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa']\n ['aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa', 'aaa']]\n\nText: Quite tricky... \nAPI:\nnumpy.array\n","label":[[85,91,"Mention"],[1033,1044,"API"]],"Comments":[]}
{"id":59664,"text":"ID:17081678\nPost:\nText: You can choose from given array using np.choose which constructs an array from an index array (in your case select_id) and a set of arrays (in your case input_array) to choose from. However you may first need to transpose input_array to match dimensions. The following shows a small example: \nCode: In [101]: input_array\nOut[101]: \narray([[ 3, 14],\n       [12,  5],\n       [75, 50]])\n\nIn [102]: input_array.shape\nOut[102]: (3, 2)\n\nIn [103]: select_id\nOut[103]: [0, 1, 1]\n\nIn [104]: output_array = np.choose(select_id, input_array.T)\n\nIn [105]: output_array\nOut[105]: array([ 3,  5, 50])\n\nAPI:\nnumpy.choose\n","label":[[62,71,"Mention"],[617,629,"API"]],"Comments":[]}
{"id":59665,"text":"ID:17115673\nPost:\nText: Cause of the Problem \nText: The error is because you're trying to index beyond the bounds of the sediment_transport grid (e.g. the i+1 and j+1 portions). Right now, you're trying to get a value that doesn't exist when you're at a boundary of the grid. Also, it's not raising an error, but you're currently grabbing the opposite edge when you're at i=0 or j=0 (due to the i-1 and j-1 parts). \nText: You mentioned that you wanted the values of elevation_change to be 0 at the boundaries (which certainly seems reasonable). Another common boundary condition is to \"wrap\" the values and grab a value from the opposite edge. It probably doesn't make much sense in this case, but I'll show it in a couple of examples because it's easy to implement with some of the methods. \nText: Tempting but Incorrect \nText: It's tempting to just catch the exception and set the value to 0. For example: \nCode: for [i, j], flow in np.ndenumerate(flow_direction_np):\n    try:\n        if flow == 32:\n            ...\n        elif ...\n            ...\n    except IndexError:\n        elevation_change[i, j] = 0\n\nText: However, this approach is actually incorrect. Negative indexing is valid, and will return the opposite edge of the grid. Therefore, this would basically implement a \"zero\" boundary condition on the right and bottom edges of the grid, and a \"wrap-around\" boundary condition on the left and top edges. \nText: Padding with zeros \nText: In the case of \"zero\" boundary conditions, there's a very simple way to avoid indexing problems: Pad the sediment_transport grid with zeros. This way, if we index beyond the edge of the original grid, we'll get a 0. (Or whatever constant value you'd like to pad the array with.) \nText: Side note: This is the perfect place to use numpy.pad. However, it was added in v1.7. I'm going to skip using it here, as the OP mentions ArcGIS, and Arc doesn't ship with an up-to-date version of numpy. \nText: For example: \nCode: padded_transport = np.zeros((rows + 2, cols + 2), float)\npadded_transport[1:-1, 1:-1] = sediment_transport  \n# The two lines above could be replaced with:\n#padded_transport = np.pad(sediment_transport, 1, mode='constant')\n\nfor [i, j], flow in np.ndenumerate(flow_direction):\n    # Need to take into account the offset in the \"padded_transport\"\n    r, c = i + 1, j + 1\n\n    if flow == 32:\n        elevation_change[i, j] = padded_transport[r - 1, c - 1]\n    elif flow == 16:\n        elevation_change[i, j] = padded_transport[r, c - 1]\n    elif flow == 8:\n        elevation_change[i, j] = padded_transport[r + 1, c - 1]\n    elif flow == 4:\n        elevation_change[i, j] = padded_transport[r + 1, c]\n    elif flow == 64:\n        elevation_change[i, j] = padded_transport[r - 1, c]\n    elif flow == 128:\n        elevation_change[i, j] = padded_transport[r - 1, c + 1]\n    elif flow == 1:\n        elevation_change[i, j] = padded_transport[r, c + 1]\n    elif flow == 2:\n        elevation_change[i, j] = padded_transport[r + 1, c + 1]\n\nText: DRY (Don't Repeat Yourself) \nText: We can write this code a bit more compactly by using a dict: \nCode: elevation_change = np.zeros_like(sediment_transport)\nnrows, ncols = flow_direction.shape\nlookup = {32: (-1, -1),\n          16:  (0, -1), \n          8:   (1, -1),\n          4:   (1,  0),\n          64: (-1,  0),\n          128:(-1,  1),\n          1:   (0,  1),\n          2:   (1,  1)}\n\npadded_transport = np.zeros((nrows + 2, ncols + 2), float)\npadded_transport[1:-1, 1:-1] = sediment_transport  \n\nfor [i, j], flow in np.ndenumerate(flow_direction):\n    # Need to take into account the offset in the \"padded_transport\"\n    r, c = i + 1, j + 1\n    # This also allows for flow_direction values not listed above...\n    dr, dc = lookup.get(flow, (0,0))\n    elevation_change[i,j] = padded_transport[r + dr, c + dc]\n\nText: At this point, it's a bit superfluous to pad the original array. Implement different boundary conditions by padding is very easy if you use numpy.pad, but we could just write the logic out directly: \nCode: elevation_change = np.zeros_like(sediment_transport)\nnrows, ncols = flow_direction.shape\nlookup = {32: (-1, -1),\n          16:  (0, -1), \n          8:   (1, -1),\n          4:   (1,  0),\n          64: (-1,  0),\n          128:(-1,  1),\n          1:   (0,  1),\n          2:   (1,  1)}\n\nfor [i, j], flow in np.ndenumerate(flow_direction):\n    dr, dc = lookup.get(flow, (0,0))\n    r, c = i + dr, j + dc\n    if not ((0 <= r < nrows) & (0 <= c < ncols)):\n        elevation_change[i,j] = 0\n    else:\n        elevation_change[i,j] = sediment_transport[r, c]\n\nText: \"Vectorizing\" the Calculation \nText: Iterating through numpy arrays in python is rather slow for reasons I won't delve into here. Therefore, there are more efficient ways to implement this in numpy. The trick is to use roll along with boolean indexing. \nText: For \"wrap-around\" boundary conditions, it's as simple as: \nCode: elevation_change = np.zeros_like(sediment_transport)\nnrows, ncols = flow_direction.shape\nlookup = {32: (-1, -1),\n          16:  (0, -1), \n          8:   (1, -1),\n          4:   (1,  0),\n          64: (-1,  0),\n          128:(-1,  1),\n          1:   (0,  1),\n          2:   (1,  1)}\n\nfor value, (row, col) in lookup.iteritems():\n    mask = flow_direction == value\n    shifted = np.roll(mask, row, 0)\n    shifted = np.roll(shifted, col, 1)\n    elevation_change[mask] = sediment_transport[shifted]\n\nreturn elevation_change\n\nText: If you're not familiar with numpy, this probably looks a bit like greek. There are two parts to this. The first is using boolean indexing. As a quick example of what this does: \nCode: In [1]: import numpy as np\n\nIn [2]: x = np.arange(9).reshape(3,3)\n\nIn [3]: x\nOut[3]: \narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\nIn [4]: mask = np.array([[False, False, True],\n...                      [True, False, False],\n...                      [True, False, False]])\n\n\nIn [5]: x[mask]\nOut[5]: array([2, 3, 6])\n\nText: As you can see, if we index an array with a boolean grid of the same shape, the values where it is True, will be returned. Similarly, you can set values this way. \nText: The next trick is numpy.roll. This will shift the values by a given amount in one direction. They'll \"wrap-around\" at the edges. \nCode: In [1]: import numpy as np\n\nIn [2]: np.array([[0,0,0],[0,1,0],[0,0,0]])\nOut[2]: \narray([[0, 0, 0],\n       [0, 1, 0],\n       [0, 0, 0]])\n\nIn [3]: x = _\n\nIn [4]: np.roll(x, 1, axis=0)\nOut[4]: \narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 1, 0]])\n\nIn [5]: np.roll(x, 1, axis=1)\nOut[5]: \narray([[0, 0, 0],\n       [0, 0, 1],\n       [0, 0, 0]])\n\nText: Hopefully that makes a bit of sense, at any rate. \nText: To implement \"zero\" boundary conditions (or arbitrary boundary conditions using numpy.pad), we'd do something like this: \nCode: def vectorized(flow_direction, sediment_transport):\n    elevation_change = np.zeros_like(sediment_transport)\n    nrows, ncols = flow_direction.shape\n    lookup = {32: (-1, -1),\n              16:  (0, -1), \n              8:   (1, -1),\n              4:   (1,  0),\n              64: (-1,  0),\n              128:(-1,  1),\n              1:   (0,  1),\n              2:   (1,  1)}\n\n    # Initialize an array for the \"shifted\" mask\n    shifted = np.zeros((nrows+2, ncols+2), dtype=bool)\n\n    # Pad \"sediment_transport\" with zeros\n    # Again, `np.pad` would be better and more flexible here, as it would\n    # easily allow lots of different boundary conditions...\n    tmp = np.zeros((nrows+2, ncols+2), sediment_transport.dtype)\n    tmp[1:-1, 1:-1] = sediment_transport\n    sediment_transport = tmp\n\n    for value, (row, col) in lookup.iteritems():\n        mask = flow_direction == value\n\n        # Reset the \"shifted\" mask\n        shifted.fill(False)\n        shifted[1:-1, 1:-1] = mask\n\n        # Shift the mask by the right amount for the given value\n        shifted = np.roll(shifted, row, 0)\n        shifted = np.roll(shifted, col, 1)\n\n        # Set the values in elevation change to the offset value in sed_trans\n        elevation_change[mask] = sediment_transport[shifted]\n\n    return elevation_change\n\nText: Advantage to Vectorization \nText: The \"vectorized\" version is much faster, but will use more RAM. \nText: For a 1000 by 1000 grid: \nCode: In [79]: %timeit vectorized(flow_direction, sediment_transport)\n10 loops, best of 3: 170 ms per loop\n\nIn [80]: %timeit iterate(flow_direction, sediment_transport)\n1 loops, best of 3: 5.36 s per loop\n\nIn [81]: %timeit lookup(flow_direction, sediment_transport)\n1 loops, best of 3: 3.4 s per loop\n\nText: These results are from comparing the following implementations with randomly generated data: \nCode: import numpy as np\n\ndef main():\n    # Generate some random flow_direction and sediment_transport data...\n    nrows, ncols = 1000, 1000\n    flow_direction = 2 ** np.random.randint(0, 8, (nrows, ncols))\n    sediment_transport = np.random.random((nrows, ncols))\n\n    # Make sure all of the results return the same thing...\n    test1 = vectorized(flow_direction, sediment_transport)\n    test2 = iterate(flow_direction, sediment_transport)\n    test3 = lookup(flow_direction, sediment_transport)\n    assert np.allclose(test1, test2)\n    assert np.allclose(test2, test3)\n\n\ndef vectorized(flow_direction, sediment_transport):\n    elevation_change = np.zeros_like(sediment_transport)\n    sediment_transport = np.pad(sediment_transport, 1, mode='constant')\n    lookup = {32: (-1, -1),\n              16:  (0, -1), \n              8:   (1, -1),\n              4:   (1,  0),\n              64: (-1,  0),\n              128:(-1,  1),\n              1:   (0,  1),\n              2:   (1,  1)}\n\n    for value, (row, col) in lookup.iteritems():\n        mask = flow_direction == value\n        shifted = np.pad(mask, 1, mode='constant')\n        shifted = np.roll(shifted, row, 0)\n        shifted = np.roll(shifted, col, 1)\n        elevation_change[mask] = sediment_transport[shifted]\n\n    return elevation_change\n\ndef iterate(flow_direction, sediment_transport):\n    elevation_change = np.zeros_like(sediment_transport)\n    padded_transport = np.pad(sediment_transport, 1, mode='constant')  \n\n    for [i, j], flow in np.ndenumerate(flow_direction):\n        r, c = i + 1, j + 1\n        if flow == 32:\n            elevation_change[i, j] = padded_transport[r - 1, c - 1]\n        elif flow == 16:\n            elevation_change[i, j] = padded_transport[r, c - 1]\n        elif flow == 8:\n            elevation_change[i, j] = padded_transport[r + 1, c - 1]\n        elif flow == 4:\n            elevation_change[i, j] = padded_transport[r + 1, c]\n        elif flow == 64:\n            elevation_change[i, j] = padded_transport[r - 1, c]\n        elif flow == 128:\n            elevation_change[i, j] = padded_transport[r - 1, c + 1]\n        elif flow == 1:\n            elevation_change[i, j] = padded_transport[r, c + 1]\n        elif flow == 2:\n            elevation_change[i, j] = padded_transport[r + 1, c + 1]\n    return elevation_change\n\ndef lookup(flow_direction, sediment_transport):\n    elevation_change = np.zeros_like(sediment_transport)\n    nrows, ncols = flow_direction.shape\n    lookup = {32: (-1, -1),\n              16:  (0, -1), \n              8:   (1, -1),\n              4:   (1,  0),\n              64: (-1,  0),\n              128:(-1,  1),\n              1:   (0,  1),\n              2:   (1,  1)}\n\n    for [i, j], flow in np.ndenumerate(flow_direction):\n        dr, dc = lookup.get(flow, (0,0))\n        r, c = i + dr, j + dc\n        if not ((0 <= r < nrows) & (0 <= c < ncols)):\n            elevation_change[i,j] = 0\n        else:\n            elevation_change[i,j] = sediment_transport[r, c]\n\n    return elevation_change\n\nif __name__ == '__main__':\n    main()\n\nAPI:\nnumpy.roll\n","label":[[4799,4803,"Mention"],[11678,11688,"API"]],"Comments":[]}
{"id":59666,"text":"ID:17291741\nPost:\nText: You can use either numpy broadcasting or a combination of np.repeat and numpy.reshape: \nCode: my_dtype = numpy.dtype([('Image', h5py.special_dtype(ref=h5py.Reference)), \n             ('NextLevel', h5py.special_dtype(ref=h5py.Reference))])\nref_array = array( (image.ref, dataset.ref), dtype=my_dtype)\ndataset = numpy.repeat(ref_array, n*n)\ndataset = dataset.reshape( (n,n) )\n\nText: Note that repkeat returns a flattened array, hence the use of numpy.reshape. It seems repeat is faster than just broadcasting it: \nCode: %timeit empty_dataset=np.empty(2*2,dtype=my_dtype); empty_dataset[:]=ref_array\n100000 loops, best of 3: 9.09 us per loop\n\n%timeit repeat_dataset=np.repeat(ref_array, 2*2).reshape((2,2))\n100000 loops, best of 3: 5.92 us per loop\n\nAPI:\nnumpy.repeat\nnumpy.repeat\n","label":[[82,91,"Mention"],[415,422,"Mention"],[776,788,"API"],[789,801,"API"]],"Comments":[]}
{"id":59667,"text":"ID:17384910\nPost:\nText: Perhaps it would be more convenient to write it using np.save instead? \nText: Alternatively, you can also use: \nCode: numpy.array_str(array[i], max_line_width=1000000)\n\nAPI:\nnumpy.save\n","label":[[78,85,"Mention"],[198,208,"API"]],"Comments":[]}
{"id":59668,"text":"ID:17395568\nPost:\nText: You don't need to worry about the 0s, they shouldn't effect how the averages compare since there will presumably be one in each row. Hence, you can do something like this to get the index of the row with the highest average: \nCode: >>> import numpy as np \n>>> complete_matrix = np.array([\n...     [0, 1, 2, 4],\n...     [1, 0, 3, 5],\n...     [2, 3, 0, 6],\n...     [4, 5, 6, 0]])\n>>> np.argmax(np.mean(complete_matrix, axis=1))\n3\n\nText: Reference: \nText: mean argmax \nAPI:\nnumpy.mean\nnumpy.argmax\n","label":[[477,481,"Mention"],[482,488,"Mention"],[495,505,"API"],[506,518,"API"]],"Comments":[]}
{"id":59669,"text":"ID:17439554\nPost:\nText: Here is one way of doing this: \nCode: def unscramble(A_shuf,ind_shuf):\n    order = np.lexsort(ind_shuf[::-1])\n    return A_shuf.flat[order].reshape(A_shuf.shape)\n\nText: You essentially have the rank of each item in the form of n indices, ie a = (0, 0), b = (0, 1), c = (0, 2), d = (1, 0) ... and so on. If you argsort the ranks you'll the the reordering you need to put the items in ascending order. You could use lexsort or you could use ravel_multi_index to get the ranks as integers and apply the argsort on the integer ranks. Let me know if the explanation isn't clear. \nAPI:\nnumpy.ravel_multi_index\n","label":[[463,480,"Mention"],[604,627,"API"]],"Comments":[]}
{"id":59670,"text":"ID:17478237\nPost:\nText: Here is a partial solution. I have corrected the python side, which was not working for me as posted. \nText: First, I think you should be able to multiply matrices in PHP by writing your own function. \nText: It can not be as difficult to do matrix multiplication in PHP as it is to deal with JSON, python, and starting a new process and moving data back and forth. \nText: There is an unmaintained PHP library for matrix multiplication at http:\/\/pear.php.net\/package\/Math_Matrix \nText: Ok, so if you want to do it this Rube-Goldberg-ish way here is the corrected python code. It needed imports for json and sys, and .tolist() to deal with getting json to encode the matrix result (json won't encode matrix as-is because it isn't a simple array). I discarded the unit test library for mtx in favor of using the overloaded * instead of matrix_multiply. \nCode: #!\/usr\/bin\/python\nimport json\nimport sys\nimport numpy as np\nfrom numpy import matrix\nprint json.loads(sys.argv[1])\narr=json.loads(sys.argv[1])\narr1=arr[0]\narr2=arr[1]\nprint arr1\nprint arr2\nA=np.asmatrix(arr1)\nprint A\nB=np.asmatrix(arr2)\nprint B\nZ1 = A*B\nprint Z1\nprint json.dumps(Z1.tolist())\n\nText: This is a test prototype. For a \"production\" version you should delete all of the prints except the last one. \nText: Test run: \nCode: .\/matrix_multiply.py \"[[[2,0],[0,1]],[[1,3],[2,4]]]\"\n[[[2, 0], [0, 1]], [[1, 3], [2, 4]]]\n[[2, 0], [0, 1]]\n[[1, 3], [2, 4]]\n[[2 0]\n [0 1]]\n[[1 3]\n [2 4]]\n[[2 6]\n [2 4]]\n[[2, 6], [2, 4]]\n\nText: looks fine. \nText: I haven't written any PHP in over 10 years, so I will leave that part to someone else. \nAPI:\nnumpy.matrix\n","label":[[807,810,"Mention"],[1620,1632,"API"]],"Comments":[]}
{"id":59671,"text":"ID:17567754\nPost:\nText: This is not related to the system, but caused by the formatting employed by numpy. The easiest fix is to modify your script to invoke set_printoptions like: \nCode: if __name__ == '__main__':\n    alpha = float(sys.argv[1])\n    beta = float(sys.argv[2])\n    Iterations = float(sys.argv[3])\n    numpy.set_printoptions(threshold=Iterations)   # Force threshold to iteration count.\n    sys.stdout.write(str(Weibull_Random(alpha,beta,Iterations)))\n\nText: In this snippet, I force numpy to set the summarization threshold to the number of iterations. \nAPI:\nnumpy.set_printoptions\n","label":[[158,174,"Mention"],[574,596,"API"]],"Comments":[]}
{"id":59672,"text":"ID:17637348\nPost:\nText: According to the Basic slicing documentation: \nText: All arrays generated by basic slicing are always views of the original array. \nText: Use ndarray.copy or copy to get copy. \nAPI:\nnumpy.copy\n","label":[[182,186,"Mention"],[206,216,"API"]],"Comments":[]}
{"id":59673,"text":"ID:17806818\nPost:\nText: first thing there are a couple of typos in your code. It's: \nCode: X = numpy.zeros((50000,25)) # it's a tuple going in\nC = pdist(X, 'euclidean') # euclidean with an e\n\nText: of course it does not matter for the question. \nText: The Euclidean pdist is just a call for normnp (http:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.linalg.norm.html). It's a very general function. If it does not work in your case due to memory constraints you can always create something yourself. Two 50000 length vectors do not take that much memory and this can make one pairwise comparison: \nCode: np.sqrt(np.sum(np.square(X[0])) + np.sum(np.square(X[1])))\n\nText: And then you only need to loop through the whole thing. \nText: Hope it helps, P \nAPI:\nnumpy.linalg.norm\n","label":[[291,297,"Mention"],[763,780,"API"]],"Comments":[]}
{"id":59674,"text":"ID:17959303\nPost:\nCode: numpy.nonzero(data[:,0]==6)[0]\n\nText: data[:,0]==6 returns an array of booleans, 1 when the condition is true, 0 when it is false \nText: nonzero returns the index of nonzero elements inside of a container \nText: you may also be interested to know that you can do things like \nCode: data[data[:,0]==6,2]\n\nText: to grab all the elements from the 2nd column when the first column is zero \nAPI:\nnumpy.nonzero\n","label":[[161,168,"Mention"],[415,428,"API"]],"Comments":[]}
{"id":59675,"text":"ID:18066500\nPost:\nText: packbits to turn it into a uint8 array for writing, then np.unpackbits after reading it back. np.packbits pads the axis you're packing along with zeros to get to a multiple of 8, so make sure you keep track of how many zeros you'll need to chop off the end when you unpack the array. \nAPI:\nnumpy.packbits\nnumpy.unpackbits\nnumpy.packbits\n","label":[[24,32,"Mention"],[81,94,"Mention"],[118,129,"Mention"],[314,328,"API"],[329,345,"API"],[346,360,"API"]],"Comments":[]}
{"id":59676,"text":"ID:18133725\nPost:\nCode: [np.all((x[i], y[i])) for i in range(1000)]\n\nText: can be rewritten as \nCode: x = []\nfor i in range(1000):\n    x.append(numpy.all((x[i],y[i])))\n\nText: so you are calling dll on a very small list \nText: numpy methods usually shine on much larger lists \nCode: timeit.timeit('all(x)','x = numpy.arange(1,100000)',number=1)\n#~.0175\ntimeit.timeit('numpy.all(x)','x = numpy.arange(1,100000)',number=1)\n#~.00043\n\nAPI:\nnumpy.all\n","label":[[194,197,"Mention"],[435,444,"API"]],"Comments":[]}
{"id":59677,"text":"ID:18163730\nPost:\nText: You can use np.newaxis: \nCode: >>> np.arange(10)[:, np.newaxis]\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6],\n       [7],\n       [8],\n       [9]])\n\nText: np.newaxis is just an alias for None, and was added by numpy developers mainly for readability. Therefore np.arange(10)[:, None] would produce the same exact result as the above solution. \nText: Edit: \nText: Another option is: \nCode: np.expand_dims(np.arange(10), axis=1)\n\nText: expand_dims \nAPI:\nnumpy.expand_dims\n","label":[[495,506,"Mention"],[513,530,"API"]],"Comments":[]}
{"id":59678,"text":"ID:18163999\nPost:\nText: Thats because np.newaxis is an alias for None as it says in the documentation: None can also be used instead of newaxis. \nAPI:\nnumpy.newaxis\n","label":[[38,48,"Mention"],[151,164,"API"]],"Comments":[]}
{"id":59679,"text":"ID:18192719\nPost:\nText: If you want to make a set-style data structure out of the collection of columns, here is one way to do it (I'm sure there are more efficient ways for larger data): \nCode: group = {}\nfor i in range(array.shape[1]):\n    tup = tuple(array[:,i])\n    if tup in group.keys():\n        group[tup].append(i)\n    else:\n        group[tup] = [i]\n\nText: Executed for your example of array gives: \nCode: In [132]: group\nOut[132]:\n{(0, 1): [0],\n (0, 2): [6],\n (1, 0): [5],\n (1, 1): [4, 7, 8],\n (2, 0): [2, 9],\n (2, 1): [1, 3]}\n\nText: Since ndarruay is (like list) not hashable, the columns can't serve as the dict key themselves. I chose to just use the tuple-equivalent of the column, but there are many other choices. \nText: Also, I assume that a list of the column indices is desired in group. If this is really true, you could consider using a defaultdict instead of a regular dict. But you can also use many other containers for storing the column indices. \nText: Updated \nText: I believe I understand better what the question is asking: Given an arbitrary collection of pre-defined groups of columns, how to determine whether any two given groups contain a column in common. \nText: If we assume you have already built the set-like structure in my answer above, you can take the two groups, look at their constituent columns, and ask if any columns wind up being in the same part of the set dictionary: \nText: Suppose we define: \nCode: my_partition['first']  = [0,1,2]\nmy_partition['second'] = [3,4]\nmy_partition['third']  = [5,6,7]\nmy_partition['fourth'] = [8, 9]\n\n# Define a helper to back-out the column that serves as a key for the set-like structure.\n# Take 0th element, column index should only be part of one subset.\nget_key = lambda x: [k for k,v in group.iteritems() if x in v][0]\n\n# use itertools\nimport itertools\n\n# Print out the common columns between each pair of groups.\nfor pair_x, pair_y in itertools.combinations(my_partition.keys(), 2):\n    print pair_x, pair_y, (set(map(get_key, my_partition[pair_x])) &\n                           set(map(get_key, my_partition[pair_y])))\n\nText: Whenever that's not the empty set, it means some columns were in common between the two groups. \nText: Executed for your problem: \nCode: In [163]: for pair_x, pair_y in itertools.combinations(my_partition.keys(), 2):\n    print pair_x, pair_y, set(map(get_key, my_partition[pair_x])) & set(map(get_key, my_partition[pair_y]))\n   .....:\nsecond fourth set([(1, 1)])\nsecond third set([(1, 1)])\nsecond first set([(2, 1)])\nfourth third set([(1, 1)])\nfourth first set([(2, 0)])\nthird first set([])\n\nAPI:\nnumpy.ndarray\n","label":[[549,557,"Mention"],[2610,2623,"API"]],"Comments":[]}
{"id":59680,"text":"ID:18217289\nPost:\nText: Based on the info you provided, typemaps would work. But my experience as an intermittent SWIG user (typically a couple weeks where I use it a lot then hiatus till next project\/phase) is that few people have the time to grok that feature. \nText: In your case I believe SWIG typemaps are more of a convenience than a requirement so I would use one of two approaches: \nText: create a Python function that does the conversion from ar to Matrix (Matrix is the one exported from C++ to Python by SWIG) use the builtin numpy.i typemap to call a C++ function that accepts a C++ type in that typemap, and define that function to call int some_function(Matrix& in) \nText: The advantage of option 1 is that you can automate the transformation by rebinding the Python function: \nCode: old_some_func = some_function\ndef some_function(numpy_array):\n    tempMat = Matrix()\n    # convert numpy_array to SWIG'd Matrix class\n    old_some_func(tempMat)\n\nText: The performance hit of doing this is likely to be negligible, but you should test. Without SWIG (i.e. if you were using the C API) this technique would have the additional advantage of not requiring you to change your C++ library (see the extend directive of SWIG). \nText: The advantage of option 2 is that the conversion is at the C\/C++ level, so depending on what's involved, you may get improved performance. Say one of the numpy.i typemaps maps ar to float[numValues] array, and your C++ Matrix holds single precision floating point values. In that case you define a C++ some_function(float*, numValues) in the .i file for your project, and that function calls some_function(Matrix). Check whether your C++ Matrix class can store a pointer to array data, this way you will avoid copying data across one and maybe even two layers (Python -> SWIG some_function(array) -> some_function(Matrix)). \nText: But keep in mind: your computations inside your C++ some_function may make any performance difference between the two options insignificant. You have to test. Then go with the one that is easiest and most maintainable (probably option 1). \nAPI:\nnumpy.array\nnumpy.array\n","label":[[452,454,"Mention"],[1415,1417,"Mention"],[2115,2126,"API"],[2127,2138,"API"]],"Comments":[]}
{"id":59681,"text":"ID:18232374\nPost:\nText: When using load you can pass the file name, and if the extension is .npz, it will first decompress the file: \nCode: np.savez_compressed('filename.npz', array1=array1, array2=array2)\nb = np.load('filename.npz')\n\nText: and do b['array1'] and so forth to retrieve the data from each array... \nAPI:\nnumpy.load\n","label":[[35,39,"Mention"],[319,329,"API"]],"Comments":[]}
{"id":59682,"text":"ID:18244534\nPost:\nText: You'll also reach another snag with numpy when you hit 65 characters, but pandas works around this because each str object is stored as an opaque pointer to a Python object, not a string_ type. \nCode: In [18]: from pandas.util.testing import rands\n\nIn [19]: s = Series([rands(120) for _ in range(10)])\n\nIn [20]: s\nOut[20]:\n0    LdeUwCKNFi4SWWfnAsKK3VIdDegy35lokoOr5DfCePoGn2...\n1    xXmofyBFUfCiApbqNEDtJs6JhU0QAhIG8sQRCKkKMdTZuZ...\n2    t3XcQFDQhg8BxAc9vFeo5Ky6beMxp9IGj54u3OzELR8lRf...\n3    tWufKLo4OiW8lMpB8NiHzy0REAnAtAmLrDJyLzi1GBSRwS...\n4    bysGao2rhiqxfmv54eDT6qcshlk0E7srrRLnuBDRRu7oVg...\n5    AYIZFysXR9vispYQEfwqaZ20YYvR52pPkBtd2acOapK3Mv...\n6    eLAwKopRuynrY75dn7vEfUnqhoSDLh5mGSBclFDaItwyxJ...\n7    oj8ilX2EvhegAI4FvZQxJU0hTDR04aLySNdCXPmqOLa6CF...\n8    5mEX5o23PMg5yWEE6bofk5tqzPCFNNCIn1v3ynYxicVXa8...\n9    c2fS5Z1w7IxKq72x5KM8WhNChfrEJoFavdD1DQUJn4NCNP...\ndtype: object\n\nIn [21]: s.astype(str).map(len)\nOut[21]:\n0    120\n1    120\n2    120\n3    120\n4    120\n5    120\n6    120\n7    120\n8    120\n9    120\ndtype: int64\n\nIn [22]: map(len, s.values.astype(str))\nOut[22]: [64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n\nText: To be fair to numpy, this was fixed in pull request #3270 and is fixed in numpy 1.8. \nText: EDIT: to address the initial issue (which was converting an int array to a str array), since you've tagged this as pandas you can do \nCode: In [4]: s = Series([1, 22, 333, 4444])\n\nIn [5]: s\nOut[5]:\n0       1\n1      22\n2     333\n3    4444\ndtype: int64\n\nIn [6]: s.astype(str)\nOut[6]:\n0       1\n1      22\n2     333\n3    4444\ndtype: object\n\nText: This will work in older-than-1.7 numpy, but you'll have to upgrade to a later version of pandas, one at or after f0c1bd. Alternatively you can do \nCode: In [3]: s = Series([1, 22, 333, 4444])\n\nIn [4]: s.map(str)\nOut[4]:\n0       1\n1      22\n2     333\n3    4444\ndtype: object\n\nText: which should work on any pandas version that has the map method on Series objects and any numpy version that is supported by pandas. \nAPI:\nnumpy.string_\n","label":[[204,211,"Mention"],[2009,2022,"API"]],"Comments":[]}
{"id":59683,"text":"ID:18312439\nPost:\nText: The printed values are not correct. In your case y is smaller than 1 when using float64 and bigger or equal to 1 when using float32. this is expected since rounding errors depend on the size of the float. \nText: To avoid this kind of problems, when dealing with floating point numbers you should always decide a \"minimum error\", usually called epsilon and, instead of comparing for equality, checking whether the result is at most distant epsilon from the target value: \nCode: In [13]: epsilon = 1e-11\n\nIn [14]: number = np.float64(1) - 1e-16\n\nIn [15]: target = 1\n\nIn [16]: abs(number - target) < epsilon   # instead of number == target\nOut[16]: True\n\nText: In particular, numpy already provides np.allclose which can be useful to compare arrays for equality given a certain tolerance. It works even when the arguments aren't arrays(e.g. np.allclose(1 - 1e-16, 1) -> True). \nText: Note however than set_printoptoisn doesn't affect how np.float32\/64 are printed. It affects only how arrays are printed: \nCode: In [1]: import numpy as np\n\nIn [2]: np.float(1) - 1e-16\nOut[2]: 0.9999999999999999\n\nIn [3]: np.array([1 - 1e-16])\nOut[3]: array([ 1.])\n\nIn [4]: np.set_printoptions(precision=16)\n\nIn [5]: np.array([1 - 1e-16])\nOut[5]: array([ 0.9999999999999999])\n\nIn [6]: np.float(1) - 1e-16\nOut[6]: 0.9999999999999999\n\nText: Also note that doing print y or evaluating y in the interactive interpreter gives different results: \nCode: In [1]: import numpy as np\n\nIn [2]: np.float(1) - 1e-16\nOut[2]: 0.9999999999999999\n\nIn [3]: print(np.float64(1) - 1e-16)\n1.0\n\nText: The difference is that print calls str while evaluating calls repr: \nCode: In [9]: str(np.float64(1) - 1e-16)\nOut[9]: '1.0'\n\nIn [10]: repr(np.float64(1) - 1e-16)\nOut[10]: '0.99999999999999989'\n\nAPI:\nnumpy.set_printoptions\n","label":[[923,939,"Mention"],[1781,1803,"API"]],"Comments":[]}
{"id":59684,"text":"ID:18518561\nPost:\nText: You could use average which allows you to specify weights: \nCode: >>> bin_avg[index] = np.average(items_in_bin, weights=my_weights)\n\nText: So to calculate the weights you could find the x coordinates of each data point in the bin and calculate their distances to the bin center. \nAPI:\nnumpy.average\n","label":[[38,45,"Mention"],[309,322,"API"]],"Comments":[]}
{"id":59685,"text":"ID:18528805\nPost:\nText: You can use the resize() method of an ndarray object: \nCode: import numpy as np\n\na = np.array([1,2,3])\na.resize(2,3)\nprint a\n#array([[1, 2, 3],\n#       [0, 0, 0]])\n\nText: EDIT: \nText: Note that a is extended in-place, which means that the original array has now a reference to an extra, contiguous memory block. \nText: In your case, based on the comments, you can previously create a copy: \nCode: atb_mat = atbvec.copy()\natb_mat.resize(dim, nb)\n\nText: or, preferably, use resze to obtain a brand new array without references: \nCode: atb_mat = numpy.resize(atbvec, (dim, nb))\n\nText: In this case, though, the array is filled with the previous values from atbvec, example: \nCode: a = np.array([1,2,3])\nb = np.resize(a, (3, 4))\n#array([[1, 2, 3, 1],\n#       [2, 3, 1, 2],\n#       [3, 1, 2, 3]])\n\nText: See that the memory block is copied until it fills the new contiguous-size. \nAPI:\nnumpy.ndarray\nnumpy.resize\n","label":[[62,69,"Mention"],[496,501,"Mention"],[905,918,"API"],[919,931,"API"]],"Comments":[]}
{"id":59686,"text":"ID:18601018\nPost:\nText: It's a class instance (aka an object): \nCode: In [2]: numpy.r_\nOut[2]: <numpy.lib.index_tricks.RClass at 0x1923710>\n\nText: A class is a construct which is used to define a distinct type - as such a class allows instances of itself. Each instance can have properties (member\/instance variables and methods). \nText: One of the methods a class can have is the __getitem__ method, this is called whenever you append [something,something...something] to the name of the instance. In the case of the r_ instance the method returns a numpy array. \nText: Take the following class for example: \nCode: class myClass(object)\n    def __getitem__(self,i)\n        return i*2\n\nText: Look at these outputs for the above class: \nCode: In [1]: a = myClass()\n\nIn [2]: a[3]\nOut[2]: 6\n\nIn [3]: a[3,4]\nOut[3]: (3, 4, 3, 4)\n\nText: I am calling the __getitem__ method of myClass (via the [] parentheses) and the __getitem__ method is returning (the contents of a list * 2 in this case)- it is not the class\/instance behaving as a function - it is the __getitem__ function of the myClass instance which is being called. \nText: On a final note, you will notice that to instantiate myClass I had to do a = myClass() whereas to get an instance of RClass you use _r This is because numpy instantiates RClass and binds it to the name np.r_ itself. This is the relevant line in the numpy source code. In my opinion this is rather ugly and confusing! \nAPI:\nnumpy.r_\nnumpy.r_\nnumpy.r_\n","label":[[518,520,"Mention"],[1258,1260,"Mention"],[1328,1333,"Mention"],[1449,1457,"API"],[1458,1466,"API"],[1467,1475,"API"]],"Comments":[]}
{"id":59687,"text":"ID:18662291\nPost:\nText: This is what nepeat is for. \nCode: >>> x = numpy.array([1, 2, 1])\n>>> numpy.repeat(x, 4)\narray([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1])\n\nAPI:\nnumpy.repeat\n","label":[[37,43,"Mention"],[163,175,"API"]],"Comments":[]}
{"id":59688,"text":"ID:18684433\nPost:\nText: As the name implies multivariate_normal generates normal distributions, this means that there is a non-null probability of finding points outside of any given interval. You can generate correlated uniform distributions but this a little more convoluted. Take a look here for two possible methods. \nText: If you want to go with the normal distribution you can set up the sigmas so that your half-interval correspond to 3 standard deviations (you can also filter out the bad points if needed). In this way you will have ~99% of your points inside your interval, ex: \nCode: import numpy as np\nfrom matplotlib.pyplot import scatter\n\nxx = np.array([-0.51, 51.2])\nyy = np.array([0.33, 51.6])\nmeans = [xx.mean(), yy.mean()]  \nstds = [xx.std() \/ 3, yy.std() \/ 3]\ncorr = 0.8         # correlation\ncovs = [[stds[0]**2          , stds[0]*stds[1]*corr], \n        [stds[0]*stds[1]*corr,           stds[1]**2]] \n\nm = np.random.multivariate_normal(means, covs, 1000).T\nscatter(m[0], m[1])\n\nAPI:\nnumpy.random.multivariate_normal\n","label":[[44,63,"Mention"],[1004,1036,"API"]],"Comments":[]}
{"id":59689,"text":"ID:18689712\nPost:\nText: pandas.isnull() (also pd.isna(), in newer versions) checks for missing values in both numeric and string\/object arrays. From the documentation, it checks for: \nText: NaN in numeric arrays, None\/NaN in object arrays \nText: Quick example: \nCode: import pandas as pd\nimport numpy as np\ns = pd.Series(['apple', np.nan, 'banana'])\npd.isnull(s)\nOut[9]: \n0    False\n1     True\n2    False\ndtype: bool\n\nText: The idea of using np.nan to represent missing values is something that pandas introduced, which is why pandas has the tools to deal with it. \nText: Datetimes too (if you use pd.NaT you won't need to specify the dtype) \nCode: In [24]: s = Series([Timestamp('20130101'),np.nan,Timestamp('20130102 9:30')],dtype='M8[ns]')\n\nIn [25]: s\nOut[25]: \n0   2013-01-01 00:00:00\n1                   NaT\n2   2013-01-02 09:30:00\ndtype: datetime64[ns]``\n\nIn [26]: pd.isnull(s)\nOut[26]: \n0    False\n1     True\n2    False\ndtype: bool\n\nAPI:\nnumpy.nan\n","label":[[442,448,"Mention"],[945,954,"API"]],"Comments":[]}
{"id":59690,"text":"ID:18691371\nPost:\nText: You can use choie if you use numpy 1.7.0+: \nCode: >>> import numpy as np\n>>> array1 = np.array([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1])\n>>> np.random.choice(array1, 5)\narray([ 0. ,  0. ,  0.3,  1. ,  0.3])\n>>> np.random.choice(array1, 5, replace=False)\narray([ 0.6,  0.8,  0.1,  0. ,  0.4])\n\nText: To get 5 elements that the sum is equal to 1, \nText: generate 4 random numbers. substract the sum of 4 numbers from 1 -> x if x included in array1, use that as final number; or repeat \nCode: >>> import numpy as np\n>>> \n>>> def solve(arr, total, n):\n...     while True:\n...         xs = np.random.choice(arr, n-1)\n...         remain = total - xs.sum()\n...         if remain in arr:\n...             return np.append(xs, remain)\n... \n>>> array1 = np.array([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1])\n>>> print solve(array1, 1, 5)\n[ 0.1  0.3  0.4  0.2  0. ]\n\nText: Another version (assume given array is sorted): \nCode: EPS = 0.0000001\ndef solve(arr, total, n):\n    while True:\n        xs = np.random.choice(arr, n-1)\n        t = xs.sum()\n        i = arr.searchsorted(total - t)\n        if abs(t + arr[i] - total) < EPS:\n            return np.append(xs, arr[i])\n\nAPI:\nnumpy.random.choice\n","label":[[36,41,"Mention"],[1169,1188,"API"]],"Comments":[]}
{"id":59691,"text":"ID:18692353\nPost:\nText: You forgot the outer brackets. \nCode: A = np.matrix([[1,0,0,a,0,0],[0,1,0,0,a,0],[0,0,1,0,0,a],[0,0,0,b,0,0],[0,0,0,0,b,0],[0,0,0,0,0,b]])\n\nText: The np.matrix constructor takes a single array-like argument to initialize the matrix, not a sequence of positional arguments representing the rows. (Additional positional arguments set things like the dtype.) \nAPI:\nnumpy.matrix\n","label":[[174,183,"Mention"],[386,398,"API"]],"Comments":[]}
{"id":59692,"text":"ID:18758049\nPost:\nText: Take a look at reshape . \nCode: >>> arr = numpy.zeros((50,100,25))\n>>> arr.shape\n# (50, 100, 25)\n\n>>> new_arr = arr.reshape(5000,25)\n>>> new_arr.shape   \n# (5000, 25)\n\n# One shape dimension can be -1. \n# In this case, the value is inferred from \n# the length of the array and remaining dimensions.\n>>> another_arr = arr.reshape(-1, arr.shape[-1])\n>>> another_arr.shape\n# (5000, 25)\n\nAPI:\nnumpy.reshape\n","label":[[39,46,"Mention"],[412,425,"API"]],"Comments":[]}
{"id":59693,"text":"ID:18761200\nPost:\nText: Numpy doesn't care what the axes of your matplotlib graph are. \nText: I presume that you think log(y) is some polynomial function of log(x), and you want to find that polynomial? If that is the case, then run np.polyfit on the logarithms of your data set: \nCode: import numpy as np\nlogx = np.log(x)\nlogy = np.log(y)\ncoeffs = np.polyfit(logx,logy,deg=3)\npoly = np.poly1d(coeffs)\n\nText: poly is now a polynomial in log(x) that returns log(y). To get the fit to predict y values, you can define a function that just exponentiates your polynomial: \nCode: yfit = lambda x: np.exp(poly(np.log(x)))\n\nText: You can now plot your fitted line on your matplotlib loglog plot: \nCode: plt.loglog(x,yfit(x))\n\nText: And show it like this \nCode: plt.show()\n\nAPI:\nnumpy.polyfit\n","label":[[233,243,"Mention"],[771,784,"API"]],"Comments":[]}
{"id":59694,"text":"ID:18789056\nPost:\nText: It seems you may have overwritten the variable t with an array somewhere. What your error message means is that t is a ndarray which has no ppf method. The t you intended to import shouldn't be an ndarray but rather a distribution generator. \nText: Either find where it became an array and use another name there, or import with better names. \nText: For example, try changing your import line to this: \nCode: from scipy.stats import distrbutions as dists\n\nText: and then change the problem line to: \nCode: tval = dists.t.ppf(1-alpha\/2, dof)\n\nText: Alternatively: \nCode: from scipy.stats.distributions import t as tdist\ntval = tdist.ppf(1-alpha\/2, dof)\n\nAPI:\nnumpy.ndarray\n","label":[[143,150,"Mention"],[682,695,"API"]],"Comments":[]}
{"id":59695,"text":"ID:18793834\nPost:\nText: diff is useful in this case. You can count the number of -1's in the diffed array. \nText: Note, you'd also need to check the last element -- if it's True, there wouldn't be a -1 in the diffed array to indicate that. Better yet, you can append False to the array before diffing. \nCode: import numpy as np\na = np.array([False, False, False, True, True, True, False, False, False], dtype=bool)\nd = np.diff(np.asarray(a, dtype=int))\nd\n=> array([ 0,  0,  1,  0,  0, -1,  0,  0])\n(d < 0).sum()\n=> 1\n\nText: To append False at the end: \nCode: b = np.append(a, [ False ])\nd = np.diff(np.asarray(b, dtype=int))\n...\n\nText: Now, \"the sequence ...,True, False, ..., True... never occurs\" iff (d<0).sum() < 2. \nText: A trick to avoid the append operation (and make your code more obscure) is by doing: (d<0).sum() + a[-1] < 2 (i.e., if a[-1] is True, count it as a block). This would only work if a is not empty, of course. \nAPI:\nnumpy.diff\n","label":[[24,28,"Mention"],[940,950,"API"]],"Comments":[]}
{"id":59696,"text":"ID:18817152\nPost:\nText: The problem in my case was that array created int64-bit numbers by default. So I had to explicitly convert it to int32: \nCode: points = np.array([[910, 641], [206, 632], [696, 488], [458, 485]])\n# points.dtype => 'int64'\ncv2.polylines(img, np.int32([points]), 1, (255,255,255))\n\nText: (Looks like a bug in cv2 python binding, it should've verified dtype) \nAPI:\nnumpy.array\n","label":[[56,61,"Mention"],[385,396,"API"]],"Comments":[]}
{"id":59697,"text":"ID:18842047\nPost:\nText: Although the docstrings don't document this functionality, the source indicates it is possible to pass an array to the poisson function. \nCode: >>> import numpy\n>>> # 1 dimension array of 1M random var's uniformly distributed between 1 and 2\n>>> numpyarray = numpy.random.rand(1e6) + 1 \n>>> # pass to poisson\n>>> poissonarray = numpy.random.poisson(lam=numpyarray)\n>>> poissonarray\narray([4, 2, 3, ..., 1, 0, 0])\n\nText: The poisson random variable returns discrete multiples of one, and approximates a bell curve as lambda grows beyond one. \nCode: >>> import matplotlib.pyplot\n>>> count, bins, ignored = matplotlib.pyplot.hist(\n            numpy.random.poisson(\n                    lam=numpy.random.rand(1e6) + 10), \n                    14, normed=True)\n>>> matplotlib.pyplot.show()\n\nText: This method of passing the array to the poisson generator appears to be quite efficient. \nCode: >>> timeit.Timer(\"numpy.random.poisson(lam=numpy.random.rand(1e6) + 1)\",\n                 'import numpy').repeat(3,1)\n[0.13525915145874023, 0.12136101722717285, 0.12127304077148438]\n\nAPI:\nnumpy.random.poisson\n","label":[[143,150,"Mention"],[1098,1118,"API"]],"Comments":[]}
{"id":59698,"text":"ID:18871667\nPost:\nText: You can use np.apply_along_axis to eliminate the loop. \nCode: hist, bin_edges = apply_along_axis(lambda x: histogram(x, bins=bins), 0, B)\n\nAPI:\nnumpy.apply_along_axis\n","label":[[36,55,"Mention"],[168,190,"API"]],"Comments":[]}
{"id":59699,"text":"ID:18926071\nPost:\nText: Given an integer array of 0s and 1s: \nCode: M = np.random.random_integers(0,1,(5,5))\nprint(M)\n# [[1 0 0 1 1]\n#  [0 0 1 1 0]\n#  [0 1 1 0 1]\n#  [1 1 1 0 1]\n#  [0 1 1 0 0]]\n\nText: Here are three ways you could NOT the array: \nText: Convert to a boolean array and use the ~ operator to bitwise NOT the array: print((~(M.astype(np.bool))).astype(M.dtype)) # [[0 1 1 0 0] # [1 1 0 0 1] # [1 0 0 1 0] # [0 0 0 1 0] # [1 0 0 1 1]] Use logiral_not and cast the resulting boolean array back to integers: print(np.logical_not(M).astype(M.dtype)) # [[0 1 1 0 0] # [1 1 0 0 1] # [1 0 0 1 0] # [0 0 0 1 0] # [1 0 0 1 1]] Just subtract all your integers from 1: print(1 - M) # [[0 1 1 0 0] # [1 1 0 0 1] # [1 0 0 1 0] # [0 0 0 1 0] # [1 0 0 1 1]] \nText: The third way will probably be quickest for most non-boolean dtypes. \nAPI:\nnumpy.logical_not\n","label":[[451,462,"Mention"],[838,855,"API"]],"Comments":[]}
{"id":59700,"text":"ID:18959440\nPost:\nText: The text file contain UTF-8 BOM characters. np.loadtxt does not accept encoding, but you can pass iterable instead of filename. \nText: Try following: \nCode: stockFile = '....'\n\n\nimport numpy as np\nimport matplotlib.dates as mdates\nimport codecs\n\nwith codecs.open(stockFile, encoding='utf-8-sig') as f:\n    date, closep, highp, lowp, openp, volume = np.loadtxt(f, delimiter=',', unpack=True, converters={0: mdates.strpdate2num('%d-%b-%y')})\n\nAPI:\nnumpy.loadtxt\n","label":[[68,78,"Mention"],[470,483,"API"]],"Comments":[]}
{"id":59701,"text":"ID:19032827\nPost:\nText: Assuming that by \"sparse matrix\" you don't actually mean a scipy.sparse matrix, but merely a ndarray with relatively few nonzero entries, then I think nonzero is exactly what you're looking for. Starting from an array: \nCode: >>> a = (np.random.random((5,5)) < 0.10)*1\n>>> a\narray([[0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0]])\n\nText: nonzero returns the indices (here x and y) where the nonzero entries live: \nCode: >>> a.nonzero()\n(array([1, 2, 3]), array([4, 2, 0]))\n\nText: We can assign these to i and j: \nCode: >>> i, j = a.nonzero()\n\nText: We can also use them to index back into a, which should give us only 1s: \nCode: >>> a[i,j]\narray([1, 1, 1])\n\nText: We can even modify a using these indices: \nCode: >>> a[i,j] = 2\n>>> a\narray([[0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 2],\n       [0, 0, 2, 0, 0],\n       [2, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0]])\n\nText: If you want a combined array from the indices, you can do that too: \nCode: >>> np.array(a.nonzero()).T\narray([[1, 4],\n       [2, 2],\n       [3, 0]])\n\nText: (there are lots of ways to do this reshaping; I chose one almost at random.) \nAPI:\nnumpy.ndarray\n","label":[[117,124,"Mention"],[1190,1203,"API"]],"Comments":[]}
{"id":59702,"text":"ID:19059737\nPost:\nText: np.cov seems to be deterministic: \nCode: import numpy\n\nrandoms = numpy.random.random((1043, 261))\n\ncovs = [numpy.cov(randoms) for _ in range(10)]\nall((c==covs[0]).all() for c in covs)\n#>>> True\n\nText: I'd imagine the problem is elsewhere. \nText: Also note that this result holds with numbers 1000th the size \nAPI:\nnumpy.cov\n","label":[[24,30,"Mention"],[338,347,"API"]],"Comments":[]}
{"id":59703,"text":"ID:19090059\nPost:\nText: To see how the mappings have to be applied, see numpy.i. Herein the typemap you want to use is defined as \nCode: (DATA_TYPE* IN_ARRAY3, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3)\n\nText: But you are applying it as \nCode: %apply (double* IN_ARRAY3, int DIM1, int DIM2, int DIM3) {(double*** WF, int dim1, int dim2, int dim3)}\n\nText: So you put a double* onto a double***. Although C will accept this argument your program segfaults when accessing the memory. \nText: You need to define your internal three dimensional memory either as a single vector of type double* or you need an additional wrapper function which does the correct mapping either by copying it over (slow) or providing the appropriate memory addresses (fast). \nText: Since you are already adding a temporary function, you can easily use that to create the proper memory layout for GetCharges(...,double*** WF) but you need to change the function definition to \nCode: %apply (double* IN_ARRAY3, int DIM1, int DIM2, int DIM3) {(double* WF, int dim1, int dim2, int dim3)}\n%apply (double* INPLACE_ARRAY1, int DIM1) {(double* charges, int number4)}\n\nText: ... \nCode: void GetCharges_temp(double* pos_x, int number1, double* pos_y, int number2, double* pos_z, int number3, double *charges, int number4, double* WF, int dim1, int dim2, int dim3, double resolution, double  shape)\n\nText: Edit \nText: To answer your comment, I will add here a few more lines: Since I usually interface exisiting C code I was assuming that you have already a required memory layout. Sorry, about that. \nText: So the real question is, how do you access the elements of WF in C? You get a double* with total length of DIM1*DIM2*DIM3. In Numpy you access an element via WF[i][j][k]. To do the same in C you need to calculate the correct offset for this element as WF[k+DIM3*(j +DIM2*i)] if WF is (C contiguous) row major and WF[i+DIM1*(j +DIM2*k)] if WF is (Fortran contiguous) column major. You can check ordering in numpy by accessing WF.flags. Also, you can force C row major ordering upon WF in numpy by using either np.ascontiguousarray or numpy.require. \nText: So essentially all numpy arrays of any dimension are vectors of dim n1*...*nd, which are accessed in the same way in C: by calculating the correct offset. \nText: In row-major stored elements the fast index is the last one (here k) whereas in column major the fast index is the first one (here i). The fast index should be in the inner most loop in C to gain maximum speed up. \nAPI:\nnumpy.ascontiguousarray\n","label":[[2082,2102,"Mention"],[2510,2533,"API"]],"Comments":[]}
{"id":59704,"text":"ID:19222204\nPost:\nText: Just use numpy.sum(): \nCode: result = np.sum(matrix)\n\nText: or equivalently, the .sum() method of the array: \nCode: result = matrix.sum()\n\nText: By default this sums over all elements in the array - if you want to sum over a particular axis, you should pass the axis argument as well, e.g. matrix.sum(0) to sum over the first axis. \nText: As a side note your \"matrix\" is actually a numpy.ndarray, not a matrix - they are different classes that behave slightly differently, so it's best to avoid confusing the two. \nAPI:\nnumpy.matrix\n","label":[[427,433,"Mention"],[544,556,"API"]],"Comments":[]}
{"id":59705,"text":"ID:19429688\nPost:\nText: I get a different error when I apply your code, so I'll clean up some part: \nCode: x = np.asarray([np.linspace(3000, 7000, 1000.0), np.linspace(4000, 8000, 1000.0), np.linspace(2000, 9000, 1000.0)])\ny = np.asarray([np.linspace(10, 200, 1000.0), np.linspace(20, 200, 1000.0), np.linspace(30, 200, 1000.0)])\nxgrid = np.linspace(6520,6620, 1000.0)\n\nText: since np.interp only accepts 1D objects: \nCode: ygrid = np.array([np.interp(xgrid, i, j) for i, j in zip(x, y)])\n\nAPI:\nnumpy.interp\n","label":[[382,391,"Mention"],[495,507,"API"]],"Comments":[]}
{"id":59706,"text":"ID:19524187\nPost:\nText: Summary \nText: In a nutshell, just move the view before the slicing. \nText: Instead of: \nCode: ar2 = zeros((1000,2000),dtype=uint16)\nar2 = ar2[:,1000:]\nar2 = ar2.view(dtype=uint8)\n\nText: Do: \nCode: ar2 = zeros((1000,2000),dtype=uint16)\nar2 = ar2.view(dtype=uint8) # ar2 is now a 1000x4000 array...\nar2 = ar2[:,2000:] # Note the 2000 instead of 1000! \n\nText: What's happening is that the sliced array isn't contiguous (as @Craig noted) and view errs on the conservative side and doesn't try to re-interpret non-contiguous memory buffers. (It happens to be possible in this exact case, but in some cases it would result in a non-evenly-strided array, which numpy doesn't allow.) \nText: If you're not very familiar with numpy, it's possible that you're misunderstanding view, and you actually want astype instead. \nText: What does view do? \nText: First off, let's take a detailed look at what view does. In this case, it re-interprets the memory buffer of a numpy array as a new datatype, if possible. That means that the number of elements in the array will often change when you use view. (You can also use it to view the array as a different subclass of ndarray, but we'll skip that part for now.) \nText: You may already be aware of the following (your problem is a bit more subtle), but if not, here's an explanation. \nText: As an example: \nCode: In [1]: import numpy as np\n\nIn [2]: x = np.zeros(2, dtype=np.uint16)\n\nIn [3]: x\nOut[3]: array([0, 0], dtype=uint16)\n\nIn [4]: x.view(np.uint8)\nOut[4]: array([0, 0, 0, 0], dtype=uint8)\n\nIn [5]: x.view(np.uint32)\nOut[5]: array([0], dtype=uint32)\n\nText: If you want to make a copy of the array with the new datatype instead, use astype: \nCode: In [6]: x\nOut[6]: array([0, 0], dtype=uint16)\n\nIn [7]: x.astype(np.uint8)\nOut[7]: array([0, 0], dtype=uint8)\n\nIn [8]: x.astype(np.uint32)\nOut[8]: array([0, 0], dtype=uint32)\n\nText: Now let's take a look at what happens with when viewing a 2D array. \nCode: In [9]: y = np.arange(4, dtype=np.uint16).reshape(2, 2)\n\nIn [10]: y\nOut[10]:\narray([[0, 1],\n       [2, 3]], dtype=uint16)\n\nIn [11]: y.view(np.uint8)\nOut[12]:\narray([[0, 0, 1, 0],\n       [2, 0, 3, 0]], dtype=uint8)\n\nText: Notice that the shape of the array has changed, and that the changes have happened along the last axis (in this case, extra columns have been added). \nText: At first glance it may appear that extra zeros have been added. It's not that extra zeros are being inserted, it's that the uint16 representation of 2 is equivalent to two uint8s, one with a value of 2 and one with a value of 0. Therefore, any uint16 under 255 will result in the value and a zero, while any value over that will result in two smaller uint8s. As an example: \nCode: In [13]: y * 100\nOut[14]:\narray([[  0, 100],\n       [200, 300]], dtype=uint16)\n\nIn [15]: (y * 100).view(np.uint8)\nOut[15]:\narray([[  0,   0, 100,   0],\n       [200,   0,  44,   1]], dtype=uint8)\n\nText: What's happening behind the scenes \nText: Numpy arrays consist of a \"raw\" memory buffer that's interpreted through a shape, a dtype, and strides (and an offset, but let's ignore that for now). For more detail, there are several good overviews: the official documentation, the numpy book, or scipy-lectures. \nText: This allows numpy to be very memory efficient and \"slice and dice\" the underlying memory buffer in many different ways without making a copy. \nText: Strides tell numpy how many bytes to jump within the memory buffer to go one increment along a particular axis. \nText: For example: \nCode: In [17]: y\nOut[17]:\narray([[0, 1],\n       [2, 3]], dtype=uint16)\n\nIn [18]: y.strides\nOut[18]: (4, 2)\n\nText: So, to go one row deeper in the array, numpy needs to step forward 4 bytes in the memory buffer, while to go one column farther in the array, numpy needs to step 2 bytes. Transposing the array just amounts to reversing the strides (and shape, but in this case, y is 2x2): \nCode: In [19]: y.T.strides\nOut[19]: (2, 4)\n\nText: When we view the array as uint8, the strides change. We still step forward 4 bytes per row, but only one byte per column: \nCode: In [20]: y.view(np.uint8).strides\nOut[20]: (4, 1)\n\nText: However, numpy arrays have to have the one stride length per dimension. This is what \"evenly-strided\" means. In other words, do move forward one row\/column\/whatever, numpy needs to be able to step the same amount through the underlying memory buffer each time. In other words, there's no way to tell numpy to step different amounts for each row\/column\/whatever. \nText: For that reason, view takes a very conservative route. If the array isn't contiguous, and the view would change the shape and strides of the array, it doesn't try to handle it. As @Craig noted, it's because the slice of y isn't contiguous that view isn't working. \nText: There are plenty of cases (yours is one) where the resulting array would be valid, but the view method doesn't try to be too smart about it. \nText: To really play around with what's possible, you can use as_strided or directly use the __array_interface__. It's a good learning tool to experiment with it, but you have to really understand what you're doing to use it effectively. \nText: Hopefully that helps a bit, anyway! Sorry for the long-winded answer! \nAPI:\nnumpy.lib.stride_tricks.as_strided\n","label":[[4992,5002,"Mention"],[5251,5285,"API"]],"Comments":[]}
{"id":59707,"text":"ID:19549750\nPost:\nText: The function in1d seems to do what you want. The only problems is that it only works on 1d arrays, so you should use it like this: \nCode: In [9]: np.in1d(fake, [0,2,6,8]).reshape(fake.shape)\nOut[9]: \narray([[ True, False,  True],\n       [False, False, False],\n       [ True, False,  True]], dtype=bool)\n\nText: I have no clue why this is limited to 1d arrays only. Looking at its source code, it first seems to flatten the two arrays, after which it does some clever sorting tricks. But nothing would stop it from unflattening the result at the end again, like I had to do by hand here. \nAPI:\nnumpy.in1d\n","label":[[37,41,"Mention"],[616,626,"API"]],"Comments":[]}
{"id":59708,"text":"ID:19567186\nPost:\nText: To replace nan to 0, use numpy.nan_to_num: \nCode: >>> a = numpy.array([1,2,numpy.nan,4])\n>>> numpy.nan_to_num(a)\narray([ 1.,  2.,  0.,  4.])\n\nText: Use np.isnan to convert nan to True, non-nan numbers to False. Then substract them from 1. \nCode: >>> numpy.isnan(a)\narray([False, False,  True, False], dtype=bool)\n>>> 1 - numpy.isnan(a)\narray([ 1.,  1.,  0.,  1.])\n\nAPI:\nnumpy.isnan\n","label":[[176,184,"Mention"],[394,405,"API"]],"Comments":[]}
{"id":59709,"text":"ID:19597672\nPost:\nText: If I understand your problem correctly, you might get some help with shuffle \nCode: >>> def rand_bin_array(K, N):\n    arr = np.zeros(N)\n    arr[:K]  = 1\n    np.random.shuffle(arr)\n    return arr\n\n>>> rand_bin_array(5,15)\narray([ 0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n        0.,  0.])\n\nAPI:\nnumpy.random.shuffle\n","label":[[93,100,"Mention"],[341,361,"API"]],"Comments":[]}
{"id":59710,"text":"ID:19621153\nPost:\nText: Update, I've updated the answer to work with binary or gray scale images. Notice that image intensities are now just scalars instead of (R, G, B) values and all images, masks and structure elements are 2d-arrays instead of 3d arrays. You may need to adjust the value of white_pixel (or otherwise modify this code to suit your needs). \nCode: import numpy as np\nfrom skimage.morphology import binary_dilation\n# Setup\ncoordx = [110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 100, 101, 102,\n          103, 104, 105, 106, 107, 108, 109, 110]\ncoordy = [110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 110, 111, 112,\n          113, 114, 115, 116, 117, 118, 119, 120]\nimg = np.random.random((128, 128))\nimg[110, 110] = 1.\nimg[109, 110] = 1.\n\n\n# values grater than white_pixel will get detected as white pixels\nwhite_pixel = 1\n\nmask = np.zeros((128, 128), dtype=bool)\nmask[coordx, coordy] = 1\n\nstructure = np.ones((7, 7))\nmask = binary_dilation(mask, structure)\n\nis_white = (img * mask) >= white_pixel\n\n# This will tell you which pixels are white\nprint np.where(is_white)\n\n# This will tell you if any pixels are white\nprint np.any(is_white)\n\nText: Original answer: \nText: You only need to use where if you wan to know which pixels are white. I would just multiply the image by a mask and use np.any, something like this: \nCode: # Setup\ncoordx = [110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 100, 101, 102,\n          103, 104, 105, 106, 107, 108, 109, 110]\ncoordy = [110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 110, 111, 112,\n          113, 114, 115, 116, 117, 118, 119, 120]\nwhite_pixel = np.array([255, 255, 255])\nimg = np.random.randint(0, 256, (128, 128, 3))\nimg[110, 110, :] = 255\nimg[109, 110, :] = 255\n\nmask = np.zeros((128, 128, 1), dtype=bool)\nmask[coordx, coordy] = 1\n\nstructure = np.ones((7, 7, 1))\nmask = binary_dilation(mask, structure)\n\nis_white = np.all((img * mask) == white_pixel, axis=-1)\n\n# This will tell you which pixels are white\nprint np.where(is_white)\n\n# This will tell you if any pixels are white\nprint np.any(is_white)\n\nAPI:\nnumpy.where\n","label":[[1223,1228,"Mention"],[2102,2113,"API"]],"Comments":[]}
{"id":59711,"text":"ID:19647414\nPost:\nText: np.append is pretty different from list.append in python. I know that's thrown off a few programers new to numpy. np.append is more like concatenate, it makes a new array and fills it with the values from the old array and the new value(s) to be appended. For example: \nCode: import numpy\n\nold = numpy.array([1, 2, 3, 4])\nnew = numpy.append(old, 5)\nprint old\n# [1, 2, 3, 4]\nprint new\n# [1, 2, 3, 4, 5]\nnew = numpy.append(new, [6, 7])\nprint new\n# [1, 2, 3, 4, 5, 6, 7]\n\nText: I think you might be able to achieve your goal by doing something like: \nCode: result = numpy.zeros((10,))\nresult[0:2] = [1, 2]\n\n# Or\nresult = numpy.zeros((10, 2))\nresult[0, :] = [1, 2]\n\nText: Update: \nText: If you need to create a numpy array using loop, and you don't know ahead of time what the final size of the array will be, you can do something like: \nCode: import numpy as np\n\na = np.array([0., 1.])\nb = np.array([2., 3.])\n\ntemp = []\nwhile True:\n    rnd = random.randint(0, 100)\n    if rnd > 50:\n        temp.append(a)\n    else:\n        temp.append(b)\n    if rnd == 0:\n         break\n\n result = np.array(temp)\n\nText: In my example result will be an (N, 2) array, where N is the number of times the loop ran, but obviously you can adjust it to your needs. \nText: new update \nText: The error you're seeing has nothing to do with types, it has to do with the shape of the numpy arrays you're trying to concatenate. If you do np.append(a, b) the shapes of a and b need to match. If you append an (2, n) and (n,) you'll get a (3, n) array. Your code is trying to append a (1, 0) to a (2,). Those shapes don't match so you get an error. \nAPI:\nnumpy.append\nnumpy.append\n","label":[[24,33,"Mention"],[138,147,"Mention"],[1644,1656,"API"],[1657,1669,"API"]],"Comments":[]}
{"id":59712,"text":"ID:19661331\nPost:\nText: You can use np.nan instead of None. \nCode: import matplotlib.pyplot as pyplot\nimport numpy\n\nx = range(5)\nk = numpy.array([(1.,0.001), (1.1, 0.002), (numpy.nan, numpy.nan), \n                 (1.2, 0.003), (0.99, 0.004)])\n\nFig, ax = pyplot.subplots()\n\n# This plots a gap---as desired\nax.plot(x, k[:,0], 'k-')\n\nax.plot(range(len(y)), y[:,0]+y[:,1], 'k--')\n\nText: Or you could mask the x value as well, so the indices were consistent between x and y \nCode: import matplotlib.pyplot as pyplot\nimport numpy\n\nx = range(5)\ny = numpy.array([(1.,0.001), (1.1, 0.002), (numpy.nan, numpy.nan), \n                 (1.2, 0.003), (0.99, 0.004)])\n\nFig, ax = pyplot.subplots()\n\n\nax.plot(range(len(y)), y[:,0]+y[:,1], 'k--')\nimport matplotlib.pyplot as pyplot\nimport numpy\n\nx = range(5)\nk = numpy.array([(1.,0.001), (1.1, 0.002), (None, None), \n                 (1.2, 0.003), (0.99, 0.004)])\n\nFig, ax = pyplot.subplots()\n\n# This plots a gap---as desired\nax.plot(x, k[:,0], 'k-')\n\n# I'd like to plot\n#     k[:,0] + k[:,1]\n# but I can't add None\n\narr_none = np.array([None])\nmask = (k[:,0] == arr_none) | (k[:,1] == arr_none)\n\nax.plot(numpy.arange(len(y))[mask], k[mask,0]+k[mask,1], 'k--')\n\nAPI:\nnumpy.nan\n","label":[[36,42,"Mention"],[1200,1209,"API"]],"Comments":[]}
{"id":59713,"text":"ID:19667038\nPost:\nText: You can just append a value to the end of an array\/list using append or numpy.append: \nCode: # Python list\na = [1, 2, 3]\na.append(1)\n# => [1, 2, 3, 1]\n\n# Numpy array\nimport numpy as np\na = np.array([1, 2, 3])\na = np.append(a, 1)\n# => [1, 2, 3, 1]\n\nText: Note, as pointed out by @BrenBarn, that the np.append approach creates a whole new array each time it is executed, which makes it inefficient. \nAPI:\nnumpy.append\n","label":[[322,331,"Mention"],[427,439,"API"]],"Comments":[]}
{"id":59714,"text":"ID:19703987\nPost:\nText: I've found a solution using ctypes.POINTER(ctypes.c_double) and np.ctypeslib.as_array - according to the numpy.ctypeslib documentation, this will share the memory with the array: \nCode: callback_func = ctypes.CFUNCTYPE(\n    None,            # return\n    ctypes.POINTER(ctypes.c_double), # x\n    ctypes.c_int     # n\n)\n\nText: [...] \nCode: @callback_func\ndef callback(x, n):\n    x = npct.as_array(x, (n,))\n    print(\"x: {0}, n: {1}\".format(x, n))\n\nText: Anyone with a more elegant solution, perhaps using the ndpointer objects? \nAPI:\nnumpy.ctypeslib.as_array\n","label":[[88,109,"Mention"],[556,580,"API"]],"Comments":[]}
{"id":59715,"text":"ID:19705530\nPost:\nText: Your process is horribly inefficient. When handling such huge amounts of data, you really need to know your tools. \nText: For your problem, np.concatenate is forbidden - it needs at least twice the memory of the inputs. Plus it will copy every bit of data, so it's slow, too. \nText: Use np.memmap to load the arrays. That will use only a few bytes of memory while still being pretty efficient. Join them using np.vstack. Call this only once (i.e. don't bigArray=vstack(bigArray,newArray)!!!). Load all the arrays in a list allArrays and then call bigArray = vstack(allArrays) If that is really too slow, you need to know the size of the array in advance, create an array of this size once and then load the data into the existing array (instead of creating a new one every time). Depending on how often the files on disk change, it might be much more efficient to concatenate them with the OS tools to create one huge file and then load that (or use numpy.memmap) \nAPI:\nnumpy.memmap\n","label":[[311,320,"Mention"],[994,1006,"API"]],"Comments":[]}
{"id":59716,"text":"ID:19713300\nPost:\nText: An easy way to create a block matrix is bmat (as pointed out by @inquisitiveIdiot). Judging by the block matrix you're looking to create, you need a 3x2 matrix of zeros: \nCode: >>> import numpy as np\n>>> z = np.zeros( (3, 2) )\n\nText: You can then create a block matrix by passing a 2x2 array of the blocks to numpy.bmat: \nCode: >>> M = np.bmat( [[a, b], [z, c]] )\n>>> M\nmatrix([[  1.,   2.,   5.,   6.,   7.],\n        [  3.,   4.,   8.,   9.,  10.],\n        [  0.,   0.,   1.,   2.,   3.],\n        [  0.,   0.,   4.,   5.,   6.],\n        [  0.,   0.,   7.,   8.,   9.]])\n\nText: Another (IMO more complicated) method is to use hstack and numpy.vstack. \nCode: >>> M = np.vstack( (np.hstack((a, b)), np.hstack((z, c))) )\n>>> M\nmatrix([[  1.,   2.,   5.,   6.,   7.],\n        [  3.,   4.,   8.,   9.,  10.],\n        [  0.,   0.,   1.,   2.,   3.],\n        [  0.,   0.,   4.,   5.,   6.],\n        [  0.,   0.,   7.,   8.,   9.]])\n\nAPI:\nnumpy.bmat\nnumpy.hstack\n","label":[[64,68,"Mention"],[650,656,"Mention"],[955,965,"API"],[966,978,"API"]],"Comments":[]}
{"id":59717,"text":"ID:19784069\nPost:\nText: I think you might be able to use np.split for this. You can split an array into sub-arrays by doing something like: \nCode: import numpy as np\n\ndef findsplit(a):\n    diff = a[1:] != a[:-1]\n    edges = np.where(diff)[0]\n    return edges + 1\n\narray = np.array([0,0,0,1,1,1,1,2,2,3,4,4,4])\ns = np.split(array, findsplit(array))\nfor a in s:\n    print a\n# [0 0 0]\n# [1 1 1 1]\n# [2 2]\n# [3]\n# [4 4 4]\n\nText: To get the nested dictionaries you discribe in your question you could do something like: \nCode: byFront = np.split(array, findsplit(array['Front']))\nfront_dict = {}\nfor sameFront in byFront:\n    back_dict = {}\n    byBack = np.split(sameFront, findsplit(sameFront['Back']))\n    for sameBack in byBack:\n        back_dict[sameBack['Back'][0]] = sameBack\n    front_dict[sameFront['Front'][0]] = back_dict\n\nAPI:\nnumpy.split\n","label":[[57,65,"Mention"],[833,844,"API"]],"Comments":[]}
{"id":59718,"text":"ID:19827819\nPost:\nText: I would just use plain lists here, when you create a np.array with sub-lists of different lengths (or different data types in your array) you get an array of type object, which lacks many useful numpy features and is rarely a good idea to implement. \nCode: x = [['foo', '333', 32.3],\n     ['bar', 4.0],\n     ['baz', '555', '2232',  -1.9]]\n\nresult = sorted(x, key=lambda k : k[-1], reverse=True)\n\nText: Result: \nCode: >>> result\n[['foo', '333', 32.3], ['bar', 4.0], ['baz', '555', '2232', -1.9]]\n\nAPI:\nnumpy.array\n","label":[[77,85,"Mention"],[525,536,"API"]],"Comments":[]}
{"id":59719,"text":"ID:19867192\nPost:\nText: my problem appears to be resolved. I was using a django module from inside which I was calling multiprocessing.pool.map_async. My worker function was a function inside the class itself. That was the problem. Multiprocessesing cannot call a function of the same class inside another process because subprocesses do not share memory. So inside the subprocess there is no live instance of the class. Probably that is why it is not getting called. As far as I understood. I removed the function from the class and put it in the same file but outside of the class, just before the class definition starts. It worked. I got moderate speedup also. And One more thing is people who are facing the same problem please do not read large arrays and pass between processes. Pickling and Unpickling would take a lot of time and you won't get speed up rather speed down. Try to read arrays inside the subprocess itself. \nText: And if possible please use np.memmap arrays, they are quite fast. \nAPI:\nnumpy.memmap\n","label":[[964,973,"Mention"],[1009,1021,"API"]],"Comments":[]}
{"id":59720,"text":"ID:19956324\nPost:\nText: The problem you have is that although nl and npred are ndarray objects they can contain heterogenous data. See doc numpy.dtype: \nText: A numpy array is homogeneous, and contains elements described by a dtype object. A dtype object can be constructed from different combinations of fundamental numeric types. \nText: so if n1 is an array of strings and npred an array of ints you can't perform the add operation: \nCode: >>> import numpy as np\n>>> a = np.array(['a', 'b', 'c'])\n>>> b = np.array([1, 2, 3])\n>>> type(a), type(b)\n (numpy.ndarray, numpy.ndarray)\n>>> a + b\n unsupported operand type(s) for +: 'numpy.ndarray' and 'numpy.ndarray\n\nText: If you want to know the content type of your arrays: \nCode: >>> a.dtype, b.dtype\n (dtype('S1'), dtype('int64'))\n\nText: So, you must know which data type contain each array. It is NOT a problem of dimensions. \nAPI:\nnumpy.ndarray\n","label":[[79,86,"Mention"],[882,895,"API"]],"Comments":[]}
{"id":59721,"text":"ID:19956889\nPost:\nText: In your first attempt you are trying to save a float64 variable, and documentation says that savetxt expects an array_like object. \nText: In your second attempt you missed the brackets to specify a matrix g_=array([g]), however if you save the txt inside the loop you will be overwriting your output file each time. \nText: I guess this is what you want: \nCode: import numpy as np\n\ng = list()\nk = range(8,15)\nfor i in k:\n    q = range(i)\n    g.append(np.mean(q))\n\nnp.savetxt('myfile.txt', np.array(g), fmt='%.2f')\n\nText: Output of myfile.txt: \nCode: 3.50\n4.00\n4.50\n5.00\n5.50\n6.00\n6.50\n\nAPI:\nnumpy.float64\nnumpy.savetxt\n","label":[[71,78,"Mention"],[117,124,"Mention"],[614,627,"API"],[628,641,"API"]],"Comments":[]}
{"id":59722,"text":"ID:19976162\nPost:\nText: Be careful, you are not computing the continuous time Fourier transform, computers work with discrete data, so does Numpy, if you take a look to fft documentation it says: \nText: numpy.fft.fft(a, n=None, axis=-1)[source] Compute the one-dimensional discrete Fourier Transform. This function computes the one-dimensional n-point discrete Fourier Transform (DFT) with the efficient Fast Fourier Transform (FFT) algorithm \nText: That means that your are computing the DFT which is defined by equation: \nText: the continuous time Fourier transform is defined by: \nText: And if you do the maths to look for the relationship between them: \nText: As you can see there is a constant factor 1\/N which is exactly your scale value dt (x[n] - x[n-1] where n is in [0,T] interval is equivalent to 1\/N). \nText: Just a comment on your code, it is not a good practice to import everything from numpy import * instead use: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\n# create data\nN = 4097\nT = 100.0\nt = np.linspace(-T\/2,T\/2,N)\nf = np.exp(-np.pi*t**2)\n\n# perform FT and multiply by dt\ndt = t[1]-t[0]\nft = np.fft.fft(f) * dt      \nfreq = np.fft.fftfreq(N, dt)\nfreq = freq[:N\/2+1]\n\n# plot results\nplt.plot(freq, np.abs(ft[:N\/2+1]),'o')\nplt.plot(freq, np.exp(-np.pi * freq**2),'r')\nplt.legend(('numpy fft * dt', 'exact solution'), loc='upper right')\nplt.xlabel('f')\nplt.ylabel('amplitude')\nplt.xlim([0, 1.4])\nplt.show()\n\nAPI:\nnumpy.fft.fft\n","label":[[169,172,"Mention"],[1445,1458,"API"]],"Comments":[]}
{"id":59723,"text":"ID:20030280\nPost:\nText: np.genfromtxt accepts generators, so you can chain genfromtext and ifilter: \nCode: from itertools import ifilter\nwith open(fname, 'rb') as inp:\n     filtered_inp = ifilter(lambda x: x.startswith('A'), inp)\n     a = np.genfromtxt(filtered_inp)\n\nAPI:\nnumpy.genfromtxt\n","label":[[24,37,"Mention"],[273,289,"API"]],"Comments":[]}
{"id":59724,"text":"ID:20141196\nPost:\nText: Try this: \nCode: import numpy as np\n\ndata = np.random.randint(0,100,size=(1,100))\nnp.savetxt('data.csv',data,delimiter=',',fmt='%d')     #numpy savetxt\n\nText: and the output: \nText: [data.csv] \nCode: 70,53,95,60,91,...\n\nText: You can also define other formats: \nCode: fmt='\"%d\"'\n\nText: which results in: \nCode: \"70\",\"53\",\"95\",\"60\",\"91\",...\n\nText: You can find more about np.savetxt at {here}. \nAPI:\nnumpy.savetxt\n","label":[[395,405,"Mention"],[423,436,"API"]],"Comments":[]}
{"id":59725,"text":"ID:20161762\nPost:\nText: You can use np.array and convert the whole Image into a ndarray object: \nCode: import numpy as np\na = np.array(Image.open('d:\\eye.jpg')).swapaxes(0,1)\n\nText: Where a[i,j] is a position in this array that will give you the same result as rgb1.getpixel((i,j)) (with the difference that the latter returns a tuple). \nAPI:\nnumpy.array\nnumpy.ndarray\n","label":[[36,44,"Mention"],[80,87,"Mention"],[343,354,"API"],[355,368,"API"]],"Comments":[]}
{"id":59726,"text":"ID:20209858\nPost:\nText: What is my_array[[0]]? my_array is a 1d array of records defined by my_dtype. \nText: my_array[0] is one of those records, a tuple. Notice that some entries are float, some integers. If it was a row of a 2d array, all entries would be of the same type (e.g. float). \nText: To convert it to a 2d array of floats, you might try: \nCode: np.array(my_array.tolist())\n\nText: Another way is to convert all the fields to the same type, and reshape it. Something along this line (tested on a different recarray): \nCode: x = array([(1.0, 2), (3.0, 4)], dtype=[('x', '<f8'), ('y', '<i4')])\nx.astype([('x', '<f8'), ('y', '<f8')]).view(dtype='f8').reshape(2,2)\n\nText: See also: How to convert np.recarray to numpy.array? \nAPI:\nnumpy.recarray\n","label":[[703,714,"Mention"],[737,751,"API"]],"Comments":[]}
{"id":59727,"text":"ID:20228722\nPost:\nText: I'd use vectorize to \"vectorize\" your function. Note that despite the name, vectorize is not intended to make your code run faster -- Just simplify it a bit. \nText: Here's some examples: \nCode: >>> import numpy as np\n>>> @np.vectorize\n... def foo(a, b):\n...    return a + b\n... \n>>> foo([1,3,5], [2,4,6])\narray([ 3,  7, 11])\n>>> foo(np.arange(9).reshape(3,3), np.arange(9).reshape(3,3))\narray([[ 0,  2,  4],\n       [ 6,  8, 10],\n       [12, 14, 16]])\n\nText: With your code, it should be enough to decorate func with np.vectorize and then you can probably just call it as func(X, Y) -- No raveling or reshapeing necessary: \nCode: import numpy as np\nimport collections as c\n\n# some arbitrary lookup table\na = c.defaultdict(int)\na[1] = 2\na[2] = 3\na[3] = 2\na[4] = 3\n\n@np.vectorize\ndef func(x,y):\n    # some arbitrary function\n    return a[x] + a[y]\n\nX,Y = np.mgrid[1:3, 1:4]\nX = X.T\nY = Y.T\n\nZ = func(X, Y)\n\nAPI:\nnumpy.vectorize\n","label":[[32,41,"Mention"],[933,948,"API"]],"Comments":[]}
{"id":59728,"text":"ID:20270042\nPost:\nText: from this post: How to apply norm to each row of a matrix? \nText: If the operation supports axis, use the axis parameter, it's usually faster, \nText: Otherwise, np.apply_along_axis could help. \nText: Here is the numpy.count_nonzero. \nText: So here is the simple answer: \nCode: import numpy as np\n\narr = np.eye(3)\nnp.apply_along_axis(np.count_nonzero, 0, arr)\n\nAPI:\nnumpy.linalg.norm\n","label":[[53,57,"Mention"],[389,406,"API"]],"Comments":[]}
{"id":59729,"text":"ID:20627118\nPost:\nText: This works... \nCode: In [10]: np.vstack([np.hstack([Mat_1, Mat_2]), np.hstack([Mat_3, Mat_4])])\nOut[10]:\narray([['1', '2', 'a', 'b'],\n       ['3', '4', 'c', 'd'],\n       ['5', '6', 'e', 'f'],\n       ['7', '8', 'g', 'h']],\n      dtype='|S1')\n\nText: EDIT: \nText: It looks like the np.bmat function was written just for this purpose: \nCode: In [11]: np.bmat([[Mat_1, Mat_2], [Mat_3, Mat_4]])\nOut[11]:\nmatrix([['1', '2', 'a', 'b'],\n        ['3', '4', 'c', 'd'],\n        ['5', '6', 'e', 'f'],\n        ['7', '8', 'g', 'h']],\n       dtype='|S1')\n\nAPI:\nnumpy.bmat\n","label":[[303,310,"Mention"],[569,579,"API"]],"Comments":[]}
{"id":59730,"text":"ID:20672567\nPost:\nText: Seems like wehre is exactly what you want: \nCode: >>> import numpy as np\n>>> k = np.array([True, False, False, True, False])\n>>> np.where(k, 2, 5)\narray([2, 5, 5, 2, 5])\n\nAPI:\nnumpy.where\n","label":[[35,40,"Mention"],[200,211,"API"]],"Comments":[]}
{"id":59731,"text":"ID:20700103\nPost:\nText: Specify dtype: \nCode: >>> import numpy as np\n>>> list1=['10','20','30']\n>>> list2=['40','50','60']\n>>> list3=['70','80','90']\n>>> np.array([list1, list2, list3], dtype=int)\narray([[10, 20, 30],\n       [40, 50, 60],\n       [70, 80, 90]])\n\nText: According to ar documentation: \nText: dtype : data-type, optional The desired data-type for the array. If not given, then the type will be determined as the minimum type required to hold the objects in the sequence. ... \nAPI:\nnumpy.array\n","label":[[281,283,"Mention"],[494,505,"API"]],"Comments":[]}
{"id":59732,"text":"ID:20768687\nPost:\nText: Will np.float32 help? \nCode: >>>PI=3.1415926535897\n>>> print PI*PI\n9.86960440109\n>>> PI32=numpy.float32(PI)\n>>> print PI32*PI32\n9.86961\n\nText: If you want to do math operation on float32, convert the operands to float32 may help you. \nAPI:\nnumpy.float32\n","label":[[29,39,"Mention"],[264,277,"API"]],"Comments":[]}
{"id":59733,"text":"ID:20775459\nPost:\nText: Change the dtype of mat to numpy.object. Default dtype of array returned by zeros is float, so you can't set an item of type ndarray to any of its element. \nCode: >>> import numpy as np\n>>> v = mat.astype(np.object)\n>>> v[0][0] = vec\n>>> v\narray([[array([ 0.,  0.,  0.,  0.,  0.]), 0.0, 0.0, 0.0, 0.0],\n       [0.0, 0.0, 0.0, 0.0, 0.0],\n       [0.0, 0.0, 0.0, 0.0, 0.0],\n       [0.0, 0.0, 0.0, 0.0, 0.0],\n       [0.0, 0.0, 0.0, 0.0, 0.0]], dtype=object)\n\nAPI:\nnumpy.zeros\n","label":[[100,105,"Mention"],[484,495,"API"]],"Comments":[]}
{"id":59734,"text":"ID:20867432\nPost:\nText: The array elements that display as 2.68530063 may not be exactly equal to 2.68530063. NumPy truncates array elements for display purposes, and rounding error in the computations that produced your array may have caused elements to have values slightly off from what you'd expect. You can use isclose to perform comparisons within a tolerance: \nCode: distances[numpy.isclose(distances, 0)]  = 1\ndistances[distances > 2.68530063] = 0\ndistances[numpy.isclose(distances, 2.68530063)]  = -1\n\nText: Also, note that the order you perform these assignments in is important. When you compare distances > 2.68530063, the distances values that used to equal 0 will now be 1. That might cause bugs if you're not careful. \nAPI:\nnumpy.isclose\n","label":[[316,323,"Mention"],[739,752,"API"]],"Comments":[]}
{"id":59735,"text":"ID:20953194\nPost:\nText: That's because nda implements the __abs__(self) method. Just provide it for your own class, and abs() will magically work. For non-builtin types you can also provide this facility after-the-fact. E.g. \nCode: class A:\n    \"A class without __abs__ defined\"\n    def __init__(self, v):\n        self.v = v\n\ndef A_abs(a):\n    \"An 'extension' method that will be added to `A`\"\n    return abs(a.v)\n\n# Make abs() work with an instance of A\nA.__abs__ = A_abs\n\nText: However, this will not work for built-in types, such as list or dict. \nAPI:\nnumpy.ndarray\n","label":[[39,42,"Mention"],[556,569,"API"]],"Comments":[]}
{"id":59736,"text":"ID:20989347\nPost:\nText: If you have a numpy.ndarray, then try this: \nCode: >>> import numpy\n>>> lst = [6.72599983, -7.15100002, 4.68499994]\n>>> numpy.asarray(lst)\narray([ 6.72599983, -7.15100002,  4.68499994])\n>>> list(numpy.asarray(lst))\n[6.7259998300000001, -7.1510000199999997, 4.68499994]\n\nText: If you have wrongly casted the array into a string, then you need to use that cleaning trick with ast to get it into a list. \nCode: >>> import ast, numpy\n>>> s = str(numpy.asarray(lst))\n>>> s\n'[ 6.72599983 -7.15100002  4.68499994]'\n>>> list(ast.literal_eval(\",\".join(s.split()).replace(\"[,\", \"[\")))\n[6.72599983, -7.15100002, 4.68499994]\n\nAPI:\nnumpy.array\n","label":[[331,336,"Mention"],[643,654,"API"]],"Comments":[]}
{"id":59737,"text":"ID:21015708\nPost:\nText: Use np.array to use shape attribute. \nCode: >>> import numpy as np\n>>> X = np.array([\n...     [[-9.035250067710876], [7.453250169754028], [33.34074878692627]],\n...     [[-6.63700008392334], [5.132999956607819], [31.66075038909912]],\n...     [[-5.1272499561309814], [8.251499891281128], [30.925999641418457]]\n... ])\n>>> X.shape\n(3L, 3L, 1L)\n\nText: NOTE X.shape returns 3-items tuple for the given array; [n, T] = X.shape raises ValueError. \nAPI:\nnumpy.array\n","label":[[28,36,"Mention"],[469,480,"API"]],"Comments":[]}
{"id":59738,"text":"ID:21022829\nPost:\nText: np.diff does exactly that with its parameter axis: \nCode: >>> a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n>>> np.diff(a, axis = 0)\narray([[3, 3, 3],\n       [3, 3, 3]])\n>>> np.diff(a, axis=1)\narray([[1, 1],\n       [1, 1],\n       [1, 1]])\n\nAPI:\nnumpy.diff\n","label":[[24,31,"Mention"],[263,273,"API"]],"Comments":[]}
{"id":59739,"text":"ID:21031019\nPost:\nText: In numpy, cov defaults to a \"delta degree of freedom\" of 1 while var defaults to a ddof of 0. From the notes to var \nCode: Notes\n-----\nThe variance is the average of the squared deviations from the mean,\ni.e.,  ``var = mean(abs(x - x.mean())**2)``.\n\nThe mean is normally calculated as ``x.sum() \/ N``, where ``N = len(x)``.\nIf, however, `ddof` is specified, the divisor ``N - ddof`` is used\ninstead.  In standard statistical practice, ``ddof=1`` provides an\nunbiased estimator of the variance of a hypothetical infinite population.\n``ddof=0`` provides a maximum likelihood estimate of the variance for\nnormally distributed variables.\n\nText: So you can get them to agree by taking: \nCode: In [69]: cov(x,x)#defaulting to ddof=1\nOut[69]: \narray([[ 0.5,  0.5],\n       [ 0.5,  0.5]])\n\nIn [70]: x.var(ddof=1)\nOut[70]: 0.5\n\nIn [71]: cov(x,x,ddof=0)\nOut[71]: \narray([[ 0.25,  0.25],\n       [ 0.25,  0.25]])\n\nIn [72]: x.var()#defaulting to ddof=0\nOut[72]: 0.25\n\nAPI:\nnumpy.var\n","label":[[136,139,"Mention"],[983,992,"API"]],"Comments":[]}
{"id":59740,"text":"ID:21033701\nPost:\nText: Use np.loadtxt with the parameter usecols for selecting only the columns containing float numbers. \nCode: >>> import numpy as np\n>>> cols = range(0,5) + range(6,35)\n>>> data = np.loadtxt(\"data.txt\", delimiter=\",\", usecols=cols, dtype=np.float)\n>>> data\n[[  1.96600000e+03   1.00000000e+00   1.00000000e+00   1.00000000e+00\n    6.00000000e+01   3.90000000e+00   1.70000000e+00   8.60000000e+01\n    1.02400000e+05   0.00000000e+00   0.00000000e+00   2.64000000e+02\n    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n    0.00000000e+00   0.00000000e+00   0.00000000e+00   2.30000000e+02\n    2.10000000e+00   0.00000000e+00   0.00000000e+00   2.41000000e+01\n    7.77770000e+04   0.00000000e+00   9.99999999e+08   8.00000000e+00\n    1.00000000e-01   0.00000000e+00   8.80000000e+01   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  1.96600000e+03   1.00000000e+00   1.00000000e+00   2.00000000e+00\n    6.00000000e+01   4.40000000e+00   0.00000000e+00   7.30000000e+01\n    1.02500000e+05   0.00000000e+00   0.00000000e+00   2.65000000e+02\n    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n    0.00000000e+00   0.00000000e+00   0.00000000e+00   2.70000000e+02\n    3.60000000e+00   0.00000000e+00   0.00000000e+00   2.41000000e+01\n    7.77770000e+04   0.00000000e+00   9.99999999e+08   8.00000000e+00\n    1.00000000e-01   0.00000000e+00   8.80000000e+01   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  1.96600000e+03   1.00000000e+00   1.00000000e+00   3.00000000e+00\n    6.00000000e+01   2.80000000e+00  -6.00000000e-01   7.90000000e+01\n    1.02500000e+05   0.00000000e+00   0.00000000e+00   2.58000000e+02\n    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n    0.00000000e+00   0.00000000e+00   0.00000000e+00   3.10000000e+02\n    2.10000000e+00   0.00000000e+00   0.00000000e+00   2.41000000e+01\n    7.77770000e+04   0.00000000e+00   9.99999999e+08   8.00000000e+00\n    1.00000000e-01   0.00000000e+00   8.80000000e+01   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]]\n\nText: If you want to include the 6th column, then you will have to load the matrix as object, you can't mix floats with strings. \nCode: >>> data = np.loadtxt(\"data.txt\", delimiter=\",\", dtype=np.object)\n\nText: So if you need this column, load it separately. \nAPI:\nnumpy.loadtxt\n","label":[[28,38,"Mention"],[2333,2346,"API"]],"Comments":[]}
{"id":59741,"text":"ID:21042520\nPost:\nText: np.concatenate will definitely be a valid choice - without sample data and sample code and corresponding error messages we cannot help further. \nText: Alternatives would be numpy.r_, s_ \nText: EDIT \nText: This code snippet: \nCode: import sys\nimport numpy as np\nfilenames = sys.argv[1:]\narrays = [np.loadtxt(filename) for filename in filenames]\nfinal_array = np.concatenate(arrays, axis=0)\n\nAPI:\nnumpy.concatenate\nnumpy.s_\n","label":[[24,38,"Mention"],[207,209,"Mention"],[419,436,"API"],[437,445,"API"]],"Comments":[]}
{"id":59742,"text":"ID:21043878\nPost:\nText: Problem is you're using numpy.math.cos here, which expects you to pass a scalar. Use cos if you want to apply cos to an iterable. \nCode: In [30]: import numpy as np\n\nIn [31]: np.cos(np.array([1, 2, 3]))                                                             \nOut[31]: array([ 0.54030231, -0.41614684, -0.9899925 ])\n\nText: Error: \nCode: In [32]: np.math.cos(np.array([1, 2, 3]))                                                        \n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-32-8ce0f3c0df04> in <module>()\n----> 1 np.math.cos(np.array([1, 2, 3]))\n\nTypeError: only length-1 arrays can be converted to Python scalars\n\nAPI:\nnumpy.cos\n","label":[[109,112,"Mention"],[776,785,"API"]],"Comments":[]}
{"id":59743,"text":"ID:21059308\nPost:\nText: Here's a numpy version of the rolling maximum drawdown function. windowed_view is a wrapper of a one-line function that uses stride.as_strided to make a memory efficient 2d windowed view of the 1d array (full code below). Once we have this windowed view, the calculation is basically the same as your max_dd, but written for a numpy array, and applied along the second axis (i.e. axis=1). \nCode: def rolling_max_dd(x, window_size, min_periods=1):\n    \"\"\"Compute the rolling maximum drawdown of `x`.\n\n    `x` must be a 1d numpy array.\n    `min_periods` should satisfy `1 <= min_periods <= window_size`.\n\n    Returns an 1d array with length `len(x) - min_periods + 1`.\n    \"\"\"\n    if min_periods < window_size:\n        pad = np.empty(window_size - min_periods)\n        pad.fill(x[0])\n        x = np.concatenate((pad, x))\n    y = windowed_view(x, window_size)\n    running_max_y = np.maximum.accumulate(y, axis=1)\n    dd = y - running_max_y\n    return dd.min(axis=1)\n\nText: Here's a complete script that demonstrates the function: \nCode: import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef windowed_view(x, window_size):\n    \"\"\"Creat a 2d windowed view of a 1d array.\n\n    `x` must be a 1d numpy array.\n\n    `numpy.lib.stride_tricks.as_strided` is used to create the view.\n    The data is not copied.\n\n    Example:\n\n    >>> x = np.array([1, 2, 3, 4, 5, 6])\n    >>> windowed_view(x, 3)\n    array([[1, 2, 3],\n           [2, 3, 4],\n           [3, 4, 5],\n           [4, 5, 6]])\n    \"\"\"\n    y = as_strided(x, shape=(x.size - window_size + 1, window_size),\n                   strides=(x.strides[0], x.strides[0]))\n    return y\n\n\ndef rolling_max_dd(x, window_size, min_periods=1):\n    \"\"\"Compute the rolling maximum drawdown of `x`.\n\n    `x` must be a 1d numpy array.\n    `min_periods` should satisfy `1 <= min_periods <= window_size`.\n\n    Returns an 1d array with length `len(x) - min_periods + 1`.\n    \"\"\"\n    if min_periods < window_size:\n        pad = np.empty(window_size - min_periods)\n        pad.fill(x[0])\n        x = np.concatenate((pad, x))\n    y = windowed_view(x, window_size)\n    running_max_y = np.maximum.accumulate(y, axis=1)\n    dd = y - running_max_y\n    return dd.min(axis=1)\n\n\ndef max_dd(ser):\n    max2here = pd.expanding_max(ser)\n    dd2here = ser - max2here\n    return dd2here.min()\n\n\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    n = 100\n    s = pd.Series(np.random.randn(n).cumsum())\n\n    window_length = 10\n\n    rolling_dd = pd.rolling_apply(s, window_length, max_dd, min_periods=0)\n    df = pd.concat([s, rolling_dd], axis=1)\n    df.columns = ['s', 'rol_dd_%d' % window_length]\n    df.plot(linewidth=3, alpha=0.4)\n\n    my_rmdd = rolling_max_dd(s.values, window_length, min_periods=1)\n    plt.plot(my_rmdd, 'g.')\n\n    plt.show()\n\nText: The plot shows the curves generated by your code. The green dots are computed by rolling_max_dd. \nText: Timing comparison, with n = 10000 and window_length = 500: \nCode: In [2]: %timeit rolling_dd = pd.rolling_apply(s, window_length, max_dd, min_periods=0)\n1 loops, best of 3: 247 ms per loop\n\nIn [3]: %timeit my_rmdd = rolling_max_dd(s.values, window_length, min_periods=1)\n10 loops, best of 3: 38.2 ms per loop\n\nText: rolling_max_dd is about 6.5 times faster. The speedup is better for smaller window lengths. For example, with window_length = 200, it is almost 13 times faster. \nText: To handle NA's, you could preprocess the Series using the fillna method before passing the array to rolling_max_dd. \nAPI:\nnumpy.lib.stride_tricks.as_strided\n","label":[[149,166,"Mention"],[3565,3599,"API"]],"Comments":[]}
{"id":59744,"text":"ID:21087900\nPost:\nText: Using np.insert and numpy.diff: \nCode: >>> import numpy as np\n>>> a = np.array([[1, 2, 8], [8, 2, 7], [7, 2, 5]])\n>>> np.insert(np.diff(a), 0, 0, axis=1)\narray([[ 0,  1,  6],\n       [ 0, -6,  5],\n       [ 0, -5,  3]])\n\nAPI:\nnumpy.insert\n","label":[[30,39,"Mention"],[248,260,"API"]],"Comments":[]}
{"id":59745,"text":"ID:21190090\nPost:\nText: Not sure if it can be done with numpy.sort, but you can use argsore for sure: \nCode: >>> arr\narray([[ 105.,    4.],\n       [  53.,  520.],\n       [ 745.,  902.],\n       [  19.,   nan],\n       [ 184.,   nan],\n       [  22.,   10.],\n       [ 104.,   26.]])\n>>> arr[np.argsort(arr[:,1])]\narray([[ 105.,    4.],\n       [  22.,   10.],\n       [ 104.,   26.],\n       [  53.,  520.],\n       [ 745.,  902.],\n       [  19.,   nan],\n       [ 184.,   nan]])\n\nAPI:\nnumpy.argsort\n","label":[[84,91,"Mention"],[477,490,"API"]],"Comments":[]}
{"id":59746,"text":"ID:21194602\nPost:\nText: Here it goes a workaround. For a given coo_matrix called m (obtained with m.tocoo()): \nText: 1) create a memmaup array for writing: \nCode: mm = np.memmap('test.memmap', mode='w+', dtype=m.dtype, shape=m.shape)\n\nText: 2) copy the data to the memmap array, which should work: \nCode: for i,j,v in zip(m.row, m.col, m.data):\n    mm[i,j] = v\n\nText: 3) You can access the memmap as detailed in the documentation... \nAPI:\nnumpy.memmap\n","label":[[129,136,"Mention"],[439,451,"API"]],"Comments":[]}
{"id":59747,"text":"ID:21201814\nPost:\nText: You are inserting lists of length 3 into q. When you finish the loop that creates q, q is a list of 4 items, where each item is a list of length 3. So np.array(q) creates an array with shape 4x3. You could change the second-to-last line to this: \nCode: q = np.array(q).T\n\nText: Or, you can use numpy more effectively to eliminate all the explicit for loops. For example, if you are using numpy 1.8, the norm function accepts an axis argument. \nText: Here's vectorized version of your code. \nText: First, some setup for this example. \nCode: In [152]: np.set_printoptions(precision=3)\n\nIn [153]: np.random.seed(111)\n\nText: Create some data to work with. \nCode: In [154]: v = 5e-9 * np.random.randint(0, 3, size=(3, 4))\n\nIn [155]: v\nOut[155]: \narray([[  0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00],\n       [  1.000e-08,   5.000e-09,   1.000e-08,   1.000e-08],\n       [  1.000e-08,   0.000e+00,   5.000e-09,   0.000e+00]])\n\nText: Compute the square root of the norms of the columns by using the argument axis=0 in numpy.linalg.norm. \nCode: In [156]: L = np.sqrt(np.linalg.norm(v, axis=0))\n\nIn [157]: L\nOut[157]: array([  1.189e-04,   7.071e-05,   1.057e-04,   1.000e-04])\n\nText: Use whexre to select the values by which the columns of v are to be divided to create q. \nCode: In [158]: scale = np.where(L > 0.0001, L, 1000.0)\n\nIn [159]: scale\nOut[159]: array([  1.189e-04,   1.000e+03,   1.057e-04,   1.000e+03])\n\nText: q is has shape (3, 4), and scale has shape (4,), so we can use broadcasting to divide each column of q by the corresponding value in scale. \nCode: In [160]: q = v \/ scale\n\nIn [161]: q\nOut[161]: \narray([[  0.000e+00,   0.000e+00,   0.000e+00,   0.000e+00],\n       [  8.409e-05,   5.000e-12,   9.457e-05,   1.000e-11],\n       [  8.409e-05,   0.000e+00,   4.729e-05,   0.000e+00]])\n\nText: Repeated here are the three lines of the vectorized code: \nCode: L = np.sqrt(np.linalg.norm(v, axis=0))\nscale = np.where(L > 0.0001, L, 1000.0)\nq = v \/ scale\n\nAPI:\nnumpy.where\n","label":[[1209,1215,"Mention"],[1995,2006,"API"]],"Comments":[]}
{"id":59748,"text":"ID:21235029\nPost:\nText: save is what you need : \nCode: numpy.save(file, arr)\nSave an array to a binary file in NumPy .npy format.\n\nAPI:\nnumpy.save\n","label":[[24,28,"Mention"],[136,146,"API"]],"Comments":[]}
{"id":59749,"text":"ID:21242776\nPost:\nText: It looks like you could use np.bincount here: \nCode: import numpy as np\n\ndef radial_profile(data, center):\n    y, x = np.indices((data.shape))\n    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n    r = r.astype(np.int)\n\n    tbin = np.bincount(r.ravel(), data.ravel())\n    nr = np.bincount(r.ravel())\n    radialprofile = tbin \/ nr\n    return radialprofile \n\nAPI:\nnumpy.bincount\n","label":[[52,63,"Mention"],[392,406,"API"]],"Comments":[]}
{"id":59750,"text":"ID:21254228\nPost:\nText: Use isinstance: \nCode: >>> f = numpy.float64(1.4)\n>>> isinstance(f, numpy.float64)\nTrue\n>>> isinstance(f, float)\nTrue\n\nText: np.float64 is inherited from python native float type. That because it is both float and float64 (@Bakuriu thx for pointing out). But if you will check python float instance variable for float64 type you will get False in result: \nCode: >>> f = 1.4\n>>> isinstance(f, numpy.float64)\nFalse\n>>> isinstance(f, float)\nTrue\n\nAPI:\nnumpy.float64\n","label":[[149,159,"Mention"],[473,486,"API"]],"Comments":[]}
{"id":59751,"text":"ID:21336836\nPost:\nText: You need to use cconcatentae instead of array addition \nCode: c = numpy.concatenate((a, b))\n\nText: Implementation \nCode: import numpy as np\na = np.arange(53)\nb = np.arange(82)\nc = np.concatenate((a, b))\n\nText: Output \nCode: c.shape\n(135, )\n\nAPI:\nnumpy.concatenate\n","label":[[40,52,"Mention"],[270,287,"API"]],"Comments":[]}
{"id":59752,"text":"ID:21340171\nPost:\nText: You can use decode to decode the bytes literal: \nCode: In [1]: import numpy as np\n\nIn [2]: a = np.eye(2, dtype='S17')                                                                                   \n\nIn [3]: a\nOut[3]: \narray([[b'1', b''],                                                                                                \n       [b'', b'1']],                                                                                               \n      dtype='|S17')                                                                                                \n\nIn [4]: np.char.decode(a, 'ascii')                                                                                 \nOut[4]: \narray([['1', ''],                                                                                                  \n       ['', '1']],                                                                                                 \n      dtype='<U1')  \n\nAPI:\nnumpy.char.decode\n","label":[[36,42,"Mention"],[978,995,"API"]],"Comments":[]}
{"id":59753,"text":"ID:21343127\nPost:\nText: Matplotlib can only handle real values. Your options are to take the real or imaginary parts of the results, or magnitude and maybe even phase. These can be done with real or numpy.imag, or abs and numpy.angle. \nText: Ultimately, I guess it just depends on what you want to know about your FFT. People are usually most interested in the magnitude of FFT data, which suggests abs. This gives you an idea of the \"power\" in the various frequencies. \nAPI:\nnumpy.real\nnumpy.abs\n","label":[[191,195,"Mention"],[214,217,"Mention"],[476,486,"API"],[487,496,"API"]],"Comments":[]}
{"id":59754,"text":"ID:21404655\nPost:\nText: You can use split and then filter the resulting list. Here is one example assuming that the column with the values is labeled \"value\": \nCode: events = np.split(df, np.where(np.isnan(df.value))[0])\n# removing NaN entries\nevents = [ev[~np.isnan(ev.value)] for ev in events if not isinstance(ev, np.ndarray)]\n# removing empty DataFrames\nevents = [ev for ev in events if not ev.empty]\n\nText: You will have a list with all the events separated by the NaN values. \nAPI:\nnumpy.split\n","label":[[36,41,"Mention"],[488,499,"API"]],"Comments":[]}
{"id":59755,"text":"ID:21407851\nPost:\nText: Use where with numpy.column_stack: \nCode: >>> np.column_stack(np.where(b))\narray([[1, 2],\n       [2, 0],\n       [2, 1],\n       [2, 2]])\n\nAPI:\nnumpy.where\n","label":[[28,33,"Mention"],[166,177,"API"]],"Comments":[]}
{"id":59756,"text":"ID:21436602\nPost:\nText: I'll extend slightly the answer by @VladimirF as I suspect you don't want to limit yourself to the exact print example. \nText: a>7 returns a logical array corresponding to a with .true. at index where the condition is met, .false. otherwise. The pack intrinsic takes such a mask and returns an array with those elements with .true. in the mask. \nText: However, you can do other things with the mask which may fit under your np.where desire. For example, there is the where construct (and where statement) and the merge intrinsic. Further you can use pack again with the mask to get the indices and do more involved manipulations. \nAPI:\nnumpy.where\n","label":[[448,456,"Mention"],[660,671,"API"]],"Comments":[]}
{"id":59757,"text":"ID:21454043\nPost:\nText: You can use diff for this: \nCode: In [13]: import numpy as np\n\nIn [14]: a = np.array([1., 5., 4., 2., 10., 8., 3., 1.])                                         \n\nIn [15]: np.diff(a)\/a[1:]                                                                        \nOut[15]: \narray([ 0.8       , -0.25      , -1.        ,  0.8       , -0.25      ,                          \n       -1.66666667, -2.        ])      \n\nIn [16]: np.round(np.diff(a)\/a[1:], 3)\nOut[16]: array([ 0.8  , -0.25 , -1.   ,  0.8  , -0.25 , -1.667, -2.   ])\n\nText: In pure Python you can do this using zip with either use map or a list comprehension. Here's an example using map: \nCode: >>> def calculate(x_y):\n    x, y = x_y\n    return round((x-y)\/x, 3)\n\n>>> map(calculate, zip(lis[1:], lis[:-1]))\n[0.8, -0.25, -1.0, 0.8, -0.25, -1.667, -2.0]\n\nAPI:\nnumpy.diff\n","label":[[36,40,"Mention"],[837,847,"API"]],"Comments":[]}
{"id":59758,"text":"ID:21470538\nPost:\nText: To solve the problem shown. If you want a new array: \nCode: def g(x):\n    return np.ones_like(x)*3\n\nText: or if you want to set all elements in an array to 3 in place: \nCode: def g(x):\n    x[:] = 3\n\nText: Note there is no return statement here as you are simply updating array x so that all elements are 3. \nText: The issue with def g(x): return(3) as shown is there is no reference to numpy inside the function. You state for any given input return 3. Stating x=3 will run into similar issues as you are updating the pointer x to point to 3 instead of the numpy array. While the statement x[:]=3 accesses an internal function known as a view from the np.ndarray class instead of the usual use of the = statement that simply updates a pointer. \nAPI:\nnumpy.ndarray\n","label":[[676,686,"Mention"],[774,787,"API"]],"Comments":[]}
{"id":59759,"text":"ID:21482780\nPost:\nText: I think using tile might be better: \nCode: In [422]: np.tile((0,1,2), (5,1))\nOut[422]: \narray([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]])\n\nIn [473]: tile(arange(5)[:,None], 3)\nOut[473]: \narray([[0, 0, 0],\n       [1, 1, 1],\n       [2, 2, 2],\n       [3, 3, 3],\n       [4, 4, 4]])\n\nText: Time efficiency: \nText: for a small matrix of shape (5,3), the for-loop is faster: \nCode: In [490]: timeit np.tile((0,1,2), (5,1))\n10000 loops, best of 3: 38.3 us per loop\n\nIn [491]: %%timeit\n     ...: bx = np.zeros((5,3),np.int16)\n     ...: for j in range(3):\n     ...:     bx[:,j]=j\n     ...: \n100000 loops, best of 3: 16.5 us per loop\n\nText: but for a large matrix of shape (5, 1000), tile is faster: \nCode: In [488]: timeit n=1000; tile(xrange(n), (5,1))\n1000 loops, best of 3: 313 us per loop\n\nIn [489]: %%timeit\n     ...: n=1000\n     ...: bx=zeros((5, n))\n     ...: for j in range(n):\n     ...:     bx[:,j]=j\n     ...: \n100 loops, best of 3: 3.97 ms per loop\n\nText: Anyway, tile makes code cleaner. \nAPI:\nnumpy.tile\n","label":[[38,42,"Mention"],[1061,1071,"API"]],"Comments":[]}
{"id":59760,"text":"ID:21563036\nPost:\nText: Simplest solution \nText: Use dot or a.dot(b). See the documentation here. \nCode: >>> a = np.array([[ 5, 1 ,3], \n                  [ 1, 1 ,1], \n                  [ 1, 2 ,1]])\n>>> b = np.array([1, 2, 3])\n>>> print a.dot(b)\narray([16, 6, 8])\n\nText: This occurs because numpy arrays are not matrices, and the standard operations *, +, -, \/ work element-wise on arrays. \nText: Note that while you can use matrix (as of early 2021) where * will be treated like standard matrix multiplication, matrix is deprecated and may be removed in future releases.. See the note in its documentation (reproduced below): \nText: It is no longer recommended to use this class, even for linear algebra. Instead use regular arrays. The class may be removed in the future. \nText: Thanks @HopeKing. \nText: Other Solutions \nText: Also know there are other options: \nText: As noted below, if using python3.5+ and numpy v1.10+, the @ operator works as you'd expect: >>> print(a @ b) array([16, 6, 8]) If you want overkill, you can use numpy.einsum. The documentation will give you a flavor for how it works, but honestly, I didn't fully understand how to use it until reading this answer and just playing around with it on my own. >>> np.einsum('ji,i->j', a, b) array([16, 6, 8]) As of mid 2016 (numpy 1.10.1), you can try the experimental numpy.matmul, which works like np.dot with two major exceptions: no scalar multiplication but it works with stacks of matrices. >>> np.matmul(a, b) array([16, 6, 8]) inner functions the same way as ydot for matrix-vector multiplication but behaves differently for matrix-matrix and tensor multiplication (see Wikipedia regarding the differences between the inner product and dot product in general or see this SO answer regarding numpy's implementations). >>> np.inner(a, b) array([16, 6, 8]) # Beware using for matrix-matrix multiplication though! >>> b = a.T >>> np.dot(a, b) array([[35, 9, 10], [ 9, 3, 4], [10, 4, 6]]) >>> np.inner(a, b) array([[29, 12, 19], [ 7, 4, 5], [ 8, 5, 6]]) If you have multiple 2D arrays to dot together, you may consider the np.linalg.multi_dot function, which simplifies the syntax of many nested np.dots. Note that this only works with 2D arrays (i.e. not for matrix-vector multiplication). >>> np.dot(np.dot(a, a.T), a).dot(a.T) array([[1406, 382, 446], [ 382, 106, 126], [ 446, 126, 152]]) >>> np.linalg.multi_dot((a, a.T, a, a.T)) array([[1406, 382, 446], [ 382, 106, 126], [ 446, 126, 152]]) \nText: Rarer options for edge cases \nText: If you have tensors (arrays of dimension greater than or equal to one), you can use np.tensordot with the optional argument axes=1: >>> np.tensordot(a, b, axes=1) array([16, 6, 8]) Don't use vdot if you have a matrix of complex numbers, as the matrix will be flattened to a 1D array, then it will try to find the complex conjugate dot product between your flattened matrix and vector (which will fail due to a size mismatch n*m vs n). \nAPI:\nnumpy.dot\nnumpy.matrix\nnumpy.matrix\nnumpy.dot\nnumpy.inner\nnumpy.dot\nnumpy.tensordot\nnumpy.vdot\n","label":[[53,56,"Mention"],[424,430,"Mention"],[511,517,"Mention"],[1367,1373,"Mention"],[1502,1507,"Mention"],[1534,1538,"Mention"],[2593,2605,"Mention"],[2700,2704,"Mention"],[2950,2959,"API"],[2960,2972,"API"],[2973,2985,"API"],[2986,2995,"API"],[2996,3007,"API"],[3008,3017,"API"],[3018,3033,"API"],[3034,3044,"API"]],"Comments":[]}
{"id":59761,"text":"ID:21591863\nPost:\nText: When you call print y_axis on a numpy array, you are getting a truncated version of the numbers that numpy is actually storing internally. The way in which it is truncated depends on how numpy's printing options are set. \nCode: >>> arr = np.array([22\/7, 1\/13])           # init array\n>>> arr                                    # np.array default printing\narray([ 3.14285714,  0.07692308])\n>>> arr[0]                                 # int default printing\n3.1428571428571428\n>>> np.set_printoptions(precision=24)      # increase np.array print \"precision\"\n>>> arr                                    # np.array high-\"precision\" print\narray([ 3.142857142857142793701541,  0.076923076923076927347012])\n>>> float.hex(arr[0])                      # actual underlying representation\n'0x1.9249249249249p+1'\n\nText: The reason it looks like you're \"gaining accuracy\" when you print out the .tolist()ed form of y_axis is that by default, more digits are printed when you call print on a list than when you call print on a numpy array. \nText: In actuality, the numbers stored internally by either a list or a numpy array should be identical (and should correspond to the last line above, generated with float.hex(arr[0])), since numpy uses float64 by default, and python float objects are also 64 bits by default. \nAPI:\nnumpy.float64\n","label":[[1252,1259,"Mention"],[1332,1345,"API"]],"Comments":[]}
{"id":59762,"text":"ID:21632101\nPost:\nText: How do I get rows [1,2,3] and columns [0,2,3] with a single step of indexing? \nText: You could use np.ix_ instead but this is neither less typing nor is it faster. In fact its slower: \nCode: %timeit M[np.ix_([1,2,3],[0,2,3])]\n100000 loops, best of 3: 17.8 s per loop\n\n%timeit M[[1,2,3],:][:, [0,2,3]]\n100000 loops, best of 3: 10.9 s per loop\n\nText: How to force a view (if possible)? \nText: You can use stride.as_strided to ask for a tailored view of an array. Here is an example of its use from scipy-lectures. \nText: This would allow you to get a view instead of a copy in your very first example: \nCode: from numpy.lib.stride_tricks import as_strided\n\nM = np.array(range(9)).reshape((3,3))\nsub_1 = M[:,[0,2]][[0,2],:]\nsub_2 = as_strided(M, shape=(2, 2), strides=(48,16))\n\nprint sub_1\nprint ''\nprint sub_2\n\n[[0 2]\n [6 8]]\n\n[[0 2]\n [6 8]]\n\n# change the initial array\nM[0,0] = -1\n\nprint sub_1\nprint ''\nprint sub_2\n\n[[0 2]\n [6 8]]\n\n[[-1  2]\n [ 6  8]]\n\nText: As you can see sub_2 is indeed a view since it reflects changes made to the initial array M. \nText: The strides argument passed to as_strided specifies the byte-sizes to \"walk\" in each dimension: \nText: The datatype of the initial array M is int64 (on my machine) so an int is 8 bytes in memory. Since Numpy arranges arrays by default in C-style (row-major order), one row of M is consecutive in memory and takes 24 bytes. Since you want every other row you specify 48 bytes as stride in the first dimension. For the second dimension you want also every other element -- which now sit next to each other in memory -- so you specify 16 bytes as stride. \nText: For your latter example Numpy is not able to return a view because the requested indices are to irregular to be described through shape and strides. \nAPI:\nnumpy.lib.stride_tricks.as_strided\nnumpy.int64\n","label":[[429,446,"Mention"],[1225,1230,"Mention"],[1797,1831,"API"],[1832,1843,"API"]],"Comments":[]}
{"id":59763,"text":"ID:21638984\nPost:\nText: The I attribute only exists on matrix objects, not ndarrays. You can use LA.inv to invert arrays: \nCode: inverse = numpy.linalg.inv(x)\n\nText: Note that the way you're generating matrices, not all of them will be invertible. You will either need to change the way you're generating matrices, or skip the ones that aren't invertible. \nCode: try:\n    inverse = numpy.linalg.inv(x)\nexcept numpy.linalg.LinAlgError:\n    # Not invertible. Skip this one.\n    pass\nelse:\n    # continue with what you were doing\n\nText: Also, if you want to go through all 3x3 matrices with elements drawn from [0, 10), you want the following: \nCode: for comb in itertools.product(range(10), repeat=9):\n\nText: rather than combinations_with_replacement, or you'll skip matrices like \nCode: numpy.array([[0, 1, 0],\n             [0, 0, 0],\n             [0, 0, 0]])\n\nAPI:\nnumpy.linalg.inv\n","label":[[97,103,"Mention"],[865,881,"API"]],"Comments":[]}
{"id":59764,"text":"ID:21685920\nPost:\nText: Have you tried memory-map yet? It can be called via memmap . That is designed for files that are too large to load into ram. \nText: Here is the description I copied from the docstring: \nText: Create a memory-map to an array stored in a binary file on disk. Memory-mapped files are used for accessing small segments of large files on disk, without reading the entire file into memory. Numpys memmaps are array-like objects. This differs from Pythons mmap module, which uses file-like objects. This subclass of ndarray has some unpleasant interactions with some operations, because it doesnt quite fit properly as a subclass. An alternative to using this subclass is to create the mmap object yourself, then create an ndarray with ndarray.new directly, passing the object created in its buffer= parameter. This class may at some point be turned into a factory function which returns a view into an mmap buffer. \nText: And it is fairly simple to use. You can refer to the doc for more examples. \nAPI:\nnumpy.memmap\n","label":[[76,82,"Mention"],[1028,1040,"API"]],"Comments":[]}
{"id":59765,"text":"ID:21685983\nPost:\nText: I'd really use np.genfromtxt to read the csv files. Here is an example: \nCode: import numpy as np\nX, Y, Z = np.genfromtxt('targets.csv', delimiter=',', unpack=True)\n\nText: This is much easier than csv, and will return a nda immediately. \nAPI:\nnumpy.ndarray\n","label":[[244,247,"Mention"],[267,280,"API"]],"Comments":[]}
{"id":59766,"text":"ID:21765862\nPost:\nText: You can use np.memmap and let the operating system decide which parts of the image file to page in or out of RAM. If you use 64-bit Python the virtual memory space is astronomic compared to the available RAM. \nAPI:\nnumpy.memmap\n","label":[[36,45,"Mention"],[239,251,"API"]],"Comments":[]}
{"id":59767,"text":"ID:21766579\nPost:\nCode: def _get_payoff(self, actual, predicted):\n    pred_factor = numpy.abs(0.5 - predicted)\n    payoff_selector = 2*numpy.isclose(actual, 1) + (predicted < 0.5)\n    payoff = numpy.choose(payoff_selector,\n                          [\n                              self.fp_payoff,\n                              self.tn_payoff,\n                              self.tp_payoff,\n                              self.fn_payoff,\n                          ])\n    return numpy.sum(payoff * pred_factor)\n\ndef get_total_payoff(self):\n    return self._get_payoff(self.target, predictions)\n\nText: We use choose to generate an array of payoff selections and multiply that with an array of absolute differences between 0.5 and the prediction values, then sum. np.isclose is used to test whether the actual values are close to 1. We can ignore the predicted == 0.5 case, since multiplying by numpy.abs(0.5 - predicted) gives the correct result of 0 anyway. If self.target and predictions are guaranteed to be 1D, dot is likely to perform better than separately multiplying and summing. \nAPI:\nnumpy.choose\nnumpy.isclose\nnumpy.dot\n","label":[[604,610,"Mention"],[758,768,"Mention"],[1010,1013,"Mention"],[1089,1101,"API"],[1102,1115,"API"],[1116,1125,"API"]],"Comments":[]}
{"id":59768,"text":"ID:21832479\nPost:\nText: All non-empty process address space is mmaped. Either using mmap syscall or indirectly via brk\/sbrk syscalls . \nText: You probably need to find another way to distinguish numpy arrays. \nText: np.memmap documentation says these arrays have additional attributes filename, offset and mode, may be you can use these to detect whether an array is backed by a file. \nAPI:\nnumpy.memmap\n","label":[[216,225,"Mention"],[391,403,"API"]],"Comments":[]}
{"id":59769,"text":"ID:21853034\nPost:\nText: You can use numpy.loadtxt, np.column_stack and sdvetxt for this: \nCode: >>> import numpy as np\n>>> a1 = np.loadtxt('ra.txt')\n>>> a2 = np.loadtxt('dec.txt')\n>>> np.savetxt('out.txt', np.column_stack((a1, a2)), delimiter=' ', fmt='%.5f')\n>>> !cat out.txt\n115.07433 -88.84627\n114.75551 -88.68575\n114.85937 -88.68499\n111.18574 -88.63843\n192.78617 -88.70691\n189.71056 -88.69978\n0.47592 -87.12709\n2.22369 -87.12417\n2.97497 -87.11521\n5.44140 -86.95877\n0.29367 -86.94902\n308.83178 -87.04636\n\nAPI:\nnumpy.column_stack\nnumpy.savetxt\n","label":[[51,66,"Mention"],[71,78,"Mention"],[513,531,"API"],[532,545,"API"]],"Comments":[]}
{"id":59770,"text":"ID:21894930\nPost:\nText: You want to use either np.where or numpy.argwhere: \nCode: import numpy as np\nA = np.array([[99, 2, 3],\n              [0, 59, 2],\n              [54, 4, 2]])\nnp.where(A > 50)\n# (array([0, 1, 2]), array([0, 1, 0]))\nnp.argwhere(A > 50)\n# array([[0, 0],\n#        [1, 1],\n#        [2, 0]])\n\nAPI:\nnumpy.where\n","label":[[47,55,"Mention"],[314,325,"API"]],"Comments":[]}
{"id":59771,"text":"ID:21931086\nPost:\nText: You should use array to create an array not numpy.ndarray. ndarray is a low-level interface, and in most cases array should be used to create an array. \nCode: In [5]: arr1 = numpy.array([1.0, 2.0])                                                            \n\nIn [6]: arr1\nOut[6]: array([ 1.,  2.])\n\nText: Signature of numpy.ndarray: \nCode: ndarray(shape, dtype=float, buffer=None, offset=0, strides=None, order=None)         \n\nText: So, the first argument is shape not the array. So, numpy filled your array with some random data. \nText: From the docstring of numpy.ndarray: \nText: Arrays should be constructed using array, zeros or empty. \nAPI:\nnumpy.array\nnumpy.ndarray\nnumpy.array\n","label":[[39,44,"Mention"],[83,90,"Mention"],[135,140,"Mention"],[670,681,"API"],[682,695,"API"],[696,707,"API"]],"Comments":[]}
{"id":59772,"text":"ID:21931851\nPost:\nText: You can use np.column_stack and numpy.ndarray.flatten: \nCode: In [12]: numpy.column_stack((A, A)).flatten()                                                    \nOut[12]: array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5])\n\nText: Timing comparison: \nCode: In [27]: A = numpy.array([1, 2, 3, 4, 5]*1000)                                                   \n\nIn [28]: %timeit numpy.column_stack((A, A)).flatten()                                            \n10000 loops, best of 3: 44.7 s per loop                                                         \n\nIn [29]: %timeit numpy.repeat(A, 2)                                                              \n10000 loops, best of 3: 104 s per loop                                                          \n\nIn [30]: %timeit numpy.tile(A,2).reshape(2,-1).flatten('F')                                      \n10000 loops, best of 3: 129 s per loop     \n\nAPI:\nnumpy.column_stack\n","label":[[36,51,"Mention"],[906,924,"API"]],"Comments":[]}
{"id":59773,"text":"ID:21961923\nPost:\nText: You can use hsitack to append columns, like this: \nCode: import numpy as np\n\ndef some_function(x):\n    return 3*x\n\ninput = np.ones([10,26])\ninput = np.hstack([input,np.empty([input.shape[0],1])])\nfor row in input:\n    row[-1] = some_function(row[0])\n\noutput = input\n\nAPI:\nnumpy.hstack\n","label":[[36,43,"Mention"],[296,308,"API"]],"Comments":[]}
{"id":59774,"text":"ID:21975533\nPost:\nText: Try using np.bincount \nCode: In [19]: Counter(c2)\nOut[19]: Counter({1: 100226, 0: 99774})\n\nIn [20]: uniqueCounter(c2)\nOut[20]: defaultdict(<type 'int'>, {0: 99774, 1: 100226})\n\nIn [21]: np.bincount(c2)\nOut[21]: array([ 99774, 100226])\n\nText: Some timings: \nCode: In [16]: %timeit np.bincount(c2)\n1000 loops, best of 3: 2 ms per loop\n\nIn [17]: %timeit uniqueCounter(c2)\n1 loops, best of 3: 161 ms per loop\n\nIn [18]: %timeit Counter(c2)\n1 loops, best of 3: 362 ms per loop\n\nAPI:\nnumpy.bincount\n","label":[[34,45,"Mention"],[501,515,"API"]],"Comments":[]}
{"id":59775,"text":"ID:22012510\nPost:\nText: As @JChoi points out, the standard_normal method of the Generator class in random accepts a dtype argument that allows the direct generation of np.float32 samples. It only accepts float32 and float64 for dtype, so it won't help with numpy.float16. \nText: I don't know of a random number generator in numpy or scipy that generates 16 bit floats natively. \nText: To avoid the large temporary, you could generate the values in batches. For example, the following creates an array of 10000000 samples of float16 values. \nCode: In [125]: n = 10000000  # Number of samples to generate\n\nIn [126]: k = 10000     # Batch size\n\nIn [127]: a = np.empty(n, dtype=np.float16)\n\nIn [128]: for i in range(0, n, k):\n   .....:     a[i:i+k] = np.random.normal(loc=0, scale=1, size=k)\n   .....:\n\nAPI:\nnumpy.random\nnumpy.float32\nnumpy.float32\nnumpy.float64\n","label":[[99,105,"Mention"],[168,178,"Mention"],[204,211,"Mention"],[216,223,"Mention"],[804,816,"API"],[817,830,"API"],[831,844,"API"],[845,858,"API"]],"Comments":[]}
{"id":59776,"text":"ID:22029876\nPost:\nText: As you have specified python2.7, you can use the import_module function from the backported importlib module. \nCode: >>> np = importlib.import_module('numpy')\n>>> cos = getattr(np, 'cos')\n>>> cos\n<ufunc 'cos'>\n>>> cos(3.14159)\n-0.99999999999647926\n\nText: You would need to parse the command line options (using argparse as @HenryKeiter suggests in the comments), then split the function from the module string, import the module and use getattr to access the function. \nCode: >>> def loadfunc(dotted_string):\n...     params = dotted_string.split('.')\n...     func = params[-1]\n...     module = '.'.join(params[:-1])\n...     mod =  importlib.import_module(module)\n...     return getattr(mod, func)\n...\n>>> loadfunc('os.getcwd')\n<built-in function getcwd>\n>>> loadfunc('numpy.sin')\n<ufunc 'sin'>\n>>> f = loadfunc('numpy.cos')\n>>> f(3.14159)\n-0.99999999999647926\n\nText: Note this doesn't load functions from the global namespace, and you might want to add some error checking. \nText: EDIT \nText: There is also no need for the lambda function in this line \nCode: foo(lambda t: numpy.cos(t), x)\n\nText: You can just pass cos directly without evaluating it \nCode: foo(numpy.cos, x)\n\nAPI:\nnumpy.cos\n","label":[[1139,1142,"Mention"],[1205,1214,"API"]],"Comments":[]}
{"id":59777,"text":"ID:22074424\nPost:\nText: 1. The meaning of shapes in NumPy \nText: You write, \"I know literally it's list of numbers and list of lists where all list contains only a number\" but that's a bit of an unhelpful way to think about it. \nText: The best way to think about NumPy arrays is that they consist of two parts, a data buffer which is just a block of raw elements, and a view which describes how to interpret the data buffer. \nText: For example, if we create an array of 12 integers: \nCode: >>> a = numpy.arange(12)\n>>> a\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\nText: Then a consists of a data buffer, arranged something like this: \nCode: \n  0   1   2   3   4   5   6   7   8   9  10  11 \n\n\nText: and a view which describes how to interpret the data: \nCode: >>> a.flags\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : True\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  UPDATEIFCOPY : False\n>>> a.dtype\ndtype('int64')\n>>> a.itemsize\n8\n>>> a.strides\n(8,)\n>>> a.shape\n(12,)\n\nText: Here the shape (12,) means the array is indexed by a single index which runs from 0 to 11. Conceptually, if we label this single index i, the array a looks like this: \nCode: i= 0    1    2    3    4    5    6    7    8    9   10   11\n\n  0   1   2   3   4   5   6   7   8   9  10  11 \n\n\nText: If we reshape an array, this doesn't change the data buffer. Instead, it creates a new view that describes a different way to interpret the data. So after: \nCode: >>> b = a.reshape((3, 4))\n\nText: the array b has the same data buffer as a, but now it is indexed by two indices which run from 0 to 2 and 0 to 3 respectively. If we label the two indices i and j, the array b looks like this: \nCode: i= 0    0    0    0    1    1    1    1    2    2    2    2\nj= 0    1    2    3    0    1    2    3    0    1    2    3\n\n  0   1   2   3   4   5   6   7   8   9  10  11 \n\n\nText: which means that: \nCode: >>> b[2,1]\n9\n\nText: You can see that the second index changes quickly and the first index changes slowly. If you prefer this to be the other way round, you can specify the order parameter: \nCode: >>> c = a.reshape((3, 4), order='F')\n\nText: which results in an array indexed like this: \nCode: i= 0    1    2    0    1    2    0    1    2    0    1    2\nj= 0    0    0    1    1    1    2    2    2    3    3    3\n\n  0   1   2   3   4   5   6   7   8   9  10  11 \n\n\nText: which means that: \nCode: >>> c[2,1]\n5\n\nText: It should now be clear what it means for an array to have a shape with one or more dimensions of size 1. After: \nCode: >>> d = a.reshape((12, 1))\n\nText: the array d is indexed by two indices, the first of which runs from 0 to 11, and the second index is always 0: \nCode: i= 0    1    2    3    4    5    6    7    8    9   10   11\nj= 0    0    0    0    0    0    0    0    0    0    0    0\n\n  0   1   2   3   4   5   6   7   8   9  10  11 \n\n\nText: and so: \nCode: >>> d[10,0]\n10\n\nText: A dimension of length 1 is \"free\" (in some sense), so there's nothing stopping you from going to town: \nCode: >>> e = a.reshape((1, 2, 1, 6, 1))\n\nText: giving an array indexed like this: \nCode: i= 0    0    0    0    0    0    0    0    0    0    0    0\nj= 0    0    0    0    0    0    1    1    1    1    1    1\nk= 0    0    0    0    0    0    0    0    0    0    0    0\nl= 0    1    2    3    4    5    0    1    2    3    4    5\nm= 0    0    0    0    0    0    0    0    0    0    0    0\n\n  0   1   2   3   4   5   6   7   8   9  10  11 \n\n\nText: and so: \nCode: >>> e[0,1,0,0,0]\n6\n\nText: See the NumPy internals documentation for more details about how arrays are implemented. \nText: 2. What to do? \nText: Since np.reshape just creates a new view, you shouldn't be scared about using it whenever necessary. It's the right tool to use when you want to index an array in a different way. \nText: However, in a long computation it's usually possible to arrange to construct arrays with the \"right\" shape in the first place, and so minimize the number of reshapes and transposes. But without seeing the actual context that led to the need for a reshape, it's hard to say what should be changed. \nText: The example in your question is: \nCode: numpy.dot(M[:,0], numpy.ones((1, R)))\n\nText: but this is not realistic. First, this expression: \nCode: M[:,0].sum()\n\nText: computes the result more simply. Second, is there really something special about column 0? Perhaps what you actually need is: \nCode: M.sum(axis=0)\n\nAPI:\nnumpy.reshape\n","label":[[6044,6054,"Mention"],[6845,6858,"API"]],"Comments":[]}
{"id":59778,"text":"ID:22085909\nPost:\nText: Perhaps linspace \nCode: import numpy as np\n\ndef split_into_parts(number, n_parts):\n    return np.linspace(0, number, n_parts+1)[1:]\n\nText: Result: \nCode: >>> split_into_parts(1, 3)\narray([ 0.33333333,  0.66666667,  1.        ])\n\nAPI:\nnumpy.linspace\n","label":[[32,40,"Mention"],[258,272,"API"]],"Comments":[]}
{"id":59779,"text":"ID:22101517\nPost:\nText: You are not creating an instance of numpy.float64: \nCode: >>> import numpy  \n>>> import numbers\n>>> numpy.float64\n<class 'numpy.float64'>\n>>> numpy.float64()\n0.0\n>>> isinstance(numpy.float64, numbers.Real)\nFalse\n>>> isinstance(numpy.float64(), numbers.Real) # Notice the () after numpy.float64\nTrue\n>>>\n\nText: Instead, your current code is simply checking if the np.float64 class itself is an instance of numbers.Real. This will never be true. \nText: Note that you get the same behavior for any number type: \nCode: >>> import numbers\n>>> isinstance(int, numbers.Real)\nFalse\n>>> isinstance(int(), numbers.Real)\nTrue\n>>> isinstance(float, numbers.Real)\nFalse\n>>> isinstance(float(), numbers.Real)\nTrue\n>>>\n\nAPI:\nnumpy.float64\n","label":[[387,397,"Mention"],[734,747,"API"]],"Comments":[]}
{"id":59780,"text":"ID:22109379\nPost:\nText: I suggest using mtx instead of ndarray, it keeps a dimension of 2 regardless of how many rows you have: \nCode: In [17]: x\nOut[17]: \narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\nIn [18]: m=np.asmatrix(x)\n\nIn [19]: m[1]\nOut[19]: matrix([[3, 4, 5]])\n\nIn [20]: m[1][0, 1]\nOut[20]: 4\n\nIn [21]: x[1][0, 1]\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-21-bef99eb03402> in <module>()\n----> 1 x[1][0, 1]\n\nIndexError: too many indices\n\nText: Thx for @askewchan mentioning, if you want to use the numpy array arithmetic, use np.atleast_2d: \nCode: In [85]: np.atleast_2d(x[1])[0, 1]\nOut[85]: 4\n\nAPI:\nnumpy.matrix\n","label":[[40,43,"Mention"],[745,757,"API"]],"Comments":[]}
{"id":59781,"text":"ID:22149930\nPost:\nText: It specifies the axis along which the means are computed. By default axis=0. This is consistent with the np.mean usage when axis is specified explicitly (in numpy.mean, axis==None by default, which computes the mean value over the flattened array) , in which axis=0 along the rows (namely, index in pandas), and axis=1 along the columns. For added clarity, one may choose to specify axis='index' (instead of axis=0) or axis='columns' (instead of axis=1). \nCode: +------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|----axis=1----->\n+------------+---------+--------+\n             |         |\n             | axis=0  |\n                      \n\nAPI:\nnumpy.mean\n","label":[[129,136,"Mention"],[753,763,"API"]],"Comments":[]}
{"id":59782,"text":"ID:22159018\nPost:\nText: Use sqrt rather than math.sqrt. nsqrt expects a scalar or array as input, on the other hand math.sqrt can only handle scalars. \nCode: >>> import numpy as np\n>>> import math\n>>> a = np.arange(5)\n>>> np.sqrt(a)\narray([ 0.        ,  1.        ,  1.41421356,  1.73205081,  2.        ])\n#error\n>>> math.sqrt(a)\nTraceback (most recent call last):\n  File \"<ipython-input-78-c7d50051514f>\", line 1, in <module>\n    math.sqrt(a)\nTypeError: only length-1 arrays can be converted to Python scalars\n\n>>> \n\nAPI:\nnumpy.sqrt\nnumpy.sqrt\n","label":[[28,32,"Mention"],[56,61,"Mention"],[523,533,"API"],[534,544,"API"]],"Comments":[]}
{"id":59783,"text":"ID:22231215\nPost:\nText: This might be how you arrived in this situation: \nCode: import numpy as np\narr = np.array([(1490775.0, 12037425.0)], dtype=[('foo','<f8'),('bar','<f8')])\narr.flags.writeable = False\n\nG = dict()\nG[arr[0]] = 0\n\nprint(type(G.keys()[0]))\n# <type 'numpy.void'>\n\nprint(type(G.keys()[0][0]))\n# <type 'numpy.float64'>\n\nprint(type(G.keys()[0][1]))\n# <type 'numpy.float64'>\n\nprint(type(G))\n# <type 'dict'>\n\nText: A tuple of floats is not a key in G: \nCode: print((1490775.0, 12037425.0) in G)\n# False\n\nText: But the np.void instance is a key in G: \nCode: print(arr[0] in G)\n# True\n\nText: You will probaby be better off not using numpy.voids as keys. Instead, if you really need a dict, then perhaps convert the array to a list first: \nCode: In [173]: arr.tolist()\nOut[173]: [(1490775.0, 12037425.0)]\nIn [174]: G = {item:0 for item in arr.tolist()}\n\nIn [175]: G\nOut[175]: {(1490775.0, 12037425.0): 0}\n\nIn [176]: (1490775.0, 12037425.0) in G\nOut[176]: True\n\nAPI:\nnumpy.void\n","label":[[530,537,"Mention"],[975,985,"API"]],"Comments":[]}
{"id":59784,"text":"ID:22314373\nPost:\nText: savetxt saves an array to a file, and it looks like maxi is a python Float. It's sort of unusual to use savetxt to save a scalar (why not just use Python's built-in file i\/o functions?), but if you must, this might work: \nCode: savetxt('myfilelocation', [maxi], fmt='%1.4f')\n\nText: Notice that I've put maxi in a list. Also, make sure that myfilelocation doesn't end with a .gz. If it does, numpy will compress it. When it doubt, just use .txt, or no extension at all. \nAPI:\nnumpy.savetxt\n","label":[[24,31,"Mention"],[499,512,"API"]],"Comments":[]}
{"id":59785,"text":"ID:22402180\nPost:\nText: It's a NaN. It's just that is doesn't work the way you think it does with NumPy arrays. When you assign \nCode: a[:] = numpy.NAN\n\nText: NumPy doesn't actually fill a with references to the np.NAN object. Instead, it fills the array with doubles with NaN values at C level. \nText: When you then access an array element with a[0], NumPy has no record of the original object used to initialize that cell. It just has the numeric value. It has to construct a new Python object to wrap that value, and the new wrapper is not the same object as numpy.NAN. Thus, the is check returns False. \nText: Note that generally, comparing numbers with is is a bad idea anyway. Usually, what you want is ==, to compare their numeric values. However, == also returns False for NaNs, so what you need is numpy.isnan: \nCode: >>> numpy.isnan(a[0])\nTrue\n>>> numpy.isnan(a)\narray([ True,  True], dtype=bool)\n\nAPI:\nnumpy.NAN\n","label":[[212,218,"Mention"],[913,922,"API"]],"Comments":[]}
{"id":59786,"text":"ID:22424132\nPost:\nText: sorry for the previous answer, \nText: use np.apply_along_axis \nCode: a = np.arange(12).reshape((4,3))\ndef sum(array):\n    return np.sum(array)\n\nnp.apply_along_axis(sum, 1, a)\n>>> array([ 3, 12, 21, 30])\n\nAPI:\nnumpy.apply_along_axis\n","label":[[66,85,"Mention"],[233,255,"API"]],"Comments":[]}
{"id":59787,"text":"ID:22444297\nPost:\nText: The tofile method writes the data \"flattened\". It writes the elements sequentially to the file, losing any shape information about the array. \nText: Your use of the function savetxt needs a couple tweaks. To separate the fields with spaces, use delimiter=' ' (that's a space character, not an empty string). Don't include '\\n' in the fmt argument. The following works for me: \nCode: numpy.savetxt('newdata.txt', massout, delimiter=' ', fmt='%s')\n\nText: P.S. With a 2-d numpy array, it is more efficient and stylistically preferable to index the array as massout[i, j] instead of massout[i][j]. That is, you should write \nCode:     massout[i, 1] = '27'\n\nText: instead of \nCode:     massout[i][1] = '27'\n\nAPI:\nnumpy.savetxt\n","label":[[198,205,"Mention"],[732,745,"API"]],"Comments":[]}
{"id":59788,"text":"ID:22529187\nPost:\nText: Best syntax I found so far: \nCode: import numpy\ncimport numpy\ncimport cython\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef primes(int kmax):\n    cdef int n, k, i\n    cdef numpy.ndarray[int] p = numpy.empty(kmax, dtype=numpy.int32)\n    k = 0\n    n = 2\n    while k < kmax:\n        i = 0\n        while i < k and n % p[i] != 0:\n            i = i + 1\n        if i == k:\n            p[k] = n\n            k = k + 1\n        n = n + 1\n    return p\n\nText: Note where I used int32 instead of int. Anything on the left side of a cdef is a C type (thus int = int32 and float = float32), while anything on the RIGHT side of it (or outside of a cdef) is a python type (int = int64 and float = float64) \nAPI:\nnumpy.int32\n","label":[[500,505,"Mention"],[729,740,"API"]],"Comments":[]}
{"id":59789,"text":"ID:22620104\nPost:\nText: Monkeypatching \nText: If I understand your situation correctly, your use case is not really about decoration (modifying a function you write, in a standard manner) but rather about monkey patching: Modifying a function somebody else wrote without actually changing that function's definition's source code. \nText: The idiom for what you then need is something like \nCode: import numpy as np  # provide local access to the numpy module object\n\noriginal_np_sqrt = np.sqrt\n\ndef my_improved_np_sqrt(x):\n  # do whatever you please, including:\n  # - contemplating the UncertainQuantity-ness of x and\n  # - calling original_np_sqrt as needed\n\nnp.sqrt = my_improved_np_sqrt\n\nText: Of course, this can change only the future meaning of numpy.sqrt, not the past one. So if anybody has imported numpy before the above and has already used nsqrt in a way you would have liked to influence, you lose. (And the name to which they map numpy does not matter.) \nText: But after the above code was executed, the meaning of sqrt in all modules (whether they imported numpy before it or after it) will be that of my_improved_np_sqrt, whether the creators of those modules like it or not (and of course unless some more monkeypatching of sqrt is going on elsewhere). \nText: Note that \nText: When you do weird things, Python can become a weird platform! When you do weird things, Python can become a weird platform! When you do weird things, Python can become a weird platform! \nText: This is why monkey patching is not normally considered good design style. So if you take that route, make sure you announce it very prominently in all relevant documentation. \nText: Oh, and if you do not want to modify other code than that which is directly or indirectly executed from your own methods, you could introduce a decorator that performs monkeypatching before the call and un-monkeypatching (reassigning original_np_sqrt) after the call and apply that decorator to all your functions in question. Make sure you handle exceptions in that decorator then, so that the un-monkeypatching is really executed in all cases. \nAPI:\nnumpy.sqrt\nnumpy.sqrt\nnumpy.sqrt\n","label":[[852,857,"Mention"],[1029,1033,"Mention"],[1241,1245,"Mention"],[2121,2131,"API"],[2132,2142,"API"],[2143,2153,"API"]],"Comments":[]}
{"id":59790,"text":"ID:22670647\nPost:\nText: Have a look at linspace if you don't mind the dependency. \nCode: linspace(min, max, n+1)\n\nAPI:\nnumpy.linspace\n","label":[[39,47,"Mention"],[119,133,"API"]],"Comments":[]}
{"id":59791,"text":"ID:22671394\nPost:\nText: using np.repeat : \nCode: np.repeat(np.arange(c.size), c)\n\nAPI:\nnumpy.repeat\n","label":[[30,39,"Mention"],[87,99,"API"]],"Comments":[]}
{"id":59792,"text":"ID:22673349\nPost:\nText: This isn't about Nan vs. float(\"nan\"), it's that you've got two separate float nans. \nCode: >>> np.nan is np.nan\nTrue\n>>> float(\"nan\") is float(\"nan\")\nFalse\n\nText: and so \nCode: >>> Counter([1,2,2, np.nan, np.nan])\nCounter({nan: 2, 2: 2, 1: 1})\n>>> Counter([1,2,2, float(\"nan\"), float(\"nan\")])\nCounter({2: 2, nan: 1, 1: 1, nan: 1})\n\nText: but \nCode: >>> f = float(\"nan\")\n>>> Counter([1,2,2, f, f])\nCounter({nan: 2, 2: 2, 1: 1})\n\nAPI:\nnumpy.nan\n","label":[[41,44,"Mention"],[458,467,"API"]],"Comments":[]}
{"id":59793,"text":"ID:22686289\nPost:\nText: There is another version of repeat in numpy: np.ndarray.repeat \nText: Please see the documentation here \nText: Hope this helps \nAPI:\nnumpy.ndarray.repeat\n","label":[[69,86,"Mention"],[157,177,"API"]],"Comments":[]}
{"id":59794,"text":"ID:22737457\nPost:\nText: ladtxt takes a filename(string) and returns a numpy array. So you don't need the with clause: \nCode: filenames2 = [\"AAA\", \"BBB\", \"CCC\", \"DDD\", \"EEE\"]\nfor filename2 in filenames2:\n    data = np.loadtxt(filename2)\n    a1 = data[:,0]\n    # ...\n    np.savetxt('output_filename.txt', z, ...)\n\nText: So far I don't see any need to make zero matrices in your code. data will get filed with the contents of the file and a1, b1, ..., will get their contents from data. \nAPI:\nnumpy.loadtxt\n","label":[[24,30,"Mention"],[490,503,"API"]],"Comments":[]}
{"id":59795,"text":"ID:22778484\nPost:\nText: shorter, faster and clearer answer, avoiding meshgrid: \nCode: import numpy as np\n\ndef func(x, y):\n    return np.sin(y * x)\n\nxaxis = np.linspace(0, 4, 10)\nyaxis = np.linspace(-1, 1, 20)\nresult = func(xaxis[:,None], yaxis[None,:])\n\nText: This will be faster in memory if you get something like x^2+y as function, since than x^2 is done on a 1D array (instead of a 2D one), and the increase in dimension only happens when you do the \"+\". For meshgrid, x^2 will be done on a 2D array, in which essentially every row is the same, causing massive time increases. \nText: Edit: the \"x[:,None]\", makes x to a 2D array, but with an empty second dimension. This \"None\" is the same as using \"x[:,numpy.newaxis]\". The same thing is done with Y, but with making an empty first dimension. \nText: Edit: in 3 dimensions: \nCode: def func2(x, y, z):\n    return np.sin(y * x)+z\n\nxaxis = np.linspace(0, 4, 10)\nyaxis = np.linspace(-1, 1, 20)\nzaxis = np.linspace(0, 1, 20)\nresult2 = func2(xaxis[:,None,None], yaxis[None,:,None],zaxis[None,None,:])\n\nText: This way you can easily extend to n dimensions if you wish, using as many None or : as you have dimensions. Each : makes a dimension, and each None makes an \"empty\" dimension. The next example shows a bit more how these empty dimensions work. As you can see, the shape changes if you use None, showing that it is a 3D object in the next example, but the empty dimensions only get filled up whenever you multiply with an object that actually has something in those dimensions (sounds complicated, but the next example shows what i mean) \nCode: In [1]: import numpy\n\nIn [2]: a = numpy.linspace(-1,1,20)\n\nIn [3]: a.shape\nOut[3]: (20,)\n\nIn [4]: a[None,:,None].shape \nOut[4]: (1, 20, 1)\n\nIn [5]: b = a[None,:,None] # this is a 3D array, but with the first and third dimension being \"empty\"\nIn [6]: c = a[:,None,None] # same, but last two dimensions are \"empty\" here\n\nIn [7]: d=b*c \n\nIn [8]: d.shape # only the last dimension is \"empty\" here\nOut[8]: (20, 20, 1)\n\nText: edit: without needing to type the None yourself \nCode: def ndm(*args):\n    return [x[(None,)*i+(slice(None),)+(None,)*(len(args)-i-1)] for i, x in enumerate(args)]\n\n\nx2,y2,z2  = ndm(xaxis,yaxis,zaxis)\nresult3 = func2(x2,y2,z2)\n\nText: This way, you make the None-slicing to create the extra empty dimensions, by making the first argument you give to ndm as the first full dimension, the second as second full dimension etc- it does the same as the 'hardcoded' None-typed syntax used before. \nText: Short explanation: doing x2, y2, z2 = ndm(xaxis, yaxis, zaxis) is the same as doing \nCode: x2 = xaxis[:,None,None]\ny2 = yaxis[None,:,None]\nz2 = zaxis[None,None,:]\n\nText: but the ndm method should also work for more dimensions, without needing to hardcode the None-slices in multiple lines like just shown. This will also work in numpy versions before 1.8, while np.meshgrid only works for higher than 2 dimensions if you have numpy 1.8 or higher. \nAPI:\nnumpy.meshgrid\n","label":[[2878,2889,"Mention"],[2969,2983,"API"]],"Comments":[]}
{"id":59796,"text":"ID:22830249\nPost:\nText: That line produces a truth matrix. \nText: The numpy.random.randint() function, with a size argument, produces a new ndarray object with size elements randomly picked between 0 and 2 (exclusive), so in this case 100 0 or 1 values: \nCode: >>> numpy.random.randint(2, size=100)\narray([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n       0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 0])\n\nText: The < 1 then produces an array of boolean values (True or False): \nCode: >>> numpy.random.randint(2, size=100) < 1\narray([False, False, False, False,  True, False,  True,  True,  True,\n       False, False,  True,  True,  True, False, False,  True,  True,\n        True, False, False,  True,  True,  True, False, False,  True,\n       False, False, False, False, False,  True,  True, False,  True,\n       False, False, False,  True, False,  True, False,  True, False,\n       False,  True,  True,  True, False,  True,  True, False, False,\n       False,  True,  True, False, False, False,  True, False,  True,\n        True,  True, False, False, False,  True, False, False, False,\n       False, False,  True,  True,  True,  True, False,  True, False,\n        True,  True, False,  True, False,  True, False,  True, False,\n       False,  True, False, False, False,  True,  True, False,  True, False], dtype=bool)\n\nText: This array is then converted to a Pandas Series object. \nAPI:\nnumpy.ndarray\n","label":[[140,147,"Mention"],[1616,1629,"API"]],"Comments":[]}
{"id":59797,"text":"ID:22895054\nPost:\nText: Using outer and ravel \nCode: >>> import numpy as np\n>>> a = np.array((2,3))\n>>> b = np.array((5,6,7))\n>>> np.outer(a,b).ravel()\narray([10, 12, 14, 15, 18, 21])\n\nText: Edit: \nText: Subraction: we can't use numpy.outer, but we can use numpy.newaxis: \nCode: >>> (a[:, np.newaxis] - b).ravel()\narray([-3, -4, -5, -2, -3, -4])\n\nAPI:\nnumpy.outer\nnumpy.ravel\n","label":[[30,35,"Mention"],[40,45,"Mention"],[352,363,"API"],[364,375,"API"]],"Comments":[]}
{"id":59798,"text":"ID:22913489\nPost:\nText: Slice with np.newaxis to put additional axes into the shape of an array: \nCode: >>> my_array = numpy.array([1, 2, 3, 4, 5])\n>>> my_array[:, numpy.newaxis]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5]])\n\nText: (numpy.newaxis is None, so you can also just use None directly. I find newaxis more readable, but it's a matter of personal preference.) \nAPI:\nnumpy.newaxis\n","label":[[35,45,"Mention"],[390,403,"API"]],"Comments":[]}
{"id":59799,"text":"ID:22956238\nPost:\nText: Oh, it's much \"worse\" than that: \nCode: In [2]: numpy.power(10,-1)   \nOut[2]: 0\n\nText: But this is a hint to what's going on: 10 is an integer, and np.power doesn't coerce the numbers to floats. But this works: \nCode: In [3]: numpy.power(10.,-1)\nOut[3]: 0.10000000000000001\n\nIn [4]: numpy.power(10.,-100)\nOut[4]: 1e-100\n\nText: Note, however, that the power operator, **, does convert to float: \nCode: In [5]: 10**-1\nOut[5]: 0.1\n\nAPI:\nnumpy.power\n","label":[[172,180,"Mention"],[458,469,"API"]],"Comments":[]}
{"id":59800,"text":"ID:22991529\nPost:\nText: Either bar.getClose() or self.__stoch[-1] is returning a ndarray while both should be returning floats. \nAPI:\nnumpy.ndarray\n","label":[[81,88,"Mention"],[134,147,"API"]],"Comments":[]}
{"id":59801,"text":"ID:23056488\nPost:\nText: Easiest is to use Python's builtins, i.e. string type: \nCode: A = \"123231213\"\nB = \"2312\"\nresult = A.replace(B, \"\")\n\nText: To efficiently convert ar to an from str, use these functions: \nCode: x = numpy.frombuffer(\"3452353\", dtype=\"|i1\")\nx\narray([51, 52, 53, 50, 51, 53, 51], dtype=int8)\nx.tostring()\n\"3452353\"\n\nText: (*) thus mixes up ascii codes (1 != \"1\"), but substring search will work just fine. Your data type should better fit in one char, or you may get a false match. \nText: To sum it up, a quick hack looks like this: \nCode: A = numpy.array([1, 2, 3, 2, 3, 1, 2, 1, 3])\nB = numpy.array([2, 3, 1, 2])\nnumpy.fromstring(A.tostring().replace(B.tostring(), \"\"), dtype=A.dtype)\narray([1, 2, 3, 1, 3])\n# note, here dtype is some int, I'm relying on the fact that:\n# \"1 matches 1\" is equivalent to \"0001 matches 00001\"\n# this holds as long as values of B are typically non-zero.\n#\n# this trick can conceptually be used with floating point too,\n# but beware of multiple floating point representations of same number\n\nText: In depth explanation: \nText: Assuming size of A and B is arbitrary, naive approach runs in quadratic time. However better, probabilistic algorithms exit, for example Rabin-Karp, which relies on sliding window hash. \nText: Which is the main reason text oriented functions, such as xxx in str or str.replace or re will be much faster than custom numpy code. \nText: If you truly need this function to be integrated with numpy, you can always write an extension, but it's not easy :) \nAPI:\nnumpy.array\n","label":[[169,171,"Mention"],[1534,1545,"API"]],"Comments":[]}
{"id":59802,"text":"ID:23081015\nPost:\nText: The final output only takes about 100 MB. However, the final output isn't the only array you allocate. \nCode: np.random.rand(rows,cols)\n\nText: This is an array of 100 million float64s. It takes about 800 MB. \nCode: np.random.rand(rows,cols)*100\n\nText: This is another array of 100 million float64s. It also takes about 800 MB. While it's being computed, both this array and the previous array must remain in memory, for a peak memory usage of about 1.6 GB, 16 times higher than you were expecting. \nText: NumPy doesn't seem to provide a way to directly generate random uint8s. However, you can cut this function's peak memory usage down to about 500 MB by using randint to generate int32s instead of float64s and skip a temporary allocation: \nCode: return np.random.randint(0, 100, (rows, cols)).astype('uint8')\n\nText: If that's still too high, you can generate random numbers in chunks and slice-assign them into a result array, lowering the number of temporary int32s you need to keep in memory at once: \nCode: data = np.zeros([rows, cols], dtype='uint8')\nfor chunk_start in xrange(0, rows, rows\/10):\n    data[chunk_start: chunk_start+rows\/10] = (\n            np.random.randint(0, 100, (rows\/10, cols)))\nreturn data\n\nText: This version should have a peak memory usage of about 140 MB. \nAPI:\nnumpy.random.randint\n","label":[[686,693,"Mention"],[1317,1337,"API"]],"Comments":[]}
{"id":59803,"text":"ID:23103848\nPost:\nText: This behavior is consistent with the rest of Python and Numpy. \nText: Whenever you take a slice of something, you should expect to get back that same type as the sliced object. This is true of regular python lists (and tuples, and strings), as well as all Numpy arrays. When you index into an object, you should expect to receive whatever type the element is. e.g.: \nCode: pylist = [0,1,2,3,4,5]\ntype(pylist[0:1]) # <type 'list'>\ntype(pylist[0]) # <type 'int'>\n\nnparr = array([0,1,2,3,4,5])\ntype(nparr[0:1]) # <type 'numpy.ndarray'>\ntype(nparr[0]) # <type 'numpy.int64'>\n\nText: Therefore, you can only perform those operations which are valid for the type which you end up with. In the case of eoid type, it does not support slicing, so that is why you receive an exception. Although, actually, what you are doing with RA[0][['f1','f2']] is advanced indexing, but that won't work with np.void either. \nText: If what you want to do is retrieve the 'f1' and 'f2' fields from the first element then you can do: \nCode: RA[['f1','f2']][0]\n\nText: You can also use slicing here: \nCode: RA[['f1','f2']][0:1]\n\nText: This works because you are operating on a Numpy array which supports advanced indexing, and then selecting the element from this returned view (which is of type numpy.ndarray). \nText: I should also note, that slicing behavior is not identical between Numpy and native python objects, but that the type of objects they return is consistent. \nAPI:\nnumpy.void\nnumpy.void\n","label":[[718,722,"Mention"],[909,916,"Mention"],[1477,1487,"API"],[1488,1498,"API"]],"Comments":[]}
{"id":59804,"text":"ID:23201848\nPost:\nText: The ndarray generated by np.arange does generally not include the end value: \nCode: In [15]: np.arange(99.0, 100., 0.1)\nOut[15]: array([ 99. ,  99.1,  99.2,  99.3,  99.4,  99.5,  99.6,  99.7,  99.8,  99.9])\n\nText: Note that there is a built-in method histogram to do this for you; \nCode: In [13]: np.histogram(lst, list(np.arange(95.0,100.2,0.1)))\nOut[13]: \n(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 1, 2, 0, 6, 0]), array([  95. ,   95.1,   95.2,   95.3,   95.4,   95.5,   95.6,   95.7,\n         95.8,   95.9,   96. ,   96.1,   96.2,   96.3,   96.4,   96.5,\n         96.6,   96.7,   96.8,   96.9,   97. ,   97.1,   97.2,   97.3,\n         97.4,   97.5,   97.6,   97.7,   97.8,   97.9,   98. ,   98.1,\n         98.2,   98.3,   98.4,   98.5,   98.6,   98.7,   98.8,   98.9,\n         99. ,   99.1,   99.2,   99.3,   99.4,   99.5,   99.6,   99.7,\n         99.8,   99.9,  100. ,  100.1,  100.2]))\n\nAPI:\nnumpy.arange\nnumpy.histogram\n","label":[[49,58,"Mention"],[275,284,"Mention"],[1043,1055,"API"],[1056,1071,"API"]],"Comments":[]}
{"id":59805,"text":"ID:23232143\nPost:\nText: Thanks Warren Weckesser! The link you suggested helped me a lot. I now have two working scripts: one for writing complex numbers using savetxt and one for reading\/loading the complex numbers from the file using numpy.loadtxt. \nText: For future references, the codes are listed below. \nText: Writing: \nCode: import numpy\n\nd1 = -0.240921619563-0.0303165074169j\nd2 = -0.340921619563-0.0403165074169j\nd3 = -0.440921619563-0.0503165074169j\nd4 = -0.540921619563-0.0603165074169j\n\narray = numpy.array([d1, d2, d3, d4])\n\nsave = open(\"test.dat\",\"w\")\nnumpy.savetxt(save, array.reshape(1, array.shape[0]), newline = \"\\r\\n\", fmt = '%.4f%+.4fj '*4)\n\nsave.close()\n\nText: Reading\/Loading: \nCode: import numpy\n\ncoeffs = numpy.loadtxt(\"test.dat\", dtype = numpy.complex128)\n\nAPI:\nnumpy.savetxt\n","label":[[159,166,"Mention"],[786,799,"API"]],"Comments":[]}
{"id":59806,"text":"ID:23269723\nPost:\nText: If you can convert your data to numpy.arrays then you can use np.amax and cfabs as below. \nCode: import numpy as np\n\na = np.array([(1, 2), (-3, 4), (-2, 2), (0, 1), (1, 3)])\n\na_abs = np.fabs(a)\nprint(a_abs)\n# [[ 1.  2.]\n#  [ 3.  4.]\n#  [ 2.  2.]\n#  [ 0.  1.]\n#  [ 1.  3.]]\n\na_max = np.amax(a_abs)\n\nprint(a_max)\n# 4.0\n\nText: np.fabs will simply return an array of the same shape but with the absolute value for each element in a multidimensional array. \nText: np.amax will return the maximum value of an array. If you select an axis using a keyword-argument (such as np.amax(a, axis=0)) then it will act along that axis. The default for the axis keyword however is None which will cause it to act along the flattened array. \nAPI:\nnumpy.amax\nnumpy.fabs\n","label":[[86,93,"Mention"],[98,103,"Mention"],[753,763,"API"],[764,774,"API"]],"Comments":[]}
{"id":59807,"text":"ID:23305298\nPost:\nText: This should give you the same result under the assumptions below. \nCode: A1 = (A1+offsets)*(priorita\/norms)\n\nText: Assumptions: \nText: A1: n*m matrix offsets: n*1 vector norms: n*1 vector priorita: n*1 vector These are all ar \nAPI:\nnumpy.array\n","label":[[247,249,"Mention"],[256,267,"API"]],"Comments":[]}
{"id":59808,"text":"ID:23319127\nPost:\nText: Answering the titled question and ignore the others (please read the FAQ!), a minimal working example for digitize is \nCode: import numpy as np\n\n# Five bins, spaced from 0, 10\nbins = np.linspace(0,10,5)\n\n# Some random test data, from 0,10\ndata = np.random.random(size=20)*10\nprint data\n\n# Binned data\nprint np.digitize(data,bins)\n\nText: Which returns \nCode: [ 9.29893458  0.88322852  4.9592157   7.33677397  0.20901007  5.77875637\n  2.49152666  3.55982666  5.33997896  3.76318862  0.35513614  7.12985682\n  2.57747437  4.62240375  8.02503782  5.43143368  6.29290487  2.79342587\n  3.11806151  5.79996645]\n[4 1 2 3 1 3 1 2 3 2 1 3 2 2 4 3 3 2 2 3]\n\nText: It is useful to read over the docs, they tell you the proper input and output! \nAPI:\nnumpy.digitize\n","label":[[130,138,"Mention"],[761,775,"API"]],"Comments":[]}
{"id":59809,"text":"ID:23319185\nPost:\nText: This is because you are calling the string method str on the numpy array. Use the numpy function np.savetxt instead. It would look something like \nCode: with open('testfile.csv', 'w') as FOUT:\n    np.savetxt(FOUT, a_test)\n\nText: Note that the format would not necessarily be readable by a CSV reader. If that is your intention, you can use https:\/\/docs.python.org\/2\/library\/csv.html. \nAPI:\nnumpy.savetxt\n","label":[[121,131,"Mention"],[414,427,"API"]],"Comments":[]}
{"id":59810,"text":"ID:23349458\nPost:\nText: First, data=numpy.array(a) is already enough, no need to use numpy.array([b for b in a]). \nText: data is now a 3D ndarray with the shape (2,2,3), and has 3 axes 0, 1, 2. The first axis has a length of 2, the second axis's length is also 2 and the third axis's length is 3. \nText: Therefore both numpy.apply_along_axis(my_func, 0, data) and numpy.apply_along_axis(my_func, 1, data) will result in a 2D array of shape (2,3). In both cases the shape is (2,3), those of the remaining axes, 2nd and 3rd or 1st and 3rd. \nText: numpy.apply_along_axis(my_func, 2, data) returns the (2,2) shape array you showed, where (2,2) is the shape of the first 2 axes, as you apply along the 3rd axis (by giving index 2). \nText: The way to understand it is whichever axis you apply along will be 'collapsed' into the shape of your my_func, which in this case returns a single value. The order and shape of the remaining axis will remain unchanged. \nText: The alternative way to think of it is: apply_along_axis means apply that function to the values on that axis, for each combination of the remaining axis\/axes. Fetch the result, and organize them back into the shape of the remaining axis\/axes. So, if my_func returns a tuple of 4 values: \nCode: def my_func(x):\n    return (x[0] + x[-1]) * 2,1,1,1\n\nText: we will expect numpy.apply_along_axis(my_func, 0, data).shape to be (4,2,3). \nText: See also appky_over_axes for applying a function repeatedly over multiple axes \nAPI:\nnumpy.apply_over_axes\n","label":[[1406,1421,"Mention"],[1482,1503,"API"]],"Comments":[]}
{"id":59811,"text":"ID:23444573\nPost:\nText: I think, as seberg suggested, this is an issue with the BLAS library used. If you look at how dot is implemented here and here you'll find a call to cblas_dgemm() for the double-precision matrix-times-matrix case. \nText: This C program, which reproduces some of your examples, gives the same output when using \"plain\" BLAS, and the right answer when using ATLAS. \nCode: #include <stdio.h>\n#include <math.h>\n\n#include \"cblas.h\"\n\nvoid onebyone(double a11, double b11, double expectc11)\n{\n  enum CBLAS_ORDER order=CblasRowMajor;\n  enum CBLAS_TRANSPOSE transA=CblasNoTrans;\n  enum CBLAS_TRANSPOSE transB=CblasNoTrans;\n  int M=1;\n  int N=1;\n  int K=1;\n  double alpha=1.0;\n  double A[1]={a11};\n  int lda=1;\n  double B[1]={b11};\n  int ldb=1;\n  double beta=0.0;\n  double C[1];\n  int ldc=1;\n\n  cblas_dgemm(order, transA, transB,\n              M, N, K,\n              alpha,A,lda,\n              B, ldb,\n              beta, C, ldc);\n\n  printf(\"dot([ %.18g],[%.18g]) -> [%.18g]; expected [%.18g]\\n\",a11,b11,C[0],expectc11);\n}\n\nvoid onebytwo(double a11, double b11, double b12,\n              double expectc11, double expectc12)\n{\n  enum CBLAS_ORDER order=CblasRowMajor;\n  enum CBLAS_TRANSPOSE transA=CblasNoTrans;\n  enum CBLAS_TRANSPOSE transB=CblasNoTrans;\n  int M=1;\n  int N=2;\n  int K=1;\n  double alpha=1.0;\n  double A[]={a11};\n  int lda=1;\n  double B[2]={b11,b12};\n  int ldb=2;\n  double beta=0.0;\n  double C[2];\n  int ldc=2;\n\n  cblas_dgemm(order, transA, transB,\n              M, N, K,\n              alpha,A,lda,\n              B, ldb,\n              beta, C, ldc);\n\n  printf(\"dot([ %.18g],[%.18g, %.18g]) -> [%.18g, %.18g]; expected [%.18g, %.18g]\\n\",\n         a11,b11,b12,C[0],C[1],expectc11,expectc12);\n}\n\nint\nmain()\n{\n  onebyone(0, 0, 0);\n  onebyone(2, 3, 6);\n  onebyone(NAN, 0, NAN);\n  onebyone(0, NAN, NAN);\n  onebytwo(0, 0,0, 0,0);\n  onebytwo(2, 3,5, 6,10);\n  onebytwo(0, NAN,0, NAN,0);\n  onebytwo(NAN, 0,0, NAN,NAN);\n  return 0;\n}\n\nText: Output with BLAS: \nCode: dot([ 0],[0]) -> [0]; expected [0]\ndot([ 2],[3]) -> [6]; expected [6]\ndot([ nan],[0]) -> [nan]; expected [nan]\ndot([ 0],[nan]) -> [0]; expected [nan]\ndot([ 0],[0, 0]) -> [0, 0]; expected [0, 0]\ndot([ 2],[3, 5]) -> [6, 10]; expected [6, 10]\ndot([ 0],[nan, 0]) -> [0, 0]; expected [nan, 0]\ndot([ nan],[0, 0]) -> [nan, nan]; expected [nan, nan]\n\nText: Output with ATLAS: \nCode: dot([ 0],[0]) -> [0]; expected [0]\ndot([ 2],[3]) -> [6]; expected [6]\ndot([ nan],[0]) -> [nan]; expected [nan]\ndot([ 0],[nan]) -> [nan]; expected [nan]\ndot([ 0],[0, 0]) -> [0, 0]; expected [0, 0]\ndot([ 2],[3, 5]) -> [6, 10]; expected [6, 10]\ndot([ 0],[nan, 0]) -> [nan, 0]; expected [nan, 0]\ndot([ nan],[0, 0]) -> [nan, nan]; expected [nan, nan]\n\nText: BLAS seems to have expected behaviour when the first operand has a NaN, and the wrong when the first operand is zero and the second has a NaN. \nText: Anyway, I don't think this bug is in the Numpy layer; it's in BLAS. It appears to be possible to workaround by using ATLAS instead. \nText: Above generated on Ubuntu 14.04, using Ubuntu-provided gcc, BLAS, and ATLAS. \nAPI:\nnumpy.dot\n","label":[[118,121,"Mention"],[3083,3092,"API"]],"Comments":[]}
{"id":59812,"text":"ID:23455019\nPost:\nText: This will do the rearrangement of the names and the blocks of the input array - the key in the block rearrangement is the np.ix_ function which allows for Matlab-like indexing. \nCode: boundaris = [(0, 68), (68, 1190), (1190, 2248), (2248, 3399), (3399, 4795), (4795, 6023)]\nnames = ('4', 'X', '2R', '2L', '3R', '3L')\nA = numpy.random.random((6023, 6023))\n\nneworder = [3, 2, 5, 4, 0, 1]\n\ndef rearrange(l):\n    return [l[i] for i in neworder]\n\nnewnames = rearrange(names)\nranges = numpy.concatenate([numpy.arange(l, u) for l, u in rearrange(boundaris)])\nnewA = A[numpy.ix_(ranges, ranges)]\n\nAPI:\nnumpy.ix_\n","label":[[146,152,"Mention"],[618,627,"API"]],"Comments":[]}
{"id":59813,"text":"ID:23465552\nPost:\nText: The variables Ycoord and Xcoord are probably np.ndarray objects. You have to use the array compatible and operator to check all its values for your condition. You can create a flag array and set the values to 4 in all places where your conditional is True: \nCode: check = np.logical_and(Ycoord >= Y_west, Xcoord == X_west)\nflag = np.zeros_like(Ycoord)\nflag[check] = 4\n\nText: or you have to test value-by-value in your code doing: \nCode: for i in range(len(data)):\n    if Ycoord[i] >= Y_west and Xcoord[i] == X_west:\n        flag = 4\n\nAPI:\nnumpy.ndarray\n","label":[[69,79,"Mention"],[563,576,"API"]],"Comments":[]}
{"id":59814,"text":"ID:23541816\nPost:\nText: shuffle is designed to work in-place meaning that it should return None and instead modify your array. \nCode: import numpy as np\n\nx = np.arange(9).reshape((3,3))\nprint(x)\n# [[0 1 2]\n#  [3 4 5]\n#  [6 7 8]]\n\nnp.random.shuffle(x)\nprint(x)\n# [[3 4 5]\n#  [0 1 2]\n#  [6 7 8]]\n\nAPI:\nnumpy.random.shuffle\n","label":[[24,31,"Mention"],[300,320,"API"]],"Comments":[]}
{"id":59815,"text":"ID:23585292\nPost:\nText: One can find discrete fourier transform coefficients using linear algebra, though I imagine it's only useful to understand the DFT better. The code below demonstrates this. To find coefficients and phases for a sine series will take a little bit more work, but shouldn't be too hard. The Wikipedia article cited in the code comments might help. \nText: Note that one doesn't need scipy.optimize.curve_fit, or even linear least-squares. In fact, although I've used np.linalg.solve below, that is unnecessary since basis is a unitary matrix times a scale factor. \nCode: from __future__ import division, print_function\n\nimport numpy\n\n# points in time series\nn= 101\n# final time (initial time is 0)\ntfin= 10\n\n# *end of changeable parameters*\n\n# stepsize\ndt= tfin\/(n-1)\n# sample count\ns= numpy.arange(n)\n# signal; somewhat arbitrary\ny= numpy.sinc(dt*s)\n# DFT\nfy= numpy.fft.fft(y)\n# frequency spectrum in rad\/sample\nwps= numpy.linspace(0,2*numpy.pi,n+1)[:-1]\n\n# basis for DFT\n# see, e.g., http:\/\/en.wikipedia.org\/wiki\/Discrete_Fourier_transform#equation_Eq.2\n# and section \"Properties -> Orthogonality\"; the columns of 'basis' are the u_k vectors\n# described there\nbasis= 1.0\/n*numpy.exp(1.0j * wps * s[:,numpy.newaxis])\n\n# reconstruct signal from DFT coeffs and basis\nrecon_y= numpy.dot(basis,fy)\n\n# expect yerr to be \"small\"\nyerr= numpy.max(numpy.abs(y-recon_y))\nprint('yerr:',yerr)\n\n# find coefficients by fitting to basis\nlin_fy= numpy.linalg.solve(basis,y)\n\n# fyerr should also be \"small\"\nfyerr= numpy.max(numpy.abs(fy-lin_fy))\nprint('fyerr',fyerr)\n\nText: On my system this gives \nCode: yerr: 2.20721480995e-14\nfyerr 1.76885950227e-13\n\nText: Tested on Ubuntu 14.04 with Python 2.7 and 3.4. \nAPI:\nnumpy.linalg.solve\n","label":[[487,502,"Mention"],[1718,1736,"API"]],"Comments":[]}
{"id":59816,"text":"ID:23594478\nPost:\nText: Assuming that your y-values are at the corresponding position, i.e., y[i] = f(x[i]) then you can use digitize to find the indexes of the bins that the x-values belong to and use those indexes to sum up the corresponding y-values. \nText: From the numpy example (ignore that the values are not within [0; 1]): \nCode: >>> x = np.array([0.2, 6.4, 3.0, 1.6])\n>>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])\n>>> inds = np.digitize(x, bins)\n>>> inds\narray([1, 4, 3, 2])\n\nText: then sum up the values in y: \nCode: >>> aggregate = [y[inds == i].sum() for i in np.unique(inds)]\n\nText: If you're struggling with creating the bins yourself, look at numpy.linspace. \nCode: numpy.linspace(0, 1, num=50, endpoint=True)\n\nAPI:\nnumpy.digitize\n","label":[[125,133,"Mention"],[734,748,"API"]],"Comments":[]}
{"id":59817,"text":"ID:23626238\nPost:\nText: If you want to bootstrap you could use random.choice() on your observed series. \nText: Here I'll assume you'd like to smooth a bit more than that and you aren't concerned with generating new extreme values. \nText: Use pandas.Series.quantile() and a uniform [0,1] random number generator, as follows. \nText: Training \nText: Put your random sample into a pandas Series, call this series S \nText: Production \nText: Generate a random number u between 0.0 and 1.0 the usual way, e.g., random.random() return S.quantile(u) \nText: If you'd rather use numpy than pandas, from a quick reading it looks like you can substitute numpy.percentile() in step 2. \nText: Principle of Operation: \nText: From the sample S, pandas.series.quantile() or numpy.percentile() is used to calculate the inverse cumulative distribution function for the method of Inverse transform sampling. The quantile or percentile function (relative to S) transforms a uniform [0,1] pseudo random number to a pseudo random number having the range and distribution of the sample S. \nText: Simple Sample Code \nText: If you need to minimize coding and don't want to write and use functions that only returns a single realization, then it seems pervcentele bests pandas.Series.quantile. \nText: Let S be a pre-existing sample. \nText: u will be the new uniform random numbers \nText: newR will be the new randoms drawn from a S-like distribution. \nCode: >>> import numpy as np\n\nText: I need a sample of the kind of random numbers to be duplicated to put in S. \nText: For the purposes of creating an example, I am going to raise some uniform [0,1] random numbers to the third power and call that the sample S. By choosing to generate the example sample in this way, I will know in advance -- from the mean being equal to the definite integral of (x^3)(dx) evaluated from 0 to 1 -- that the mean of S should be 1\/(3+1) = 1\/4 = 0.25 \nText: In your application, you would need to do something else instead, perhaps read a file, to create a numpy array S containing the data sample whose distribution is to be duplicated. \nCode: >>> S = pow(np.random.random(1000),3)  # S will be 1000 samples of a power distribution\n\nText: Here I will check that the mean of S is 0.25 as stated above. \nCode: >>> S.mean()\n0.25296623781420458 # OK\n\nText: get the min and max just to show how np.percentile works \nCode: >>> S.min()\n6.1091277680105382e-10\n>>> S.max()\n0.99608676594692624\n\nText: The percentile function maps 0-100 to the range of S. \nCode: >>> np.percentile(S,0)  # this should match the min of S\n6.1091277680105382e-10 # and it does\n\n>>> np.percentile(S,100) # this should match the max of S\n0.99608676594692624 # and it does\n\n>>> np.percentile(S,[0,100])  # this should send back an array with both min, max\n[6.1091277680105382e-10, 0.99608676594692624]  # and it does\n\n>>> np.percentile(S,np.array([0,100])) # but this doesn't.... \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\/usr\/lib\/python2.7\/dist-packages\/numpy\/lib\/function_base.py\", line 2803, in percentile\n    if q == 0:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nText: This isn't so great if we generate 100 new values, starting with uniforms: \nCode: >>> u = np.random.random(100)\n\nText: because it will error out, and the scale of u is 0-1, and 0-100 is needed. \nText: This will work: \nCode: >>> newR = np.percentile(S, (100*u).tolist()) \n\nText: which works fine but might need its type adjusted if you want a numpy array back \nCode: >>> type(newR)\n<type 'list'>\n\n>>> newR = np.array(newR)\n\nText: Now we have a numpy array. Let's check the mean of the new random values. \nCode: >>> newR.mean()\n0.25549728059744525 # close enough\n\nAPI:\nnumpy.percentile\nnumpy.percentile\n","label":[[1224,1235,"Mention"],[2451,2461,"Mention"],[3764,3780,"API"],[3781,3797,"API"]],"Comments":[]}
{"id":59818,"text":"ID:23764592\nPost:\nText: One possible answer to the (my) question, using np.where \nText: To find the slices of a, where t = 1: \nCode: >>> import numpy as np\n>>> out = a[np.where(t == 1),:,:]\n\nText: although this gives the slightly confusing (to me at least) output of: \nCode: >>> out.shape\n(1, 83, 50, 50)\n\nText: but if we follow through with my needing the mean \nCode: >>> out2 = np.mean(np.mean(out, axis = 0), axis = 0)\n\nText: reduces the result to the expected: \nCode: >>> out2.shape\n(50,50)\n\nText: Can anyone improve on this or see any issues here? \nAPI:\nnumpy.where\n","label":[[72,80,"Mention"],[559,570,"API"]],"Comments":[]}
{"id":59819,"text":"ID:23789477\nPost:\nText: You can use the np.timedelta64 object to perform time delta calculations on a datetime64 object, see Datetime and Timedelta Arithmetic. \nText: Since a year can be either 365 or 366 days, it is not possible to substract a year, but you could substract 365 days instead: \nCode: import numpy as np\nnp.datetime64('2014-12-31') - np.timedelta64(365,'D')\n\nText: results in: \nText: numpy.datetime64('2013-12-31') \nAPI:\nnumpy.timedelta64\nnumpy.datetime64\n","label":[[40,54,"Mention"],[102,112,"Mention"],[436,453,"API"],[454,470,"API"]],"Comments":[]}
{"id":59820,"text":"ID:23841634\nPost:\nText: In order to pass the C double* buffer to a function that requires a np.ndarray you can create a temporary buffer and assign to its memory address the address of the double* array. \nText: This malloc()-based solution is orders of magnitude faster than the other answer based on NumPy buffers. Note how to free() the inner arrays to avoid a memory leak. \nCode: import numpy as np\ncimport numpy as np\nfrom cython cimport view\nfrom libc.stdlib cimport malloc, free\n\ncdef int i\ncdef double test\nlist_size = 10\nndarray_list = <double **>malloc(list_size * sizeof(double*))\narray_length = <int *>malloc(list_size * sizeof(int*))\nfor i in range(list_size):\n    array_length[i] = i+1\n    ndarray_list[i] = <double *>malloc(array_length[i] * sizeof(double))\n    for j in range(array_length[i]):\n        ndarray_list[i][j] = j\n\nfor i in range(list_size):\n    for j in range(array_length[i]):\n        test = ndarray_list[i][j]\n\ncdef view.array buff\nfor i in range(list_size):\n    buff = <double[:array_length[i]]>ndarray_list[i]\n    print np.sum(buff)\n\n#...\n\nfor i in range(list_size):\n    free(ndarray_list[i])\nfree(ndarray_list)\nfree(array_length)\n\nAPI:\nnumpy.ndarray\n","label":[[92,102,"Mention"],[1168,1181,"API"]],"Comments":[]}
{"id":59821,"text":"ID:23869387\nPost:\nText: Numpy array multiplication operates element-wise, so a simple A*B will suffice in this case. \nText: For matrix dot, inner and outer products you need to use numpy.dot, np.inner and np.outer explicitly. \nAPI:\nnumpy.outer\n","label":[[205,213,"Mention"],[232,243,"API"]],"Comments":[]}
{"id":59822,"text":"ID:24117866\nPost:\nText: The max function has the following arguments: \nCode: max(iterable[, key=func]) -> value\nmax(a, b, c, ...[, key=func]) -> value\n\nText: In the numpy array case the numpy array is treated as an iterable. In the case of your b what will happen is that b gets iterated over. This results in: \nCode: >>> list(iter(b))\n[array([1, 2, 3]), array([4, 5, 6])]\n\nText: Now if you compare the two items with the default comparison function you'll get the same error: \nCode: >>> c = list(iter(b))\n>>> cmp(c[0], c[1])\n\nText: Numpy does not flatten the array as the iterator will loop, by default, over rows and not over elements. If you want more control over the iteration, you can use on of the array iterators such as: \nText: the nditer function. \nCode: >>> max(np.nditer(b))\narray(6)\n\nText: or the .flat attribute: \nCode: >>> max(b.flat)\n6\n\nAPI:\nnumpy.nditer\n","label":[[741,747,"Mention"],[858,870,"API"]],"Comments":[]}
{"id":59823,"text":"ID:24234197\nPost:\nText: If you want to do this with the matrix form, you have a lot of memory consumption with larger arrays. If that does not matter, then you get the difference matrix by: \nCode: diff_array = allpoints[:,None] - toberemovedpoints[None,:]\n\nText: The resulting array has as many rows as there are points in allpoints, and as many columns as there are points in toberemovedpoints. Then you can manipulate this any way you want (e.g. calculate the absolute value), which gives you a boolean array. To find which rows have any hits (absolute difference < .1), use numpy.any: \nCode: hits = numpy.any(numpy.abs(diff_array) < .1, axis=1)\n\nText: Now you have a vector which has the same number of items as there were rows in the difference array. You can use that vector to index all points (negation because we wanted the non-matching points): \nCode: return allpoints[-hits]\n\nText: This is a numpyish way of doing this. But, as I said above, it takes a lot of memory. \nText: If you have larger data, then you are better off doing it point by point. Something like this: \nCode: return allpoints[-numpy.array([numpy.any(numpy.abs(a-toberemoved) < .1) for a in allpoints ])]\n\nText: This should perform well in most cases, and the memory use is much lower than with the matrix solution. (For stylistic reasons you may want to use all instead of np.any and turn the comparison around to get rid of the negation.) \nText: (Beware, there may be pritning mistakes in the code.) \nAPI:\nnumpy.all\nnumpy.any\n","label":[[1336,1339,"Mention"],[1351,1357,"Mention"],[1485,1494,"API"],[1495,1504,"API"]],"Comments":[]}
{"id":59824,"text":"ID:24247262\nPost:\nText: I would like to use a vectorized approach instead. \nText: You sound like you might be a Matlab user -- you should be aware that numpy's vectorize function provides no performance benefit: \nText: The vectorize function is provided primarily for convenience, not for performance. The implementation is essentially a for loop. \nText: Unless it just so happens that there's already an operation in numpy that does exactly what you want, you're going to be stuck with np.vectorize and nothing to really gain over a for loop. With that being said, you should be able to do that like so: \nCode: def makeArray():\n    a = [1, 2, 3, 4]\n    def addTo(arr):\n        return f(a[math.floor(arr\/4)]) * f(a[arr % 4])\n    vecAdd = numpy.vectorize(addTo)\n    return vecAdd(numpy.arange(4 * 4).reshape(4, 4))\n\nText: EDIT: \nText: If f is actually a one-dimensional array, you can do this: \nCode: f_matrix = numpy.matrix(f)\nD = f_matrix.T * f_matrix\n\nAPI:\nnumpy.vectorize\n","label":[[487,499,"Mention"],[959,974,"API"]],"Comments":[]}
{"id":59825,"text":"ID:24261445\nPost:\nText: You can use where for this: \nCode: >>> indices = np.where(m > 0)\n>>> index_row, index_col = indices\n>>> dist = m[indices]\n>>> index_row\narray([0, 0, 1, 1, 2, 2])\n>>> index_col\narray([1, 2, 0, 2, 0, 1])\n>>> dist\narray([2, 4, 4, 4, 5, 4])\n\nAPI:\nnumpy.where\n","label":[[36,41,"Mention"],[267,278,"API"]],"Comments":[]}
{"id":59826,"text":"ID:24321749\nPost:\nText: I'll try to paraphrase the problem. \nText: input array A (N integers valued 0..M-1 or -1) pointing to probability vectors B input array B (M x 4) giving probabilities (each row sums up to 1) colour table RGB (N x 4 x 3) giving RGB triplets to choose from input vector R (N) containing uniformly distributed random values [0,1] \nText: So, for each value of A: \nText: the probability vector is picked from B the random value is use to choose which of the four alternatives will be picked form the same row in RGB \nText: In addition, there are two extra rules: \nText: if A[n]==-1, then the corresponding output colour is black if there are negative probabilities in B[n], then the corresponding output colour is black \nText: The output will be a Nx3 colour array. \nText: So, let us first construct a clean probability vector so that impossible combinations are represented by [-1,0,0,0]. \nCode: # get the number of rows:\nN = len(A)\n\n# create a boolean array to show which indices in A are valid\nA_valid = (A != -1)\n\n# get B vectors for all valid points in A\nB_vectors = B[A[A_valid]]\n\n# clean the B_vectors so that if there are <0 vectors, they are replaced by -1,0,0,0\nB_vectors[numpy.amin(B_vectors, axis=1) < 0] = [-1.0, 0.0, 0.0, 0.0]\n\n# create a clean probability table (N x 4)\nprobs = numpy.empty((N, 4))\n# fill in the probabilities where they can be picked form B\nprobs[A_valid] = B_vectors\n# fill the rest with -1,0,0,0\nprobs[-A_valid] = [-1, 0, 0, 0]\n\nText: Now we have a table with either real probabilities (positive numbers summing to 1) or (-1,0,0,0) in case there is -1 in A or an impossible probability vector in B on the specific row. \nText: The probability vectors are easier to use, if a cumulative probability is formed. For example, probability vector (.2, .3, .4, .1) is transformed into (.2, .5, .9, 1.0). In this form the random number r can be compared directly to see which bin should be chosen. \nText: The next step is to obtain the colour bins (0,1,2,3) by using this approach: \nCode: # cumulative probabilities\ncumprobs = numpy.cumsum(probs, axis=1)\n\n# color indices\ncidx = numpy.zeros(N)\n\n# compare the colour indices to the random vector r\ncidx[r > cumprobs[:,0]] = 1\ncidx[r > cumprobs[:,1]] = 2\ncidx[r > cumprobs[:,2]] = 3\n\nText: (For some strange reason, there is no function in numpy to perform this. np.digitize works only with 1-d vectors.) \nText: It should be noted that if for some row the cumulative probabilities are (.2, .5, .9, 1.0) and r for the same row is 0.95, cidx is first 0 (after the array creation), then set to 1 (because r>.2), then to 2 (because r>.5) and finally to 3 (because r>.9). \nText: Then we can create the output colour table by using cidx and RGB: \nCode: # pick the item defined by cidx for each row\nrainbow = RGB[arange(N), cidx]\n\nText: This picks the colour specified by the corresponding cidx and RGB values on that row. \nText: Finally, we have to blacken out all invalid colours: \nCode: # if the probability starts with -1, then we'll blacken the color out\nrainbow[probs[:,0] < 0.] = [0,0,0]\n\nText: Now the result should be in rainbow. \nAPI:\nnumpy.digitize\n","label":[[2355,2366,"Mention"],[3130,3144,"API"]],"Comments":[]}
{"id":59827,"text":"ID:24331782\nPost:\nText: You can use argsort to get a list with the sorted indices of your array. Using that you can then rearrange the columns of the matrix. \nCode: import numpy as np\n\nc = np.array([5,2,8,2,4])    \na = np.array([[ 0,  1,  2,  3,  4],\n              [ 5,  6,  7,  8,  9],\n              [10, 11, 12, 13, 14],\n              [15, 16, 17, 18, 19],\n              [20, 21, 22, 23, 24]])\n\ni = np.argsort(c)\na = a[:,i]\n\nAPI:\nnumpy.argsort\n","label":[[36,43,"Mention"],[432,445,"API"]],"Comments":[]}
{"id":59828,"text":"ID:24345370\nPost:\nText: You can use ndenumerate with a heap, or a partial sort as suggested by David: \nCode: a = np.array([[1, 3, 5, 2, 3],\n       [7, 6, 5, 2, 4],\n       [2, 0, 5, 6, 4]])\nheap = [(v, k) for k,v in numpy.ndenumerate(npa)]\nheapq.heapify(heap)\nheapq.nsmallest(10, heap) # for k = 10\n\nText: And you get: \nCode: [(0, (2, 1)),\n (1, (0, 0)),\n (2, (0, 3)),\n (2, (1, 3)),\n (2, (2, 0)),\n (3, (0, 1)),\n (3, (0, 4)),\n (4, (1, 4)),\n (4, (2, 4)),\n (5, (0, 2))]\n\nAPI:\nnumpy.ndenumerate\n","label":[[36,47,"Mention"],[471,488,"API"]],"Comments":[]}
{"id":59829,"text":"ID:24396734\nPost:\nText: The mode parameter determines what happens near the boundaries. If you have input vectors with length x and y (x > y): \nText: valid \/ 0: you will only receive the portion of the convolution where both signals overlap (x-y+1 points) same \/ 1: the length of the output vector is the same as the length of the longer input vector (x points) full \/ 2: all data from the area where there is even a single sample of overlap between the signals (x+y-1 points) \nText: The numbers for these modes are not very publicly defined, byt they can be found in numpy's source code. In any case xcorruses the full mode. (Actually, only the first letters of mode names matter when giving the mode for convolve or correlate.) \nText: There is some confusion as to what these functions really do. correlate has two different behaviours depending on numpy version. Internally these are known as multiarray.correlate (old) and multiarray.correlate2 (new). np.convolve reverses the second input vector and uses then multiarray.correlate (i.e. the one deprecated for correlation). \nText: So, if you want to be really sure, you test what happens. The basic function is the product between two vectors where the vectors are moved one position at a time. To clarify this, I'll use some numeric examples with two vectors. \nCode: a <= [1,2,3,4,5]\nb <= [10,20]\n\nText: let's first look at convolve: \nCode: numpy.convolve(a,b,mode='full') => [ 10, 40, 70, 100, 230, 100]\n\nText: this is because: \nCode:     1  2  3  4  5  => 1 x 10 = 10\n20 10\n\n    1  2  3  4  5  => 1 x 20 + 2 x 10 = 40\n   20 10\n\n...\n\n    1  2  3  4  5     => 5 x 20 = 100\n               20 10\n\nText: Different modes return the same data but truncated at each end. \nText: For correlation: \nCode: numpy.correlate(a,b,mode='full') => [ 20, 50, 80, 110, 140, 50]\n\n    1  2  3  4  5  => 1 x 20 = 20\n10 20\n\n    1  2  3  4  5  => 1 x 10 + 2 x 20 = 50\n   10 20\n\n...\n\n    1  2  3  4  5     => 5 x 10 = 100\n               10 20\n\nText: So, basically the only difference with real numbers is that one of the vectors is mirrored. This has some consequences, such as convolution giving the same result if a and b is swapped, correlation giving reversed result in that case. With complex numbers correlate conjugates the second vector prior to the calculations above. \nText: Back to matplotlib's xcorr graph. It receives two vectors x and y with equal lengths and calculates the cross-correlation of these vectors at different lags. \nText: It first calculates the full convolution with np.correlate between x and y as shown above. Then it draws the correlation results from the full output vector at positions -maxlags..maxlags. The rule is that the second input vector is shifted. At the leftmost graph position the second vector y is at its leftmost position (i.e. shifted to the left from x). \nText: The easiest way to check this may be: \nCode: xcorr([1.,2.,3.,4.,5.], [0,0,0,0,1.], normed=False, maxlags=4)\n\nAPI:\nnumpy.correlate\nnumpy.convolve\nnumpy.correlate\n","label":[[799,808,"Mention"],[956,967,"Mention"],[2528,2540,"Mention"],[2959,2974,"API"],[2975,2989,"API"],[2990,3005,"API"]],"Comments":[]}
{"id":59830,"text":"ID:24411581\nPost:\nText: As a workaround you can try to memory map your data explicitly & manually as explained in the joblib documentation. \nText: Edit #1: Here is the important part: \nCode: from sklearn.externals import joblib\n\njoblib.dump(X_train, some_filename)\nX_train = joblib.load(some_filename, mmap_mode='r+')\n\nText: Then pass this memmap'ed data to GridSearchCV under scikit-learn 0.15+. \nText: Edit #2: Furthermore: if you use the 32bit version of Anaconda, you will be limited to 2GB per python process which can also limit the memory. \nText: I just found a bug for save under Python 3.4 but even when fixed the subsequent call to mmap will fail with: \nCode: OSError: [WinError 8] Not enough storage is available to process this command\n\nText: So please use a 64 bit version of Python (with Anaconda as AFAIK there is currently no other 64bit packages for numpy \/ scipy \/ scikit-learn==0.15.0b1 at this time). \nText: Edit #3: I found another issue that might be causing excessive memory usage under windows: currently joblib.Parallel memory maps input data with mmap_mode='c' by default: this copy-on-write setting seems to cause windows to exhaust the paging file and sometimes triggers \"[error 1455] the paging file is too small for this operation to complete\" errors. Setting mmap_mode='r' or mmap_mode='r+' does not trigger that problem. I will run tests to see if I can change the default mode in the next version of joblib. \nAPI:\nnumpy.save\n","label":[[577,581,"Mention"],[1447,1457,"API"]],"Comments":[]}
{"id":59831,"text":"ID:24435689\nPost:\nText: So, you can calculate two numbers for each point in the 2D plane \nText: confidence (0 .. 1) class (an integer) \nText: One possibility is to calculate your own RGB map and show it with imshow. Like this: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\n\n# color vector with N x 3 colors, where N is the maximum number of classes and the colors are in RGB\nmycolors = np.array([\n  [ 0, 0, 1],\n  [ 0, 1, 0],\n  [ 1, 0, 1],\n  [ 1, 1, 0],\n  [ 0, 1, 1],\n  [ 0, 0, 0],\n  [ 0, .5, 1]])\n\n# negate the colors\nmycolors = 1 - mycolors \n\n# extents of the area\nx0 = -2\nx1 = 2\ny0 = -2\ny1 = 2\n\n# grid over the area\nX, Y = np.meshgrid(np.linspace(x0, x1, 1000), np.linspace(y0, y1, 1000))\n\n# calculate the classification and probabilities\nclasses = classify_func(X, Y)\nprobabilities = prob_func(X, Y)\n\n# create the basic color map by the class\nimg = mycolors[classes]\n\n# fade the color by the probability (black for zero prob)\nimg *= probabilities[:,:,None]\n\n# reverse the negative image back\nimg = 1 - img\n\n# draw it\nplt.imshow(img, extent=[x0,x1,y0,y1], origin='lower')\nplt.axis('equal')\n\n# save it\nplt.savefig(\"mymap.png\")\n\nText: The trick of making negative colors is there just to make the maths a bit easier to undestand. The code can of course be written much denser. \nText: I created two very simple functions to mimic the classification and probabilities: \nCode: def classify_func(X, Y):\n    return np.round(abs(X+Y)).astype('int')\n\ndef prob_func(X,Y):\n    return 1 - 2*abs(abs(X+Y)-classify_func(X,Y))\n\nText: The former gives for the given area integer values from 0 to 4, and the latter gives smoothly changing probabilities. \nText: The result: \nText: If you do not like the way the colors fade towards zero probability, you may always create some non-linearity which is the applied when multiplying with the probabilities. \nText: Here the functions classify_func and prob_func are given two arrays as the arguments, first one being the X coordinates where the values are to be calculated, and second one Y coordinates. This works well, if the underlying calculations are fully vectorized. With the code in the question this is not the case, as it only calculates single values. \nText: In that case the code changes slightly: \nCode: x = np.linspace(x0, x1, 1000)\ny = np.linspace(y0, y1, 1000)\nclasses = np.empty((len(y), len(x)), dtype='int')\nprobabilities = np.empty((len(y), len(x)))\nfor yi, yv in enumerate(y):\n    for xi, xv in enumerate(x):\n    classes[yi, xi], probabilities[yi, xi] = kNN((xv, yv), D)\n\nText: Also as your confidence estimates are not 0..1, they need to be scaled: \nCode: probabilities -= np.amin(probabilities)\nprobabilities \/= np.amax(probabilities)\n\nText: After this is done, your map should look like this with extents -4,-4..4,4 (as per the color map: green=1, magenta=2, yellow=3): \nText: To vectorize or not to vectorize - that is the question \nText: This question pops up from time to time. There is a lot of information about vectorization in the web, but as a quick search did not reveal any short summaries, I'll give some thoughts here. This is quite a subjective matter, so everything just represents my humble opinions. Other people may have different opinions. \nText: There are three factors to consider: \nText: performance legibility memory use \nText: Usually (but not always) vectorization makes code faster, more difficult to understand, and consume more memory. Memory use is not usually a big problem, but with large arrays it is something to think of (hundreds of megs are usually ok, gigabytes are troublesome). \nText: Trivial cases aside (element-wise simple operations, simple matrix operations), my approach is: \nText: write the code without vectorizations and check it works profile the code vectorize the inner loops if needed and possible (1D vectorization) create a 2D vectorization if it is simple \nText: For example, a pixel-by-pixel image processing operation may lead to a situation where I end up with one-dimensional vectorizations (for each row). Then the inner loop (for each pixel) is fast, and the outer loop (for each row) does not really matter. The code may look much simpler if it does not try to be usable with all possible input dimensions. \nText: I am such a lousy algorithmist that in more complex cases I like to verify my vectorized code against the non-vectorized versions. Hence I almost invariably first create the non-vectorized code before optimizing it at all. \nText: Sometimes vectorization does not offer any performance benefit. For example, the handy function np.vectorize can be used to vectorize practically any function, but its documentation states: \nText: The vectorize function is provided primarily for convenience, not for performance. The implementation is essentially a for loop. \nText: (This function could have been used in the code above, as well. I chose the loop version for legibility for people not very familiar with numpy.) \nText: Vectorization gives more performance only if the underlying vectorized functions are faster. They sometimes are, sometimes aren't. Only profiling and experience will tell. Also, it is not always necessary to vectorize everything. You may have an image processing algorithm which has both vectorized and pixel-by-pixel operations. There np.vectorize is very useful. \nText: I would try to vectorize the kNN search algorithm above at least to one dimension. There is no conditional code (it wouldn't be a show-stopper but it would complicates things), and the algorithm is rather straight-forward. The memory consumption will go up, but with one-dimensional vectorization it does not matter. \nText: And it may happen that along the way you notice that a n-dimensional generalization is not much more complicated. Then do that if memory allows. \nAPI:\nnumpy.vectorize\nnumpy.vectorize\n","label":[[4566,4578,"Mention"],[5292,5304,"Mention"],[5803,5818,"API"],[5819,5834,"API"]],"Comments":[]}
{"id":59832,"text":"ID:24448132\nPost:\nText: Well, if you want to use the np.savetxt function, then you can just use cStringIO: \nCode: from cStringIO import StringIO\noutput = StringIO()\nnumpy.savetxt(output, numpy_array)\ncsv_string = output.getvalue()\n\nText: For python 3 you would import StringIO or BytesIO from the io module instead. \nAPI:\nnumpy.savetxt\n","label":[[53,63,"Mention"],[322,335,"API"]],"Comments":[]}
{"id":59833,"text":"ID:24450567\nPost:\nText: numpy.take(array, indices) and numpy.choose(indices, array) behave similarly on 1-D arrays, but this is just coincidence. As pointed out by jonrsharpe, they behave differently on higher-dimensional arrays. \nText: tavke \nText: numpy.take(array, indices) picks out elements from a flattened version of array. (The resulting elements are of course not necessarily from the same row.) \nText: For example, \nCode: numpy.take([[1, 2], [3, 4]], [0, 3])\n\nText: returns \nCode: array([1, 4])\n\nText: choose \nText: numpy.choose(indices, set_of_arrays) plucks out element 0 from array indices[0], element 1 from array indices[1], element 2 from array indices[2], and so on. (Here, array is actually a set of arrays.) \nText: For example \nCode: numpy.choose([0, 1, 0, 0], [[1, 2, 3, 4], [4, 5, 6, 7]])\n\nText: returns \nCode: array([1, 5, 3, 4])\n\nText: because element 0 comes from array 0, element 1 comes from array 1, element 2 comes from array 0, and element 3 comes from array 0. \nText: More Information \nText: These descriptions are simplified  full descriptions can be found here: numpy.take, numpy.choose. For example, np.take and np.choose behave similarly when indices and array are 1-D because choose first broadcasts array. \nAPI:\nnumpy.take\nnumpy.choose\nnumpy.take\nnumpy.choose\nnumpy.choose\n","label":[[237,242,"Mention"],[512,518,"Mention"],[1134,1141,"Mention"],[1146,1155,"Mention"],[1212,1218,"Mention"],[1249,1259,"API"],[1260,1272,"API"],[1273,1283,"API"],[1284,1296,"API"],[1297,1309,"API"]],"Comments":[]}
{"id":59834,"text":"ID:24478521\nPost:\nText: This should do it: \nCode: In [11]:\n\ndef f(arrA, arrB):\n    return not set(map(tuple, arrA)).isdisjoint(map(tuple, arrB))\nIn [12]:\n\nf(A, B)\nOut[12]:\nTrue\nIn [13]:\n\nf(A, C)\nOut[13]:\nFalse\nIn [14]:\n\nf(B, C)\nOut[14]:\nFalse\n\nText: To find intersection? OK, set sounds like a logical choice. But array or list are not hashable? OK, convert them to tuple. That is the idea. \nText: A numpy way of doing involves very unreadable boardcasting: \nCode: In [34]:\n\n(A[...,np.newaxis]==B[...,np.newaxis].T).all(1)\nOut[34]:\narray([[False, False],\n       [ True, False],\n       [False, False]], dtype=bool)\nIn [36]:\n\n(A[...,np.newaxis]==B[...,np.newaxis].T).all(1).any()\nOut[36]:\nTrue\n\nText: Some timeit result: \nCode: In [38]:\n#Dan's method\n%timeit set_comp(A,B)\n10000 loops, best of 3: 34.1 s per loop\nIn [39]:\n#Avoiding lambda will speed things up\n%timeit f(A,B)\n10000 loops, best of 3: 23.8 s per loop\nIn [40]:\n#numpy way probably will be slow, unless the size of the array is very big (my guess)\n%timeit (A[...,np.newaxis]==B[...,np.newaxis].T).all(1).any()\n10000 loops, best of 3: 49.8 s per loop\n\nText: Also the numpy method will be RAM hungry, as A[...,np.newaxis]==B[...,np.newaxis].T step creates a 3D array. \nAPI:\nnumpy.array\n","label":[[314,319,"Mention"],[1235,1246,"API"]],"Comments":[]}
{"id":59835,"text":"ID:24500274\nPost:\nText: It doesn't use numpy.where, but you could use tril_indices to set the lower triangular to zero: \nCode: >>> a\narray([[ 0.05559341, -1.93583316, -1.19666435],\n       [-0.33450047,  0.63275874,  0.77152195],\n       [-0.73106122, -1.57602057,  0.41878224]])\n>>> a[np.tril_indices(3, k=-1)] = 0\n>>> a\narray([[ 0.05559341, -1.93583316, -1.19666435],\n       [ 0.        ,  0.63275874,  0.77152195],\n       [ 0.        ,  0.        ,  0.41878224]])\n\nText: Note that you need to pass k=-1 to tril_indices to not include the diagonal. \nAPI:\nnumpy.tril_indices\nnumpy.tril_indices\n","label":[[70,82,"Mention"],[507,519,"Mention"],[555,573,"API"],[574,592,"API"]],"Comments":[]}
{"id":59836,"text":"ID:24652869\nPost:\nText: In plain Python, you can generate the Cartesian product of a collection of iterables using itertools.product. \nCode: >>> arrays = range(0, 2), range(4, 6), range(8, 10)\n>>> list(itertools.product(*arrays))\n[(0, 4, 8), (0, 4, 9), (0, 5, 8), (0, 5, 9), (1, 4, 8), (1, 4, 9), (1, 5, 8), (1, 5, 9)]\n\nText: In Numpy, you can combine meshgrid (passing sparse=True to avoid expanding the product in memory) with numpy.ndindex: \nCode: >>> arrays = np.arange(0, 2), np.arange(4, 6), np.arange(8, 10)\n>>> grid = np.meshgrid(*arrays, sparse=True)\n>>> [tuple(g[i] for g in grid) for i in np.ndindex(grid[0].shape)]\n[(0, 4, 8), (0, 4, 9), (1, 4, 8), (1, 4, 9), (0, 5, 8), (0, 5, 9), (1, 5, 8), (1, 5, 9)]\n\nAPI:\nnumpy.meshgrid\n","label":[[352,360,"Mention"],[722,736,"API"]],"Comments":[]}
{"id":59837,"text":"ID:24679685\nPost:\nText: 1. Explanation \nText: Looks like you ignored the RuntimeWarning: \nCode: >>> np.triu(a)\ntwodim_base.py:450: RuntimeWarning: invalid value encountered in multiply\n  out = multiply((1 - tri(m.shape[0], m.shape[1], k - 1, dtype=m.dtype)), m)\n\nText: The source code for np.triu is as follows: \nCode: def triu(m, k=0):\n    m = asanyarray(m)\n    out = multiply((1 - tri(m.shape[0], m.shape[1], k - 1, dtype=m.dtype)), m)\n    return out\n\nText: This uses tri to get an array with ones below the diagonal and zeros above, and subtracts this from 1 to get an array with zeros below the diagonal and ones above: \nCode: >>> 1 - np.tri(4, 4, -1)\narray([[ 1.,  1.,  1.,  1.],\n       [ 0.,  1.,  1.,  1.],\n       [ 0.,  0.,  1.,  1.],\n       [ 0.,  0.,  0.,  1.]])\n\nText: Then it multiplies this element-wise with the original array. So where the original array has inf, the result has inf * 0 which is NaN. \nText: 2. Workaround \nText: Use tril_indices to generate the indices of the lower triangle, and set all those entries to zero: \nCode: >>> a = np.ones((4, 4))\n>>> a[1:, 0] = np.inf\n>>> a\narray([[  1.,   1.,   1.,   1.],\n       [ inf,   1.,   1.,   1.],\n       [ inf,   1.,   1.,   1.],\n       [ inf,   1.,   1.,   1.]])\n>>> a[np.tril_indices(4, -1)] = 0\n>>> a\narray([[ 1.,  1.,  1.,  1.],\n       [ 0.,  1.,  1.,  1.],\n       [ 0.,  0.,  1.,  1.],\n       [ 0.,  0.,  0.,  1.]])\n\nText: (Depending on what you are going to do with a, you might want to take a copy before zeroing these entries.) \nAPI:\nnumpy.triu\nnumpy.tri\nnumpy.tril_indices\n","label":[[289,296,"Mention"],[470,473,"Mention"],[948,960,"Mention"],[1513,1523,"API"],[1524,1533,"API"],[1534,1552,"API"]],"Comments":[]}
{"id":59838,"text":"ID:24726757\nPost:\nText: You run out of memory as the comments show. If that happens because you are using 32-bit Python, even the method below will fail. But for the 64-bit Python and not-so-much-RAM situation we can do a lot as calculating the correlations is easily done piecewise, as you only need two lines in the memory simultaneously. \nText: So, you may split your input into, say, 1000 row chunks, and then the resulting 1000 x 1000 matrices are easy to keep in memory. Then you can assemble your result into the big output matrix which is not necessarily in the RAM. I recommend this approach even if you have a lot of RAM, because this is much more memory-friendly. Correlation coefficient calculation is not an operation where fast random accesses would help a lot if the input can be kept in RAM. \nText: Unfortunately, the corrcoef does not do this automatically, and we'll have to roll our own correlation coefficient calculation. Fortunately, that is not as hard as it sounds. \nText: Something along these lines: \nCode: import numpy as np\n\n# number of rows in one chunk\nSPLITROWS = 1000\n\n# the big table, which is usually bigger\nbigdata = numpy.random.random((27000, 128))\n\nnumrows = bigdata.shape[0]\n\n# subtract means form the input data\nbigdata -= np.mean(bigdata, axis=1)[:,None]\n\n# normalize the data\nbigdata \/= np.sqrt(np.sum(bigdata*bigdata, axis=1))[:,None]\n\n# reserve the resulting table onto HDD\nres = np.memmap(\"\/tmp\/mydata.dat\", 'float64', mode='w+', shape=(numrows, numrows))\n\nfor r in range(0, numrows, SPLITROWS):\n    for c in range(0, numrows, SPLITROWS):\n        r1 = r + SPLITROWS\n        c1 = c + SPLITROWS\n        chunk1 = bigdata[r:r1]\n        chunk2 = bigdata[c:c1]\n        res[r:r1, c:c1] = np.dot(chunk1, chunk2.T)\n\nText: Some notes: \nText: the code above is tested above np.corrcoef(bigdata) if you have complex values, you'll need to create a complex output array res and take the complex conjugate of chunk2.T the code garbles bigdata to maintain performance and minimize memory use; if you need to preserve it, make a copy \nText: The above code takes about 85 s to run on my machine, but the data will mostly fit in RAM, and I have a SSD disk. The algorithm is coded in such order to avoid too random access into the HDD, i.e. the access is reasonably sequential. In comparison, the non-memmapped standard version is not significantly faster even if you have a lot of memory. (Actually, it took a lot more time in my case, but I suspect I ran out of my 16 GiB and then there was a lot of swapping going on.) \nText: You can make the actual calculations faster by omitting half of the matrix, because res.T == res. In practice, you can omit all blocks where c > r and then mirror them later on. On the other hand, the performance is most likely limited by the HDD preformance, so other optimizations do not necessarily bring much more speed. \nText: Of course, this approach is easy to make parallel, as the chunk calculations are completely independent. Also the memmapped array can be shared between threads rather easily. \nAPI:\nnumpy.corrcoef\n","label":[[834,842,"Mention"],[3068,3082,"API"]],"Comments":[]}
{"id":59839,"text":"ID:24766502\nPost:\nText: You can do \nCode: trace.text = str(a_numpy_array)\n\nText: For more options, see np.array_str and numpy.array2string. \nAPI:\nnumpy.array_str\n","label":[[103,115,"Mention"],[146,161,"API"]],"Comments":[]}
{"id":59840,"text":"ID:24778268\nPost:\nText: The reason for the observed difference is that random.sample samples without replacement (see here), while random_integezrs samples with replacement. \nAPI:\nnumpy.random.random_integers\n","label":[[131,147,"Mention"],[180,208,"API"]],"Comments":[]}
{"id":59841,"text":"ID:24850649\nPost:\nText: Assuming that your data-frame is sorted in the index, you may combine whede and Series.fillna to obtain the last day which vols where below some threshold. for example, starting with \nCode: >>> df\n             vols\nDate             \n2014-07-10  0.045\n2014-07-11  0.057\n2014-07-12  0.064\n2014-07-13  0.003\n2014-07-14  0.021\n2014-07-15  0.052\n2014-07-16  0.090\n\nText: this would be \nCode: >>> df['tick'] = np.where(df.vols < .05, df.index, pd.tslib.NaT)\n>>> df\n             vols        tick\nDate                         \n2014-07-10  0.045  2014-07-10\n2014-07-11  0.057         NaN\n2014-07-12  0.064         NaN\n2014-07-13  0.003  2014-07-13\n2014-07-14  0.021  2014-07-14\n2014-07-15  0.052         NaN\n2014-07-16  0.090         NaN\n\nText: and then forward fill, and obtain the day difference with respect to index: \nCode: >>> df['tick'].fillna(method='ffill', inplace=True)\n>>> df\n             vols        tick\nDate                         \n2014-07-10  0.045  2014-07-10\n2014-07-11  0.057  2014-07-10\n2014-07-12  0.064  2014-07-10\n2014-07-13  0.003  2014-07-13\n2014-07-14  0.021  2014-07-14\n2014-07-15  0.052  2014-07-14\n2014-07-16  0.090  2014-07-14\n\n>>> df['days'] = df.index.values - df['tick']\n>>> df\n             vols        tick   days\nDate                                \n2014-07-10  0.045  2014-07-10 0 days\n2014-07-11  0.057  2014-07-10 1 days\n2014-07-12  0.064  2014-07-10 2 days\n2014-07-13  0.003  2014-07-13 0 days\n2014-07-14  0.021  2014-07-14 0 days\n2014-07-15  0.052  2014-07-14 1 days\n2014-07-16  0.090  2014-07-14 2 days\n\nText: Note that you need .values in the last step otherwise - would do sort of set difference operation. \nAPI:\nnumpy.where\n","label":[[94,99,"Mention"],[1671,1682,"API"]],"Comments":[]}
{"id":59842,"text":"ID:24911119\nPost:\nText: You are using np.log function here, its second argument is not base but out array: \nCode: >>> import numpy as np\n>>> np.log(1.1, 2)\nTraceback (most recent call last):\n  File \"<ipython-input-5-4d17df635b06>\", line 1, in <module>\n    np.log(1.1, 2)\nTypeError: return arrays must be of ArrayType\n\nText: You can now either use numpy.math.log or Python's math.log: \nCode: >>> np.math.log(1.1, 2)\n0.13750352374993502\n>>> import math\n>>> math.log(1.1, 2) #This will return a float object not Numpy's scalar value\n0.13750352374993502\n\nText: Or if you're dealing only with base 2 then as @WarrenWeckesser suggested you can use numpy.log2: \nAPI:\nnumpy.log\n","label":[[38,44,"Mention"],[660,669,"API"]],"Comments":[]}
{"id":59843,"text":"ID:24940371\nPost:\nText: You can use bitwise AND or OR and np.where for this: \nCode: >>> X2d = np.array([[0,4,5,0],\n...                 [7,8,4,3],\n...                 [0,0,9,8]])\n>>> \n>>> Y2d = np.array([[1,0,4,8],\n...                 [0,3,8,5],\n...                 [0,6,0,8]])\n>>> indices = np.where(~((X2d > 0) & (Y2d > 0)))\n>>> X2d[indices] = 0\n>>> Y2d[indices] = 0\n>>> X2d\narray([[0, 0, 5, 0],\n       [0, 8, 4, 3],\n       [0, 0, 0, 8]])\n>>> Y2d\narray([[0, 0, 4, 0],\n       [0, 3, 8, 5],\n       [0, 0, 0, 8]])\n\nText: I think bitwise OR is better and clearer to read: \nCode: >>> X2d = np.array([[0,4,5,0],\n...                 [7,8,4,3],\n...                 [0,0,9,8]])\n>>> \n>>> Y2d = np.array([[1,0,4,8],\n...                 [0,3,8,5],\n...                 [0,6,0,8]])\n>>> indices = np.where((X2d == 0) | (Y2d == 0))\n>>> X2d[indices] = 0\n>>> Y2d[indices] = 0\n>>> X2d\narray([[0, 0, 5, 0],\n       [0, 8, 4, 3],\n       [0, 0, 0, 8]])\n>>> Y2d\narray([[0, 0, 4, 0],\n       [0, 3, 8, 5],\n       [0, 0, 0, 8]])\n\nAPI:\nnumpy.where\n","label":[[58,66,"Mention"],[1009,1020,"API"]],"Comments":[]}
{"id":59844,"text":"ID:24955575\nPost:\nText: You're passing a file object to gaussian_kde but it expects a NumPy array, you need to use np.loadtxt first to load the data in an array: \nCode: >>> import numpy as np\n>>> arr = np.loadtxt('chk.bed')\n>>> arr\narray([  7.25236 ,   0.197037,   0.189464,   2.60056 ,   0.      ,\n        32.721   ,  11.3978  ,   3.85692 ,   0.      ,   0.      ])\n>>> gaussian_kde(arr)\n<scipy.stats.kde.gaussian_kde object at 0x7f7350390190>\n\nAPI:\nnumpy.loadtxt\n","label":[[115,125,"Mention"],[451,464,"API"]],"Comments":[]}
{"id":59845,"text":"ID:25014320\nPost:\nText: You can use functools.partial() instead of a lambda: \nCode: from collections import defaultdict\nfrom functools import partial\n\ndefaultdict(partial(numpy.ndarray, 0))\n\nText: You always need a callable for defaultdict(), and numpy.ndarray() always needs at least one argument, so you cannot just pass in nda here. \nAPI:\nnumpy.ndarray\n","label":[[326,329,"Mention"],[342,355,"API"]],"Comments":[]}
{"id":59846,"text":"ID:25022973\nPost:\nText: however, the solution here is to alter the format the data is saved in \nText: Good news, you don't have to! \nText: loadtxt can take any iterable of lines, not just a file object. \nText: So, you can wrap your file object in a simple generator that transforms the lines on the fly, and feed that to loadtxt, and everyone will be happy. \nText: Like this: \nCode: def transform_complex(line):\n    # insert your code here\n\nwith open('file.txt', 'rb') as f:\n    lines = map(transform_complex, f)\n    arr = np.loadtxt(lines, dtype=np.complex128)\n\nText: (If you're using Python 2.x, and the file is large, you probably want to use itertools.imap rather than map.) \nText: The \"insert your code here\" part, you fill in from the answer that worked, but wasn't an acceptable solution because it required modifying the files. Since I don't see such an answer in your link, I'm not sure what that is, but for example, maybe it's this: \nCode: def transform_complex(line):\n    return line.replace(b'+ -', b'- ')\n\nText: Testing things out locally, it looks like there are actually three things wrong with your input. \nText: You can test what the output should look like using savetxt. For example: \nCode: >>> arr = np.array([1-2j])\n>>> f = io.BytesIO()\n>>> np.savetxt(f, arr)\n>>> f.getvalue()\nb' (1.000000000000000000e+00-2.000000000000000000e+00j)\\n'\n\nText: (In Python 2.x, you won't see the b prefix.) \nText: Not all of those differences turn out to be relevantyou don't have to use exponential notation, you don't need parens, etc.but it looks like these three are: \nText: No spaces allowed around the + in complex numbers. The imaginary unit has to be j, not i. No +- allowed. \nText: So: \nCode: def transform_complex(line):\n    return line.replace(b' ', b'').replace(b'+-', b'-').replace(b'i', b'j')\n\nAPI:\nnumpy.loadtxt\n","label":[[139,146,"Mention"],[1818,1831,"API"]],"Comments":[]}
{"id":59847,"text":"ID:25032893\nPost:\nText: You simply need to use np.diff for this: \nCode: >>> a = np.array([1,3,6,8,17,23,45])\n>>> np.diff(a)\narray([ 2,  3,  2,  9,  6, 22])\n\nText: Edit: \nText: Your code is working fine too, but you should not use list comprehension for this as NumPy already provides a function for this because it is going be both fast and efficient. \nCode: >>> a = np.array([1,3,6,8,17,23,45])\n>>> [R-r for R,r in zip(a[1:],a)]\n[2, 3, 2, 9, 6, 22]\n\nAPI:\nnumpy.diff\n","label":[[47,54,"Mention"],[456,466,"API"]],"Comments":[]}
{"id":59848,"text":"ID:25047906\nPost:\nText: If you want for instance the number of rows repeat n=5 times you just need to use vstack , similar to np.vstack as following: \nCode: >>>from astropy.table import vstack\n>>>t1=vstack([t,t,t,t,t])\n>>>print t1\n\n a   b   c \n--- --- ---\n0.0 0.0 0.0\n1.0 1.0 1.0\n2.0 2.0 2.0\n0.0 0.0 0.0\n1.0 1.0 1.0\n2.0 2.0 2.0\n0.0 0.0 0.0\n1.0 1.0 1.0\n2.0 2.0 2.0\n0.0 0.0 0.0\n1.0 1.0 1.0\n2.0 2.0 2.0\n0.0 0.0 0.0\n1.0 1.0 1.0\n2.0 2.0 2.0\n\nAPI:\nnumpy.vstack\n","label":[[126,135,"Mention"],[442,454,"API"]],"Comments":[]}
{"id":59849,"text":"ID:25247533\nPost:\nText: According to the numpy documentation: http:\/\/docs.scipy.org\/doc\/numpy\/reference\/arrays.dtypes.html, np.void types are defined as flexible data types. Basically, these are data types where there is no pre-defined type associated to the variable(s) you're looking at. If you look at numpy, you have data types such as float, uint8, bool, string, etc. \nText: void is to accommodate for more generic and flexible types and are for those data types that don't necessary fall into any one of these pre-defined data types. This situation is mostly encountered when you're loading in a struct where each element has multiple data types associated with multiple fields. Each structure element could have a combination of different data types, and the amalgamation of all of these data types to represent an instance of this structure element thus leads us to numpy.void. \nText: With the documentation, you can certainly do the same operations like you would with any other data type. Take a look at the generic data type methods here: http:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.generic.html#numpy.generic . In fact, all numpy data types are derived from this generic class, including numpy.void. \nText: In the first link I provided at the beginning of this post, it shows a good example of how to create a custom record type, where a record is a combination of a tuple of numbers and a string. When creating a list of these records, each type in the list is of type void and it demonstrates that a record is of this data type. However, bear in mind that this record list has a data type that is of this record, but each element of this list will be of type numpy.void. \nText: However, as a matter of self-containment, let's re-create the example here: Let's create a custom record type where it has two fields associated for each variable you create: \nText: A 16-bit string with a field named name A 2-element tuple of floating point numbers that are 64-bits each, with a field named grades \nText: As such, you'd do something like: \nCode: import numpy as np\ndt = np.dtype([('name', np.str_, 16), ('grades', np.float64, (2,))])\n\nText: As such, let's create an example list of two elements and instantiate their fields: \nCode: x = np.array([('Sarah', (8.0, 7.0)), ('John', (6.0, 7.0))], dtype=dt)\n\nText: Because we made this list into a numpy.array, we expect its data type to be so: \nCode: type(x)\n\nText: We get: \nCode: <type 'numpy.ndarray'>\n\nText: Remember, the list itself is a numpy.array, but not the individual elements. \nText: To access the second element of this list, which is the second record, we do: \nCode: x[1]\n\nText: We get: \nCode: ('John', [6.0, 7.0])\n\nText: To check the type of the second record, we do: \nCode: type(x[1])\n\nText: We get: \nCode: <type 'numpy.void'> # As expected\n\nText: Some additional bonuses for you \nText: To access the name of the second record, we do: \nCode: x[1]['name']\n\nText: We get: \nCode: 'John'\n\nText: To access the grades of the second record, we do: \nCode: x[1]['grades']\n\nText: We get: \nCode: array([ 6.,  7.])\n\nText: To check the type of the name inside the second record, we do: \nCode: type(x[1]['name'])\n\nText: We get: \nCode: <type 'numpy.string_'>\n\nText: To check the type of the grades inside the second record, we do: \nCode: type(x[1]['grades'])\n\nText: We get: \nCode: <type 'numpy.ndarray'>\n\nText: Take note that each element in this list is of type numpy.void. However, the individual fields for each element in our list is either a tuple of numbers, or a string. The collection of these elements together is of type numpy.void. \nAPI:\nnumpy.void\nnumpy.void\n","label":[[124,131,"Mention"],[1496,1500,"Mention"],[3617,3627,"API"],[3628,3638,"API"]],"Comments":[]}
{"id":59850,"text":"ID:25263336\nPost:\nText: libdispatch.dylib from Grand Central Dispatch is used internally by OSX's builtin implementation of BLAS called Accelerate when you do a dom calls. The GCD runtime does not work when programs call the POSIX fork syscall without using an exec syscall afterwards and therefore makes all Python programs that use the multiprocessing module prone to crash. sklearn's GridsearchCV uses the Python multiprocessing module for parallelization. \nText: Under Python 3.4 and later you can force Python multiprocessing to use the forkserver start method instead of the default fork mode to workaround this problem, for instance at the beginning of the main file of your program: \nCode: if __name__ == \"__main__\":\n    import multiprocessing as mp; mp.set_start_method('forkserver')\n\nText: Alternatively, you can rebuild numpy from source and make it link against ATLAS or OpenBLAS instead of OSX Accelerate. The numpy developers are working on binary distributions that include either ATLAS or OpenBLAS by default. \nAPI:\nnumpy.dot\n","label":[[161,164,"Mention"],[1032,1041,"API"]],"Comments":[]}
{"id":59851,"text":"ID:25267977\nPost:\nText: Edit I would suggest using np.log as Roger Fan demonstrated below. Since you are already using numpy arrays, this will certainly outperform using map or a list comprehension. \nText: Original answer If you have a list of z-values, you can use map to perform some function to each value, in this case log (which is ln). \nCode: >>> x = range(1,10)\n>>> x\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n>>> from math import log\n>>> map(log, x)\n[0.0, 0.6931471805599453, 1.0986122886681098, 1.3862943611198906, 1.6094379124341003, 1.791759469228055, 1.9459101490553132, 2.0794415416798357, 2.1972245773362196]\n\nText: You could use any function, so you may use np.log if you prefer. \nAPI:\nnumpy.log\nnumpy.log\n","label":[[51,57,"Mention"],[660,666,"Mention"],[688,697,"API"],[698,707,"API"]],"Comments":[]}
{"id":59852,"text":"ID:25357479\nPost:\nText: You are likely using sum instead of the built-in sum, a side effect of from numy import *. Be advised not to do so, as it will cause no end of confusion. Instead, use something like import numpy as np, and refer to the numpy namespace with the short np prefix. \nText: To answer your question, sum makes the accumulator type the same as the type of the array. On a 32-bit system, numpy coerces the list into an int32 array, which causes sum to use a 32-bit accumulator. When given a generator expression, su falls back to calling sum, which promotes integers to longs. To force the use of a 64-bit accumulator for array\/list input, use the dtype parameter: \nCode: >>> np.sum([x for x in range(800000)], dtype=np.int64)\n319999600000\n\nAPI:\nnumpy.sum\nnumpy.sum\nnumpy.sum\nnumpy.sum\n","label":[[45,48,"Mention"],[317,320,"Mention"],[460,463,"Mention"],[528,530,"Mention"],[761,770,"API"],[771,780,"API"],[781,790,"API"],[791,800,"API"]],"Comments":[]}
{"id":59853,"text":"ID:25436008\nPost:\nText: From the array docs, the object you pass in must satisfy: \nText: An array, any object exposing the array interface, an object whose __array__ method returns an array, or any (nested) sequence. \nText: You're actually passing in a list of foo objects, so this list doesn't expose the array interface and doesn't have an array method. That leaves only whether it's a nested sequence. To be a nested sequence, your foo objects would likely need to be iterable. Are they? (emulating python's container types) \nText: Not sure if this is any better, but you could probably do: \nCode: numpy.array([numpy.array(x) for x in [foo_1, ..., foo_n]])\n\nText: Here is an example of Foo, as you've described. It outputs the ndarray you'd expect (no exceptions). Hopefully you can use it as an example: \nCode: import numpy as np\n\nclass Foo(object):\n    def __init__(self):\n        self.arr = np.array([[1, 2, 3], [4, 5, 6], [7,8,9]], np.int32)\n\n    def __array__(self):\n        return self.arr\n\n    def __iter__(self):\n        for elem in self.arr:\n            yield elem\n\n    def __len__(self):\n        return len(self.arr)\n\n    def __getitem__(self, key):\n        return self.arr[key]\n\ndef main():\n    foos = [Foo() for i in range(10)]\n    print np.array(foos)\n\n\nif __name__ == '__main__':\n    main()\n\nAPI:\nnumpy.array\n","label":[[33,38,"Mention"],[1314,1325,"API"]],"Comments":[]}
{"id":59854,"text":"ID:25454732\nPost:\nText: Numpy has configurable behaviour as to how floating point errors are treated. By default underflow errors are ignored, whereas other errors trigger a warning. For each category users can change this behaviour with numpy.seterr. These settings are global  there are no namespaces here; so if a library calls numpy.seterr(all='raise') then this will affect the entire program until seterr is called again. \nText: You can confirm this is indeed the cause of your problem with \nCode: print(numpy.seterr())\n\nText: which should output something like \nCode: {'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\nText: If some of those categories have the value raise, in particular the key 'invalid', then that would explain the behaviour you are observing. \nText: You can suppress this exception by calling numpy.seterr(invalid='warn'), or, alternatively, invalid='ignore'. For a full list of possible errors, read through the documentation of numpy.seterr. \nText: You can also use a context-manager to temporarily change the behaviour: \nCode: In [12]: x = np.arange(-5, 5,dtype='float64')\n\nIn [13]: with np.errstate(divide=\"raise\"):\n    print(1\/x)\n   ....:     \n---------------------------------------------------------------------------\nFloatingPointError                        Traceback (most recent call last)\n<ipython-input-13-881589fdcb7a> in <module>()\n      1 with np.errstate(divide=\"raise\"):\n----> 2     print(1\/x)\n      3 \n\nFloatingPointError: divide by zero encountered in true_divide\n\nIn [14]: with np.errstate(divide=\"warn\"):\n    print(1\/x)\n   ....:     \n\/home\/users\/gholl\/venv\/stable-3.5\/bin\/ipython3:2: RuntimeWarning: divide by zero encountered in true_divide\n  \n[-0.2        -0.25       -0.33333333 -0.5        -1.                 inf\n  1.          0.5         0.33333333  0.25      ]\n\nIn [15]: with np.errstate(divide=\"ignore\"):\n    print(1\/x)\n   ....:     \n[-0.2        -0.25       -0.33333333 -0.5        -1.                 inf\n  1.          0.5         0.33333333  0.25      ]\n\nText: I tend to wrap my entire code inside a with np.errstate(all=\"raise\") block, and then use a context-manager ignoring a particular condition if I am sure that the problem is not hiding a bug  it usually is, though. \nText: If there was indeed a library changing the state permanently, I would raise an issue with the maintainers or send a pull request, because they really should be using a context manager, so that their changing setting only applies to their block of code and not to the rest of the program. \nAPI:\nnumpy.seterr\n","label":[[405,411,"Mention"],[2562,2574,"API"]],"Comments":[]}
{"id":59855,"text":"ID:25470132\nPost:\nText: If you have only a limited number of elements to index, you might want to store the indices as a list of indices. For this purpose the nonzero is very useful. However, if your number of elements to index is large, you use less memory by storing the boolean array (1 byte per element). \nText: So, there are four possibilities: \nText: store a boolean array store indices of non-zeros always make the comparison separately masked arrays \nText: From the memory storage point of view alternative 1 takes 8 bytes per indexed element per dimension. (Of course, the \"per dimension\" can be avoided by using flat indices.) The boolean approach takes 1 byte per element, so if you have more than 1\/8 of the elements with a True in the boolean table, it is the more space-saving solution. Solution #3 probably takes the same space as the boolean solution. \nText: (I do not know enough of the internals of NumPy to say much about the masked arrays. I suspect they behave in a similar way to the boolean indexing.) \nText: Performance-wise the situation is similar. The boolean solution is efficient, if you have a lot of elements to pick, but if you have only a few elements, then the index solution is better. \nText: Just to give some benchmarking idea: \nCode: import numpy as np\nimport time\n\ndef create_indices(prob):\n    data = np.random.random(100000000) < prob\n    return data, np.nonzero(data)\n\ndef bool_index(data):\n    return data[data]\n\ndef list_index(data, indices):\n    return data[indices]\n\nText: By using timeit with different probabilities, the results are: \nCode:     p    boolean   list\n   0.01   0.206    0.012\n   0.10   0.415    0.099\n   0.20   0.405    0.146\n   0.50   0.786    0.373\n   0.75   0.539    0.555\n   1.00   0.214    0.723\n\nText: This is actually quite interesting: Using boolean indices is worst when half of the elements are True. The use of list indices behaves as expected. \nText: This benchmark must not be taken as the whole truth. It may be that the type of the array to be indexed changes the situation (here bool which is the same as uint8), etc. However, it seems that performance-wise list indices are very good in most cases. \nAPI:\nnumpy.nonzero\n","label":[[159,166,"Mention"],[2184,2197,"API"]],"Comments":[]}
{"id":59856,"text":"ID:25557903\nPost:\nText: 1. Indexing \nText: You're misunderstanding the way that NumPy indexes images. NumPy prefers row-major indexing (y, x, c) for images for the reasons described here: \nText: The drawback of [column-major indexing] is potential performance penalties. Its common to access the data sequentially, either implicitly in array operations or explicitly by looping over rows of an image. When that is done, then the data will be accessed in non-optimal order. As the first index is incremented, what is actually happening is that elements spaced far apart in memory are being sequentially accessed, with usually poor memory access speeds. \nText: If you prefer to use column-major indexing (x, y, c) and don't mind the potential performance penalty, then use np.transpose to permute the indices: \nCode: np.array(image).transpose((1, 0, 2))\n\nText: But the NumPy documentation recommends that you just get used to it: \nText: We recommend simply learning to reverse the usual order of indices when accessing elements of an array. Granted, it goes against the grain, but it is more in line with Python semantics and the natural order of the data. \nText: 2. Colour channels \nText: The third axis of the array gives you the 4 colour channels in each pixel, here the values (red, green, blue, alpha). This is more useful for most applications than a single 32-bit number: for example, you can extract the alpha channel by writing image[...,3]. \nText: If you really want the 32-bit colour values, then you can use the ndarray.view method to get a view of the same image data with a different dtype and then use np.reshape to drop the last axis (which is now redundant): \nCode: a = np.array(image)\na.view(dtype=np.uint32).reshape(a.shape[:-1])\n\nAPI:\nnumpy.transpose\nnumpy.reshape\n","label":[[772,784,"Mention"],[1616,1626,"Mention"],[1754,1769,"API"],[1770,1783,"API"]],"Comments":[]}
{"id":59857,"text":"ID:25654717\nPost:\nText: you can concatenate arrays first with np.concatenate then use np.unique \nCode: import numpy as np\na=np.array([1,6,56,120,162,170,176,179,197,204])\nb=np.array([29,31,56,104,162,170,176,179,197,204])\nnew_array = np.unique(np.concatenate((a,b),0))\n\nprint new_array\n\nText: result: \nCode: [  1   6  29  31  56 104 120 162 170 176 179 197 204]\n\nAPI:\nnumpy.concatenate\n","label":[[62,76,"Mention"],[368,385,"API"]],"Comments":[]}
{"id":59858,"text":"ID:25691669\nPost:\nText: The source code for unique in version 1.8.1 starts with the following: \nCode: try:\n    ar = ar.flatten()\nexcept AttributeError:\n    if not return_inverse and not return_index:\n        return np.sort(list(set(ar)))\n    else:\n        ar = np.asanyarray(ar).flatten()\n\nText: If the input isn't an array and return_inverse and return_index are not present, the routine delegates to Python built-ins to find unique elements. The way it does so is bugged; it does not perform the flattening that the documentation guarantees: \nText: Input array. This will be flattened if it is not already 1-D. \nText: As Jaime points out in the comments, this has been fixed in the current NumPy master branch. \nText: I believe you can get your desired result by packing your two lists into a structured array. I don't know whether unique takes structured arrays, but if not, you can replicate its behavior by using numpy.sort, which documents how to use it with structured arrays. \nAPI:\nnumpy.unique\nnumpy.unique\n","label":[[44,50,"Mention"],[834,840,"Mention"],[990,1002,"API"],[1003,1015,"API"]],"Comments":[]}
{"id":59859,"text":"ID:25713844\nPost:\nText: A simple way to \"resample\" your array is to group it into chunks, then average each chunk: \nText: (Chunking function is from this answer) \nCode: #  Chunking function \ndef chunks(l, n):\n    for i in xrange(0, len(l), n):\n        yield l[i:i+n]\n\n# Resampling function\ndef resample(arr, newLength):\n    chunkSize = len(arr)\/newLength\n    return [np.mean(chunk) for chunk in chunks(arr, chunkSize)]\n\n# Example:\nimport numpy as np\nx = np.linspace(-1,1,15)\ny = resample(x, 5)\nprint y\n# Result:\n# [-0.85714285714285721, -0.4285714285714286, -3.7007434154171883e-17, 0.42857142857142844, 0.8571428571428571]\n\nText: As you can see, the range of the resampled array does drift inward, but this effect would be much smaller for larger arrays. \nText: It's not clear to me whether the arrays will always be generated by np.linspace or not. If so, there are simpler ways of doing this, like simply picking each nth member of the original array, where n is determined by the \"compression\" ratio: \nCode: def linearResample(arr, newLength):\n    spacing = len(arr) \/ newLength\n    return arr[::spacing]\n\nAPI:\nnumpy.linspace\n","label":[[831,842,"Mention"],[1115,1129,"API"]],"Comments":[]}
{"id":59860,"text":"ID:25749403\nPost:\nText: I guess that you're looking for the savntxt which saves in a human readable format instead of the save which saves as a binary format. \nCode: import numpy as np\nmatrix=np.random.random((10,10,42))\nnx,ny,nz=np.shape(matrix)\nCXY=np.zeros([ny, nx])\nfor i in range(ny):\n    for j in range(nx):\n        CXY[i,j]=np.max(matrix[j,i,:])\n\n#Binary data\nnp.save('maximums.npy', CXY)\n\n#Human readable data\nnp.savetxt('maximums.txt', CXY)\n\nText: This code saves the array first as a binary file and then as a file you can open in a regular text editor. \nAPI:\nnumpy.savetxt\nnumpy.save\n","label":[[60,67,"Mention"],[122,126,"Mention"],[570,583,"API"],[584,594,"API"]],"Comments":[]}
{"id":59861,"text":"ID:25828097\nPost:\nText: Put all the data into one NumPy array, then call sum once: \nCode: arr.sum(axis=0)\n\nText: NumPy arrays are no faster than regular Python code when all you use it for is to access individual values item-by-item, as is done here: \nCode: c_ar[0] + d_ar[0] + e_ar[0] + m_ar[0] + p_ar[0]\n\nText: Also, for arrays this small, regular Python code may be faster than using NumPy arrays: \nCode: c = [1,1,1,1,0,0,0,1,1,1,1,0,0,0]\nd = [0,0,0,1,1,1,1,0,0,0,1,1,1,1]\ne = [0,0,0,1,1,1,1,0,0,0,1,1,1,1]\nm = [0,1,1,0,1,1,0,0,1,1,0,1,1,0]\np = [1,1,0,0,0,1,1,1,1,0,0,0,1,1]\narr = np.row_stack([c,d,e,m,p])\n\nCode: In [226]: %timeit c[0] + d[0] + e[0] + m[0] + p[0]\n10000000 loops, best of 3: 189 ns per loop\n\nIn [231]: %timeit arr[:,0].sum()\n100000 loops, best of 3: 4.73 s per loop\n\nIn [239]: %timeit [c[i] + d[i] + e[i] + m[i] + p[i] for i in range(len(c))]\n100000 loops, best of 3: 3.68 s per loop\n\nIn [240]: %timeit arr.sum(axis=0)\n100000 loops, best of 3: 5.04 s per loop\n\nAPI:\nnumpy.sum\n","label":[[73,76,"Mention"],[989,998,"API"]],"Comments":[]}
{"id":59862,"text":"ID:25875749\nPost:\nText: Multiple things are really wrong here. \nText: 1 - The problem with your data \nText: Look at them carefully: \nCode: 4814  2464  27  0.000627707861971  117923.0\n4211  736  2  4.64968786645  05  2576.0\n2075  1339  30  0.000697453179968  499822.0\n2441  2381  3  6.97453179968  05  1968.0\n4694  1738  1  2.32484393323  05  5702.0\n4406  3008  12  0.000278981271987  8483.0\n3622  1396  3  6.97453179968  05  2564.0\n5425  478  1  2.32484393323  05  428.0\n4489  1715  6  0.000139490635994  19045.0\n3695  3387  2  4.64968786645  05  16195.0\n\nText: Sometimes you got 6 columns as in: \nCode: 4211  736  2  4.64968786645  05  2576.0\n\nText: and sometimes you only got 5: \nCode: 4814  2464  27  0.000627707861971  117923.0\n\nText: So the first thing is to learn how to write data correctly. \nText: 2 - Write the data correctly \nText: Imagine that all you data are in a 2D numpy array called data. \nText: You could call: \nCode: numpy.savetxt(\"input.txt\", data)\n\nText: or, to get more control over formating: \nCode: numpy.savetxt(\"input.txt\", data, fmt=\"%d %d %d %.6f %d %.1f\")\n\nText: The fmt= parameter is a way to tell numpy how you want to save your data (%d means write it as an integer, %f means write it as a float, %.5f means write it as a float with only 5 decimals). \nText: If you want to write it yourself, you could do something like: \nCode: fmt = \"%d %d %d %.6f %d %.1f\"\nwith open(\"input.txt\", \"w\") as f:\n    for row in data:\n        f.write(fmt%row+\"\\n\")\n\nText: If the lines with 5 columns instead of 6 are what you really want to write, then use another delimiter like ,. This way, \nCode: 4814,2464,27,0.000627707861971,,117923.0\n\nText: is obviously containing 6 columns. \nText: 3 - Loading valid data \nText: What I call valid data is consistent data, data which always contains the same number of columns. \nText: You should really use np.loadtxt or genfromtxt (the latter one is it use if data are missing). Note that you can specify a delimiter for both of them using the delimiter argument. \nCode: data = numpy.loadtxt(\"valid_input.txt\")\ncol = data[:,2]\n\nText: or equivalently you could use the usecols argument together with the unpack one. \nText: 4 - Loading invalid data \nText: For your data, the method with usecols is working is you select only the third column (column 2 in Python lingua) if you don't have any other wrongness before column 2 elsewhere. \nText: You could do it by hand which would bring us to another wrongness: \nText: 5 - The problems with your first implementation \nText: There, you just replace the variable data with a single value (the one in cols[2]): \nCode: with open('input.txt', 'r') as f:\n    for rows in f:\n        cols = rows.split()\n        data = cols[2]\n\nText: There you try to sort a single value: \nCode:         sorted_data = np.sort(data)\n\nText: There you want to get the length of a single value: \nCode:         cdf = np.arange(len(data))\/float(len(data))\n        plt.plot(sorted_data, cdf, '-bs')\n\nplt.show()\n\nText: I'm really surprised numpy does not complain. \nText: You are getting one row at a time: you need to store these values somewhere (in a list for instance) and then do your stuff about it. \nText: 6 - The problem with your second implementation \nText: loadtxt can't load your data (it tries to load everything by default) because it can't infer what you want to do with 6 columns or 5 columns depending on the row. So the only thing it can do is failing. \nText: 7 - The problem with you \nText: First, don't get offended: what I'm gonna say is to help you improve. I'm not judging you in any way, just showing you how you should react in front of this kind of errors, trivial or not. \nText: Read the errors. Try to understand what's happening. Look for those errors on the internet. Ask someone. \nText: The problem is that you seem to have just copy-pasted the errors without having actually looked at them so without having tried to understand them (but I may be wrong, I'm not in your head :)). \nText: But what's for sure is that you have not copy-pasted them in your favorite search engine because answers are plenty. Again, I may be wrong. Maybe you did this but without seeing how these answers could apply to your case. Though, the first on Google answer about \nCode: ValueError: x and y must have same first dimension\n\nText: is pretty explicit. You don't even have to mention this is matplotlib or Python. Then you would have discovered that sorted_data is not the same length as cdf. With a little more work, you would have figured out what I said before about your implementations. \nText: 8 - Prove me I'm wrong \nText: As you've seen, I've not given a \"canonical answer\" and I won't since I consider that you have not done your part of the job. But you can still do it: I've given you all the tools you need to answer your own question. That don't mean that you have to do it all alone on a remote island: I've almost given a complete answer (really), the doc can help and Google too :). All you have to do is searching a tiny bit for it. Once you have something working, edit your question (or answer to your own question). \nAPI:\nnumpy.loadtxt\nnumpy.genfromtxt\nnumpy.loadtxt\n","label":[[1856,1866,"Mention"],[1870,1880,"Mention"],[3230,3237,"Mention"],[5117,5130,"API"],[5131,5147,"API"],[5148,5161,"API"]],"Comments":[]}
{"id":59863,"text":"ID:25895469\nPost:\nText: Take a closer look at the docstrings for np.random.randint and numpy.random.random. numpy.random.randint(5000) returns a single random integer between 0 and 4999 (inclusive). numpy.random.random(5000) returns an array of 5000 samples from the uniform distibution on 0 <= x < 1. \nAPI:\nnumpy.random.randint\n","label":[[65,82,"Mention"],[308,328,"API"]],"Comments":[]}
{"id":59864,"text":"ID:25922418\nPost:\nText: Numpy arrays use element-wise multiplication by default. Check out np.einsum and numpy.tensordot. I think what you're looking for is something like this: \nCode: results = np.einsum('ij,jkl->ikl',factor,input)\n\nAPI:\nnumpy.einsum\n","label":[[91,100,"Mention"],[239,251,"API"]],"Comments":[]}
{"id":59865,"text":"ID:25948444\nPost:\nText: Well, it's not much different but doing bulk operations on Numpy arrays is bound to have much less overhead: \nCode: import itertools\nimport numpy\n\nwhich = numpy.array(list(itertools.combinations(range(10), 2)))\ngrid = numpy.zeros((len(which), 10), dtype=\"int8\")\n\n# Magic\ngrid[numpy.arange(len(which))[None].T, which] = 1\n\ngrid\n#>>> array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n#>>>        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n#>>>        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n#>>>        [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n#>>>        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n#>>> ...\n\nText: The bulk of the time is then spent doing numpy.array(list(itertools.combinations(range(10), 2))). I tried using np.fromiter but I didn't get any speed improvements. Since half the time is literally generating the tuples, the only real way to improve further is to generate the combinations in something like C or Cython. \nAPI:\nnumpy.fromiter\n","label":[[699,710,"Mention"],[914,928,"API"]],"Comments":[]}
{"id":59866,"text":"ID:26038873\nPost:\nText: That's exactly what repeat does: \nCode: >>> a = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n>>> s = np.array([3, 3, 9, 3, 6, 3])\n>>> np.repeat(a, s)\narray([ 0.1,  0.1,  0.1,  0.2,  0.2,  0.2,  0.3,  0.3,  0.3,  0.3,  0.3,\n        0.3,  0.3,  0.3,  0.3,  0.4,  0.4,  0.4,  0.5,  0.5,  0.5,  0.5,\n        0.5,  0.5,  0.6,  0.6,  0.6])\n\nText: In pure Python you can do something like: \nCode: >>> from itertools import repeat, chain, imap\n>>> list(chain.from_iterable(imap(repeat, a, s)))\n[0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.4, 0.4, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6, 0.6, 0.6]\n\nText: But of course it is going to be way slower than its NumPy equivalent: \nCode: >>> s = [3, 3, 9, 3, 6, 3]*1000\n>>> a = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]*1000\n>>> %timeit list(chain.from_iterable(imap(repeat, a, s)))\n1000 loops, best of 3: 1.21 ms per loop\n>>> %timeit np.repeat(a_a, s_a) #a_a and s_a are NumPy arrays of same size as a and b\n10000 loops, best of 3: 202 s per loop\n\nAPI:\nnumpy.repeat\n","label":[[44,50,"Mention"],[1032,1044,"API"]],"Comments":[]}
{"id":59867,"text":"ID:26047775\nPost:\nText: If you can use numpy 1.9, you can use np.unique with the argument return_counts=True. I.e. \nCode: unique_items, counts = np.unique(x, return_counts=True)\n\nText: In fact, itemfreq was updated to use np.unique, but scipy currently supports numpy versions back to 1.5, so it doesn't use the return_counts argument. \nText: Here's the complete implementation of itemfreq in scipy 0.14: \nCode: def itemfreq(a):\n    items, inv = np.unique(a, return_inverse=True)\n    freq = np.bincount(inv)\n    return np.array([items, freq]).T\n\nAPI:\nnumpy.unique\n","label":[[62,71,"Mention"],[551,563,"API"]],"Comments":[]}
{"id":59868,"text":"ID:26065148\nPost:\nCode: In [22]: a\nOut[22]: array([[1, 2]])\nIn [23]: b\nOut[23]: array([[2, 3]])\nIn [24]: np.einsum('ij,ij->ij',a,b)\nOut[24]: array([[2, 6]])\nIn [29]: a*b\nOut[29]: array([[2, 6]])\n\nText: Here the repetition of the indices in all parts, including output, is interpreted as element by element multiplication. Nothing is summed. a[i,j]*b[i,j] = c[i,j] for all i,j. \nCode: In [25]: np.einsum('ij,ji->ij',a,b)\nOut[25]: \narray([[2, 4],\n       [3, 6]])\nIn [28]: np.dot(a.T,b).T\nOut[28]: \narray([[2, 4],\n       [3, 6]])\nIn [38]: np.outer(a,b)\nOut[38]: \narray([[2, 3],\n       [4, 6]])\n\nText: Again no summation because the same indices appear on left and right sides. a[i,j]*b[j,i] = c[i,j], in other words: \nCode: [[1*2, 2*2],\n [1*3, 2*3]]\n\nText: In effect an outer product. A look at how a is broadcasted against b.T might help: \nCode: In [69]: np.broadcast_arrays(a,b.T)\nOut[69]: \n[array([[1, 2],\n        [1, 2]]), \n array([[2, 2],\n        [3, 3]])]\n\nText: On the left side of the statement, repeated indices indicate which dimensions are multiplied. Matching left and right sides determines whether they are summed or not. \nCode: np.einsum('ij,ji->j',a,b) # array([ 5, 10]) sum on i only\nnp.einsum('ij,ji->i',a,b) # array([ 5, 10]) sum on j only\nnp.einsum('ij,ji',a,b) # 15 sum on i and j\n\nText: A while back I worked out a pure Python equivalent to einsum, with most of focus on how it parsed the string. The goal is the create an nditer with which it does a sum of products calculation. But it's not a trivial script to follow, even in Python: \nText: https:\/\/github.com\/hpaulj\/numpy-einsum\/blob\/master\/einsum_py.py \nText: A simpler sequence showing these summation rules: \nCode: In [53]: c=np.array([[1,2],[3,4]])\nIn [55]: np.einsum('ij',c)\nOut[55]: \narray([[1, 2],\n       [3, 4]])\nIn [56]: np.einsum('ij->i',c)\nOut[56]: array([3, 7])\nIn [57]: np.einsum('ij->j',c)\nOut[57]: array([4, 6])\nIn [58]: np.einsum('ij->',c)\nOut[58]: 10\n\nText: Using arrays that don't have a 1 dimension removes the broadcasting complication: \nCode: In [71]: b2=np.arange(1,7).reshape(2,3)\nIn [72]: np.einsum('ij,ji',a2,b2)\n...\nValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (2,3)->(2,3) (2,3)->(3,2) \n\nText: Or should I say, it exposes the attempted broadcasting. \nText: Ellipsis adds a level of complexity to the einsum interpretation. I developed the above mentioned github code when I solved a bug in the uses of .... But I didn't put much effort into refining the documentation. \nText: Ellipsis broadcasting in np.einsum \nText: The ellipses are most useful when you want an expression that can handle various sizes of arrays. If your arrays always 2D, it doesn't do anything extra. \nText: By way of example, consider a generalization of the dot, one that multiplies the last dimension of A with the 2nd to the last of B. With ellipsis we can write an expression that can handle a mix of 2d, 3D and larger arrays: \nCode: np.einsum('...ij,...jk',np.ones((2,3)),np.ones((3,4)))  # (2,4)\nnp.einsum('...ij,...jk',np.ones((5,2,3)),np.ones((3,4)))  # (5,2,4)\nnp.einsum('...ij,...jk',np.ones((5,2,3)),np.ones((5,3,4))) # (5,2,4)\nnp.einsum('...ij,...jk',np.ones((5,2,3)),np.ones((7,5,3,4)))  # (7,5,2,4)\nnp.einsum('...ij,...jk->...ik',np.ones((5,2,3)),np.ones((7,5,3,4)) # (7, 5, 2, 4)\n\nText: The last expression uses the default right hand side indexing ...ik, ellipsis plus the non-summing indices. \nText: Your original example could be written as \nCode: np.einsum('...j,j...->...j',a,b)\n\nText: Effectively it fills in the i (or more dimensions) to match the dimensions of the arrays. \nText: which would also work if a or b was 1d: \nCode: np.einsum('...j,j...->...j',a,b[0,:])\n\nText: np.dot way of generalizing to larger dimensions is different \nCode: dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n\nText: is expressed in einsum as: \nCode: np.einsum('ijo,kom->ijkm',np.ones((2,3,4)),np.ones((3,4,2)))\n\nText: which can be generalized with \nCode: np.einsum('...o,kom->...km',np.ones((4,)),np.ones((3,4,2)))\n\nText: or \nCode: np.einsum('ijo,...om->ij...m',np.ones((2,3,4)),np.ones((3,4,2)))\n\nText: But I don't think I can completely replicate it in einsum. That is, I can't tell it to fill in indices for A, followed by different ones for B. \nAPI:\nnumpy.einsum\n","label":[[2552,2561,"Mention"],[4277,4289,"API"]],"Comments":[]}
{"id":59869,"text":"ID:26086869\nPost:\nText: If you abandon names for a moment, this isn't too hard: \nCode: import numpy\n\na = numpy.arange(12).reshape(3,4)\nb = numpy.arange(20).reshape(4,5).T + 10\n\nb_rows = [0, 1, 0, 2, 1]\ncolumns_ids = numpy.array([False, True, False, True])\n\nb[:, columns_ids] = a[:, columns_ids][b_rows]\n\nb\n#>>> array([[10,  1, 20,  3],\n#>>>        [11,  5, 21,  7],\n#>>>        [12,  1, 22,  3],\n#>>>        [13,  9, 23, 11],\n#>>>        [14,  5, 24,  7]])\n\nText: If you really want an external array of names: \nCode: a = numpy.arange(12).reshape(3,4)\nb = numpy.arange(20).reshape(4,5).T + 10\n\na_rownames = numpy.array([\"m1\", \"m2\", \"m3\"])\nb_rownames = numpy.array([\"m1\", \"m2\", \"m1\", \"m3\", \"m2\"])\n\n_, b_rows = numpy.where(a_rownames == b_rownames[:, numpy.newaxis])\n\nb[:, columns_ids] = a[:, columns_ids][b_rows]\n\nb\n#>>> array([[10,  1, 20,  3],\n#>>>        [11,  5, 21,  7],\n#>>>        [12,  1, 22,  3],\n#>>>        [13,  9, 23, 11],\n#>>>        [14,  5, 24,  7]])\n\nText: This piece of magic has three steps: \nCode: _, b_rows = numpy.where(a_rownames == b_rownames[:, numpy.newaxis])\n\nText: First, we generate two perpendicular arrays: \nCode: a_rownames\n#>>> array(['m1', 'm2', 'm3'],\n#>>>       dtype='<U2')\n\nb_rownames[:, numpy.newaxis]\n#>>> array([['m1'],\n#>>>        ['m2'],\n#>>>        ['m1'],\n#>>>        ['m3'],\n#>>>        ['m2']],\n#>>>       dtype='<U2')\n\nText: Then our == comparison will use \"broadcasting\" to repeat these arrays until the dimensions match: \nCode: a_rownames == b_rownames[:, numpy.newaxis]\n#>>> array([[ True, False, False],\n#>>>        [False,  True, False],\n#>>>        [ True, False, False],\n#>>>        [False, False,  True],\n#>>>        [False,  True, False]], dtype=bool)\n\nText: np.where gives the xs and ys to index this array to get all the True values. We're only interested in the ys, so we ignore the xs. \nCode: _, b_rows = numpy.where(a_rownames == b_rownames[:, numpy.newaxis])\n\nText: The other piece of magic: \nCode: b[:, columns_ids] = a[:, columns_ids][b_rows]\n\nText: Requires knowing two different types of indexing: \nText: indexing with boolean arrays filters the array indexing with integer arrays gets the rows at those indexes \nText: Indexing like \nCode: array[xs]\n\nText: or \nCode: array[xs, :]\n\nText: (they're the same due to broadcasting) will give you all the indexes that match due to the rules above, filtered in the first axis. \nText: Using \nCode: array[:, ys]\n\nText: will filter in the second axis. \nText: So first we filter the columns (second axis) \nCode: b[:, columns_ids]\n#>>> array([[ 1,  3],\n#>>>        [ 5,  7],\n#>>>        [ 1,  3],\n#>>>        [ 9, 11],\n#>>>        [ 5,  7]])\n\na[:, columns_ids]\n#>>> array([[ 1,  3],\n#>>>        [ 5,  7],\n#>>>        [ 9, 11]])\n\nText: Then we filter the rows on a (first axis): \nCode: a[:, columns_ids][b_rows]\n#>>> array([[ 1,  3],\n#>>>        [ 5,  7],\n#>>>        [ 1,  3],\n#>>>        [ 9, 11],\n#>>>        [ 5,  7]])\n\nText: They are now the same shape, so you can do slice assignment: \nCode: b[:, columns_ids] = a[:, columns_ids][b_rows]\n\nAPI:\nnumpy.where\n","label":[[1715,1723,"Mention"],[3052,3063,"API"]],"Comments":[]}
{"id":59870,"text":"ID:26170590\nPost:\nText: You need import numpy not from numpy import * \nText: You cannot call add if you have not imported numpy. \nText: If you used from numpy import * you would use add(array1, array2) but the best way is to use: \nCode: import numpy as np\n\nnp.add()\n\nText: from numpy import * imports all the methods into your current namespace which is not a good idea as there are builtin and numpy methods with the same names and you can end up getting different outputs to what you might expect. \nCode: In [1]: import numpy as np\n\nIn [2]: np.__version__\nOut[2]: '1.9.0'\n\nIn [3]: np.add(1,2)\nOut[3]: 3\n\nAPI:\nnumpy.add\n","label":[[93,96,"Mention"],[611,620,"API"]],"Comments":[]}
{"id":59871,"text":"ID:26172737\nPost:\nText: You can convert bigbuf to an array with fromstring \nText: For example: \nCode: In [21]: bigbuf = \"\\1\\0\\0\\0\\2\\0\\0\\0\"\n\nIn [22]: fromstring(bigbuf, dtype=np.int32)\nOut[22]: array([1, 2], dtype=int32)\n\nAPI:\nnumpy.fromstring\n","label":[[64,74,"Mention"],[226,242,"API"]],"Comments":[]}
{"id":59872,"text":"ID:26179873\nPost:\nText: You could use all and index broadcasting for this \nCode: filter_matrix = np.array(filterColumns)\ncombination_array = np.array(combination)\nbool_matrix = filter_matrix == combination_array[newaxis, :]   #not sure of the newaxis position\nsubset = raw_data[bool_matrix]\n\nText: There are however simpler ways of doing the same thing if your filters are within the matrix, notably through numpy argsort and numpy roll over an axis. First you roll axes until your axes until you've ordered your filters as first columns, then you sort on them and slice the array vertically to get the rest of the matrix. \nText: In general if an for loop can be avoided in Python, better avoid it. \nText: Update: \nText: Here is the full code without a for loop: \nCode: import numpy as np\n\n# select filtering indexes\nfilter_indexes = [1, 3]\n# generate the test data\nraw_data = np.random.randint(0, 4, size=(50,5))\n\n\n# create a column that we would use for indexing\nindex_columns = raw_data[:, filter_indexes]\n\n# sort the index columns by lexigraphic order over all the indexing columns\nargsorts = np.lexsort(index_columns.T)\n\n# sort both the index and the data column\nsorted_index = index_columns[argsorts, :]\nsorted_data = raw_data[argsorts, :]\n\n# in each indexing column, find if number in row and row-1 are identical\n# then group to check if all numbers in corresponding positions in row and row-1 are identical\nautocorrelation = np.all(sorted_index[1:, :] == sorted_index[:-1, :], axis=1)\n\n# find out the breakpoints: these are the positions where row and row-1 are not identical\nbreakpoints = np.nonzero(np.logical_not(autocorrelation))[0]+1\n\n# finally find the desired subsets \nsubsets = np.split(sorted_data, breakpoints)\n\nText: An alternative implementation would be to transform the indexing matrix into a string matrix, sum row-wise, get an argsort over the now unique indexing column and split as above. \nText: For conveniece, it might be more interesting to first roll the indexing matrix until they are all in the beginning of the matrix, so that the sorting done above is clear. \nAPI:\nnumpy.all\n","label":[[38,41,"Mention"],[2099,2108,"API"]],"Comments":[]}
{"id":59873,"text":"ID:26213437\nPost:\nText: As I understand, vectorize is essentially a Python loop and therefor very inefficient. The high memory consumption that you see is likely caused by it. \nText: The way you're splitting this array is very regular, so just slice it: \nCode: tmp1 = big_matrix[:,  ::2, ...]\ntmp2 = big_matrix[:, 1::2, ...]\n\nText: This creates \"views\" to the original array and as a result doesn't require additional memory. \nText: Looking at the answers here, a simple way to construct the complex array is: \nCode: final_matrix = tmp1 + 1j * tmp2\n\nText: Or more memory efficient: \nCode: final_matrix = 1j * tmp2\nfinal_matrix += tmp1\n\nText: If you're only interested in the overall total, you could also separately sum the real and imaginary parts and combine them in the end. \nAPI:\nnumpy.vectorize\n","label":[[41,50,"Mention"],[784,799,"API"]],"Comments":[]}
{"id":59874,"text":"ID:26284774\nPost:\nText: The size of the native datatype differs from the size of the string representation of the datatype. \nText: savetxt has a fmt argument that defaults to '%.18e', which formats each of your zeros as 0.000000000000000000e+00. That is 24 characters per item plus one for the delimiter. \nText: To get a smaller file you can change the format (beware of losing significant digits) or use np.save to save in binary or np.savez to save as a compressed archive. \nAPI:\nnumpy.savetxt\nnumpy.save\nnumpy.savez\n","label":[[131,138,"Mention"],[405,412,"Mention"],[434,442,"Mention"],[482,495,"API"],[496,506,"API"],[507,518,"API"]],"Comments":[]}
{"id":59875,"text":"ID:26309616\nPost:\nText: The problem is that np.sum doesn't know how to handle sparse matrices. The following works as expected: \nCode: coomatrix.sum()\n\nAPI:\nnumpy.sum\n","label":[[44,50,"Mention"],[157,166,"API"]],"Comments":[]}
{"id":59876,"text":"ID:26334263\nPost:\nText: Yes, memory is the problem \nText: Your estimate also needs to take into account a memory-allocation already done for the list-representation of the 180000 x 7680 x float32, so without other details on dynamic memory-releases \/ garbage-collections, the numpy.asarray() method needs a bit more than just another space of 1800000 x 7680 x np.float32 bytes. \nText: If you try to test with less than a third length of the list, you may inspect the resulting effective-overhead of the array data-representation, so as to have exact data for your memory-feasible design \nText: Memory-profiling may help to point out the bottleneck and understand the code requirements, that may sometimes help to save half of the allocation space needed for data, compared to an original mode of data-flow and operations: (Fig.: Courtesy scikit-learn testing numpy-based or BLAS-direct calling method impact on memory-allocation envelopes ) \nAPI:\nnumpy.float32\nnumpy.array\n","label":[[360,370,"Mention"],[503,508,"Mention"],[947,960,"API"],[961,972,"API"]],"Comments":[]}
{"id":59877,"text":"ID:26413862\nPost:\nText: Without seeing the rest of your code, the most likely candidate is sum. \nText: In vanilla python (as when your script runs), sum is the python built-in function, which doesn't know about numpy arrays. In IPython's Pylab mode (as at Canopy's ipython prompt), which implicitly starts with from numpy import * (terribly confusing, and one reason that the IPython team is now discouraging use of their Pylab mode, which I would guess Canopy will follow before long), sum is the numpy function, which behaves quite differently. \nText: Look up both of these sum functions, and try using sum instead of sum in your script. \nText: For more context: \nText: @Senderle's first comment on your question points to the more general explanation of almost all such discrepancies in ipython -- when you run a script, it doesn't know any of the values in your global ipython namespace. As the script runs, by default, its global namespace is inserted into your ipython namespace, but not the other way around. So Ipython commands can inspect the results of a running script, but a running script cannot see\/use the values (including imports) that were previously defined at the IPython prompt (unless they were explicitly passed to the running script). \nText: The most common example of this is the one described in this article: Modules are already available in Canopy's Python (PyLab) prompt, but not in a script, but it also applies to data values as senderle was pointing to. \nText: So to return to your problem -- 100-to-1 odds that you (or Pylab startup) have defined something at the IPython prompt which is not defined in the same way in the running script, and that accounts for the discrepancy. If it's not sum, then I suggest that you narrow it down to the simplest possible case (just a few lines), and then it should jump out at you; or if not, you can post it here and it will jump out at someone else. \nAPI:\nnumpy.sum\n","label":[[605,608,"Mention"],[1929,1938,"API"]],"Comments":[]}
{"id":59878,"text":"ID:26421159\nPost:\nText: Consider this code \nCode: >>> import numpy as np\n>>> np.identity(5)\narray([[ 1.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.]])\n>>> np.identity(5)+np.ones([5,5])\narray([[ 2.,  1.,  1.,  1.,  1.],\n       [ 1.,  2.,  1.,  1.,  1.],\n       [ 1.,  1.,  2.,  1.,  1.],\n       [ 1.,  1.,  1.,  2.,  1.],\n       [ 1.,  1.,  1.,  1.,  2.]])\n>>> np.identity(5) == np.identity(5)+np.ones([5,5])\narray([[False, False, False, False, False],\n       [False, False, False, False, False],\n       [False, False, False, False, False],\n       [False, False, False, False, False],\n       [False, False, False, False, False]], dtype=bool)\n>>> \n\nText: Note the the result of the comparison is a matrix, not a boolean value. Dict comparisons will compare values using the values cmp methods, which means that when comparing matrix values, the dict comparison will get a composite result. What you want to do is use all to collapse the composite array result into a scalar boolean result \nCode: >>> np.all(np.identity(5) == np.identity(5)+np.ones([5,5]))\nFalse\n>>> np.all(np.identity(5) == np.identity(5))\nTrue\n>>> \n\nText: You would need to write your own function to compare these dictionaries, testing value types to see if they are matricies, and then comparing using numpy.all, otherwise using ==. Of course, you can always get fancy and start subclassing dict and overloading cmp if you want too. \nAPI:\nnumpy.all\n","label":[[1027,1030,"Mention"],[1519,1528,"API"]],"Comments":[]}
{"id":59879,"text":"ID:26456194\nPost:\nText: The numbers returned by rnd.rand will be between 0 and 1. Knowing that, you can just multiply the result to the given range: \nCode: # 0 to 0.001\nA = numpy.random.rand(2,3) * 0.01\n\n# 0.75 to 1.5\nmin = 0.75\nmax = 1.5\nA = ( numpy.random.rand(2,3) * (max - min) ) + min\n\nText: Or as DSM suggested: \nCode: A = numpy.random.uniform(low=0.75, high=1.5, size=(2,3) )\n\nAPI:\nnumpy.random.rand\n","label":[[48,56,"Mention"],[389,406,"API"]],"Comments":[]}
{"id":59880,"text":"ID:26457443\nPost:\nText: Assuming your image is stored as a ndarray (Test this with print type(test))... \nText: Your image will be represented by an NxMx3 array. Basically this means you have a N by M image with a color depth of 3- your RGB values. Taking the mean of those 3 will leave you with an NxMx1 array, where the 1 is now the average intensity. Numpy does this very well: \nCode: test = test.mean(2)\n\nText: The parameter given, 2, specifies the dimension to take the mean along. It could be either 0, 1, or 2, because your image matrix is 3 dimensional. This should return an NxM array. You basically will be left with a gray-scale, (color depth of 1) image. Try to show the value that gets returned! If you get Nx3 or Mx3, you know you have just taken the average along the wrong axis. Note that you can check the dimensions of a numpy array with: \nCode: test.shape\n\nText: Shape will be a tuple describing the dimensions of your image. \nAPI:\nnumpy.ndarray\n","label":[[59,66,"Mention"],[950,963,"API"]],"Comments":[]}
{"id":59881,"text":"ID:26473011\nPost:\nText: You can use np.resize on a first and then add b's items at the required indices using insert on the re-sized array: \nCode: In [101]: a = np.arange(1, 4)\n\nIn [102]: b = np.arange(4, 6)                                           \n\nIn [103]: np.insert(np.resize(a, (b.shape[0], a.shape[0])), 2, b, axis=1)                                                                       \nOut[103]: \narray([[1, 2, 4, 3],                                                    \n       [1, 2, 5, 3]])  \n\nAPI:\nnumpy.resize\nnumpy.insert\n","label":[[36,45,"Mention"],[110,116,"Mention"],[511,523,"API"],[524,536,"API"]],"Comments":[]}
{"id":59882,"text":"ID:26475523\nPost:\nText: Only the first part of your question \nCode: from numpy import *\n\nx = array([[ 0.46006547,  0.5580928 ,  0.70164242,  0.84519205,  1.4       ],\n           [ 0.00912908,  0.00912908,  0.05      ,  0.05      ,  0.05      ]])\n\nlow, high = array([ 0.633,  0.01 ]), array([ 1.325,  0.99 ])\n\n# construct an array of two rows of bools expressing your conditions\nindices1 = array((x[0,:]<low[0], x[1,:]<low[1]))\nprint indices1\n\n# do an or of the values along the first axis\nindices1 = any(indices1, axis=0)\n# now it's a single row array\nprint indices1\n\n# use the indices1 to extract what you want,\n# the double transposition because the elements\n# of a 2d array are  the rows\ntmp1 = x.T[indices1].T\nprint tmp1\n\n# [[ True  True False False False]\n#  [ True  True False False False]]\n# [ True  True False False False]\n# [[ 0.46006547  0.5580928 ]\n#  [ 0.00912908  0.00912908]]\n\nText: next construct similarly indices2 and tmp2, the indices of the remnant are the negation of the oring of the first two indices. (i.e., numpy.logical_not(numpy.logical_or(i1,i2))). \nText: Addendum \nText: Another approach, possibly faster if you have thousands of entries, implies searchsorted \nCode: from numpy import *\n\nx = array([[ 0.46006547,  0.5580928 ,  0.70164242,  0.84519205,  1.4       ],\n           [ 0.00912908,  0.00912908,  0.05      ,  0.05      ,  0.05      ]])\n\nlow, high = array([ 0.633,  0.01 ]), array([ 1.325,  0.99 ])\n\nl0r = searchsorted(x[0,:], low[0], side='right')\nl1r = searchsorted(x[1,:], low[1], side='right')\n\nh0l = searchsorted(x[0,:], high[0], side='left')\nh1l = searchsorted(x[1,:], high[1], side='left')\n\nlr = max(l0r, l1r)\nhl = min(h0l, h1l)\n\nprint lr, hl\nprint x[:,:lr]\nprint x[:,lr:hl]\nprint x[:,hl]\n\n# 2 4\n# [[ 0.46006547  0.5580928 ]\n#  [ 0.00912908  0.00912908]]\n# [[ 0.70164242  0.84519205]\n#  [ 0.05        0.05      ]]\n# [ 1.4   0.05]\n\nText: Excluding overlaps can be obtained by hl = max(lr, hl). NB in previuos approach the array slices are copied to new objects, here you get views on x and you have to be explicit if you want new objects. \nText: Edit An unnecessary optimization \nText: If we use only the upper part of x in the second couple of sortedsearches (if you look at the code you'll see what I mean...) we get two benefits, 1) a very small speedup of the searches (sortedsearch is always fast enough) and 2) the case of overlap is automatically managed. \nText: As a bonus, code for copying the segments of x in the new arrays. NB x was changed to force overlap \nCode: from numpy import *\n\n# I changed x to force overlap\nx = array([[ 0.46006547,  1.4 ,        1.4,   1.4,  1.4       ],\n           [ 0.00912908,  0.00912908,  0.05,  0.05, 0.05      ]])\n\nlow, high = array([ 0.633,  0.01 ]), array([ 1.325,  0.99 ])\n\nl0r = searchsorted(x[0,:], low[0], side='right')\nl1r = searchsorted(x[1,:], low[1], side='right')\nlr = max(l0r, l1r)\n\nh0l = searchsorted(x[0,lr:], high[0], side='left')\nh1l = searchsorted(x[1,lr:], high[1], side='left')\n\nhl = min(h0l, h1l) + lr\n\nt1 = x[:,range(lr)]\nxn = x[:,range(lr,hl)]\nncol = shape(x)[1]\nt2 = x[:,range(hl,ncol)]\n\nprint x\ndel(x)\nprint\nprint t1\nprint\n# note that xn is a void array \nprint xn\nprint\nprint t2\n\n# [[ 0.46006547  1.4         1.4         1.4         1.4       ]\n#  [ 0.00912908  0.00912908  0.05        0.05        0.05      ]]\n# \n# [[ 0.46006547  1.4       ]\n#  [ 0.00912908  0.00912908]]\n# \n# []\n# \n# [[ 1.4   1.4   1.4 ]\n#  [ 0.05  0.05  0.05]]\n\nAPI:\nnumpy.searchsorted\n","label":[[1175,1187,"Mention"],[3449,3467,"API"]],"Comments":[]}
{"id":59883,"text":"ID:26484230\nPost:\nText: Looking at the source, it appears that loadtxt contains a lot of code to handle many different formats. In case you have a well defined input file, it is not too difficult to write your own function optimized for your particular file format. Something like this (untested): \nCode: def load_big_file(fname):\n    '''only works for well-formed text file of space-separated doubles'''\n\n    rows = []  # unknown number of lines, so use list\n    with open(fname) as f:\n        for line in f:\n            line = [float(s) for s in line.split()]\n            rows.append(np.array(line, dtype = np.double))\n    return np.vstack(rows)  # convert list of vectors to array\n\nText: An alternative solution, if the number of rows and columns is known before, might be: \nCode: def load_known_size(fname, nrow, ncol)\n    x = np.empty((nrow, ncol), dtype = np.double)\n    with open(fname) as f:\n        for irow, line in enumerate(f):\n            for icol, s in enumerate(line.split()):\n                x[irow, icol] = float(s)\n    return x\n\nText: In this way, you don't have to allocate all the intermediate lists. \nText: EDIT: Seems that the second solution is a bit slower, the list comprehension is probably faster than the explicit for loop. Combining the two solutions, and using the trick that Numpy does implicit conversion from string to float (only discovered that just now), this might possibly be faster: \nCode: def load_known_size(fname, nrow, ncol)\n    x = np.empty((nrow, ncol), dtype = np.double)\n    with open(fname) as f:\n        for irow, line in enumerate(f):\n            x[irow, :] = line.split()\n    return x\n\nText: To get any further speedup, you would probably have to use some code written in C or Cython. I would be interested to know how much time these functions take to open your files. \nAPI:\nnumpy.loadtxt\n","label":[[63,70,"Mention"],[1827,1840,"API"]],"Comments":[]}
{"id":59884,"text":"ID:26562070\nPost:\nText: On Windows a 32 bit process is only given a maximum of 2GB (or GiB?) memory and loadtxt is notorious for being heavy on memory, so that explains why the first approach doesn't work. \nText: The second problem you appear to be facing is that the particular file you are testing with has missing data, i.e. not all lines have the same number of values. This is easy to check, for example: \nCode: import numpy as np\n\nnumbers_per_line = []\nwith open(filename) as infile:\n    for line in infile:\n        numbers_per_line.append(line.count(delimiter) + 1)\n\n# Check where there might be problems\nnumbers_per_line = np.array(numbers_per_line)\nexpected_number = 100\nprint np.where(numbers_per_line != expected_number)\n\nAPI:\nnumpy.loadtxt\n","label":[[104,111,"Mention"],[738,751,"API"]],"Comments":[]}
{"id":59885,"text":"ID:26582411\nPost:\nText: Earlier this year I created a wrapper for odeint that makes it easy to solve complex array differential equations: https:\/\/github.com\/WarrenWeckesser\/odeintw \nText: You can check out the whole package using git and install it using the script setup.py, or you can grab the one essential file, _odeintw.py, rename it to odeintw.py, and copy it to wherever you need it. The following script uses the function odeintw.odeintw to solve your system. It uses odeintw by stacking your three matrices A, B and C into a three-dimensional array M with shape (3, 2, 2). \nText: You can use numpy.linalg.eigvals to compute the eigenvalues of A(t). The script shows an example and a plot. The eigenvalues are complex, so you might have to experiment a bit to find a nice way to visualize them. \nCode: import numpy as np\nfrom numpy import linspace, array\nfrom odeintw import odeintw\nimport matplotlib.pyplot as plt\n\n\ndef system(M, t):\n    A, B, C = M\n    dA_dt = A.dot(C) + B.dot(C)\n    dB_dt = B.dot(C)\n    dC_dt = C\n    return array([dA_dt, dB_dt, dC_dt])\n\n\nt = np.linspace(0, 1.5, 1000)\n\n#A_initial= [1, 2, 2.3, 4.3, 2.1, 5.2, 2.13, 3.43]\nA_initial = np.array([[1 + 2.1j, 2 + 5.2j], [2.3 + 2.13j, 4.3 + 3.43j]])\n\n# B_initial= [7, 2.7, 1.23, 3.3, 3.1, 5.12, 1.13, 3]\nB_initial = np.array([[7 + 3.1j, 2.7 + 5.12j], [1.23 + 1.13j, 3.3 + 3j]])\n\n# C_initial= [0.5, 0.9, 0.63, 0.43, 0.21, 0.5, 0.11, 0.3]\nC_initial = np.array([[0.5 + 0.21j, 0.9 + 0.5j], [0.63 + 0.11j, 0.43 + 0.3j]])\n\nM_initial = np.array([A_initial, B_initial, C_initial])\nsol = odeintw(system, M_initial, t)\n\nA = sol[:, 0, :, :]\nB = sol[:, 1, :, :]\nC = sol[:, 2, :, :]\n\nplt.figure(1)\nplt.plot(t, A[:, 0, 0].real, label='A(t)[0,0].real')\nplt.plot(t, A[:, 0, 0].imag, label='A(t)[0,0].imag')\nplt.legend(loc='best')\nplt.grid(True)\nplt.xlabel('t')\n\nA_evals = np.linalg.eigvals(A)\n\nplt.figure(2)\nplt.plot(t, A_evals[:,0].real, 'b.', markersize=3, mec='b')\nplt.plot(t, A_evals[:,0].imag, 'r.', markersize=3, mec='r')\nplt.plot(t, A_evals[:,1].real, 'b.', markersize=3, mec='b')\nplt.plot(t, A_evals[:,1].imag, 'r.', markersize=3, mec='r')\nplt.ylim(-200, 1200)\nplt.grid(True)\nplt.title('Real and imaginary parts of the eigenvalues of A(t)')\nplt.xlabel('t')\nplt.show()\n\nText: Here are the plots generated by the script: \nAPI:\nscipy.integrate.odeint\n","label":[[66,72,"Mention"],[2291,2313,"API"]],"Comments":[]}
{"id":59886,"text":"ID:26604096\nPost:\nText: You can use argpartition on flattened version of array first to get the indices of top k items, and then you can convert those 1D indices as per the array's shape using numpy.unravel_index: \nCode: >>> arr = np.arange(100*100*100).reshape(100, 100, 100)\n>>> np.random.shuffle(arr)\n>>> indices =  np.argpartition(arr.flatten(), -2)[-2:]\n>>> np.vstack(np.unravel_index(indices, arr.shape)).T\narray([[97, 99, 98],\n       [97, 99, 99]])\n)\n>>> arr[97][99][98]\n999998\n>>> arr[97][99][99]\n999999\n\nAPI:\nnumpy.argpartition\n","label":[[36,48,"Mention"],[518,536,"API"]],"Comments":[]}
{"id":59887,"text":"ID:26695200\nPost:\nText: One can register a custom from-python converter with Boost.Python that handles conversions from NumPy array scalars, such as numpy.uint8, to C++ scalars, such as unsigned char. A custom from-python converter registration has three parts: \nText: A function that checks if a PyObject is convertible. A return of NULL indicates that the PyObject cannot use the registered converter. A construct function that constructs the C++ type from a PyObject. This function will only be called if converter(PyObject) does not return NULL. The C++ type that will be constructed. \nText: Extracting the value from the NumPy array scalar requires a few NumPy C API calls: \nText: import_array() must be called within the initialization of an extension module that is going to use the NumPy C API. Depending on how the extension(s) are using the NumPy C API, other requirements for importing may need to occur. PyArray_CheckScalar() checks if a PyObject is a NumPy array scalar. PyArray_DescrFromScalar() gets the data-type-descriptor object for an array scalar. The data-type-descriptor object contains information about how to interpret the underlying bytes. For example, its type_num data member contains an enum value that corresponds to a C-type. PyArray_ScalarAsCtype() can be used to extract the C-type value from a NumPy array scalar. \nText: Here is a complete example demonstrating using a helper class, enable_numpy_scalar_converter, to register specific NumPy array scalars to their corresponding C++ types. \nCode: #include <boost\/cstdint.hpp>\n#include <boost\/python.hpp>\n#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n#include <numpy\/arrayobject.h>\n\n\/\/ Mockup functions.\n\n\/\/\/ @brief Mockup function that will explicitly extract a uint8_t\n\/\/\/        from the Boost.Python object.\nboost::uint8_t test_generic_uint8(boost::python::object object)\n{\n  return boost::python::extract<boost::uint8_t>(object)();\n}\n\n\/\/\/ @brief Mockup function that uses automatic conversions for uint8_t.\nboost::uint8_t test_specific_uint8(boost::uint8_t value) { return value; }\n\n\/\/\/ @brief Mokcup function that uses automatic conversions for int32_t.\nboost::int32_t test_specific_int32(boost::int32_t value) { return value; }\n\n\n\/\/\/ @brief Converter type that enables automatic conversions between NumPy\n\/\/\/        scalars and C++ types.\ntemplate <typename T, NPY_TYPES NumPyScalarType>\nstruct enable_numpy_scalar_converter\n{\n  enable_numpy_scalar_converter()\n  {\n    \/\/ Required NumPy call in order to use the NumPy C API within another\n    \/\/ extension module.\n    import_array();\n\n    boost::python::converter::registry::push_back(\n      &convertible,\n      &construct,\n      boost::python::type_id<T>());\n  }\n\n  static void* convertible(PyObject* object)\n  {\n    \/\/ The object is convertible if all of the following are true:\n    \/\/ - is a valid object.\n    \/\/ - is a numpy array scalar.\n    \/\/ - its descriptor type matches the type for this converter.\n    return (\n      object &&                                                    \/\/ Valid\n      PyArray_CheckScalar(object) &&                               \/\/ Scalar\n      PyArray_DescrFromScalar(object)->type_num == NumPyScalarType \/\/ Match\n    )\n      ? object \/\/ The Python object can be converted.\n      : NULL;\n  }\n\n  static void construct(\n    PyObject* object,\n    boost::python::converter::rvalue_from_python_stage1_data* data)\n  {\n    \/\/ Obtain a handle to the memory block that the converter has allocated\n    \/\/ for the C++ type.\n    namespace python = boost::python;\n    typedef python::converter::rvalue_from_python_storage<T> storage_type;\n    void* storage = reinterpret_cast<storage_type*>(data)->storage.bytes;\n\n    \/\/ Extract the array scalar type directly into the storage.\n    PyArray_ScalarAsCtype(object, storage);\n\n    \/\/ Set convertible to indicate success. \n    data->convertible = storage;\n  }\n};\n\nBOOST_PYTHON_MODULE(example)\n{\n  namespace python = boost::python;\n\n  \/\/ Enable numpy scalar conversions.\n  enable_numpy_scalar_converter<boost::uint8_t, NPY_UBYTE>();\n  enable_numpy_scalar_converter<boost::int32_t, NPY_INT>();\n\n  \/\/ Expose test functions.\n  python::def(\"test_generic_uint8\",  &test_generic_uint8);\n  python::def(\"test_specific_uint8\", &test_specific_uint8);\n  python::def(\"test_specific_int32\", &test_specific_int32);\n}\n\nText: Interactive usage: \nCode: >>> import numpy\n>>> import example\n>>> assert(42 == example.test_generic_uint8(42))\n>>> assert(42 == example.test_generic_uint8(numpy.uint8(42)))\n>>> assert(42 == example.test_specific_uint8(42))\n>>> assert(42 == example.test_specific_uint8(numpy.uint8(42)))\n>>> assert(42 == example.test_specific_int32(numpy.int32(42)))\n>>> example.test_specific_int32(numpy.int8(42))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nBoost.Python.ArgumentError: Python argument types in\n    example.test_specific_int32(numpy.int8)\ndid not match C++ signature:\n    test_specific_int32(int)\n>>> example.test_generic_uint8(numpy.int8(42))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: No registered converter was able to produce a C++ rvalue of type\n  unsigned char from this Python object of type numpy.int8\n\nText: A few things to note from the interactive usage: \nText: Boost.Python was able to extract boost::uint8_t from both np.uint8 and int Python objects. The enable_numpy_scalar_converter does not support promotions. For instance, it should be safe for test_specific_int32() to accept a int8 object that is promoted to a larger scalar type, such as int. If one wishes to perform promotions: convertible() will need to check for compatible NPY_TYPES construct() should use PyArray_CastScalarToCtype() to cast the extracted array scalar value to the desired C++ type. \nAPI:\nnumpy.uint8\nnumpy.int8\n","label":[[5323,5331,"Mention"],[5489,5493,"Mention"],[5774,5785,"API"],[5786,5796,"API"]],"Comments":[]}
{"id":59888,"text":"ID:26744493\nPost:\nText: The error occurs because you cannot determine whether a complete array is True or False. What would be the boolean state of an array where all elements are True but one? \nText: min takes an iterable for an argument and compares each element to the other, each comparison results in a boolean value. Iterating over a 1-d numpy array produces individual elements - min works for a 1-d numpy array. \nCode: >>> a\narray([-4, -3, -2, -1,  0,  1,  2,  3,  4])\n>>> for thing in a:\n     print thing,\n\n\n-4 -3 -2 -1 0 1 2 3 4\n>>> min(a)\n-4\n>>>\n\nText: Iterating over a 2-d numpy array produces rows. \nCode: >>> b\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\n>>> for thing in b:\n     print thing\n\n[-4 -3 -2]\n[-1  0  1]\n[2 3 4]\n>>> \n\nText: min won't won't work for 2-d arrays because it is comparing arrays and - The truth value of an array with more than one element is ambiguous. \nCode: >>> c\narray([0, 1, 2, 3])\n>>> c < 2\narray([ True,  True, False, False], dtype=bool)\n>>> bool(c < 2)\n\nTraceback (most recent call last):\n  File \"<pyshell#74>\", line 1, in <module>\n    bool(c < 2)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n>>> \n>>> bool(np.array((True, True)))\n\nTraceback (most recent call last):\n  File \"<pyshell#75>\", line 1, in <module>\n    bool(np.array((True, True)))\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n>>> \n>>> bool(np.array((True, False)))\n\nTraceback (most recent call last):\n  File \"<pyshell#76>\", line 1, in <module>\n    bool(np.array((True, False)))\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n>>> \n\nText: If you need to find the element with the minimum value, use agmin or the ndarray.min method. \nCode: >>> \n>>> np.amin(b)\n-4\n>>> b.min()\n-4\n>>> \n\nAPI:\nnumpy.amin\n","label":[[1799,1804,"Mention"],[1888,1898,"API"]],"Comments":[]}
{"id":59889,"text":"ID:26799683\nPost:\nText: Short Answer \nText: The relation between w and the 1 errors in polyfit is \nText: w = 1\/sigma \nText: which is different from what everybody will expect. \nText: Edit: following my comment on Github, the np.polyfit 1.16 documentation now explicitly states \"use 1\/sigma (not 1\/sigma**2)\" to avoid people thinking there is a typo in the formula. \nText: Explanation \nText: In least-squares fitting one generally defines the weights vector in such a way that the fit minimizes the squared error (see e.g. Wikipedia or NIST) \nText: chi2 = np.sum(weights*(p(x) - y)**2) \nText: In the common situation where the 1 errors (standard deviations) \"sigma\" are known one has the familiar textbook relation where the weights are the reciprocal of the variance \nText: weights = 1\/sigma**2 \nText: However the np.polyfit documentation defines the weight as \"weights to apply to the y-coordinates\". This definition is not quite correct. The weights apply to the fit residuals, not only to the y-coordinates. \nText: More importantly, looking at the math in the Numpy (v1.9.1) code it appears that the Numpy code solves the linear problem below in the last-squares sense, where the w vector does indeed multiply the y-coordinate \nText: (vander*w[:, np.newaxis]).dot(x) == y*w \nText: But solving the above array expression in the least-squares sense is equivalent to minimizing the expression below with w inside the parenthesis \nText: chi2 = np.sum((w*(vander.dot(x) - y))**2) \nText: Or, using the notation of the Numpy documentation \nText: chi2 = np.sum((w*(p(x) - y))**2) \nText: in such a way that the relation between w and the 1 errors is \nText: w = 1\/sigma \nText: which is different from what everybody will expect. \nAPI:\nnumpy.polyfit\nnumpy.polyfit\nnumpy.polyfit\n","label":[[88,95,"Mention"],[226,236,"Mention"],[816,826,"Mention"],[1731,1744,"API"],[1745,1758,"API"],[1759,1772,"API"]],"Comments":[]}
{"id":59890,"text":"ID:26856448\nPost:\nText: reshape will copy the data if it can't make a proper view, whereas setting the shape will raise an error instead of copying the data. \nText: It is not always possible to change the shape of an array without copying the data. If you want an error to be raise if the data is copied, you should assign the new shape to the shape attribute of the array. \nAPI:\nnumpy.reshape\n","label":[[24,31,"Mention"],[380,393,"API"]],"Comments":[]}
{"id":59891,"text":"ID:26901037\nPost:\nText: In previous releases (0.7.5 and prior), ufuncify only worked on single dimension arrays for the first argument (not very exciting). As of 0.7.6 (not released yet, but should be in a week!) ufuncify creates actual instances of np.ufunc by default (wraps C code in numpy api). Your code above only needs a small change to make it work. \nCode: In [1]: import numpy as np\n\nIn [2]: from sympy import sin, cos, lambdify\n\nIn [3]: from sympy.abc import x,y,z\n\nIn [4]: from sympy.utilities.autowrap import ufuncify\n\nIn [5]: from sympy.printing.theanocode import theano_function\n\nIn [6]: xg, yg, zg = np.mgrid[0:1:50*1j, 0:1:50*1j, 0:1:50*1j]\n\nIn [7]: f  = sym.sin(x)*sym.cos(y)*sym.sin(z)\n\nIn [8]: ufunc_f = ufuncify([x,y,z], f)\n\nIn [9]: theano_f = theano_function([x, y, z], f, dims={x: 3, y: 3, z: 3})\n\nIn [10]: lambda_f = lambdify([x, y, z], f)\n\nIn [11]: type(ufunc_f)\nOut[11]: numpy.ufunc\n\nIn [12]: type(theano_f)\nOut[12]: theano.compile.function_module.Function\n\nIn [13]: type(lambda_f)\nOut[13]: function\n\nIn [14]: %timeit ufunc_f(xg, yg, zg)\n10 loops, best of 3: 21 ms per loop\n\nIn [15]: %timeit theano_f(xg, yg, zg)\n10 loops, best of 3: 20.7 ms per loop\n\nIn [16]: %timeit lambda_f(xg, yg, zg)\n10 loops, best of 3: 22.3 ms per loop\n\nText: ufuncify and theano_function are comparable, and slightly faster than lambdify for this simple expression. The difference is greater using the more complicated expression given below: \nCode: In [17]: f = sin(x)*cos(y)*sin(z) + sin(4*(x - y**2*sin(z)))\n\nIn [18]: ufunc_f = ufuncify([x,y,z], f)\n\nIn [19]: theano_f = theano_function([x, y, z], f, dims={x: 3, y: 3, z: 3})\n\nIn [20]: lambda_f = lambdify([x, y, z], f)\n\nIn [21]: %timeit ufunc_f(xg, yg, zg)\n10 loops, best of 3: 29.2 ms per loop\n\nIn [22]: %timeit theano_f(xg, yg, zg)\n10 loops, best of 3: 29.2 ms per loop\n\nIn [23]: %timeit lambda_f(xg, yg, zg)\n10 loops, best of 3: 42.1 ms per loop\n\nText: This is much faster than using the python version, as no intermediate arrays are created, the loop is traversed and the calculation ran in C. Theano produces equivalent speeds, as they also compile to native code. For the large expressions that I see when doing multibody dynamics, ufuncify (and the related autowrap) perform significantly faster than lambdify. I don't have much experience with theano, so I can't say how well their approach scales either, but I'd assume it would be similar. \nText: As I said above, this is only available in sympy 0.7.6 and up. Should be released soon, but until then you can grab the source from github. Docs on ufuncify new behavior can be found here \nAPI:\nnumpy.ufunc\n","label":[[250,258,"Mention"],[2605,2616,"API"]],"Comments":[]}
{"id":59892,"text":"ID:26952455\nPost:\nText: The docs for matrix are here. A quick scan over it should give you this, for a single-element matrix: \nCode: In [9]: b = np.matrix({\"key\": \"value\", (\"A\", \"B\"): (1,2,3)})\n\nIn [10]: b.item()\nOut[10]: {('A', 'B'): (1, 2, 3), 'key': 'value'}\n\nText: For a multi-element matrix, pass the index of the object you want to item(). \nAPI:\nnumpy.matrix\n","label":[[37,43,"Mention"],[352,364,"API"]],"Comments":[]}
{"id":59893,"text":"ID:27090202\nPost:\nText: If you want the output of the convolution to be the same size as the input Kp1 you could do the convolution using the 'same' option: \nCode: Kp1smo=np.convolve(Kp1,np.ones(5)\/5),'same')\n\nText: According to the documentation for convolve this will return a result of size max(M,N), where M and N are the size of the two input vectors. If Kp1 is larger than 5 this will be the size of Kp1. \nText: With no argument, np.convolve defaults to 'full' mode which gives a result of size (N+M-1) \nAPI:\nnumpy.convolve\nnumpy.convolve\n","label":[[251,259,"Mention"],[436,447,"Mention"],[515,529,"API"],[530,544,"API"]],"Comments":[]}
{"id":59894,"text":"ID:27097761\nPost:\nText: If you pass the axis=None parameter to argsort, it returns the sorted indexes of the flattened array (in ascending order). The unravel_index function converts the indexes of a flattened array to the indexes of an array of a given shape. \nCode: >> a = np.array([[1,2,3],[4,3,1]])\n>> np.unravel_index(a.argsort(axis=None), dims=a.shape)\n   (array([0, 1, 0, 0, 1, 1], dtype=int64),\n    array([0, 2, 1, 2, 1, 0], dtype=int64))\n\nText: The result of unravel_index is a tuple of arrays, where each array is the indexes along each respective axis. To pair the indices into coordinates, we can use zip: \nCode: >> ix = np.unravel_index(a.argsort(axis=None), dims=a.shape)\n>> zip(*ix)[-3:]\n   [(0, 2), (1, 1), (1, 0)]\n\nText: This is, again, in ascending order. We can use a negative stride to get this in descending order. \nCode: >> zip(*ix)[:2:-1]\n   [(1, 0), (1, 1), (0, 2)]\n\nText: Here's the whole thing on one line, where n is the number of \"top\" coordinates you want. \nCode: >> zip(*np.unravel_index(a.argsort(axis=None), dims=a.shape))[::-1][:n]\n\nAPI:\nnumpy.unravel_index\n","label":[[151,164,"Mention"],[1071,1090,"API"]],"Comments":[]}
{"id":59895,"text":"ID:27222121\nPost:\nText: np.append works differently, it'll return a single list with the elements inside the list you have as argument so it's not what you want: \nCode: >>> np.append([1, 2, 3], [4, 5, 6])\narray([1, 2, 3, 4, 5, 6])\n\nText: Your use case can be done with a simple Python list like this: \nCode: >>> z = 5\n>>> flux = np.array([1, 2, 3])\n>>> lambdas = np.array([4, 5, 6])\n>>> data = [z, flux, lambdas]\n>>> data\n[5, array([1, 2, 3]), array([4, 5, 6])]\n# Alternatively you can do data.append(z), data.append(flux) and then data.append(lambdas)\n\nText: If you want the result to be a numpy array, you'll have to shape it like a multidimensional array: \nCode: >>> data = np.array([np.array([z]), flux, lambdas]) # Notice the z integer is passed as an array\n>>> data\narray([array([5]), array([1, 2, 3]), array([4, 5, 6])], dtype=object)\n\nText: If you don't shape it like that, you'll get your error: \nCode: >>> data = np.array([z, flux, lambdas])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: setting an array element with a sequence.\n\nAPI:\nnumpy.append\n","label":[[24,33,"Mention"],[1085,1097,"API"]],"Comments":[]}
{"id":59896,"text":"ID:27240499\nPost:\nText: all tests whether all array elements along a given axis evaluate to True. \nCode: >>> import numpy as np\n>>> ratio_J_a = np.array([\n...     250.44244741,  186.92848637,  202.67726408,  143.01112845,\n...     132.95878384,  176.49130164,  178.9892571 ,  118.07516559,\n...     205.59639112,  183.64142204\n... ])\n>>> np.all(ratio_J_a > 100)\nTrue\n>>> np.all(ratio_J_a < 100)\nFalse\n\nText: Why you got wrong result: \nText: np.all(ratio_J_a) is evaluated as True because non-zero numbers are treated as a truth value. And True is equal to 1. 1 > 100 is False, 1 < 100 is True. \nAPI:\nnumpy.all\n","label":[[24,27,"Mention"],[598,607,"API"]],"Comments":[]}
{"id":59897,"text":"ID:27265750\nPost:\nText: Even though this is an old question, I was wondering the same thing and I didn't see a solution I liked. \nText: When reading binary data with Python I have found np.fromfile or fromstring to be much faster than using the Python struct module. Binary data with mixed types can be efficiently read into a numpy array, using the methods above, as long as the data format is constant and can be described with a numpy data type object (numpy.dtype). \nCode: import numpy as np\nimport pandas as pd\n\n# Create a dtype with the binary data format and the desired column names\ndt = np.dtype([('a', 'i4'), ('b', 'i4'), ('c', 'i4'), ('d', 'f4'), ('e', 'i4'),\n               ('f', 'i4', (256,))])\ndata = np.fromfile(file, dtype=dt)\ndf = pd.DataFrame(data)\n\n# Or if you want to explicitly set the column names\ndf = pd.DataFrame(data, columns=data.dtype.names)\n\nText: Edits: \nText: Removed unnecessary conversion of data.to_list(). Thanks fxx Added example of leaving off the columns argument \nAPI:\nnumpy.fromfile\nnumpy.fromstring\n","label":[[186,197,"Mention"],[201,211,"Mention"],[1008,1022,"API"],[1023,1039,"API"]],"Comments":[]}
{"id":59898,"text":"ID:27298879\nPost:\nText: It would appear that np.bool_ behaves slightly differently to vanilla Python bool: \nCode: >>> int(True+True) == int(True) + int(True)\nTrue\n>>> int(numpy.bool_(1)+numpy.bool_(1)) == int(numpy.bool_(1)) + int(numpy.bool_(1))\nFalse\n\nText: This is because: \nCode: >>> True+True\n2\n>>> numpy.bool_(1)+numpy.bool_(1)\nTrue\n>>> int(numpy.bool_(1)+numpy.bool_(1))\n1\n\nText: Basically, the addition operation for obol_ is logical, rather than numerical; to get the same behaviour with bool: \nCode: >>> int(True and True)\n1\n\nText: This is fine if you only use it for truthiness, as intended, but if you try to use it in an integer context without being explicit about that, you end up surprised. As soon as you're explicit, expected behaviour is restored: \nCode: >>> int(numpy.bool_(1)) + int(numpy.bool_(1))\n2\n\nAPI:\nnumpy.bool_\nnumpy.bool_\n","label":[[45,53,"Mention"],[425,430,"Mention"],[828,839,"API"],[840,851,"API"]],"Comments":[]}
{"id":59899,"text":"ID:27328478\nPost:\nText: From the output of print(matrix), it is apparent that matrix is an instance of scipy.sparse.coo_matrix. Such a matrix is not a numpy array; numpy knows nothing about scipy sparse matrices. In particular, savetxt doesn't handle scipy's sparse matrices. \nText: For a suggestion on how to save a sparse matrix to a text file, see my answer to a different question here: How to format in numpy savetxt such that zeros are saved only as \"0\" \nAPI:\nnumpy.savetxt\n","label":[[228,235,"Mention"],[466,479,"API"]],"Comments":[]}
{"id":59900,"text":"ID:27361757\nPost:\nText: I haven't found the reason it is happening, it has to have something to do with which side of the operator the array is and the datatype. If you use the numpy comparison functions instead, it will work. \nCode: d = [datetime.datetime(n,n,n) for n in xrange(1,10)]\na = np.array(d).reshape((3,3))\nc = datetime.datetime(4,4,4)\n\n>>> a\narray([[datetime.datetime(1, 1, 1, 0, 0), datetime.datetime(2, 2, 2, 0, 0),\n        datetime.datetime(3, 3, 3, 0, 0)],\n       [datetime.datetime(4, 4, 4, 0, 0), datetime.datetime(5, 5, 5, 0, 0),\n        datetime.datetime(6, 6, 6, 0, 0)],\n       [datetime.datetime(7, 7, 7, 0, 0), datetime.datetime(8, 8, 8, 0, 0),\n        datetime.datetime(9, 9, 9, 0, 0)]], dtype=object)\n>>> c\ndatetime.datetime(4, 4, 4, 0, 0)\n>>> a >= c\narray([[False, False, False],\n       [ True,  True,  True],\n       [ True,  True,  True]], dtype=bool)\n>>> c <= a\n\nTraceback (most recent call last):\n  File \"<pyshell#18>\", line 1, in <module>\n    c <= a\nTypeError: can't compare datetime.datetime to numpy.ndarray\n\n>>> np.less_equal(c, a)\narray([[False, False, False],\n       [ True,  True,  True],\n       [ True,  True,  True]], dtype=bool)\n\nText: Or if you have the option of using datetime64 the Python operators work. \nCode: >>> a\narray([['0001-01-01', '0002-02-02', '0003-03-03'],\n       ['0004-04-04', '0005-05-05', '0006-06-06'],\n       ['0007-07-07', '0008-08-08', '0009-09-09']], dtype='datetime64[D]')\n>>> c\nnumpy.datetime64('0004-04-04')\n>>> a >= c\narray([[False, False, False],\n       [ True,  True,  True],\n       [ True,  True,  True]], dtype=bool)\n>>> c <= a\narray([[False, False, False],\n       [ True,  True,  True],\n       [ True,  True,  True]], dtype=bool)\n>>>\n\nAPI:\nnumpy.datetime64\n","label":[[1210,1220,"Mention"],[1713,1729,"API"]],"Comments":[]}
{"id":59901,"text":"ID:27455438\nPost:\nText: I'm not sure why you want to set the seedespecially to a random number, even more especially to a random float (note that random.seed wants a large integer). \nText: But if you do, it's simple: call the rnd.seed function. \nText: Note that NumPy's seeds are arrays of 32-bit integers, while Python's seeds are single arbitrary-sized integers (although see the docs for what happens when you pass other types). \nText: So, for example: \nCode: In [1]: np.random.seed(0)    \nIn [2]: s = np.random.randn(10)\nIn [3]: s\nOut[3]:\narray([ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n       -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ])\nIn [4]: np.random.seed(0)\nIn [5]: s = np.random.randn(10)\nIn [6]: s\nOut[6]:\narray([ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n       -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ])\n\nText: Same seed used twice (I took the shortcut of passing a single int, which NumPy will internally convert into an array of 1 int32), same random numbers generated. \nAPI:\nnumpy.random.seed\n","label":[[227,235,"Mention"],[1084,1101,"API"]],"Comments":[]}
{"id":59902,"text":"ID:27466588\nPost:\nText: It sounds like you could use reshape to get what you're after. Say you have a list of 12 elements: \nCode: >>> import numpy as np\n>>> x = np.arange(12)\n>>> x\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\nText: We'll reshape it to give rows of four elements each: \nCode: >>> x.reshape(-1,4)\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\nText: You can give reshape more than two dimensions, too, so say x was 5x5 RGBA image in a 100-element 1-d array, you could do y = x.reshape(5,5,4), so that y[0][0] gives the four channels of the (0,0) pixel, y[0][1] contains the four channels of the (0,1) pixel, and so on. \nAPI:\nnumpy.reshape\n","label":[[53,60,"Mention"],[682,695,"API"]],"Comments":[]}
{"id":59903,"text":"ID:27477917\nPost:\nText: The problem is: \nText: features.transpose().dot(features) may not be invertible. And inv works only for full-rank matrix according to the documents. However, a (non-zero) regularization term always makes the equation nonsingular. \nText: By the way, you are right about the implementation. But it is not efficient. An efficient way to solve this equation is the least squares method. \nText: np.linalg.lstsq(features, labels) can do the work for np.linalg.pinv(features).dot(labels). \nText: In a general way, you can do this \nCode: def get_model(A, y, lamb=0):\n    n_col = A.shape[1]\n    return np.linalg.lstsq(A.T.dot(A) + lamb * np.identity(n_col), A.T.dot(y))\n\nAPI:\nnumpy.linalg.inv\n","label":[[109,112,"Mention"],[691,707,"API"]],"Comments":[]}
{"id":59904,"text":"ID:27505416\nPost:\nText: You can use np.ndenumerate \nCode: a = np.array([[[1, 1],\n               [2, 2]],\n\n              [[3, 3],\n               [4, 4]]])\n\nfor items in np.ndenumerate(a):\n    print(items)\n\nText: Output \nCode: ((0, 0, 0), 1)\n((0, 0, 1), 1)\n((0, 1, 0), 2)\n((0, 1, 1), 2)\n((1, 0, 0), 3)\n((1, 0, 1), 3)\n((1, 1, 0), 4)\n((1, 1, 1), 4)\n\nText: To remove the parentheses you can unpack everything \nCode: for indexes, value in np.ndenumerate(a):\n    x,y,z = indexes\n    print(x,y,z,value)\n\nText: Output \nCode: 0 0 0 1\n0 0 1 1\n0 1 0 2\n0 1 1 2\n1 0 0 3\n1 0 1 3\n1 1 0 4\n1 1 1 4\n\nText: To handle the file writing \nCode: with open('file.txt', 'w') as f:\n    for indexes, value in np.ndenumerate(a):\n        x,y,z = indexes\n        f.write('{} {} {} {}\\n'.format(x,y,z,value))\n\nAPI:\nnumpy.ndenumerate\n","label":[[36,50,"Mention"],[782,799,"API"]],"Comments":[]}
{"id":59905,"text":"ID:27517680\nPost:\nText: Use ny with axis=0 (to flatten along the first axis, i.e. flatten along the rows): \nCode: >>> np.any(a, axis=0)\narray([False,  True,  True,  True, False, False,  True,  True,  True,  True], dtype=bool)\n\nText: Of course, you can convert the boolean array into integers easily: \nCode: >>> np.any(a, axis=0)*1\narray([0, 1, 1, 1, 0, 0, 1, 1, 1, 1])\n\nAPI:\nnumpy.any\n","label":[[28,30,"Mention"],[375,384,"API"]],"Comments":[]}
{"id":59906,"text":"ID:27573104\nPost:\nText: np.bincount has a weights parameter which does just what you need: \nCode: In [36]: np.bincount(a, weights=b)\nOut[36]: array([  13.,  103.,    6.])\n\nAPI:\nnumpy.bincount\n","label":[[24,35,"Mention"],[177,191,"API"]],"Comments":[]}
{"id":59907,"text":"ID:27627472\nPost:\nText: I would suggest loading the data using loadtxt (see documentation), then array-slicing to correct the column order. \nText: Edit: actually, you can use the usecols parameter to directly specify the desired column order, like: \nCode: import numpy as np\n\ndef read_file(file_path, input_format=\"xyzb\", desired_format=\"bxyz\", delimiter=\" \"):\n    usecols = [input_format.index(col) for col in desired_format]\n    with open(file_path, \"r\") as inf:\n        return np.loadtxt(inf, usecols=usecols, delimiter=delimiter)\n\nAPI:\nnumpy.loadtxt\n","label":[[63,70,"Mention"],[540,553,"API"]],"Comments":[]}
{"id":59908,"text":"ID:27629550\nPost:\nText: OK this one was fun. I still can't help thinking it can all be done with a single ingenious call to np.tensordot but at any rate this seems to have eliminated all Python-level loops: \nCode: import numpy\n\ndef slow( a, b=None ):\n\n    if b is None: b = a\n    a = numpy.asmatrix( a )\n    b = numpy.asmatrix( b )\n\n    out = 0.0\n    for ai in a:\n        for bj in b:\n            dij = bj - ai\n            out += numpy.outer( dij, dij )\n    return out\n\ndef opsum( a, b=None ):\n\n    if b is None: b = a\n    a = numpy.asmatrix( a )\n    b = numpy.asmatrix( b )\n\n    RA, CA = a.shape\n    RB, CB = b.shape    \n    if CA != CB: raise ValueError( \"input matrices should have the same number of columns\" )\n\n    out = -numpy.outer( a.sum( axis=0 ), b.sum( axis=0 ) );\n    out += out.T\n    out += RB * ( a.T * a )\n    out += RA * ( b.T * b )\n    return out\n\ndef test( a, b=None ):\n    print( \"ground truth:\" )\n    print( slow( a, b ) )\n    print( \"optimized:\" )\n    print( opsum( a, b ) )  \n    print( \"max abs discrepancy:\" )\n    print( numpy.abs( opsum( a, b ) - slow( a, b ) ).max() )\n    print( \"\" )\n\n# OP example\ntest( [[1,2], [3,4]] )\n\n# non-symmetric example\na = [ [ 1, 2, 3 ], [-4, 5, 6 ], [7, -8, 9 ], [ 10, 11, -12 ] ]\na = numpy.matrix( a, dtype=float )\nb = a[ ::2, ::-1 ] + 15\ntest( a, b )\n\n# non-integer example\ntest( numpy.random.randn( *a.shape ), numpy.random.randn( *b.shape ) )\n\nText: With that (rather arbitrary) example input, timing of opsum (measured using timeit opsum(a,b) in IPython) looks only about a factor of 35 better than slow. But of course it scales much better: scale up the numbers of data-points by a factor of 100, and the number of features by a factor of 10, and then we're already looking at about a factor-10,000 increase in speed. \nAPI:\nnumpy.tensordot\n","label":[[124,136,"Mention"],[1786,1801,"API"]],"Comments":[]}
{"id":59909,"text":"ID:27644817\nPost:\nText: np.random.binomial(N, p, size = q) np.random.binomial(1, p, size = q) np.random.binomial(N,p, size= q) \nText: 1st and 3rd are similar, i can see. These two are binomial random number generator \nText: And, 2nd one is bernoulli random number generator \nText: Explanation of binomial: \nText: A binomial random variable counts how often a particular event occurs in a fixed number of tries or trials. \nText: Here, \nText: n = number of trials p = probability event of interest occurs on any one trial size = number of times you want to run this experiment \nText: Suppose, You wanna check how many times you will get six if you roll dice 10 times. Here, \nText: n = 10, p = (1\/6) # probability of getting six in each roll \nText: But, You have to do this experiment multiple times. \nText: Let, In 1st experiment, you get 3 six \nText: In 2nd expwriment, you get 2 six \nText: In 3rd experiment, you get 2 six \nText: In Pth experiment, you get 2 six, here P is the size \nText: Explanation of bernoulli: \nText: Suppose you perform an experiment with two possible outcomes: either success or failure. Success happens with probability p, while failure happens with probability 1-p. A random variable that takes value 1 in case of success and 0 in case of failure is called a Bernoulli random variable. \nText: Here, \nText: n = 1, Because you need to check whether it is success or failure one time p = probability of success size = number of times you will check this \nText: You can also read this, rnd.binomial \nText: Also, Difference between Binomial and Bernoulli \nAPI:\nnumpy.random.binomial\n","label":[[1508,1520,"Mention"],[1582,1603,"API"]],"Comments":[]}
{"id":59910,"text":"ID:27645934\nPost:\nText: 1. Is there any tutorial\/example on the usage of InfogainLoss layer?: A nice example can be found here: using InfogainLoss to tackle class imbalance. \nText: 2. Should the input to this layer, the class probabilities, be the output of a Softmax layer? Historically, the answer used to be YES according to Yair's answer. The old implementation of \"InfogainLoss\" needed to be the output of \"Softmax\" layer or any other layer that makes sure the input values are in range [0..1]. \nText: The OP noticed that using \"InfogainLoss\" on top of \"Softmax\" layer can lead to numerical instability. His pull request, combining these two layers into a single one (much like \"SoftmaxWithLoss\" layer), was accepted and merged into the official Caffe repositories on 14\/04\/2017. The mathematics of this combined layer are given here. \nText: The upgraded layer \"look and feel\" is exactly like the old one, apart from the fact that one no longer needs to explicitly pass the input through a \"Softmax\" layer. \nText: 3. How can I convert an np.array into a binproto file: \nText: In python \nCode: H = np.eye( L, dtype = 'f4' ) \nimport caffe\nblob = caffe.io.array_to_blobproto( H.reshape( (1,1,L,L) ) )\nwith open( 'infogainH.binaryproto', 'wb' ) as f :\n    f.write( blob.SerializeToString() )\n\nText: Now you can add to the model prototext the INFOGAIN_LOSS layer with H as a parameter: \nCode: layer {\n  bottom: \"topOfPrevLayer\"\n  bottom: \"label\"\n  top: \"infoGainLoss\"\n  name: \"infoGainLoss\"\n  type: \"InfogainLoss\"\n  infogain_loss_param {\n    source: \"infogainH.binaryproto\"\n  }\n}\n\nText: 4. How to load H as part of a DATA layer \nText: Quoting Evan Shelhamer's post: \nText: There's no way at present to make data layers load input at different rates. Every forward pass all data layers will advance. However, the constant H input could be done by making an input lmdb \/ leveldb \/ hdf5 file that is only H since the data layer will loop and keep loading the same H. This obviously wastes disk IO. \nAPI:\nnumpy.array\n","label":[[1043,1051,"Mention"],[2001,2012,"API"]],"Comments":[]}
{"id":59911,"text":"ID:27668679\nPost:\nText: I think the problem starts from the matrix A itself as a 16225 * 10000 size matrix already occupies about 12GB of memory if each element is a double precision floating point number. That together with how numpy creates temporary copies to do the dot operation will cause the error. The extra copies is because numpy uses the underlying BLAS operations for dot which needs the matrices to be stored in contiguous C order \nText: Check out these links if you want more discussions about improving dot performance \nText: http:\/\/wiki.scipy.org\/PerformanceTips \nText: Speeding up dot \nText: https:\/\/github.com\/numpy\/numpy\/pull\/2730 \nAPI:\nnumpy.dot\n","label":[[598,601,"Mention"],[656,665,"API"]],"Comments":[]}
{"id":59912,"text":"ID:27674929\nPost:\nText: where is the equivalent of Julia's ifelse: \nCode: >>> np.where(d_sex > 0.5, 'M', 'F')\narray(['F', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'F'], \n  dtype='|S1')\n\nAPI:\nnumpy.where\n","label":[[24,29,"Mention"],[190,201,"API"]],"Comments":[]}
{"id":59913,"text":"ID:27756557\nPost:\nText: After programming Python and NumPy for 10 years (and C, C++, Matlab and Fortran 10 years before that), this is my general impression: \nText: It is often easier to write numerical code in C, C++ or Fortran than Cython. The only exception I can think of is the smallest of code snipplets. In C++ you have the luxury of using templates and the STL (and Boost if you like). \nText: Learn to use the NumPy C API. The PyArrayObject (which is what a NumPy array is called in C) has a type number you can use for dispatch. You obtain it using the macro PyArray_TYPE() on your PyArrayObject*. float64 maps to type number NPY_FLOAT64, flwat32 maps to type number NPY_FLOAT32, etc. Then you have corresponding C and C++ typedefs which you can use in your C or C++ code: If PyArray_TYPE(x) == NPY_FLOAT64, the data type to use in C or C++ is npy_float64. This way you can write C or C++ code which is totally defined by the NumPy arrays you pass in. \nText: I usually use a switch statement on PyArray_TYPE(x), and case with NPY_FLOAT64, NPY_FLOAT32, etc. For each case I call a templated C++ function with the correct template type. This keeps the amount of code I need to write down to a minimum. \nText: http:\/\/docs.scipy.org\/doc\/numpy\/reference\/c-api.html \nText: Cython is good for wrapping C and C++ and avoiding tedious Python C API coding, but here is a limit to how much you can statically type arguments. For \"down-to-the-iron\" numerical code I think it is better to use plain C++, but Cython is an excellent tool for exposing it to Python. So write your numerical stuff in C++ and use Cython to call your C++. That would be the best advice I can give Cython is an excellent tool for writing C extensions to Python, but it is not a replacement for C++ when C++ is what you really want. \nText: As for you question: The thing you want to do is not really possible. Because in C or C++, which is what Cython emits, nda is PyArrayObject* regardless of dtype. So you need to handcode the switch statement. \nAPI:\nnumpy.float64\nnumpy.float32\nnumpy.ndarray\n","label":[[607,614,"Mention"],[648,655,"Mention"],[1930,1933,"Mention"],[2025,2038,"API"],[2039,2052,"API"],[2053,2066,"API"]],"Comments":[]}
{"id":59914,"text":"ID:38385308\nPost:\nText: Your error message means exactly what it says: AttributeError: 'DataFrame' object has no attribute 'read' \nText: When you use read_csv you're actually reading the csv file into a dataframe. BTW, you don't need the 'rb' \nCode: df = pandas.read_csv('FILENAME.csv')\n\nText: You can print (df) but you can not do print(df.read()) because the dataframe object doesn't have a .read() attribute. This is what's causing your error. \nAPI:\npandas.read_csv\n","label":[[150,158,"Mention"],[453,468,"API"]],"Comments":[]}
{"id":59915,"text":"ID:38542447\nPost:\nText: Yes, with pd.DataFrame.set_index you can make 'Locality' your row index. \nCode: data.set_index('Locality', inplace=True)\n\nText: If inplace=True is not provided, set_index returns the modified dataframe as a result. \nText: Example: \nCode: > import pandas as pd\n> df = pd.DataFrame([['ABBOTSFORD', 427000, 448000],\n                     ['ABERFELDIE', 534000, 600000]],\n                    columns=['Locality', 2005, 2006])\n\n> df\n     Locality    2005    2006\n0  ABBOTSFORD  427000  448000\n1  ABERFELDIE  534000  600000\n\n> df.set_index('Locality', inplace=True)\n> df\n              2005    2006\nLocality                  \nABBOTSFORD  427000  448000\nABERFELDIE  534000  600000\n\n> df.loc['ABBOTSFORD']\n2005    427000\n2006    448000\nName: ABBOTSFORD, dtype: int64\n\n> df.loc['ABBOTSFORD'][2005]\n427000\n\n> df.loc['ABBOTSFORD'].values\narray([427000, 448000])\n\n> df.loc['ABBOTSFORD'].tolist()\n[427000, 448000]\n\nAPI:\npandas.DataFrame.set_index\n","label":[[34,56,"Mention"],[929,955,"API"]],"Comments":[]}
{"id":59916,"text":"ID:38650886\nPost:\nText: OneHotEncoder cannot process string values directly. If your nominal features are strings, then you need to first map them into integers. \nText: get_dummies is kind of the opposite. By default, it only converts string columns into one-hot representation, unless columns are specified. \nAPI:\npandas.get_dummies\n","label":[[169,180,"Mention"],[315,333,"API"]],"Comments":[]}
{"id":59917,"text":"ID:38681726\nPost:\nText: You can use concat to concatenate the two dataframes rowwise, followed by drop_duplicates to remove all the duplicated rows in them. \nCode: In [1]: import pandas as pd\ndf_1 = pd.DataFrame({\"A\":[\"foo\", \"foo\", \"foo\", \"bar\"], \"B\":[0,1,1,1], \"C\":[\"A\",\"A\",\"B\",\"A\"]})\ndf_2 = pd.DataFrame({\"A\":[\"foo\", \"bar\", \"foo\", \"bar\"], \"B\":[1,0,1,0], \"C\":[\"A\",\"B\",\"A\",\"B\"]})\n\nIn [2]: df = pd.concat([df_1, df_2])\n\nIn [3]: df\nOut[3]: \n     A  B  C\n0  foo  0  A\n1  foo  1  A\n2  foo  1  B\n3  bar  1  A\n0  foo  1  A\n1  bar  0  B\n2  foo  1  A\n3  bar  0  B\n\nIn [4]: df.drop_duplicates(keep=False)\nOut[4]: \n     A  B  C\n0  foo  0  A\n2  foo  1  B\n3  bar  1  A\n\nAPI:\npandas.concat\n","label":[[36,42,"Mention"],[663,676,"API"]],"Comments":[]}
{"id":59918,"text":"ID:38830047\nPost:\nText: You can retain the original format while converting them to list by using .date of date which returns the date part of the Timestamps. \nCode: In [14]: df.index.date.tolist()\nOut[14]: \n[datetime.date(2016, 8, 1),\n datetime.date(2016, 8, 2),\n datetime.date(2016, 8, 3),\n datetime.date(2016, 8, 4),\n datetime.date(2016, 8, 5),\n datetime.date(2016, 8, 6),\n datetime.date(2016, 8, 7),\n datetime.date(2016, 8, 8),\n datetime.date(2016, 8, 9)]\n\nAPI:\npandas.DatetimeIndex.date\n","label":[[107,111,"Mention"],[466,491,"API"]],"Comments":[]}
{"id":59919,"text":"ID:38894205\nPost:\nText: When using read_csv pass in skipinitialspace=True flag to remove whitespace after CSV delimiters. \nAPI:\npandas.read_csv\n","label":[[35,43,"Mention"],[128,143,"API"]],"Comments":[]}
{"id":59920,"text":"ID:38935669\nPost:\nText: The equivalent of \nCode: df %>% groupby(col1) %>% summarize(col2_agg=max(col2), col3_agg=min(col3))\n\nText: is \nCode: df.groupby('col1').agg({'col2': 'max', 'col3': 'min'})\n\nText: which returns \nCode:       col2  col3\ncol1            \n1        5    -5\n2        9    -9\n\nText: The returning object is a df with an index called col1 and columns named col2 and col3. By default, when you group your data pandas sets the grouping column(s) as index for efficient access and modification. However, if you don't want that, there are two alternatives to set col1 as a column. \nText: Pass as_index=False: df.groupby('col1', as_index=False).agg({'col2': 'max', 'col3': 'min'}) Call reset_index: df.groupby('col1').agg({'col2': 'max', 'col3': 'min'}).reset_index() \nText: both yield \nCode: col1  col2  col3           \n   1     5    -5\n   2     9    -9\n\nText: You can also pass multiple functions to groupby.agg. \nCode: agg_df = df.groupby('col1').agg({'col2': ['max', 'min', 'std'], \n                                 'col3': ['size', 'std', 'mean', 'max']})\n\nText: This also returns a DataFrame but now it has a MultiIndex for columns. \nCode:      col2               col3                   \n      max min       std size       std mean max\ncol1                                           \n1       5   1  1.581139    5  1.581139   -3  -1\n2       9   0  3.535534    5  3.535534   -6   0\n\nText: MultiIndex is very handy for selection and grouping. Here are some examples: \nCode: agg_df['col2']  # select the second column\n      max  min       std\ncol1                    \n1       5    1  1.581139\n2       9    0  3.535534\n\nagg_df[('col2', 'max')]  # select the maximum of the second column\nOut: \ncol1\n1    5\n2    9\nName: (col2, max), dtype: int64\n\nagg_df.xs('max', axis=1, level=1)  # select the maximum of all columns\nOut: \n      col2  col3\ncol1            \n1        5    -1\n2        9     0\n\nText: Earlier (before version 0.20.0) it was possible to use dictionaries for renaming the columns in the agg call. For example \nCode: df.groupby('col1')['col2'].agg({'max_col2': 'max'})\n\nText: would return the maximum of the second column as max_col2: \nCode:       max_col2\ncol1          \n1            5\n2            9\n\nText: However, it was deprecated in favor of the rename method: \nCode: df.groupby('col1')['col2'].agg(['max']).rename(columns={'max': 'col2_max'})\n\n      col2_max\ncol1          \n1            5\n2            9\n\nText: It can get verbose for a DataFrame like agg_df defined above. You can use a renaming function to flatten those levels in that case: \nCode: agg_df.columns = ['_'.join(col) for col in agg_df.columns]\n\n      col2_max  col2_min  col2_std  col3_size  col3_std  col3_mean  col3_max\ncol1                                                                        \n1            5         1  1.581139          5  1.581139         -3        -1\n2            9         0  3.535534          5  3.535534         -6         0\n\nText: For operations like groupby().summarize(newcolumn=max(col2 * col3)), you can still use agg by first adding a new column with assign. \nCode: df.assign(new_col=df.eval('col2 * col3')).groupby('col1').agg('max') \n\n      col2  col3  new_col\ncol1                     \n1        5    -1       -1\n2        9     0        0\n\nText: This returns maximum for old and new columns but as always you can slice that. \nCode: df.assign(new_col=df.eval('col2 * col3')).groupby('col1')['new_col'].agg('max')\n\ncol1\n1   -1\n2    0\nName: new_col, dtype: int64\n\nText: With groupby.apply this would be shorter: \nCode: df.groupby('col1').apply(lambda x: (x.col2 * x.col3).max())\n\ncol1\n1   -1\n2    0\ndtype: int64\n\nText: However, groupby.apply treats this as a custom function so it is not vectorized. Up to now, the functions we passed to agg ('min', 'max', 'min', 'size' etc.) are vectorized and these are aliases for those optimized functions. You can replace df.groupby('col1').agg('min') with df.groupby('col1').agg(min), df.groupby('col1').agg(np.min) or df.groupby('col1').min() and they will all execute the same function. You will not see the same efficiency when you use custom functions. \nText: Lastly, as of version 0.20, agg can be used on DataFrames directly, without having to group first. See examples here. \nAPI:\npandas.DataFrame\n","label":[[325,327,"Mention"],[4253,4269,"API"]],"Comments":[]}
{"id":59921,"text":"ID:38938507\nPost:\nText: It is my understanding that .apply is not generally faster than iteration over the axis. I believe underneath the hood it is merely a loop over the axis, except you are incurring the overhead of a function call each time in this case. \nText: If we look at the source code, we can see that essentially we are iterating over the indicated axis and applying the function, building the individual results as series into a dictionary, and the finally calling the dataframe constructor on the dictionary returning a new DataFrame: \nCode:     if axis == 0:\n        series_gen = (self._ixs(i, axis=1)\n                      for i in range(len(self.columns)))\n        res_index = self.columns\n        res_columns = self.index\n    elif axis == 1:\n        res_index = self.index\n        res_columns = self.columns\n        values = self.values\n        series_gen = (Series.from_array(arr, index=res_columns, name=name,\n                                        dtype=dtype)\n                      for i, (arr, name) in enumerate(zip(values,\n                                                          res_index)))\n    else:  # pragma : no cover\n        raise AssertionError('Axis must be 0 or 1, got %s' % str(axis))\n\n    i = None\n    keys = []\n    results = {}\n    if ignore_failures:\n        successes = []\n        for i, v in enumerate(series_gen):\n            try:\n                results[i] = func(v)\n                keys.append(v.name)\n                successes.append(i)\n            except Exception:\n                pass\n        # so will work with MultiIndex\n        if len(successes) < len(res_index):\n            res_index = res_index.take(successes)\n    else:\n        try:\n            for i, v in enumerate(series_gen):\n                results[i] = func(v)\n                keys.append(v.name)\n        except Exception as e:\n            if hasattr(e, 'args'):\n                # make sure i is defined\n                if i is not None:\n                    k = res_index[i]\n                    e.args = e.args + ('occurred at index %s' %\n                                       pprint_thing(k), )\n            raise\n\n    if len(results) > 0 and is_sequence(results[0]):\n        if not isinstance(results[0], Series):\n            index = res_columns\n        else:\n            index = None\n\n        result = self._constructor(data=results, index=index)\n        result.columns = res_index\n\n        if axis == 1:\n            result = result.T\n        result = result._convert(datetime=True, timedelta=True, copy=False)\n\n    else:\n\n        result = Series(results)\n        result.index = res_index\n\n    return result\n\nText: Specifically: \nCode: for i, v in enumerate(series_gen):\n                results[i] = func(v)\n                keys.append(v.name)\n\nText: Where series_gen was constructed based on the requested axis. \nText: To get more performance out of a function, you can follow the advice given here. \nText: Essentially, your options are: \nText: Write a C extension Use numba (a JIT compiler) Use eval to squeeze performance out of large Dataframes \nAPI:\npandas.eval\n","label":[[3014,3018,"Mention"],[3072,3083,"API"]],"Comments":[]}
{"id":59922,"text":"ID:38948404\nPost:\nText: You can define a new variable that identifies survey period and use groupby to avoid for loop. It should be much faster when flow_df is large. \nCode: #convert both to datetime, if they are not\ndf['Survey_Date'] = pd.to_datetime(df['Survey_Date'])\nflow_df['date'] = pd.to_datetime(flow_df['date'])\n\n#Merge Survey_Date to flow_df. Most rows of flow_df['Survey_Date'] should be NaT\nflow_df = flow_df.merge(df, left_on='date', right_on='Survey_Date', how='outer')\n\n# In case not all Survey_Date in flow_df['date'] or data not sorted by date.\nflow_df['date'].fillna(flow_df['Survey_Date'], inplace=True)\nflow_df.sort_values('date', inplace=True)\n\n#Identify survey period. In your example: [1990-09-28, 1991-07-26) is represented by 0; [1991-07-26, 1991-11-23) = 1; etc.\nflow_df['survey_period'] = flow_df['Survey_Date'].notnull().cumsum()\n\n#calc Avg_Stage in each survey_period. I did .shift(1) because you want to align period [1990-09-28, 1991-07-26) to 1991-07-26 \ndf['Avg_Stage'] = (flow_df.groupby('survey_period')['flow'].max()*0.3048**3).shift(1)\n\nAPI:\npandas.DataFrame.groupby\n","label":[[92,99,"Mention"],[1079,1103,"API"]],"Comments":[]}
{"id":59923,"text":"ID:38977090\nPost:\nText: I discovered it is quite easy: \nCode: df['quantile'] = pd.qcut(df['b'], 2, labels=False)\n\n   a    b  quantile\n0  1    1         0\n1  2   10         0\n2  3  100         1\n3  4  100         1\n\nText: Interesting to know \"difference between pd.qcut and pandas.cut\" \nAPI:\npandas.qcut\n","label":[[261,268,"Mention"],[291,302,"API"]],"Comments":[]}
{"id":59924,"text":"ID:39018494\nPost:\nText: Edit: Since you are using a DataFrame don't use the csv module or json module. Instead, use pandas.io for both reading and writing. \nText: Original Answer: \nText: Short answer: use json. \nText: CSV is fine for saving tables of strings. Anything further than that and you need to manually convert the strings back into Python objects. \nText: If your data has just lists, dictionaries and basic literals like strings and numbers json would be the right tool for this job. \nText: Given: \nCode: example = {'x': [1, 2], 'y': [3, 4]}\n\nText: Save to file: \nCode: with open('f.txt','w') as f:\n    json.dump(example, f)\n\nText: Load from file: \nCode: with open('f.txt') as f:\n    reloaded_example = json.load(f)\n\nAPI:\npandas.DataFrame\n","label":[[52,61,"Mention"],[732,748,"API"]],"Comments":[]}
{"id":59925,"text":"ID:39028112\nPost:\nText: The end solution was to use read_sq with chunksize \nText: I found this post useful as well. \nCode: import sqlite3\nimport pandas as pd\nconn = sqlite3.connection('my_db.db')\nfor df in pd.read_sql(\"SELECT * from table ORDER BY A ASC\", conn, chunksize = 100000):\n    group  = df.groupby('A')\n    last   = group.first().tail(1).index.values[0]\n    last_a = 0\n    for a, g_df in group:\n        if (a == last_a):\n            g_df = l_df.append(g_df)\n\n        ....calculations....\n\n        if (a == last):\n            l_df = g_df\n            l_a  = a\n\nText: It is really important to have logic that ties together the groupby data frames that are split into two different chunks. \nAPI:\npandas.read_sql\n","label":[[52,59,"Mention"],[702,717,"API"]],"Comments":[]}
{"id":59926,"text":"ID:39065193\nPost:\nText: Using pandas.io.stata.StataReader.data to read from a stata file has been deprecated in pandas 0.18.1 version and hence you are getting that warning. \nText: Instead, you must use read_stata to read the file as shown: \nCode: df = pd.read_stata('sample_data.dta')\ndf.dtypes                                        ## Return the dtypes in this object\n\nAPI:\npandas.read_stata\n","label":[[203,213,"Mention"],[377,394,"API"]],"Comments":[]}
{"id":59927,"text":"ID:39088060\nPost:\nText: replace can take regex as an argument. Assuming that there are no preceding and trailing spaces in a string \"unclassified\" that you want to replace, that regex should be ^unclassified$: \nCode: df['item_name1'].str.replace('^unclassified$', 'replaced_string')\n\n0           replaced_string\n1    SantaCruz unclassified\n2                      text\n3                  texttext\n4           replaced_string\n5         unclassified text\n6                      text\n7                      text\n8                      text\n9         unclassified text\nName: item_name1, dtype: object\n\nAPI:\npandas.Series.str.replace\n","label":[[24,31,"Mention"],[602,627,"API"]],"Comments":[]}
{"id":59928,"text":"ID:39161842\nPost:\nText: I ran into a similar problem while manipulating df in the debug perspective of Eclipse (Mars 2) under Windows 7. \nText: When trying to examine the content of the DataFrame the console would output : \nCode: tput: unknown terminal \"emacs\"\n\nText: while the value would display (pending), and the debug session would freeze forever. \nText: I identified that the tput command was indeed a unix command shipped in by cygwing : \nCode: D:\\smouton>where tput\nd:\\smouton\\cygwin64\\bin\\tput.exe\n\nText: The workaround I set up is to modify the PATH variable before launching Eclipse. This is conveniently done by launching the following batch file instead of Eclipse executable : \nCode: REM Remove reference to \"unixy\" stuff before calling eclipse\nREM This avoids 'tput: unknown terminal \"emacs\"' error when manipulating pandas dataframe\nSET PATH=%PATH:;C:\\MinGW\\bin;d:\\smouton\\cygwin64\\bin;=;%\nREM launch eclipse\nSTART \"\" \"C:\\Program Files (x86)\\Eclipse\\eclipse.exe\"\n\nText: This file simply removes C:\\MinGW\\bin and d:\\smouton\\cygwin64\\bin from PATH, then starts eclipse. \nText: I suppose a similar workaround can solve the OP's issue with IPython as well. \nAPI:\npandas.DataFrame\n","label":[[72,74,"Mention"],[1175,1191,"API"]],"Comments":[]}
{"id":59929,"text":"ID:39184505\nPost:\nText: You can use pd.to_numeric and set errors='coerce' \nText: pd.to_numeric \nText: df['D'] = pd.to_numeric(df.D, errors='coerce') \nText: Which will give you: \nCode:     A   B   C   D\n0   1   2   5.0 7.0\n1   0   4   NaN 15.0\n2   4   8   9.0 10.0\n3   11  5   8.0 0.0\n4   11  5   8.0 NaN\n\nText: Deprecated solution (pandas <= 0.20 only): \nCode: df.convert_objects(convert_numeric=True)\n\nText: pd.DataFrame.convert_objects \nText: Here's the dev note in the convert_objects source code: # TODO: Remove in 0.18 or 2017, which ever is sooner. So don't make this a long term solution if you use it. \nAPI:\npandas.to_numeric\npandas.DataFrame.convert_objects\n","label":[[81,94,"Mention"],[409,437,"Mention"],[616,633,"API"],[634,666,"API"]],"Comments":[]}
{"id":59930,"text":"ID:39238699\nPost:\nText: df.drop takes level as an optional argument \nText: df.drop('1995-96', level='start') \nText: As of v0.18.1, its docstring says: \nText: \"\"\" Signature: df.drop(labels, axis=0, level=None, inplace=False, errors='raise') Docstring: Return new object with labels in requested axis removed. Parameters ---------- labels : single label or list-like axis : int or axis name level : int or level name, default None For MultiIndex inplace : bool, default False If True, do operation inplace and return None. errors : {'ignore', 'raise'}, default 'raise' If 'ignore', suppress error and existing labels are dropped. .. versionadded:: 0.16.1 \"\"\" \nAPI:\npandas.DataFrame.drop\n","label":[[24,31,"Mention"],[663,684,"API"]],"Comments":[]}
{"id":59931,"text":"ID:39252835\nPost:\nText: The problem was due to an error in the input file so simple use of usecols in pd.read_csv worked. \nText: code below demonstrates the selection of a few columns of data \nCode: import csv\nimport pandas\n\nlow_memory=False\n\n\n\n    #read only the selected columns\n    df = pandas.read_csv('DataB - Copy - Copy.csv',delimiter=',', dtype = object,\n    usecols=['TIMESTAMP', 'igmmx_U_77m', 'igmmx_U_58m', ])\n    print df # see what the data looks like\n    outfile = open('DataB_GreaterGabbardOnly.csv','wb')#somewhere to write the data to\n    df.to_csv(outfile)#save selection to the blank .csv created above\n\nAPI:\npandas.read_csv\n","label":[[102,113,"Mention"],[629,644,"API"]],"Comments":[]}
{"id":59932,"text":"ID:39414644\nPost:\nText: Thanks @zhqiat. I think upgrading pandas to version 0.19 will solve the problem. unfortunately I couldn't found an easy way to accomplish that. I found a tutorial to upgrade Pandas but for ubuntu (winXP user). \nText: I finally chose the workaround, using the method posted here, basically converting all columns, one by one, to a numeric type of Series \nCode: result[col] = result[col].apply(lambda x: x.str.replace(\".\",\"\").str.replace(\",\",\".\"))\n\nText: I know that this solution ain't the best, but works. Thanks \nAPI:\npandas.Series\n","label":[[370,376,"Mention"],[543,556,"API"]],"Comments":[]}
{"id":59933,"text":"ID:39434395\nPost:\nText: It's Series object. You can find that out using type(). \nCode: In [157]: phone\nOut[157]: \n  current_cellphone  |  months of     usage  |.1  previous_cellphone\n0          Motorola  |      11  |  Motorola  NaN                 NaN\n1            Huawei  |      21  |     Nokia  NaN                 NaN\n2          Motorola  |      13  |  Motorola  NaN                 NaN\n3             Nokia  |       2  |    iphone  NaN                 NaN\n4            Huawei  |      20  |    Huawei  NaN                 NaN\n5          Motorola  |      15  |  Motorola  NaN                 NaN\n6              Sony  |       9  |       HTC  NaN                 NaN\n\nIn [158]: vc = phone['current_cellphone'].value_counts()\n\nIn [159]: vc\nOut[159]: \nMotorola    3\nHuawei      2\nNokia       1\nSony        1\nName: current_cellphone, dtype: int64\n\nIn [160]: type(vc)\nOut[160]: pandas.core.series.Series\n\nText: To extract the information from the series: \nCode: In [169]: vc.values\nOut[169]: array([3, 2, 1, 1])\n\nIn [170]: vc.keys()\nOut[170]: Index([u'Motorola', u'Huawei', u'Nokia', u'Sony'], dtype='object')\n\nIn [176]: vc.to_dict()\nOut[176]: {'Huawei': 2, 'Motorola': 3, 'Nokia': 1, 'Sony': 1}\n\nIn [177]: vc.to_dict().keys()\nOut[177]: ['Nokia', 'Huawei', 'Motorola', 'Sony']\n\nIn [178]: vc.to_dict().values()\nOut[178]: [1, 2, 3, 1]\n\nText: Converting to dataframe: \nCode: In [180]: pd.DataFrame(vc)\nOut[180]: \n          current_cellphone\nMotorola                  3\nHuawei                    2\nNokia                     1\nSony                      1\n\nAPI:\npandas.Series\n","label":[[29,35,"Mention"],[1551,1564,"API"]],"Comments":[]}
{"id":59934,"text":"ID:39459076\nPost:\nText: I think the following is pretty close, the core idea is simply to convert those dictionaries into json and relying on read_json to parse them. \nCode: dictionary_example={\n        \"1234\":{'choice':0,'choice_set':{0:{'A':100,'B':200,'C':300},1:{'A':200,'B':300,'C':300},2:{'A':500,'B':300,'C':300}}},\n       \"234\":{'choice':1,'choice_set':{0:{'A':100,'B':400},1:{'A':100,'B':300,'C':1000}}},\n       \"1876\":{'choice':2,'choice_set':{0:{'A': 100,'B':400,'C':300},1:{'A':100,'B':300,'C':1000},2:{'A':600,'B':200,'C':100}}}\n\n    }\n\ndf = pd.read_json(json.dumps(dictionary_example)).T\n\n\ndef to_s(r):\n    return pd.read_json(json.dumps(r)).unstack()\n\nflattened_choice_set = df[\"choice_set\"].apply(to_s)\n\nflattened_choice_set.columns = ['_'.join((str(col[0]), col[1])) for col in flattened_choice_set.columns] \n\nresult = pd.merge(df, flattened_choice_set, \n         left_index=True, right_index=True).drop(\"choice_set\", axis=1)\n\nresult\n\nAPI:\npandas.read_json\n","label":[[142,151,"Mention"],[957,973,"API"]],"Comments":[]}
{"id":59935,"text":"ID:39528057\nPost:\nText: It is usually a bad idea to use a python for loop to iterate over a large pd.DataFrame or a numpy.ndarray. You should rather use the available build in functions on them as they are optimized and in many cases actually not written in python but in a compiled language. In your case you should use the methods max and min that both give you an option skipna to skip nan values in your DataFrame without the need to actually drop them manually. Furthermore, you can choose a axis to minimize along. So you can specifiy axis=1 to get the minimum along columns. \nText: This will add up to something similar as what @EdChum just mentioned in the comments: \nCode: data.max(axis=1, skipna=True) - data.min(axis=1, skipna=True)\n\nAPI:\npandas.DataFrame\npandas.DataFrame.max\npandas.DataFrame.min\n","label":[[98,110,"Mention"],[333,336,"Mention"],[341,344,"Mention"],[750,766,"API"],[767,787,"API"],[788,808,"API"]],"Comments":[]}
{"id":59936,"text":"ID:39569260\nPost:\nText: Using glob.glob will be a better option, along with using os.path.join to get to the full path: \nCode: from glob import glob\nfrom os.path import join, abspath\nfrom os import listdir, getcwd\n\nimport pandas as pd\n\ndata_frame = pd.DataFrame()\ndir_path = \"et\"\nfull_path = join(abspath(getcwd()), dir_path, \"*.csv\")\nfor file_name in glob(full_path):\n    csv_reader = pd.read_csv(file_name, names=columns)\n    # Guessing that all csv files will have the header\n    #If header is absent, use names=None\n    data_frame = data_frame.append(csv_reader, ignore_index=True)\n    # There is also a concat funtion to use. I am comfortable with append\n    # For concat, it will be data_frame = pd.concat(data_frame, csv_reader, ignore_index=True)\n\nText: abspath will make sure that the full directory from the root(in case of windows, from the main file system drive) is taken Adding *.csv with the join will make sure that you will check for csv files with the directory glob(full_path) will return a list of csv files, with absolute path, of the given directory Always make sure that you will either close the file descriptor explicitly or use the with statement to do it automatically, as it is a clean practice. Any C developer can vouch that closing the file descriptor is best. Since we need to put the value in the dataframe, I took out the with statement and added the read_csv from pandas. read_csv will make life lot better while reading the csv, in case we are into writing the csv file contents to dataframes. With read_csv and pandas append(or concat), we can write csv files easily without writing the header content from other csv files. I have given in append, because of personal opinion. Added how to use concat in the comment though. \nAPI:\npandas.read_csv\n","label":[[1407,1415,"Mention"],[1767,1782,"API"]],"Comments":[]}
{"id":59937,"text":"ID:39672184\nPost:\nText: You can use pd.Series.where function to construct new data frame based on the condition: \nCode: pairs = [('foo1', 'foo2'), ('foo3', 'foo4')]  # construct pairs of columns that need to swapped\n\ndf_out = pd.DataFrame() \n\n# for each pair, swap the values if foo3 < foo4\nfor l, r in pairs:\n    df_out[l] = df[l].where(df.foo3 < df.foo4, df[r])\n    df_out[r] = df[r].where(df.foo3 < df.foo4, df[l])\n\ndf_out\n#     foo1   foo2   foo3  foo4\n#0  cheese    egg      1     2\n#1   apple   pear      1     3\n#2 spanish french      1    10\n\nAPI:\npandas.Series.where\n","label":[[36,51,"Mention"],[556,575,"API"]],"Comments":[]}
{"id":59938,"text":"ID:39960144\nPost:\nText: Dask.dataframe handles larger-than-memory datasets through laziness. Appending concrete data to a dask.dataframe will not be productive. \nText: If your data can be handled by pd.read_csv \nText: The erad_csv function is very flexible. You say above that your parsing process is very complex, but it might still be worth looking into the options for pd.read_csv to see if it will still work. The dask.dataframe.read_csv function supports these same arguments. \nText: In particular if the concern is that your data is separated by tabs rather than commas this isn't an issue at all. Pandas supports a sep='\\t' keyword, along with a few dozen other options. \nText: Consider dask.bag \nText: If you want to operate on textfiles line-by-line then consider using dask.bag to parse your data, starting as a bunch of text. \nCode: import dask.bag as db\nb = db.read_text('myfile.tsv', blocksize=10000000)  # break into 10MB chunks\nrecords = b.str.split('\\t').map(parse)\ndf = records.to_dataframe(columns=...)\n\nText: Write to HDF5 file \nText: Once you have dask.dataframe try the .to_hdf method: \nCode: df.to_hdf('myfile.hdf5', '\/df')\n\nAPI:\npandas.read_csv\n","label":[[222,230,"Mention"],[1152,1167,"API"]],"Comments":[]}
{"id":59939,"text":"ID:39995060\nPost:\nText: You can use concat for concating all DataFrames and then stack with str.get_dummies. Last need groupby by index (level=0) with aggregating sum: \nCode: import pandas as pd\nimport numpy as np\nimport io\n\ntemp=u\"\"\"CAT;BALL\n\"\"\"\n#after testing replace io.StringIO(temp) to filename\ndf1 = pd.read_csv(io.StringIO(temp), sep=\";\", index_col=None, header=None)\n\nprint (df1)\n\ntemp=u\"\"\"DOG;BALL;APPLE\n\"\"\"\n#after testing replace io.StringIO(temp) to filename\ndf2 = pd.read_csv(io.StringIO(temp), sep=\";\", index_col=None, header=None)\n\nprint (df2)\n\n\ntemp=u\"\"\"DOG;BALL;APPLE;CAT\n\"\"\"\n#after testing replace io.StringIO(temp) to filename\ndf3 = pd.read_csv(io.StringIO(temp), sep=\";\", index_col=None, header=None)\n\nprint (df3)\n\ndf = pd.concat([df1,df2,df3], keys=['A','B','C'])\ndf.reset_index(1, drop=True, inplace=True)\nprint (df)\n     0     1      2    3\nA  CAT  BALL    NaN  NaN\nB  DOG  BALL  APPLE  NaN\nC  DOG  BALL  APPLE  CAT\n\nCode: print (df.stack().reset_index(1, drop=True).str.get_dummies())\n   APPLE  BALL  CAT  DOG\nA      0     0    1    0\nA      0     1    0    0\nB      0     0    0    1\nB      0     1    0    0\nB      1     0    0    0\nC      0     0    0    1\nC      0     1    0    0\nC      1     0    0    0\nC      0     0    1    0\n\nprint (df.stack().reset_index(1, drop=True).str.get_dummies().groupby(level=0).sum())\n   APPLE  BALL  CAT  DOG\nA      0     1    1    0\nB      1     1    0    1\nC      1     1    1    1\n\nText: Another solution with get_dummies and groupby by columns (level=0, axis=1) with aggregating sum: \nCode: print (pd.get_dummies(df, dummy_na=False, prefix='', prefix_sep='')\n         .groupby(level=0, axis=1).sum())\n\n   APPLE  BALL  CAT  DOG\nA      0     1    1    0\nB      1     1    0    1\nC      1     1    1    1\n\nText: EDIT by comment: \nText: Another approach is get dummies from each dataframe separately and then concat output: \nCode: df11 = pd.get_dummies(df1, dummy_na=False, prefix='', prefix_sep='')\n         .groupby(level=0, axis=1).sum()\n#print (df11)\ndf21 = pd.get_dummies(df2, dummy_na=False, prefix='', prefix_sep='')\n         .groupby(level=0, axis=1).sum()\n#print (df21)\ndf31 = pd.get_dummies(df3, dummy_na=False, prefix='', prefix_sep='')\n         .groupby(level=0, axis=1).sum()\n#print (df31)\n\ndf = pd.concat([df11,df21,df31], keys=['A','B','C']).fillna(0).astype(int)\ndf.reset_index(1, drop=True, inplace=True)\nprint (df)\n   APPLE  BALL  CAT  DOG\nA      0     1    1    0\nB      1     1    0    1\nC      1     1    1    1\n\nAPI:\npandas.get_dummies\n","label":[[1474,1485,"Mention"],[2500,2518,"API"]],"Comments":[]}
{"id":59940,"text":"ID:40206165\nPost:\nText: Here is a vectorized solution with where method: \nCode: import numpy as np\n\nON, OFF = 14.39, 12.50\ndf['sig'] = 0                                 #  set the initial value to be 0\ndf['sig'] = (df.sig.where(df.value < ON, 1)   #  if value > ON, set it 1\n                   .where((df.value < OFF) | (df.value > ON), np.nan)  \n                                              #  if value < ON, and value > OFF, set it nan\n                   .ffill().fillna(0))        # forward fill the nan value as they depend \n                                              # on their previous state, and fill initial \n                                              # value as 0\ndf\n\n#           value   sig\n#2016-09-21 13.30     0\n#2016-09-22 12.02     0\n#2016-09-23 12.28     0\n#2016-09-26 14.50     1\n#2016-09-27 13.10     1\n#2016-09-28 12.39     0\n#2016-09-29 14.02     0\n\nText: A similar np.where() method with maybe clearer intention \nCode: import numpy as np\ndf['sig'] = np.where(df.value > ON, 1, np.where(df.value < OFF, 0, np.nan))\ndf['sig'] = df.sig.ffill().fillna(0) \n\nAPI:\npandas.Series.where\n","label":[[59,64,"Mention"],[1087,1106,"API"]],"Comments":[]}
{"id":59941,"text":"ID:40223868\nPost:\nText: It looks like pd.tseries.frequencies.to_offset is what's used internally to convert from offset strings to a DateOffset object: \nCode: from pandas.tseries.frequencies import to_offset\n\nfreq = to_offset('7W')\n\nText: You can also get it in more of a hackier way without any imports by taking the freq attribute of a trivial DateTimeIndex: \nCode: freq = pd.date_range('2016-03-14', periods=0, freq='7W').freq\n\nText: Using either method: \nCode: print(freq)\n<7 * Weeks: weekday=6>\n\nprint(type(freq))\n<class 'pandas.tseries.offsets.Week'>\n\nAPI:\npandas.tseries.frequencies.to_offset\n","label":[[38,70,"Mention"],[563,599,"API"]],"Comments":[]}
{"id":59942,"text":"ID:40300827\nPost:\nText: You are passing in d, a list, as the STRING argument: \nCode: d = ['lower','Less']\n# ...\n    cleaning(c[x],d,c[x+1])\n    #             ^\n\nText: Your second example works, you pass in y instead, which is a single element from the b list: \nCode: b = ['upper','lower','hair','footwear']\nfor x,y in zip(range(len(a)),b):\n    # ^ one element from b   ^\n    cleaning(a[x],y,a[x+1])\n    #             ^\n\nText: The conhains method accepts regexes by default, and re.compile uses a dictionary as a cache to hold compiled patterns. Because you passed in a list, you get your error: \nCode: >>> pandas.Series(['aa', 'bb', 'cc']).str.contains(['a'])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\/Users\/mjpieters\/Development\/venvs\/stackoverflow-2.7\/lib\/python2.7\/site-packages\/pandas\/core\/strings.py\", line 1458, in contains\n    regex=regex)\n  File \"\/Users\/mjpieters\/Development\/venvs\/stackoverflow-2.7\/lib\/python2.7\/site-packages\/pandas\/core\/strings.py\", line 222, in str_contains\n    regex = re.compile(pat, flags=flags)\n  File \"\/Users\/mjpieters\/Development\/venvs\/stackoverflow-2.7\/lib\/python2.7\/re.py\", line 194, in compile\n    return _compile(pattern, flags)\n  File \"\/Users\/mjpieters\/Development\/venvs\/stackoverflow-2.7\/lib\/python2.7\/re.py\", line 237, in _compile\n    p, loc = _cache[cachekey]\nTypeError: unhashable type: 'list'\n\nText: The fix is to pass in y instead of d: \nCode: for x, y in zip(range(len(c)) ,d):\n    cleaning(c[x], y, c[x + 1])\n    cleaning(c[x], y, c[x + 2])\n\nText: You may want to come up with better variable names; one-letter names are hard to distinguish and easily lead to errors like these. \nAPI:\npandas.Series.str.contains\n","label":[[430,438,"Mention"],[1673,1699,"API"]],"Comments":[]}
{"id":59943,"text":"ID:40314011\nPost:\nText: Copy Data from OP and run df = pd.read_clipboard() \nText: Plot using df.plot \nText: Updated to pandas v1.2.4 and matplotlib v3.3.4 \nText: then using your code \nCode: df = df.replace(np.nan, 0)\ndfg = df.groupby(['home_team'])['arrests'].mean()\n\ndfg.plot(kind='bar', title='Arrests', ylabel='Mean Arrests',\n         xlabel='Home Team', figsize=(6, 5))\n\nAPI:\npandas.DataFrame.plot\n","label":[[93,100,"Mention"],[380,401,"API"]],"Comments":[]}
{"id":59944,"text":"ID:40334722\nPost:\nText: Use to_datetime which will do the conversion for you. \nCode: df1 = pd.DataFrame({'date':['2011-10-31', '2011-10-31', '2011-10-29'],'val':range(3)}).set_index('date')\ndf2 = pd.DataFrame({'date':['2011-10-31T01:00:00.000000000+0100',\n                      '2011-10-31T00:00:00.000000000+0000',\n                      '2011-10-29T11:00:00.000000000+0100'],'val':range(3)}).set_index('date')\n\nText: Test comparison: \nCode: df1.index==df2.index\n# array([False, False, False], dtype=bool)\n\npd.to_datetime(df1.index)==pd.to_datetime(df2.index)\n# array([ True,  True, False], dtype=bool)\n\nAPI:\npandas.to_datetime\n","label":[[28,39,"Mention"],[609,627,"API"]],"Comments":[]}
{"id":59945,"text":"ID:40407155\nPost:\nText: If I understand the structure of your dataframes correctly, I'd suggest merging the two dataframes and then performing the task on the merged data using numpy.where. \nCode: import numpy as np\n\nexams = exams.merge(students, on='Student Number', how='left')\nexams['Passed'] = np.where(\n    (exams['Exam Grade Date'] >= exams['Enrollment Date']) &\n    (exams['Exam Grade Date'] <= exams['Detail Date']) &\n    (exams['Grade'] >= 70),\n    1, 0)\n\nstudents = students.merge(\n    exams.groupby(['Student Number', 'Detail Date'])['Passed'].sum().reset_index(),\n    left_on=['Student Number', 'Detail Date'],\n    right_on=['Student Number', 'Detail Date'],\n    how='left')\nstudents['Passed'] = students['Passed'].fillna(0).astype('int')\n\nText: Note: you'll need to make sure the date columns are properly stored as datetimes (you can use to_adteftime to do this). \nText: numpy.where creates a new array where the values are one way (1 in the example above) if the conditions you specify are met and another (0) if they aren't met. \nText: The line exams.groupby(['Student Number', 'Detail Date'])['Passed'].sum() produces a series in which the index is Student Number and Detail Date and the values are the counts of passed exams corresponding to that Student Number and Detail Date combination. The reset_index() makes it into a dataframe for merging. \nAPI:\npandas.to_datetime\n","label":[[852,864,"Mention"],[1372,1390,"API"]],"Comments":[]}
{"id":59946,"text":"ID:40413874\nPost:\nText: You can use this \nText: data.to_csv(filename, index=False, header=False) \nText: the header means: \nText: header : boolean or list of string, default True Write out column names. If a list of string is given it is assumed to be aliases for the column names \nText: you can find more specific info in df.to_csv \nAPI:\npandas.DataFrame.to_csv\n","label":[[322,331,"Mention"],[338,361,"API"]],"Comments":[]}
{"id":59947,"text":"ID:40556694\nPost:\nText: The most concise way is probably the pd.crosstab function: \nCode: >>> pandas.crosstab(d.bowl, d.cookie)\ncookie  chocolate  vanilla\nbowl                      \none             1        5\ntwo             3        1\n\nAPI:\npandas.crosstab\n","label":[[61,72,"Mention"],[242,257,"API"]],"Comments":[]}
{"id":59948,"text":"ID:40599085\nPost:\nText: One way is to make a list and append a DataFrame to it each time you go through the loop and have new results. Then at the end you can use concat to combine all the individual DataFrames into one. \nText: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.concat.html#pandas-concat \nAPI:\npandas.concat\n","label":[[163,169,"Mention"],[321,334,"API"]],"Comments":[]}
{"id":59949,"text":"ID:40705575\nPost:\nText: You can use pd.Series.str.split just like you would use split normally. Just split on the string '::', and index the list that's created from the split method: \nCode: >>> df = pd.DataFrame({'text': [\"vendor a::ProductA\", \"vendor b::ProductA\", \"vendor a::Productb\"]})\n>>> df\n                 text\n0  vendor a::ProductA\n1  vendor b::ProductA\n2  vendor a::Productb\n>>> df['text_new'] = df['text'].str.split('::').str[0]\n>>> df\n                 text  text_new\n0  vendor a::ProductA  vendor a\n1  vendor b::ProductA  vendor b\n2  vendor a::Productb  vendor a\n\nText: Here's a non-pandas solution: \nCode: >>> df['text_new1'] = [x.split('::')[0] for x in df['text']]\n>>> df\n                 text  text_new text_new1\n0  vendor a::ProductA  vendor a  vendor a\n1  vendor b::ProductA  vendor b  vendor b\n2  vendor a::Productb  vendor a  vendor a\n\nText: Edit: Here's the step-by-step explanation of what's happening in pandas above: \nCode: # Select the pandas.Series object you want\n>>> df['text']\n0    vendor a::ProductA\n1    vendor b::ProductA\n2    vendor a::Productb\nName: text, dtype: object\n\n# using pandas.Series.str allows us to implement \"normal\" string methods \n# (like split) on a Series\n>>> df['text'].str\n<pandas.core.strings.StringMethods object at 0x110af4e48>\n\n# Now we can use the split method to split on our '::' string. You'll see that\n# a Series of lists is returned (just like what you'd see outside of pandas)\n>>> df['text'].str.split('::')\n0    [vendor a, ProductA]\n1    [vendor b, ProductA]\n2    [vendor a, Productb]\nName: text, dtype: object\n\n# using the pandas.Series.str method, again, we will be able to index through\n# the lists returned in the previous step\n>>> df['text'].str.split('::').str\n<pandas.core.strings.StringMethods object at 0x110b254a8>\n\n# now we can grab the first item in each list above for our desired output\n>>> df['text'].str.split('::').str[0]\n0    vendor a\n1    vendor b\n2    vendor a\nName: text, dtype: object\n\nText: I would suggest checking out the pd.Series.str docs, or, better yet, Working with Text Data in pandas. \nAPI:\npandas.Series.str.split\npandas.Series.str\n","label":[[36,55,"Mention"],[2012,2025,"Mention"],[2088,2111,"API"],[2112,2129,"API"]],"Comments":[]}
{"id":59950,"text":"ID:40707646\nPost:\nText: As per documentation of pandas 0.19.1 pandas.DataFrame.from_csv does not support index_col = False. Try to use pd.read_csv instead (with the same parameters). Also make sure you are using the up to date version of pandas. \nText: See if this works: \nCode: import pandas as pd\ndef merge(peak_score, profile_score, res_file):\n    df_peak = pd.read_csv(peak_score, index_col = False, sep='\\t')\n    df_profile = pd.read_csv(profile_score, index_col = False, sep='\\t')\n    result = pd.concat([df_peak, df_profile], axis=1)\n    print result.head()\n    test = []\n    for a,b in zip(result['prot_a_p'],result['prot_b_p']):\n        if a == b:\n            test.append(1)\n        else:\n            test.append(0)\n    result['test']=test\n    result = result[result['test']==0] \n    del result['test']\n    result = result.fillna(0)\n    result.to_csv(res_file)   \n\nif __name__ == '__main__':\n    pass\n\nText: Regarding the path issue when changing from Windows to OS X: \nText: In all flavours of Unix, paths are written with slashes \/, while in Windows backslashes \\ are used. Since OS X is a descendant of Unix, as other users have correctly pointed out, when you change there from Windows you need to adapt your paths. \nAPI:\npandas.read_csv\n","label":[[135,146,"Mention"],[1235,1250,"API"]],"Comments":[]}
{"id":59951,"text":"ID:40726437\nPost:\nText: You need to use read_csv instead of python's csv. \nText: There you can use the dtype argument to provide it with the correct types of data for it to use: \nText: From pandas documentation \nText: dtype : Type name or dict of column -> type, default None Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32} (unsupported with engine='python'). Use str or object to preserve and not interpret dtype. \nText: If you must parse the CSV outside pandas an importing with \"from_records\" you can use coerce_float=True. Reference \nText: coerce_float : boolean, default False Attempt to convert values to non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets \nAPI:\npandas.read_csv\n","label":[[40,48,"Mention"],[739,754,"API"]],"Comments":[]}
{"id":59952,"text":"ID:40753808\nPost:\nText: From your error, I would double check your initial dataframe to make sure you do have column1 in both (no extra spaces or anything) as an actual column, because it should work fine (no error in the following code) \nText: Additionally, there's a difference between calling merge on pd.DataFrame or on Dask.dataframe. \nText: Here's some example data: \nCode: df1 = pd.DataFrame(np.transpose([np.arange(1000),\n                           np.arange(1000)]), columns=['column1','column1_1'])\n\ndf2 = pd.DataFrame(np.transpose([np.arange(1000),\n                           np.arange(1000, 2000)]), columns=['column1','column1_2'])\n\nText: And their dask equivalent: \nCode: ddf1 = dd.from_pandas(df1, npartitions=100)\nddf2 = dd.from_pandas(df2, npartitions=100)\n\nText: Using pandas.DataFrame: \nCode: In [1]: type(dd.merge(df1, df2, on=\"column1\", how=\"outer\"))\n\nOut [1]: pandas.core.frame.DataFrame\n\nText: So this returns a pandas.DataFrame, so you cannot call compute() on it. \nText: Using dask.dataframe: \nCode: In [2]: type(dd.merge(ddf1, ddf2, on=\"column1\", how=\"outer\"))\nOut[2]: dask.dataframe.core.DataFrame\n\nText: Here you can call compute: \nCode: In [3]: dd.merge(ddf1,ddf2, how='outer').compute(num_workers=60)\n\nOut[3]:\n   column1  column1_1  column1_2\n0        0          0       1000\n1      400        400       1400\n2      100        100       1100\n3      500        500       1500\n4      300        300       1300\n\nText: Side Note: depending on the size of your data and your hardware, you might want to check if doing a pandas.join wouldn't be faster: \nCode: df1.set_index('column1').join(df2.set_index('column1'), how='outer').reset_index()\n\nText: Using a size of (1 000 000, 2) for each df it's faster than the dask solution on my hardware. \nAPI:\npandas.DataFrame\n","label":[[305,317,"Mention"],[1774,1790,"API"]],"Comments":[]}
{"id":59953,"text":"ID:40817732\nPost:\nText: I would do it this way: \nCode: data[\"Vol_POC_last_week\"] = pd.Series()                                               \ndata.loc[data.Week == 40, \"Vol_POC_last_week\"] = data.loc[data.Week == 39, \"Vol_POC\"].values\n\nText: loc \nText: DataFrame.loc Purely label-location based indexer for selection by label. \nText: The .loc attribute is the primary access method gives access to a value not to a copy of it. \nText: It has simple and obvious interface: \nText: .loc[row_indexer,column_indexer] \nText: Multi line change \nCode: for i in data.Week.unique()[:-1]:\n    data.loc[data.Week == i, \"Vol_POC_last_week\"] = data.loc[data.Week == i-1, \"Vol_POC\"].values\n\nText: Since we need to make changes according to a particular value of Week column and it is not unique the only way I see now is just the same but iterative. \nAPI:\npandas.DataFrame.loc\n","label":[[242,245,"Mention"],[840,860,"API"]],"Comments":[]}
{"id":59954,"text":"ID:40821428\nPost:\nText: You want to take sum_delivery into account only once per order, so you have to groupby before you join: \nCode: >>> items2 = items.groupby('order_id', as_index=False)['sum'].sum()\n>>> items2\n   order_id   sum\n0         1  1000\n1         2   500\n\nText: Now you can use merge to use custom column names: \nCode: >>> res = pd.merge(orders, items2, left_on = 'id', right_on = 'order_id')[['date', 'sum', 'sum_delivery']]\n>>> res\n         date   sum  sum_delivery\n0  2016-01-01  1000            10\n1  2016-01-05   500             0\n\nText: And now just do simple math and simple groupby (don't forget to use as_index=False): \nCode: >>> res['date'] = res['date'].str[:7]\n>>> res['sum2'] = res['sum'] + res['sum_delivery']\n>>> res2 = res.groupby('date', as_index=False)['sum2'].sum()\n>>> res2\n      date  sum2\n0  2016-01  1510\n\nAPI:\npandas.DataFrame.merge\npandas.DataFrame.groupby\n","label":[[291,296,"Mention"],[595,602,"Mention"],[847,869,"API"],[870,894,"API"]],"Comments":[]}
{"id":59955,"text":"ID:40852617\nPost:\nText: The keys in a dictionary have to be unique, so you would be overwriting some of the values if you did something where one state has multiple cities; \nCode: In [1]: {'Arizona': 'Flagstaff', 'Arizona': 'Phoenix'}\nOut[1]: {'Arizona': 'Phoenix'}\n\nText: On the other hand, if you know that your keys are unique, pd.DataFrame.from_dict does the job. \nText: In your case, you could pass the information as a list of pairs. \nCode: In [20]: df = pd.DataFrame([['Arizona', 'Flagstaff'], ['Arizona', 'Phoenix'], ['Alabama', 'Auburn']], columns=['State', 'City'])\n\nIn [22]: df\nOut[22]: \n     State       City\n0  Arizona  Flagstaff\n1  Arizona    Phoenix\n2  Alabama     Auburn\n\nText: If you want to get rid of the redundant information that you are passing by including the state more than once, you could do something like \nCode: In [33]: cities = {'Alabama': ['Auburn', 'Jacksonville'], 'Arizona': ['Flagstaff', 'Phoenix']}\n\nIn [34]: pd.DataFrame(((k, c) for (k, v) in cities.items() for c in v), columns=['State', 'City'])\nOut[34]: \n     State          City\n0  Arizona     Flagstaff\n1  Arizona       Phoenix\n2  Alabama        Auburn\n3  Alabama  Jacksonville\n\nAPI:\npandas.DataFrame.from_dict\n","label":[[331,353,"Mention"],[1177,1203,"API"]],"Comments":[]}
{"id":59956,"text":"ID:40979611\nPost:\nText: corrwith should work for you. An alternative with numpy.corrcoef: \nCode: import numpy as np\nimport pandas as pd\n\ndf1 = pd.DataFrame({'a': np.random.random(5), 'b': np.random.random(5)})\nresult = np.corrcoef(df1.a,df1.b)\nprint(result)\n\nText: It outputs: \nCode: [[ 1.          0.02543264]\n [ 0.02543264  1.        ]]\n\nText: Both corrwith and corrcoef are the same thing. \nAPI:\npandas.DataFrame.corrwith\n","label":[[24,32,"Mention"],[399,424,"API"]],"Comments":[]}
{"id":59957,"text":"ID:41012557\nPost:\nText: You can use mainly merge_asof and then add new column by map: \nCode: Input1.TXN_DATE = pd.to_datetime(Input1.TXN_DATE)\nInput2.TXN_DATE = pd.to_datetime(Input2.TXN_DATE)\n\nInput1 = Input1.sort_values('TXN_DATE')\nInput2 = Input2.sort_values('TXN_DATE')\ndf = pd.merge_asof(Input1, Input2, on='TXN_DATE', suffixes=('','_COMPANION')) \\\n       .sort_values('LOT') \\\n       .drop('OPERATION_COMPANION', axis=1)\ndf['LOT_TXN_DATE'] = df.LOT_COMPANION.map(Input2.set_index('LOT')['TXN_DATE'])\nprint (df)\n  LOT  OPERATION   TXN_DATE LOT_COMPANION LOT_TXN_DATE\n4  A1      100.0 2016-12-06            B5   2016-12-04\n3  A2      100.0 2016-12-05            B5   2016-12-04\n2  A3      100.0 2016-11-30            B4   2016-11-22\n1  A4      100.0 2016-11-27            B4   2016-11-22\n0  A5      100.0 2016-11-22            B4   2016-11-22\n\nAPI:\npandas.merge_asof\n","label":[[43,53,"Mention"],[853,870,"API"]],"Comments":[]}
{"id":59958,"text":"ID:41088706\nPost:\nText: In summary, what you're doing is saving the index to file and when you're reading back from the file, the column previously saved as index is loaded as a regular column. \nText: There are a few ways to deal with this: \nText: Method 1 \nText: When saving a df to disk, use index=False like this: \nCode: df.to_csv(path, index=False)\n\nText: Method 2 \nText: When reading from file, you can define the column that is to be used as index, like this: \nCode: df = pd.read_csv(path, index_col='index')\n\nText: Method 3 \nText: If method #2 does not suit you for some reason, you can always set the column to be used as index later on, like this: \nCode: df.set_index('index', inplace=True)\n\nText: After this point, your datafame should look like this: \nCode:         userid    locale    age\nindex\n    0    A1092     EN-US     31\n    1    B9032     SV-SE     23\n\nText: I hope this helps. \nAPI:\npandas.DataFrame\n","label":[[278,280,"Mention"],[903,919,"API"]],"Comments":[]}
{"id":59959,"text":"ID:41111164\nPost:\nText: pd.Series has a last_valid_index method: \nCode: pd.DataFrame(a.T).apply(pd.Series.last_valid_index)\nOut: \n0    3\n1    2\n2    6\n3    3\n4    0\n5    3\ndtype: int64\n\nAPI:\npandas.Series\n","label":[[24,33,"Mention"],[191,204,"API"]],"Comments":[]}
{"id":59960,"text":"ID:41153445\nPost:\nText: You can use df.div method: \nCode: df.div(df.sum())\n\n#          a           b           c\n#0  0.071429    0.368421    0.173913\n#1  0.214286    0.210526    0.391304\n#2  0.214286    0.052632    0.043478\n#3  0.285714    0.105263    0.130435\n#4  0.214286    0.263158    0.260870\n\nText: To divide by rows, specify axis accordingly: \nCode: df1 = df.div(df.sum(axis = 1), axis = 0)\n\nText: Here is a test that the result data frame has rowsum of one. \nCode: df1.sum(axis = 1)\n\n#0    1.0\n#1    1.0\n#2    1.0\n#3    1.0\n#4    1.0\n#dtype: float64\n\nAPI:\npandas.DataFrame.div\n","label":[[36,42,"Mention"],[564,584,"API"]],"Comments":[]}
{"id":59961,"text":"ID:41168691\nPost:\nText: Creating dataframe from dictionary object. \nCode: import pandas as pd\ndata = [{'name': 'vikash', 'age': 27}, {'name': 'Satyam', 'age': 14}]\ndf = pd.DataFrame.from_dict(data, orient='columns')\n\ndf\nOut[4]:\n   age  name\n0   27  vikash\n1   14  Satyam\n\nText: If you have nested columns then you first need to normalize the data: \nCode: data = [\n  {\n    'name': {\n      'first': 'vikash',\n      'last': 'singh'\n    },\n    'age': 27\n  },\n  {\n    'name': {\n      'first': 'satyam',\n      'last': 'singh'\n    },\n    'age': 14\n  }\n]\n\ndf = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')\n\ndf    \nOut[8]:\nage name.first  name.last\n0   27  vikash  singh\n1   14  satyam  singh\n\nText: Source: \nText: from_dict pd.json_normalize \nAPI:\npandas.DataFrame.from_dict\npandas.json_normalize\n","label":[[727,736,"Mention"],[737,754,"Mention"],[761,787,"API"],[788,809,"API"]],"Comments":[]}
{"id":59962,"text":"ID:41179085\nPost:\nText: Try something like this \nCode: import pandas as pd\n\ndf = pd.DataFrame([\"{u'users': {u'href': u'qwer:\/\/abc\\.x-data\\.orc\/v1\/i\/32\/users'}, u'self': {u'href': ...\",\"{u'users': {u'href': u'qwer:\/\/abc\\.x-data\\.orc\/v1\/i\/87\/users'}, u'self': {u'href': ...\"], columns=['_links'])\n\ndf._links.str.findall('qwer:\/\/abc\\\\\\.x-data\\\\\\.orc\/v1\/i\/\\d+\/users')\n\nText: When using regex I find it helpful to trial out the regex on http:\/\/pythex.org\/ \nText: If the data is in a dictionary format, it would be best to convert it over to a DataFrame using from_ditt \nAPI:\npandas.DataFrame.from_dict\n","label":[[554,563,"Mention"],[570,596,"API"]],"Comments":[]}
{"id":59963,"text":"ID:41307453\nPost:\nText: A list comprehension can be used. Note: axis=1 denotes that we are referring to the column and inplace=True can also be used as per drop docs. \nCode: droplist = [i for i in df.columns if i.startswith('TYPE') and '_1' not in i]\ndf1.drop(droplist,axis=1,inplace=True)\n\nAPI:\npandas.DataFrame.drop\n","label":[[156,160,"Mention"],[296,317,"API"]],"Comments":[]}
{"id":59964,"text":"ID:41328899\nPost:\nText: Suppose you have the following DataFrame: \nText: Edit \nText: I checked the docs and you should probably use the set_option API to do this: \nCode: In [13]: df\nOut[13]: \n              a             b             c\n0  4.405544e+08  1.425305e+08  6.387200e+08\n1  8.792502e+08  7.135909e+08  4.652605e+07\n2  5.074937e+08  3.008761e+08  1.781351e+08\n3  1.188494e+07  7.926714e+08  9.485948e+08\n4  6.071372e+08  3.236949e+08  4.464244e+08\n5  1.744240e+08  4.062852e+08  4.456160e+08\n6  7.622656e+07  9.790510e+08  7.587101e+08\n7  8.762620e+08  1.298574e+08  4.487193e+08\n8  6.262644e+08  4.648143e+08  5.947500e+08\n9  5.951188e+08  9.744804e+08  8.572475e+08\n\nIn [14]: pd.set_option('float_format', '{:f}'.format)\n\nIn [15]: df\nOut[15]: \n                 a                b                c\n0 440554429.333866 142530512.999182 638719977.824965\n1 879250168.522411 713590875.479215  46526045.819487\n2 507493741.709532 300876106.387427 178135140.583541\n3  11884941.851962 792671390.499431 948594814.816647\n4 607137206.305609 323694879.619369 446424361.522071\n5 174424035.448168 406285189.907148 445616045.754137\n6  76226556.685384 979050957.963583 758710090.127867\n7 876261954.607558 129857447.076183 448719292.453509\n8 626264394.999419 464814260.796770 594750038.747595\n9 595118819.308896 974480400.272515 857247528.610996\n\nIn [16]: df.describe()\nOut[16]: \n                     a                b                c\ncount        10.000000        10.000000        10.000000\nmean  479461624.877280 522785202.100082 536344333.626082\nstd   306428177.277935 320806568.078629 284507176.411675\nmin    11884941.851962 129857447.076183  46526045.819487\n25%   240956633.919592 306580799.695412 445818124.696121\n50%   551306280.509214 435549725.351959 521734665.600552\n75%   621482597.825966 772901261.744377 728712562.052142\nmax   879250168.522411 979050957.963583 948594814.816647\n\nText: End of edit \nCode: In [7]: df\nOut[7]: \n              a             b             c\n0  4.405544e+08  1.425305e+08  6.387200e+08\n1  8.792502e+08  7.135909e+08  4.652605e+07\n2  5.074937e+08  3.008761e+08  1.781351e+08\n3  1.188494e+07  7.926714e+08  9.485948e+08\n4  6.071372e+08  3.236949e+08  4.464244e+08\n5  1.744240e+08  4.062852e+08  4.456160e+08\n6  7.622656e+07  9.790510e+08  7.587101e+08\n7  8.762620e+08  1.298574e+08  4.487193e+08\n8  6.262644e+08  4.648143e+08  5.947500e+08\n9  5.951188e+08  9.744804e+08  8.572475e+08\n\nIn [8]: df.describe()\nOut[8]: \n                  a             b             c\ncount  1.000000e+01  1.000000e+01  1.000000e+01\nmean   4.794616e+08  5.227852e+08  5.363443e+08\nstd    3.064282e+08  3.208066e+08  2.845072e+08\nmin    1.188494e+07  1.298574e+08  4.652605e+07\n25%    2.409566e+08  3.065808e+08  4.458181e+08\n50%    5.513063e+08  4.355497e+08  5.217347e+08\n75%    6.214826e+08  7.729013e+08  7.287126e+08\nmax    8.792502e+08  9.790510e+08  9.485948e+08\n\nText: You need to fiddle with the pandas.options.display.float_format attribute. Note, in my code I've used import pandas as pd. A quick fix is something like: \nCode: In [29]: pd.options.display.float_format = \"{:.2f}\".format\n\nIn [10]: df\nOut[10]: \n             a            b            c\n0 440554429.33 142530513.00 638719977.82\n1 879250168.52 713590875.48  46526045.82\n2 507493741.71 300876106.39 178135140.58\n3  11884941.85 792671390.50 948594814.82\n4 607137206.31 323694879.62 446424361.52\n5 174424035.45 406285189.91 445616045.75\n6  76226556.69 979050957.96 758710090.13\n7 876261954.61 129857447.08 448719292.45\n8 626264395.00 464814260.80 594750038.75\n9 595118819.31 974480400.27 857247528.61\n\nIn [11]: df.describe()\nOut[11]: \n                 a            b            c\ncount        10.00        10.00        10.00\nmean  479461624.88 522785202.10 536344333.63\nstd   306428177.28 320806568.08 284507176.41\nmin    11884941.85 129857447.08  46526045.82\n25%   240956633.92 306580799.70 445818124.70\n50%   551306280.51 435549725.35 521734665.60\n75%   621482597.83 772901261.74 728712562.05\nmax   879250168.52 979050957.96 948594814.82\n\nAPI:\npandas.set_option\n","label":[[136,146,"Mention"],[4024,4041,"API"]],"Comments":[]}
{"id":59965,"text":"ID:41384950\nPost:\nText: your pivot table \nCode: table = pd.pivot_table(df, values=['Amount'],\n                       index=['Location', 'Employee'],\n                       columns=['Account', 'Currency'],\n                       fill_value=0, aggfunc=np.sum, dropna=True, )\nprint(table)\n\n                  Amount                  \nAccount            Basic         Net      \nCurrency             GBP   USD   GBP   USD\nLocation Employee                         \nAirport  Test 2        0  3000     0  2000\nTown     Test 1        0  4000     0  3000\n         Test 3     5000     0  4000     0\n\nText: croncat \nCode: pd.concat([\n    d.append(d.sum().rename((k, 'Total')))\n    for k, d in table.groupby(level=0)\n]).append(table.sum().rename(('Grand', 'Total')))\n\n\n                  Amount                  \nAccount            Basic         Net      \nCurrency             GBP   USD   GBP   USD\nLocation Employee                         \nAirport  2             0  3000     0  2000\n         Total         0  3000     0  2000\nTown     1             0  4000     0  3000\n         3          5000     0  4000     0\n         Total      5000  4000  4000  3000\nGrand    Total      5000  7000  4000  5000\n\nText: Old Answer \nText: for posterity \nText: build sub totals \nCode: tab_tots = table.groupby(level='Location').sum()\ntab_tots.index = [tab_tots.index, ['Total'] * len(tab_tots)]\nprint(tab_tots)\n\n               Amount                  \nAccount         Basic         Net      \nCurrency          GBP   USD   GBP   USD\nLocation                               \nAirport  Total      0  3000     0  2000\nTown     Total   5000  4000  4000  3000\n\nText: all together \nCode: pd.concat(\n    [table, tab_tots]\n).sort_index().append(\n    table.sum().rename(('Grand', 'Total'))\n)\n\nAPI:\npandas.concat\n","label":[[595,602,"Mention"],[1757,1770,"API"]],"Comments":[]}
{"id":59966,"text":"ID:41388666\nPost:\nText: You can use shift and cvmsum to get 'islands' songs: \nCode: >>> df = pd.DataFrame({'user_id': [1, 1, 1, 1, 2, 2, 2, 2], 'start_timestamp': [1, 3, 20, 26, 1, 5, 40, 42], 'end_timestamp': [2, 4, 25, 27, 2, 10, 41, 50]}, columns=['user_id', 'start_timestamp', 'end_timestamp'])\n>>> df\n   user_id  start_timestamp  end_timestamp\n0        1                1              2\n1        1                3              4\n2        1               20             25\n3        1               26             27\n4        2                1              2\n5        2                5             10\n6        2               40             41\n7        2               42             50\n\n>>> df['session_break'] = (df['start_timestamp'] - df.groupby('user_id')['end_timestamp'].shift(1) >= 15).astype('int')\n>>> df\n   user_id  start_timestamp  end_timestamp  session_break\n0        1                1              2              0\n1        1                3              4              0\n2        1               20             25              1\n3        1               26             27              0\n4        2                1              2              0\n5        2                5             10              0\n6        2               40             41              1\n7        2               42             50              0\n>>> df['session_label'] = df.groupby('user_id')['session_break'].cumsum()\n>>> df\n   user_id  start_timestamp  end_timestamp  session_break  session_label\n0        1                1              2              0              0\n1        1                3              4              0              0\n2        1               20             25              1              1\n3        1               26             27              0              1\n4        2                1              2              0              0\n5        2                5             10              0              0\n6        2               40             41              1              1\n7        2               42             50              0              1\n\nText: update \nText: To get average session duration you can do this: \nCode: >>> g = df.groupby(['user_id', 'session_label']).agg({'end_timestamp' : np.max, 'start_timestamp' : np.min})\n>>> g\n                       start_timestamp  end_timestamp\nuser_id session_label                                \n1       0                            1              4\n        1                           20             27\n2       0                            1             10\n        1                           40             50\n\n>>> (g['end_timestamp'] - g['start_timestamp']).groupby(level=0).mean()\nuser_id\n1    5.0\n2    9.5\n\nAPI:\npandas.DataFrame.shift\npandas.DataFrame.cumsum\n","label":[[36,41,"Mention"],[46,52,"Mention"],[2702,2724,"API"],[2725,2748,"API"]],"Comments":[]}
{"id":59967,"text":"ID:41397223\nPost:\nText: You may want to consider using a merge to get rid of the for and if construct. \nText: You would create a pandas data frame which maps a class to its probabilities (e.g. 'classA': [1,0,0]). Afterwards merge this mapping with the predicted classes. \nText: Example: \nCode: import pandas as pd\nimport numpy as np\n\nclasses = ['classA', 'classB', 'classC']\n\npredictionProbabilityMapping = pd.DataFrame(index=classes, columns=classes, data=[\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n])\n\ndef convertProbabilities(predictions):\n    predictionsDf = pd.DataFrame(columns=['classification'], data=predictions)\n    return pd.merge(predictionsDf, \n                    predictionProbabilityMapping, \n                    left_on='classification', right_index=True)[classes]\n\npredictions = np.array(['classB', 'classB', 'classC'])\nprint convertProbabilities(predictions)\n\nText: Which yields: \nCode:        classA  classB  classC\n0       0       1       0\n1       0       1       0\n2       0       0       1\n\nAPI:\npandas.DataFrame.merge\n","label":[[57,62,"Mention"],[1036,1058,"API"]],"Comments":[]}
{"id":59968,"text":"ID:41410549\nPost:\nText: What you encounter is actually a special case that makes it easier to compare Series or numpy.ndarray with normal python constructs. The source code reads: \nCode: def flex_wrapper(self, other, level=None, fill_value=None, axis=0):\n    # validate axis\n    if axis is not None:\n        self._get_axis_number(axis)\n    if isinstance(other, ABCSeries):\n        return self._binop(other, op, level=level, fill_value=fill_value)\n    elif isinstance(other, (np.ndarray, list, tuple)):\n        if len(other) != len(self):\n            # ---------------------------------------\n            # you never reach the `==` path because you get into this.\n            # ---------------------------------------\n            raise ValueError('Lengths must be equal')  \n        return self._binop(self._constructor(other, self.index), op,\n                           level=level, fill_value=fill_value)\n    else:\n        if fill_value is not None:\n            self = self.fillna(fill_value)\n\n        return self._constructor(op(self, other),\n                                 self.index).__finalize__(self)\n\nText: You're hitting the ValueError because pandas assumes for .eq that you wanted the value converted to a numpy.ndarray or pd.Series (if you give it an array, list or tuple) instead of actually comparing it to the tuple. For example if you have: \nCode: s = pd.Series([1,2,3])\ns.eq([1,2,3])\n\nText: you wouldn't want it to compare each element to [1,2,3]. \nText: The problem is that object arrays (as with dtype=uint) often slip through the cracks or are neglected on purpose. A simple if self.dtype != 'object' branch inside that method could resolve this issue. But maybe the developers had strong reasons to actually make this case different. I would advise to ask for clarification by posting on their bug tracker. \nText: You haven't asked how you can make it work correctly but for completness I'll include one possibility (according to the source code it seems likely you need to wrap it as Serxies yourself): \nCode: >>> s.eq(pd.Series([(1, 2)]))\n0     True\n1    False\n2    False\ndtype: bool\n\nAPI:\npandas.Series\npandas.Series\npandas.Series\n","label":[[102,108,"Mention"],[1234,1243,"Mention"],[2006,2013,"Mention"],[2113,2126,"API"],[2127,2140,"API"],[2141,2154,"API"]],"Comments":[]}
{"id":59969,"text":"ID:41426010\nPost:\nText: replace in pandas lets you use regex and ( has special meaning in regex so use \\( \nCode: df['Title'] = df['Title'].str.replace('\\(\\(\\(', '>>')\n\nText: pandas doc: replace \nAPI:\npandas.Series.str.replace\n","label":[[186,193,"Mention"],[200,225,"API"]],"Comments":[]}
{"id":59970,"text":"ID:41434647\nPost:\nText: Calling df['colA'].apply(foo) is similar to: foo(df['colA']) (where df['colA'] - is a pandas.Series), so your function should be able to accept Series as an argument - if it's not the case and foo() can accept only scalar arguments, then we have to call foo() for each row: \nCode: df[['colA']].apply(foo, axis=1)\n\nText: NOTE: df[['colA']] - is a DataFrame, as Series.apply() function doesn't have axis argument \nAPI:\npandas.Series\n","label":[[168,174,"Mention"],[441,454,"API"]],"Comments":[]}
{"id":59971,"text":"ID:41457495\nPost:\nText: You can use list comprehension with nosnull for remove NaN values: \nCode: print (x.apply(lambda y: [a  for a in y if pd.notnull(a)]))\n0    [1, 2, 3]\n1          [2]\n2    [3, 4, 5]\n3           []\ndtype: object\n\nText: Another solution with filter with condition where v!=v only for NaN: \nCode: print (x.apply(lambda a: list(filter(lambda v: v==v, a))))\n0    [1, 2, 3]\n1          [2]\n2    [3, 4, 5]\n3           []\ndtype: object\n\nText: Thank you DYZ for another solution: \nCode: print (x.apply(lambda y: list(filter(np.isfinite, y))))\n0    [1, 2, 3]\n1          [2]\n2    [3, 4, 5]\n3           []\ndtype: object\n\nAPI:\npandas.notnull\n","label":[[60,67,"Mention"],[634,648,"API"]],"Comments":[]}
{"id":59972,"text":"ID:41524268\nPost:\nText: What you're looking for is: \nCode: titanic_df[titanic_df['Sex'] == 'male']\n\nText: This is basically a SELECT * FROM titanic_df WHERE Sex == 'male', if you're familiar with SQL. \nText: Edit: If you want to create two different DataFrame objects from each level of Sex, you can store each DataFrame in a dictionary, like this: \nCode: distinct_dfs = {}\nfor level in set(titanic_df['Sex']):\n     level_df = titanic_df[titanic_df['Sex'] == level]\n     distinct_dfs[level] = level_df\n\nText: That's just one approach you could take, and would be advantageous with many different values for Sex. But, since you only have two values, this would be easiest: \nCode: female_df = titanic_df[titanic_df['Sex'] == 'female']\nmale_df = titanic_df[titanic_df['Sex'] == 'male']\n\nAPI:\npandas.DataFrame\n","label":[[250,259,"Mention"],[789,805,"API"]],"Comments":[]}
{"id":59973,"text":"ID:41701525\nPost:\nText: Use contains to match regexp. \nCode: df = df.loc[df['a'].str.contains('^N[0-9]+')]\n\nAPI:\npandas.Series.str.contains\n","label":[[28,36,"Mention"],[113,139,"API"]],"Comments":[]}
{"id":59974,"text":"ID:41775226\nPost:\nText: You can use pd.melt which transform data from wide to long format: \nCode: import pandas as pd\npd.melt(df, id_vars=\"User\", var_name = \"Question\", value_name=\"Answer\")\n\nOut[246]:\n  User  Question  Answer\n0   1   Day-1   Good\n1   2   Day-1   Good\n2   3   Day-1   Good\n3   4   Day-1   Bad\n4   5   Day-1   Ok\n5   1   Day-2   Good\n6   2   Day-2   Ok\n7   3   Day-2   Ok\n8   4   Day-2   Bad\n9   5   Day-2   Bad\n10  1   Day-3   Bad\n11  2   Day-3   Ok\n12  3   Day-3   Ok\n13  4   Day-3   Good\n14  5   Day-3   Bad\n\nText: Another option is to use stack(): \nCode: (df.set_index(\"User\").stack()\n   .rename_axis((\"User\", \"Question\"))\n   .rename(\"Answer\").reset_index())\n\nOut[248]:\n  User  Question Answer\n0   1   Day-1   Good\n1   1   Day-2   Good\n2   1   Day-3   Bad\n3   2   Day-1   Good\n4   2   Day-2   Ok\n5   2   Day-3   Ok\n6   3   Day-1   Good\n7   3   Day-2   Ok\n8   3   Day-3   Ok\n9   4   Day-1   Bad\n10  4   Day-2   Bad\n11  4   Day-3   Good\n12  5   Day-1   Ok\n13  5   Day-2   Bad\n14  5   Day-3   Bad\n\nAPI:\npandas.melt\n","label":[[36,43,"Mention"],[1019,1030,"API"]],"Comments":[]}
{"id":59975,"text":"ID:41856768\nPost:\nText: Assuming the starting cell is given as (StartRow, StartCol) and the ending cell is given as (EndRow, EndCol), I found the following worked for me: \nCode: # Get the content in the rectangular selection region\n# content is a tuple of tuples\ncontent = xlws.Range(xlws.Cells(StartRow, StartCol), xlws.Cells(EndRow, EndCol)).Value \n\n# Transfer content to pandas dataframe\ndataframe = pandas.DataFrame(list(content))\n\nText: Note: Excel Cell B5 is given as row 5, col 2 in win32com. Also, we need list(...) to convert from tuple of tuples to list of tuples, since there is no df constructor for a tuple of tuples. \nAPI:\npandas.DataFrame\n","label":[[593,595,"Mention"],[637,653,"API"]],"Comments":[]}
{"id":59976,"text":"ID:41859343\nPost:\nText: Just apply cumsum on the pd.Series df['SUM_C'] and assign it to a new column: \nCode: df['CUMSUM_C'] = df['SUM_C'].cumsum()\n\nText: Result: \nCode: df\nOut[34]: \n   A  B  SUM_C  CUMSUM_C\n0  1  1     10       10\n1  1  2     20       30\n\nAPI:\npandas.Series\n","label":[[49,58,"Mention"],[261,274,"API"]],"Comments":[]}
{"id":59977,"text":"ID:41875483\nPost:\nText: make sure you read 10 Minutes to pandas. For this case we are using fillna method \nCode: df = pd.DataFrame({'country' : ['US','FR','DE','SP'], \n    'energy_per_capita': [10,8,9,7], \n    'pop_2014' : [300,70,80,60],\n    'pop_2015': [305,72,80,np.nan]})\n\ndf['total energy consumption']= df['energy_per_capita'] *df['pop_2015'].fillna(df['pop_2014'])\nprint df\n\nText: output \nCode:   country  energy_per_capita  pop_2014  pop_2015  total energy consumption\n0      US                 10       300     305.0                    3050.0\n1      FR                  8        70      72.0                     576.0\n2      DE                  9        80      80.0                     720.0\n3      SP                  7        60       NaN                     420.0\n\nAPI:\npandas.DataFrame.fillna\n","label":[[92,98,"Mention"],[783,806,"API"]],"Comments":[]}
{"id":59978,"text":"ID:41878330\nPost:\nText: You can use pd.crosstab with cumcount column as the columns parameter: \nCode: (pd.crosstab(df.x, df.groupby('x').cumcount() + 1, df.y, \n            aggfunc = lambda x: x.iloc[0])\n   .rename(columns=\"y_{}\".format).reset_index())\n\nAPI:\npandas.crosstab\n","label":[[36,47,"Mention"],[258,273,"API"]],"Comments":[]}
{"id":59979,"text":"ID:41879821\nPost:\nText: You can try the following methods. The error comes from the fact that each sublist is interpreted as a row when using DataFrame constructor. You can either make a dictionary out of the headers and the list: \nCode: import pandas as pd\nheaders = ['id', 'fname', 'name']\ndf = pd.DataFrame(dict(zip(headers, foo)))\n\ndf\n#fname  id  lname\n#0   a   1     aa\n#1   b   2     bb\n#2   c   3     cc\n#3   d   4     dd\n#4   e   5     ee\n\nText: Or transpose the list: \nCode: df = pd.DataFrame(list(zip(*foo)), columns=headers)\n\ndf\n#  id   fname   lname\n#0  1       a      aa\n#1  2       b      bb\n#2  3       c      cc\n#3  4       d      dd\n#4  5       e      ee\n\nAPI:\npandas.DataFrame\n","label":[[142,151,"Mention"],[678,694,"API"]],"Comments":[]}
{"id":59980,"text":"ID:42037645\nPost:\nText: This question is pretty wide open, so I will show two possible ways to parse this data into a structure that can be accessed in the manner you described. \nText: Solution #1 \nText: This code uses a bit more advanced python and libraries. It uses a generator around a csv reader to allow the multiple sections of the data to be read efficiently. The data is then placed into a DataFrame per section. And each data frame is accessible in a dict. \nText: The data can be accessed like: \nCode: ratings['food']['taste']\n\nText: This will give a pandas.Series. A regular python list can be had with: \nCode: list(ratings['food']['taste'])\n\nText: Code to read data to Pandas Dataframe using a generator: \nCode: def csv_record_reader(csv_reader):\n    \"\"\" Read a csv reader iterator until a blank line is found. \"\"\"\n    prev_row_blank = True\n    for row in csv_reader:\n        row_blank = (row[0] == '')\n        if not row_blank:\n            yield row\n            prev_row_blank = False\n        elif not prev_row_blank:\n            return\n\nratings = {}\nratings_reader = csv.reader(my_csv_data)\nwhile True:\n    category_row = list(csv_record_reader(ratings_reader))\n    if len(category_row) == 0:\n        break\n    category = category_row[0][0]\n\n    # get the generator for the data section\n    data_generator = csv_record_reader(ratings_reader)\n\n    # first row of data is the column names\n    columns = next(data_generator)\n\n    # use the rest of the data to build a data frame\n    ratings[category] = pd.DataFrame(data_generator, columns=columns)\n\nText: Solution #2 \nText: Here is a solution to read the data to a dict. The data can be accessed with something like: \nCode: ratings['food']['taste']\n\nText: Code to read CSV to dict: \nCode: from collections import namedtuple\n\nratings_reader = csv.reader(my_csv_data)\nratings = {}\nneed_category = True\nneed_header = True\nfor row in ratings_reader:\n    if row[0] == '':\n        if not (need_category or need_header):\n            # this is the end of a data set\n            need_category = True\n            need_header = True\n\n    elif need_category:\n        # read the category (food, drink, ...)\n        category = ratings[row[0]] = dict(rows=[])\n        need_category = False\n\n    elif need_header:\n        # read the header (place, taste, ...)\n        for key in row:\n            category[key] = []\n        DataEnum = namedtuple('DataEnum', row)\n        need_header = False\n\n    else:\n        # read a row of data\n        row_data = DataEnum(*row)\n        category['rows'].append(row_data)\n        for k, v in row_data._asdict().items():\n            category[k].append(v)\n\nText: Test Data: \nCode: my_csv_data = [x.strip() for x in \"\"\"\n    food,,\n    ,,\n    place,taste,day\n    a,good,1\n    b,good,2\n    c,awesome,3\n    d,nice,4\n    e,ok,5\n    ,,\n    ,,\n    ,,\n    drink,,\n    ,,\n    place,taste,day\n    a,good,1\n    b,good,2\n    c,awesome,3\n    d,nice,4\n    e,ok,5\n\"\"\".split('\\n')[1:-1]]\n\nText: To read the data from a file: \nCode: with open('ratings_file.csv', 'rb') as ratings_file: \n    ratings_reader = csv.reader(ratings_file)\n\nAPI:\npandas.DataFrame\n","label":[[399,408,"Mention"],[3100,3116,"API"]],"Comments":[]}
{"id":59981,"text":"ID:42080062\nPost:\nText: Getting the delay_class \nText: Compute the time delay as you have done. \nText: Then, delay_class can be computed from function like this. The difference of two Timestamp objects yields a Timedelta object: \nCode: from pandas import Timedelta, NaT\n\ndef delay_class(delay=NaT):\n    if delay is NaT:\n        return 'A'\n        # assuming a null delay means 0, modify above line if needed.\n    if delay <= Timedelta(minutes=15)\n        return 'A'\n    if delay > Timedelta(minutes=15) and delay <= Timedelta(minutes=60):\n        return 'B'\n    if delay > Timedelta(minutes=60):\n        return 'C'\n\nText: apply this function to create a new column \nCode: df['delay class'] = df.delay.apply(delay_class)\n\nText: Cleaning up the data \nText: Investigate the rows where the delay is negative. \nCode: df_bad = df[df.delay < Timedelta(0)]\n\nText: either work with only the good data (negate the filter condition above) or modify the bad data, (for example, set them to 0) \nAPI:\npandas.Timestamp\npandas.Timedelta\n","label":[[184,193,"Mention"],[211,220,"Mention"],[987,1003,"API"],[1004,1020,"API"]],"Comments":[]}
{"id":59982,"text":"ID:42131286\nPost:\nText: Tested in python 3.11, pandas 1.5.1, matplotlib 3.6.2 \nText: Sample Data and Imports \nCode: import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(2022)  # creates a consistent sample\ny = np.random.rand(10,4)\ny[:,0]= np.arange(10)\ndf = pd.DataFrame(y, columns=[\"X\", \"A\", \"B\", \"C\"])\n\n     X         A         B         C\n0  0.0  0.499058  0.113384  0.049974\n1  1.0  0.486988  0.897657  0.647452\n2  2.0  0.721135  0.831353  0.827568\n3  3.0  0.957044  0.368044  0.494838\n4  4.0  0.619429  0.977530  0.096433\n5  5.0  0.292499  0.298675  0.752473\n6  6.0  0.523737  0.864436  0.388843\n7  7.0  0.475181  0.564672  0.349429\n8  8.0  0.037820  0.794270  0.357883\n9  9.0  0.914509  0.372662  0.964883\n\nText: Several columns can be plotted at once by supplying a list of column names to the y= parameter in pd.DataFrame.plot \nCode: ax = df.plot(x=\"X\", y=[\"A\", \"B\", \"C\"], kind=\"bar\", rot=0)\n\nText: This will produce a graph where bars are grouped. \nCode: ax = df.plot(x=\"X\", y=[\"A\", \"B\", \"C\"], kind=\"bar\", rot=0, stacked=True)\n_ = ax.legend(bbox_to_anchor=(1, 1.02), loc='upper left')\n\nText: This will produce a graph where bars are stacked. \nText: In order to have them overlapping, you would need to call .plot several times, and supply the first returned axes to the ax= parameter of the subsequent plots. \nCode: ax = df.plot(x=\"X\", y=\"A\", kind=\"bar\", rot=0)\ndf.plot(x=\"X\", y=\"B\", kind=\"bar\", ax=ax, color=\"C2\", rot=0)\ndf.plot(x=\"X\", y=\"C\", kind=\"bar\", ax=ax, color=\"C3\", rot=0)\n\nplt.show()\n\nText: This will produce a graph where bars are layered, which is neither a standard or recommended implementation because larger values plotted in a later group will cover smaller values, as can be seen at x=9.0, where C=0.964883 covers, A=0.914509 and B=0.372662. Data plotted in this way is likely to be misinterpreted. \nText: This plot only makes sense if the highest values are those from the first column plotted for all bars. This seems to be the case in the desired output from the question. Otherwise I would not recommend using this kind of plot and instead either use a stacked plot or the grouped bars from the first solution here. One could experiment with transparency (alpha) and see if the latter solution gives an appealing result. \nAPI:\npandas.DataFrame.plot\n","label":[[851,868,"Mention"],[2292,2313,"API"]],"Comments":[]}
{"id":59983,"text":"ID:42141250\nPost:\nText: Setup \nText: To create a dummy data frame which corresponds to your provided example, I used the following: \nCode: import pandas as pd\nimport numpy as np\nimport random\n\n# define sequence and target\nsequence = [\"H\", \"C\"]\ntarget = \"H\"\n\n# define shapes\nsize_col = 4\nsize_row = 100\n\n# create dummy data and dummy columns\narray_indices = np.random.choice(sequence, size=(size_row, size_col))\narray_value = np.random.random(size=(size_row, 1))\narray = np.concatenate([array_indices, array_value], axis=1)\n\ncol_indices = [\"Idx {}\".format(x) for x in range(size_col)]\ncol_values = [\"Probability\"]\ncolumns = col_indices + col_values\n\n# create pandas data frame\ndf = pd.DataFrame(array, columns=columns)\ndf[col_values] = df[col_values].astype(float)\n\nText: The resulting pd.DataFrame looks like this: \nCode: >>> print(df.head())\n\nIdx 0   Idx 1   Idx 2   Idx 3   Probability\n  C       C       C       H     0.892125\n  C       H       C       H     0.633699\n  C       C       C       C     0.228546\n  H       C       H       C     0.766639\n  C       H       C       C     0.379930\n\nText: The only difference to your data frame is the reset index (you get the same when using df.reset_index()). \nText: Solution \nText: Now, to get the sums of the rows with a target value for all indices, you may use the following: \nCode: bool_indices = df[col_indices] == target\nresult = bool_indices.apply(lambda x: df.loc[x, col_values].sum())\n\nText: First, you create a new data frame with boolean values which correspond to each index column containing the target value for each row. \nText: Second, you use these boolean series as index columns to define a subset of your actual value column and finally apply an arbitrary method like sum() on it. \nText: The result is the following: \nCode: >>> print(result)\n\n                   Idx 0       Idx 1      Idx 2       Idx 3\nProbability     23.246007   23.072544   24.775996   24.683079\n\nText: This solution is flexible in regard to your input sequence, the target and the shape of your data. \nText: In addition, if you want to use slicing with wildcards, you can use the idx on your original data frame example like: \nCode: idx = pd.IndexSlice\n\n# to get all rows which have the \"H\" at second index\ndf.loc[idx[:, \"H\"], :]\n\n# to get all rows which have the \"H\" at third index\ndf.loc[idx[:, :, \"H\"], :]\n\nAPI:\npandas.DataFrame\npandas.IndexSlice\n","label":[[785,797,"Mention"],[2116,2119,"Mention"],[2351,2367,"API"],[2368,2385,"API"]],"Comments":[]}
{"id":59984,"text":"ID:42151385\nPost:\nText: You can use get_dummies and concatenate the result with the original data frame: \nCode: pd.concat([df, pd.get_dummies(df.ph_level).rename(columns = \"{}_binary\".format)], axis = 1)\n\nAPI:\npandas.get_dummies\n","label":[[36,47,"Mention"],[210,228,"API"]],"Comments":[]}
{"id":59985,"text":"ID:42168328\nPost:\nText: You can break the lists in the fields column into multiple columns by applying Series to fields and then merging to id and name like so: \nCode: cols = df.columns[df.columns != 'fields'].tolist() # adapted from @jezrael \ndf = df[cols].join(df.fields.apply(pandas.Series))\n\nText: Then you can melt the resulting new columns using set_index and stack, and then reseting the index: \nCode: df = df.set_index(cols).stack().reset_index()\n\nText: Finally, drop the redundant column generated by reset_index and rename the generated column to \"field\": \nCode: df = df.drop(df.columns[-2], axis=1).rename(columns={0: 'field'})\n\nAPI:\npandas.Series\n","label":[[103,109,"Mention"],[645,658,"API"]],"Comments":[]}
{"id":59986,"text":"ID:42216306\nPost:\nText: The dataframe iterator returns a copy of the row, so you don't see your changes to the underlying row. \nText: Reference: iterrows \nAPI:\npandas.DataFrame.iterrows\n","label":[[145,153,"Mention"],[160,185,"API"]],"Comments":[]}
{"id":59987,"text":"ID:42522656\nPost:\nText: You can use the method replace to replace Android to Java then use groupby to aggregate the data. \nText: This should work: \nCode: rules = {'Android':'Java'}\ndf['title'].replace(rules,inplace=True)\ndf = df.groupby('title').sum().reset_index()\nprint(df)\n\nText: Output: \nCode:         title  collectionsCount  subscribersCount  entriesCount  viewsCount\n0           C              2213             14870            50       52019\n1         C++              3495             18404           101       75019\n2          Go              5321             12881           179      145851\n3        Java            801798            165516          8296    12200339\n4  JavaScript            222100             88872          2412     3548736\n5         PHP             15376             25143           375      329720\n6      Python             45234             45100          1007      930588\n7        Ruby              1543              6711            40       45162\n8       Swift             28498             30317          1180      928488\n9         iOS            163137             65896          3601     3739843\n\nAPI:\npandas.Series.replace\npandas.DataFrame.groupby\n","label":[[47,54,"Mention"],[91,98,"Mention"],[1140,1161,"API"],[1162,1186,"API"]],"Comments":[]}
{"id":59988,"text":"ID:42590721\nPost:\nText: Update: \nText: This functionality has been added to pandas 0.24.0: \nText: ExcelWriter now accepts mode as a keyword argument, enabling append to existing workbooks when using the openpyxl engine (GH3441) \nText: Previous version: \nText: Pandas has an open feature request for this. \nText: In the mean time, here is a function which adds a pd.DataFrame to an existing workbook: \nText: Code: \nCode: def add_frame_to_workbook(filename, tabname, dataframe, timestamp):\n    \"\"\"\n    Save a dataframe to a workbook tab with the filename and tabname\n    coded to timestamp\n\n    :param filename: filename to create, can use strptime formatting\n    :param tabname: tabname to create, can use strptime formatting\n    :param dataframe: dataframe to save to workbook\n    :param timestamp: timestamp associated with dataframe\n    :return: None\n    \"\"\"\n    filename = timestamp.strftime(filename)\n    sheet_name = timestamp.strftime(tabname)\n\n    # create a writer for this month and year\n    writer = pd.ExcelWriter(filename, engine='openpyxl')\n    \n    try:\n        # try to open an existing workbook\n        writer.book = load_workbook(filename)\n        \n        # copy existing sheets\n        writer.sheets = dict(\n            (ws.title, ws) for ws in writer.book.worksheets)\n    except IOError:\n        # file does not exist yet, we will create it\n        pass\n\n    # write out the new sheet\n    dataframe.to_excel(writer, sheet_name=sheet_name)\n    \n    # save the workbook\n    writer.save()\n\nText: Test Code: \nCode: import datetime as dt\nimport pandas as pd\nfrom openpyxl import load_workbook\n\ndata = [x.strip().split() for x in \"\"\"\n                   Date  Close\n    2016-10-18T13:44:59  2128.00\n    2016-10-18T13:59:59  2128.75\n\"\"\".split('\\n')[1:-1]]\ndf = pd.DataFrame(data=data[1:], columns=data[0])\n\nname_template = '.\/sample-%m-%y.xlsx'\ntab_template = '%d_%H_%M'\nnow = dt.datetime.now()\nin_an_hour = now + dt.timedelta(hours=1)\nadd_frame_to_workbook(name_template, tab_template, df, now)\nadd_frame_to_workbook(name_template, tab_template, df, in_an_hour)\n\nText: (Source) \nAPI:\npandas.DataFrame\n","label":[[362,374,"Mention"],[2097,2113,"API"]],"Comments":[]}
{"id":59989,"text":"ID:42602505\nPost:\nText: There is a pd.DataFrame.count method, which is shadowing the name of your column. This is why you're getting this error message - the bound method count is being accessed, which then obviously doesn't work. \nText: In this case, you should simply use the ['name_of_column'] syntax to access the count column in both places, and be mindful of DataFrame method names when naming columns in the future. \nText: death_2013['percent_of_total'] = death_2013['count'].apply( lambda x: (x \/ death_2013['count'].sum())) \nText: Note however that in this particular case there is no need to use apply - you can simply divide the entire Series by the mean. \nCode: death_2013['count'] \/ death_2013['count'].sum()\n\nAPI:\npandas.DataFrame.count\n","label":[[35,53,"Mention"],[728,750,"API"]],"Comments":[]}
{"id":59990,"text":"ID:42625713\nPost:\nText: map_partitions was the answer. After several days of experiments and time measurements, I've come to the following code. It gives 2-4 times speedup compared to list comprehensions or generator expressions wrapping itertuples \nCode: def func(data):\n    filtered = # filter data.image\n    result = np.histogram(filtered)\n    return result\n\n\ndef func_partition(data, additional_args):\n    result = data.apply(func, args=(bsifilter, ), axis=1)\n    return result\n\nif __name__ == '__main__':\n    dask.set_options(get=dask.multiprocessing.get)\n    n = 30000\n    df = pd.DataFrame({'image': [(np.random.random((180, 64)) * 255).astype(np.uint8) for i in np.arange(n)],\n                   'n1': np.arange(n),\n                   'n2': np.arange(n) * 2,\n                   'n3': np.arange(n) * 4\n                   }\n                  )\n\n\n    ddf = dd.from_pandas(df, npartitions=MAX_PROCESSORS)\n    dhists = ddf.map_partitions(func_partition, bfilter, meta=pd.Series(dtype=np.ndarray))\n    print 'Delayed dhists = \\n', dhists\n    hists = pd.Series(dhists.compute())\n\nAPI:\npandas.DataFrame.itertuples\n","label":[[238,248,"Mention"],[1086,1113,"API"]],"Comments":[]}
{"id":59991,"text":"ID:42632557\nPost:\nText: Use the from_dic alternative constructor. Build your \"rows\" into a list to begin with: \nCode: In [22]: import numpy as np\n\nIn [23]: nan = np.nan\n\nIn [24]: rows = []\n\nIn [25]: rows.append({'Autor': nan,\n    ...:  'Balcon\/loj': '2',\n    ...:  'Etaj': '1',\n    ...:  'Grup sanitar': 'separat',\n    ...:  'Locul de amplasare n cas': 'In mijlocul casei',\n    ...:  'Numrul casei': nan,\n    ...:  'Numrul de camere': '4 i mai multe camere',\n    ...:  'Parcare': 'deschis',\n    ...:  'Preul': nan,\n    ...:  'Sectorul': nan,\n    ...:  'Strada': nan,\n    ...:  'Suprafaa total': '90 m',\n    ...:  'Tipul cldirii': 'Dat n exploatare'})\n\nIn [26]: rows.append({'Autor': nan,\n    ...:  'Numrul casei': nan,\n    ...:  'Numrul de camere': '3 camere',\n    ...:  'Preul': nan,\n    ...:  'Sectorul': nan,\n    ...:  'Strada': nan,\n    ...:  'Suprafaa total': '103 m',\n    ...:  'Tipul cldirii': 'Dat n exploatare'})\n\nText: Then, just make sure the pass the appropriate \"orient\" argument: \nCode: In [28]: pd.DataFrame.from_dict(rows, orient='columns')\nOut[28]:\n   Autor Balcon\/loj Etaj Grup sanitar Locul de amplasare n cas  \\\n0    NaN           2    1      separat          In mijlocul casei\n1    NaN         NaN  NaN          NaN                        NaN\n\n   Numrul casei      Numrul de camere   Parcare  Preul  Sectorul  Strada  \\\n0            NaN  4 i mai multe camere  deschis     NaN       NaN     NaN\n1            NaN               3 camere       NaN     NaN       NaN     NaN\n\n  Suprafaa total     Tipul cldirii\n0            90 m  Dat n exploatare\n1           103 m  Dat n exploatare\n\nText: EDIT \nText: Actually, just noticed the normal constructor works just fine, and doesn't need any arguments! \nCode: In [31]: pd.DataFrame(rows)\nOut[31]:\n   Autor Balcon\/loj Etaj Grup sanitar Locul de amplasare n cas  \\\n0    NaN           2    1      separat          In mijlocul casei\n1    NaN         NaN  NaN          NaN                        NaN\n\n   Numrul casei      Numrul de camere   Parcare  Preul  Sectorul  Strada  \\\n0            NaN  4 i mai multe camere  deschis     NaN       NaN     NaN\n1            NaN               3 camere       NaN     NaN       NaN     NaN\n\n  Suprafaa total     Tipul cldirii\n0            90 m  Dat n exploatare\n1           103 m  Dat n exploatare\n\nAPI:\npandas.DataFrame.from_dict\n","label":[[32,40,"Mention"],[2347,2373,"API"]],"Comments":[]}
{"id":59992,"text":"ID:42632831\nPost:\nText: df cannot have 3 dimensions: \nText: DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. \nText: However, there is a way to fake 3-dimensions with MultiIndex \/ Advanced Indexing: \nText: Hierarchical indexing (MultiIndex) Hierarchical \/ Multi-level indexing is very exciting as it opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1d) and DataFrame (2d). \nText: If you really need that extra dimension go with pandas.Panel: \nText: Panel is a somewhat less-used, but still important container for 3-dimensional data. \nText: but don't miss this important disclaimer from the docs: \nText: Note: Unfortunately Panel, being less commonly used than Series and DataFrame, has been slightly neglected feature-wise. A number of methods and options available in DataFrame are not available in Panel. \nText: There is also pandas.Panel4D (experimental) in the unlikely chance that you need it. \nAPI:\npandas.DataFrame\n","label":[[24,26,"Mention"],[1177,1193,"API"]],"Comments":[]}
{"id":59993,"text":"ID:42637440\nPost:\nText: You can use pd.concat here it only involves one aggregation and doesn't invoke the merge\/join process: \nCode: agg = df.groupby(['ID', 'FROM_YEAR'], as_index=False)[[\"AREA\", \"AREA2\"]].mean()\n\npd.concat([agg.assign(TYPE = t) for t in [\"A\", \"B\"]], ignore_index=True)\n\nAPI:\npandas.concat\n","label":[[36,45,"Mention"],[294,307,"API"]],"Comments":[]}
{"id":59994,"text":"ID:42659309\nPost:\nText: You can make it easier for yourself while returning a Series instead of a pandas.DataFrame: \nCode: def act_value_calc(x):\n    start_delta = (x.Start.replace(day=31,month=12) - x.Start).days\n    full_delta = (x.End - x.Start).days\n    result1 = round( (x.Value + x.CreditNote) \/ full_delta * start_delta, 2)\n    result2 = round( (x.Value + x.CreditNote) - result1, 2)\n    return(pd.Series({'r1': result1,'r2': result2}))\n\nprint(df.apply(act_value_calc, 1))\n    r1      r2\n0   40.11   39.89\n1   85.23   84.77\n\nAPI:\npandas.Series\n","label":[[78,84,"Mention"],[537,550,"API"]],"Comments":[]}
{"id":59995,"text":"ID:42661988\nPost:\nText: One good way to convert datetime values to epoch values is to create a datetime.timedelta by subtracting the epoch time from the date to be converted. This function does that and can be applied to a pd.Series or to a column of a pandas.DataFrame. \nText: Code: \nCode: import pandas as pd\nimport datetime as dt\nfrom pytz import timezone\n\ndef convert_naive_dt_to_utc_epoch(naive_dt, tz_info):\n    # assign proper timezone to datetime\n    aware = tz_info.localize(naive_dt).astimezone(timezone('UTC'))\n\n    # get a datetime that is equal to epoch in UTC\n    utc_at_epoch = timezone('UTC').localize(dt.datetime(1970, 1, 1))\n\n    # return the number of seconds since epoch\n    return (aware - utc_at_epoch).total_seconds()\n\nText: Test Code: \nCode: data = [np.datetime64(x) for x in\n        \"2016-10-18T13:44:59 2016-02-18T13:59:59\".split()]\nseries = pd.Series(data=data, name='Date')\n\n# apply the conversion function to series to create epoch series\nepoch_series = series.apply(\n    lambda x: convert_naive_dt_to_utc_epoch(x, timezone('US\/Eastern')))\n\nprint(epoch_series)\n\nText: Results: \nCode: 0    1.476813e+09\n1    1.455822e+09\nName: Date, dtype: float64\n\nAPI:\npandas.Series\n","label":[[223,232,"Mention"],[1182,1195,"API"]],"Comments":[]}
{"id":59996,"text":"ID:42707430\nPost:\nText: Instead of loading the data into pandas directly, parse the Excel file using xlrd to reformat the file with proper city | product | count entries and ingest that into a dataframe. You should then be able to cross reference this list against the SKUs using a standard merge \nAPI:\npandas.DataFrame.merge\n","label":[[291,296,"Mention"],[303,325,"API"]],"Comments":[]}
{"id":59997,"text":"ID:42766950\nPost:\nText: I think simple pd.DataFrame.apply with anonimous function should work fine: \nCode: df.apply(lambda x: dict_of_dicts[x.col1][x.col2], axis=1)\n\nAPI:\npandas.DataFrame.apply\n","label":[[39,57,"Mention"],[171,193,"API"]],"Comments":[]}
{"id":59998,"text":"ID:42790043\nPost:\nText: You can use pd.read_csv to read data as a list of data frames and then concatenate them: \nCode: # if you use python 2 change this to \/\/ from io import BytesIO and use BytesIO instead\nfrom io import StringIO     \nimport pandas as pd\n\npd.concat([pd.read_csv(StringIO(d), sep = \";\") for d in data])\n\nText: Since your actual data is a list of responses, you may need access the text firstly: \nCode: pd.concat([pd.read_csv(StringIO(d.text), sep = \";\") for d in data])\n\nCode: data = [u'Country;Celebrity;Song Volume;CPP;Index\\r\\nus;Taylor Swift;33100;0.83;0.20\\r\\n', \n        u'Country;Celebrity;Song Volume;CPP;Index\\r\\nus;Rihanna;28100;0.76;0.33\\r\\n']\n\nAPI:\npandas.read_csv\n","label":[[36,47,"Mention"],[678,693,"API"]],"Comments":[]}
{"id":59999,"text":"ID:42961333\nPost:\nText: The most straight forward way to do this is use rcosstab which gives you a frequency table of the factors: \nCode: pd.crosstab(df.MAHC_PREDICT.astype(str) + \"_Pred\", df.MAHC_ACTUAL.astype(str) + \"_Actual\")\n\nText: For simplicity, if column and index names aren't important: \nCode: pd.crosstab(df.MAHC_PREDICT, df.MAHC_ACTUAL)\n\nText: Yeilds: \nCode: MAHC_ACTUAL  False  True \nMAHC_PREDICT                    \nFalse           126      2\nTrue             13    113\n\nAPI:\npandas.crosstab\n","label":[[72,80,"Mention"],[489,504,"API"]],"Comments":[]}
{"id":60000,"text":"ID:43087460\nPost:\nText: It seems your data file is JSON. \nText: Try the read_json method \nText: It also looks like the data is in the 'records' orientation, so something like: \nCode: pd.read_json('\/Users\/fiz\/Desktop\/xad', orient='records', encoding='utf-8')\n\nText: May be a good place to start. \nText: Sadly the read_json method does not seem to support chunking. \nAPI:\npandas.read_json\n","label":[[72,81,"Mention"],[370,386,"API"]],"Comments":[]}
{"id":60001,"text":"ID:43094422\nPost:\nText: This is happening because a is a pd.DataFrame and it corresponds to table, so during serialization it tries to represent all data for each table column. DataFrame does not know that you have only one value for each column. Values have to be extracted manually: \nCode: a = {column: values[0] for column, values in df.fillna(0).to_dict().items(orient='list')}\nreturn Response(a)\n\nText: For more details check http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.to_dict.html \nAPI:\npandas.DataFrame\n","label":[[57,69,"Mention"],[521,537,"API"]],"Comments":[]}
{"id":60002,"text":"ID:43215869\nPost:\nText: Running read_csv in Jupyter, it works fine as it is now. It just interprets the decimal values as strings. \nText: Note that, in a CSV, everything is in columns, so you may have to transpose your data frame using df.T. Also, to convert your decimals into numeric decimals in Python., you'll need to do the following: \nText: df.applymap(lambda x: float(x.replace(',','.')) \nAPI:\npandas.read_csv\n","label":[[32,40,"Mention"],[401,416,"API"]],"Comments":[]}
{"id":60003,"text":"ID:43244831\nPost:\nText: The default delimiter for pd.read_csv is comma ,, you need to explicitly specify the sep parameter to be space in order to read in space delimited file: \nCode: df = pd.read_csv(\"output.txt\", sep = \" \", index_col=0, header=None)\n\nAPI:\npandas.read_csv\n","label":[[50,61,"Mention"],[258,273,"API"]],"Comments":[]}
{"id":60004,"text":"ID:43329102\nPost:\nText: Using openpyxl \nText: Since you have indicated, that you are looking into a very user friendly way to specify the range (like the excel-syntax) and as Charlie Clark already suggested, you can use openpyxl. \nText: The following utility function takes a workbook and a column\/row range and returns a pandas DataFrame: \nCode: from openpyxl import load_workbook\nfrom openpyxl.utils import get_column_interval\nimport re\n\ndef load_workbook_range(range_string, ws):\n    col_start, col_end = re.findall(\"[A-Z]+\", range_string)\n\n    data_rows = []\n    for row in ws[range_string]:\n        data_rows.append([cell.value for cell in row])\n\n    return pd.DataFrame(data_rows, columns=get_column_interval(col_start, col_end))\n\nText: Usage: \nCode: wb = load_workbook(filename='excel-sheet.xlsx', \n                   read_only=True)\nws = wb.active\nload_workbook_range('B1:C2', ws)\n\nText: Output: \nCode:    B  C\n0  5  6\n1  8  9\n\nText: Pandas only Solution \nText: Given the following data in an excel sheet: \nCode:     A   B   C\n0   1   2   3\n1   4   5   6\n2   7   8   9\n3  10  11  12\n\nText: You can load it with the following command: pd.read_excel('excel-sheet.xlsx') \nText: If you were to limit the data being read, the pd.read_excel method offers a number of options. Use the parse_cols, skiprows and skip_footer to select the specific subset that you want to load: \nCode: pd.read_excel(\n    'excel-sheet.xlsx',    # name of excel sheet\n    names=['B','C'],       # new column header\n    skiprows=range(0,1),   # list of rows you want to omit at the beginning\n    skip_footer=1,         # number of rows you want to skip at the end\n    parse_cols='B:C'       # columns to parse (note the excel-like syntax)\n)\n\nText: Output: \nCode:    B  C\n0  5  6\n1  8  9\n\nText: Some notes: \nText: The API of the read_excel method is not meant to support more complex selections. In case you require a complex filter it is much easier (and cleaner) to load the whole data into a DataFrame and use the excellent slicing and indexing mechanisms provided by pandas. \nAPI:\npandas.read_excel\n","label":[[1229,1242,"Mention"],[2062,2079,"API"]],"Comments":[]}
{"id":60005,"text":"ID:43357863\nPost:\nCode: pd.DataFrame(\n    [dict(zip(x[::2], [len(y) for y in x[1::2]])) for x in categories_list]\n).sum()\n\na    10\nb     6\nc     3\ndtype: int64\n\nText: I'm aiming at creating a list of dictionaries. So I have to fill in the ...... with something that parses each sub-list with a dictionary [ ...... for x in catgories_list] If I use dict on a list or generator of tuples, it will magically turn that into a dictionary with keys as the first value in the tuple and values as the second value in the tuple. dict(...list of tuples...) zip will give me that generator of tuples zip(list one, list two) I know that in each sub-list, my keys are at the even indices [0, 2, 4...] and values are at the odd indices [1, 3, 5, ...] # even odd zip(x[::2], x[1::2]) but x[1::2] will be arrays, and I don't want the arrays. I want the length of the arrays. # even odd zip(x[::2], [len(y) for y in x[1::2]]) DataFrame will take a list of dictionaries and create a dataframe. Finally, use sum to count the lengths. \nAPI:\npandas.DataFrame\n","label":[[909,918,"Mention"],[1021,1037,"API"]],"Comments":[]}
{"id":60006,"text":"ID:43409453\nPost:\nText: You should try out map. It makes it dead simple: \nCode: import pandas as pd\n# Loads csv1 with default index\ndf1 = pd.read_csv(\"csv1.csv\")\n# Loads csv2 and sets the year as the index\ndf2 = pd.read_csv(\"csv2.csv\", index_col=0)\n\ndf1[\"zodiac\"] = df1[\"year\"].map(df2[\"zodiac\"])\n\ndf1.to_csv(\"output.csv\")\n\nText: Note that the index of df2 is the year - so it essentially behaves in a similar way to a dictionary of year-sign. map takes a dictionary or Series as a lookup table to the values in the column. \nAPI:\npandas.Series\n","label":[[470,476,"Mention"],[530,543,"API"]],"Comments":[]}
{"id":60007,"text":"ID:43414749\nPost:\nText: You can just use to_datetime no need to convert to string first (at least in pandas 0.19): \nCode: dates = pd.Series([151215]*8)\ndates = pd.to_datetime(dates, format=\"%y%m%d\")\nprint(dates)\n0   2015-12-15\n1   2015-12-15\n2   2015-12-15\n3   2015-12-15\n4   2015-12-15\n5   2015-12-15\n6   2015-12-15\n7   2015-12-15\ndtype: datetime64[ns]\n\nText: Converting a single value as in your example will result in Timestamp('2015-12-15 00:00:00'), but if you pass the entire series the result looks like the above. \nAPI:\npandas.to_datetime\n","label":[[41,52,"Mention"],[528,546,"API"]],"Comments":[]}
{"id":60008,"text":"ID:43417545\nPost:\nText: You can use the round function to accomplish this: \nCode: In [1]: import pandas as pd\n\nIn [2]: dfOld = pd.DataFrame({'cl': ['0.014', '0.015', '0.016']})\ndfOld.cl.apply(type).value_counts()  # prove that the values are strings\n\nOut[2]:\n<type 'str'>    3\ndtype: int64\n\nIn [3]:\ndfOld.cl.astype(float).round(decimals=2)\n\nOut[3]:\n0    0.01\n1    0.02\n2    0.02\nName: cl, dtype: float64\n\nAPI:\npandas.Series.round\n","label":[[40,45,"Mention"],[410,429,"API"]],"Comments":[]}
{"id":60009,"text":"ID:43424796\nPost:\nText: If I understand you correctly, you want to compare the values in column 4 and column 1, and if they are equal, output a new column with the value from column 3. \nText: To do this, simply use np.where as follows: \nCode: import pandas as pd\nimport numpy as np\n\ndf1 = pd.DataFrame({'CUI':['C0161894','C0029730','C0176693','C0029730','C0000074'],\n                   'ICD9\/10':[39,398,398,3989,3989],\n                   'Out':[4000001,4000002,4000003,4000004,4000005],\n                   'Lookup':['C0000005','C0000039','C0000052','C0000074','C0000074']})                       \n\n\ndf1['Match'] = np.where(df1.Lookup == df1.CUI,  df1.Out, 'No Match')\n\nText: Output: \nCode:         CUI  ICD9\/10    Lookup      Out     Match\n0  C0161894       39  C0000005  4000001  No Match\n1  C0029730      398  C0000039  4000002  No Match\n2  C0176693      398  C0000052  4000003  No Match\n3  C0029730     3989  C0000074  4000004  No Match\n4  C0000074     3989  C0000074  4000005   4000005\n\nText: Edit: \nText: In response to your comment, you can use the chunksize parameter in pd.read_csv to read in only parts of your dataframe: \nText: For data in csv as follows: \nCode:      CUI  ICD9\/10    Lookup      Out\nC0161894       39  C0000005  4000001\nC0029730      398  C0000039  4000002\nC0176693      398  C0000052  4000003\nC0029730     3989  C0000074  4000004\nC0000074     3989  C0000074  4000005\n\nText: See https:\/\/stackoverflow.com\/a\/25962187\/2254228: You can do: \nCode: chunksize = 1000\nfor chunk in pd.read_csv(data, chunksize=chunksize):\n    # process(chunk) using the solution above\n    # Output Chunk to new csv using `pd.to_csv('new_data')`\n\nText: Edit2: Here I have compiled full sample code for you. Replace the file data and new_data with whatever your data file is called and replace the file paths with your file paths. This will avoid any memory errors from your datafile being too big. \nText: For some a sample data.csv: \nCode:      CUI  ICD9\/10    Lookup      Out\nC0161894       39  C0000005  4000001\nC0029730      398  C0000039  4000002\nC0176693      398  C0000052  4000003\nC0029730     3989  C0000074  4000004\nC0000074     3989  C0000074  4000005\n\nText: Create a target csv file new_data as an empty csv file to store your new data frame: \nCode: CUI  ICD9\/10    Lookup      Out\n\nText: Then import the old data, splitting it into chunk, where chunksize = the number of lines of the file to read in: \nCode: # Read in line by line = set chunksize = 1\nchunksize = 1\n\n# Open New File\nwith open(\"Pathtonewfile\/new_data.csv\", \"w\") as f:\n\n    # Iterate over the old data.csv file, reading in one line\n    for chunk in pd.read_csv('Pathtooldfile\/data.csv', index_col = False, chunksize=chunksize):\n\n        # Carry out Lookup Calculation as above\n        chunk['Match'] = np.where(chunk.Lookup == chunk.CUI,  chunk.Out, 'No Match')\n\n        # Write the new dataframe chunk to \"new_data.csv\"\n        chunk.to_csv(f, header=False, index=False, \n                     cols=['CUI','ICD9\/10','Out','Lookup'],\n                     mode = 'a')\n\nText: This gives you an output in new_data.csv as follows: \nCode:         CUI  ICD9\/10    Lookup      Out     Match\n0  C0161894       39  C0000005  4000001  No Match\n1  C0029730      398  C0000039  4000002  No Match\n2  C0176693      398  C0000052  4000003  No Match\n3  C0029730     3989  C0000074  4000004  No Match\n4  C0000074     3989  C0000074  4000005   4000005\n\nAPI:\npandas.read_csv\n","label":[[1079,1090,"Mention"],[3417,3432,"API"]],"Comments":[]}
{"id":60010,"text":"ID:43477904\nPost:\nText: I am going to assume these are you errors for the methods you are trying respectively: \nText: MergeError: No common columns to perform merge on KeyError: 'movieId' \nText: From the DataFrames you posted I assume that your index is movieId, that is what is leading to your errors. If you reset your indices and try again you should be able to merge. So based on these assumptions the following should work: \nCode: out = pd.merge(top_ten_movies.reset_index(), movies.reset_index())\nprint(out)\n   movieId  count      mean                 genres      title\n0        1    247  3.872470              Adventure  Toy Story\n1        2    107  3.401869       Children|Fantasy    Jumanji\n2        6    104  3.884615  Action|Crime|Thriller       Heat\n\nText: Now if your index is movieId as assumed, you can use the pd.DataFrame.join method to accomplish your task. For example: \nCode: print(top_ten_movies.join(movies))\n         count      mean                 genres      title\nmovieId\n1          247  3.872470              Adventure  Toy Story\n2          107  3.401869       Children|Fantasy    Jumanji\n6          104  3.884615  Action|Crime|Thriller       Heat\n10         122  3.450820                    NaN        NaN\n25         101  3.742574                    NaN        NaN\n32         196  3.923469                    NaN        NaN\n34         148  3.601351                    NaN        NaN\n36         104  3.937500                    NaN        NaN\n39         120  3.550000                    NaN        NaN\n47         201  4.034826                    NaN        NaN\nprint(top_ten_movies.join(movies, how='inner'))\n         count      mean                 genres      title\nmovieId\n1          247  3.872470              Adventure  Toy Story\n2          107  3.401869       Children|Fantasy    Jumanji\n6          104  3.884615  Action|Crime|Thriller       Heat\n\nText: All this being said the best thing you could do for yourself is to read Merge, Join, and Concatenate in the pandas documentation. \nAPI:\npandas.DataFrame.join\n","label":[[826,843,"Mention"],[2023,2044,"API"]],"Comments":[]}
{"id":60011,"text":"ID:43479001\nPost:\nText: I believe you can just pass the list into pd.DataFrame() and you will just get NaNs for the values that don't exist. \nText: For example: \nCode: List_of_Lists = [[1,2,3,4],\n                 [5,6,7],\n                 [9,10],\n                 [11]]\ndf = pd.DataFrame(List_of_Lists)\nprint(df)\n    0     1    2    3\n0   1   2.0  3.0  4.0\n1   5   6.0  7.0  NaN\n2   9  10.0  NaN  NaN\n3  11   NaN  NaN  NaN\n\nText: Then to get the naming the way you want just use add_prefix \nCode: df = df.add_prefix('Column')\nprint(df)\n   Column0  Column1  Column2  Column3\n0        1      2.0      3.0      4.0\n1        5      6.0      7.0      NaN\n2        9     10.0      NaN      NaN\n3       11      NaN      NaN      NaN\n\nText: Now I guess there is the possibility that you also could want each list to be a column. In that case you need to transpose your List_of_Lists. \nCode: from itertools import zip_longest\n\ndf = pd.DataFrame(list(map(list, zip_longest(*List_of_Lists))))\nprint(df)\n   0    1     2     3\n0  1  5.0   9.0  11.0\n1  2  6.0  10.0   NaN\n2  3  7.0   NaN   NaN\n3  4  NaN   NaN   NaN\n\nAPI:\npandas.DataFrame.add_prefix\n","label":[[479,489,"Mention"],[1108,1135,"API"]],"Comments":[]}
{"id":60012,"text":"ID:43483381\nPost:\nText: pandas.core.groupby.GroupBy.apply does NOT have named parameter args, but apply does have it. \nText: So try this: \nCode: df.groupby('columnName').apply(lambda x: myFunction(x, arg1))\n\nText: or as suggested by @Zero: \nCode: df.groupby('columnName').apply(myFunction, ('arg1'))\n\nText: Demo: \nCode: In [82]: df = pd.DataFrame(np.random.randint(5,size=(5,3)), columns=list('abc'))\n\nIn [83]: df\nOut[83]:\n   a  b  c\n0  0  3  1\n1  0  3  4\n2  3  0  4\n3  4  2  3\n4  3  4  1\n\nIn [84]: def f(ser, n):\n    ...:     return ser.max() * n\n    ...:\n\nIn [85]: df.apply(f, args=(10,))\nOut[85]:\na    40\nb    40\nc    40\ndtype: int64\n\nText: when using GroupBy.apply you can pass either a named arguments: \nCode: In [86]: df.groupby('a').apply(f, n=10)\nOut[86]:\n    a   b   c\na\n0   0  30  40\n3  30  40  40\n4  40  20  30\n\nText: a tuple of arguments: \nCode: In [87]: df.groupby('a').apply(f, (10))\nOut[87]:\n    a   b   c\na\n0   0  30  40\n3  30  40  40\n4  40  20  30\n\nAPI:\npandas.DataFrame.apply\n","label":[[98,103,"Mention"],[971,993,"API"]],"Comments":[]}
{"id":60013,"text":"ID:43506981\nPost:\nText: The cause \nText: The error is pretty clear. If you check the types of the elements, you will find out that at some point you are tying to add datetime.time object and pandas.Timedelta. \nText: There are 2 kinds of dates, times and timedeltas: \nText: python's builtin from datetime module i.e. datetime.time, datetime.date, datetime.timedelta, ... pandas \/ numpy i.e pandas.Timestamp, Timedelta \nText: these two stacks are incompatible for basic operations as addition or comparison. \nText: Solution 1 \nText: Convert everything to pandas type and extract the times in the end \nText: You should make sure, that dtypes of your columns are something like datetime64[ns] and timedelta64[ns]. For that, try converting them explicitly using pd.to_datetime and pd.to_timedelta. \nText: Solution 2 \nText: Another approach would be just converting the Delta column to datetime.timedelta you could try \nCode: df[\"end_Time\"] = df[\"Start_Time\"] + df[\"Delta\"].map(pd.Timedelta.to_pytimedelta)\n\nText: But you may run into some more errors depending on what is in your df[\"Delta\"] and df[\"Start_Time\"] \nAPI:\npandas.Timedelta\n","label":[[407,416,"Mention"],[1114,1130,"API"]],"Comments":[]}
{"id":60014,"text":"ID:43578742\nPost:\nText: You can use pd.concat or append method for this, both methods will generate NA for columns that don't exist in the sub data frame, to fill them with zero, you can use fillna(0): \nCode: df1.append(df2).fillna(0)\n\n#  beds     property_id\n#0  1.0          1\n#1  2.0          2\n#0  0.0          3\n#1  0.0          4\n\n\npd.concat([df1, df2]).fillna(0)\n\n#  beds     property_id\n#0  1.0         1\n#1  2.0         2\n#0  0.0         3\n#1  0.0         4\n\nAPI:\npandas.concat\n","label":[[36,45,"Mention"],[473,486,"API"]],"Comments":[]}
{"id":60015,"text":"ID:43579255\nPost:\nText: One option is to create all the dates you'd like to complete using date_range and then you can do an outer join between the complete dates with each sub data frame keyed on the date column, finally fill missing values with 0: \nCode: # create complete dates\ndates = pd.DataFrame({\"date\": pd.date_range(\"2017-01-01\", \"2017-01-10\")})\n\n# convert date column to date time if it's not already\ndf['date'] = pd.to_datetime(df.date)\n\n# merge complete dates with each sub data frame separately using groupby.apply\n(df.groupby(['id', 'name'])['date', 'value']\n .apply(lambda g: g.merge(dates, how=\"outer\"))\n .fillna(0)\n .reset_index(level=[0,1])\n .reset_index(drop=True))\n\n#   id       name        date   value\n#0  C1  Company 1   2017-01-01  31.0\n#1  C1  Company 1   2017-01-02  35.0\n#2  C1  Company 1   2017-01-03  32.0\n#3  C1  Company 1   2017-01-06  36.0\n#4  C1  Company 1   2017-01-07  35.0\n#5  C1  Company 1   2017-01-08  34.0\n#6  C1  Company 1   2017-01-10  33.0\n#7  C1  Company 1   2017-01-04  0.0\n#8  C1  Company 1   2017-01-05  0.0\n#9  C1  Company 1   2017-01-09  0.0\n# ...\n\nAPI:\npandas.date_range\n","label":[[91,101,"Mention"],[1103,1120,"API"]],"Comments":[]}
{"id":60016,"text":"ID:43648208\nPost:\nText: These answers are guided by the fact that OP wanted an in place edit of an existing dataframe. Usually, I overwrite the existing dataframe with a new one. \nText: Use pd.DataFrame.fillna with a dict \nText: Pandas fillna allows us to pass a dictionary that specifies which columns will be filled in and with what. \nText: So this will work \nCode: a.fillna({'a': 0, 'b': 0})\n\n     a    b  c\n0  1.0  5.0  5\n1  2.0  0.0  1\n2  0.0  6.0  5\n3  0.0  0.0  2\n\nText: With an in place edit made possible with: \nCode: a.fillna({'a': 0, 'b': 0}, inplace=True)\n\nText: NOTE: I would've just done this a = a.fillna({'a': 0, 'b': 0}) \nText: We don't save text length but we could get cute using dict.fromkeys \nCode: a.fillna(dict.fromkeys(['a', 'b'], 0), inplace=True)\n\nText: loc \nText: We can use the same format as the OP but place it in the correct columns using loc \nCode: a.loc[:, ['a', 'b']] = a[['a', 'b']].fillna(0)\n\na\n\n     a    b  c\n0  1.0  5.0  5\n1  2.0  0.0  1\n2  0.0  6.0  5\n3  0.0  0.0  2\n\nText: update \nText: Explicitly made to make in place edits with the non-null values of another dataframe \nCode: a.update(a[['a', 'b']].fillna(0))\n\na\n\n     a    b  c\n0  1.0  5.0  5\n1  2.0  0.0  1\n2  0.0  6.0  5\n3  0.0  0.0  2\n\nText: Iterate column by column \nText: I really don't like this approach because it is unnecessarily verbose \nCode: for col in ['a', 'b']:\n    a[col].fillna(0, inplace=True)\n\na\n\n     a    b  c\n0  1.0  5.0  5\n1  2.0  0.0  1\n2  0.0  6.0  5\n3  0.0  0.0  2\n\nText: fillna with a dataframe \nText: Use the result of a[['a', 'b']].fillna(0) as the input for another fillna. In my opinion, this is silly. Just use the first option. \nCode: a.fillna(a[['a', 'b']].fillna(0), inplace=True)\n\na\n\n     a    b  c\n0  1.0  5.0  5\n1  2.0  0.0  1\n2  0.0  6.0  5\n3  0.0  0.0  2\n\nAPI:\npandas.DataFrame.fillna\npandas.DataFrame.update\n","label":[[190,209,"Mention"],[1014,1020,"Mention"],[1796,1819,"API"],[1820,1843,"API"]],"Comments":[]}
{"id":60017,"text":"ID:43701645\nPost:\nText: As mentioned in comments, cut to categorize, and then groupby: \nText: Code: \nCode: df.groupby(pd.cut(df.Date, [2000, 2008, 2016]))['TME'].sum()\n\nText: Test Code: \nCode: df = pd.read_fwf(StringIO(\n    u\"\"\"\n    Date  TME\n    2001  503.2\n    2002  529.9\n    2003  559.8\n    2004  593.2\n    2005  629.5\n    2006  652.1\n    2007  664.3\n    2008  688.2\n    2009  732.0\n    2010  759.2\n    2011  769.2\n    2012  759.8\n    2013  760.6\n    2014  753.3\n    2015  757.6\n    2016  753.9\"\"\"\n), header=1)\\\n\nprint(df.groupby(pd.cut(df.Date, [2000, 2008, 2016]))['TME'].sum())\n\nText: Results: \nCode: Date\n(2000, 2008]    4820.2\n(2008, 2016]    6045.6\nName: TME, dtype: float64\n\nAPI:\npandas.cut\n","label":[[50,53,"Mention"],[691,701,"API"]],"Comments":[]}
{"id":60018,"text":"ID:43708106\nPost:\nText: Please use \nText: df.set_index('Date').plot() \nText: or \nText: df.plot(x='Date', y='Result') \nText: because of the plot by default use index of df as the x-axis, so you should set the 'Date' column as the index, or specify which column to use as the x-axis. \nText: see more at plot \nAPI:\npandas.DataFrame.plot\n","label":[[301,305,"Mention"],[312,333,"API"]],"Comments":[]}
{"id":60019,"text":"ID:43742507\nPost:\nText: Option 1: join This solution requires that you set the index of D2 and use the on parameter \nCode: D1.join(D2.set_index('ID'), on='ID')\n\n   ID val1 val2  Target\n0   1    x    y       0\n1   1    x    y       0\n2   2    a    b       1\n3   2    a    c       1\n\nText: Note: if D2 doesn't include all values in D1.ID and you want a null value for the rows of D1 where that is true, then use the how='left' option. \nCode: D1.join(D2.set_index('ID'), on='ID', how='left')\n\nText: from comments: \nText: Why does this require setting the index of D2? The other answers don't do that.  ErikE @ErikE that is the difference between merge and join. pd.DataFrame.merge will perform its merging on column values by default. While join looks at the index by default. I can override joins behavior by specifying a column to join on with on='ID'. However, that override ability is limited to the left object only. So, I have to set the index of the right object in order to execute appropriately.  piRSquared \nText: Option 2: map + assign This solution is going to turn D2 into something dict like, a pd.Series with the index being the 'ID's and the values being the 'Target'. map converts the 'ID' column on D1 into new values and we assign it to a new column with assign. \nCode: D1.assign(Target=D1.ID.map(D2.set_index('ID').Target))\n\n\n   ID val1 val2  Target\n0   1    x    y       0\n1   1    x    y       0\n2   2    a    b       1\n3   2    a    c       1\n\nAPI:\npandas.DataFrame.merge\n","label":[[660,678,"Mention"],[1471,1493,"API"]],"Comments":[]}
{"id":60020,"text":"ID:43761049\nPost:\nText: You could just put those two columns together, sort the pairs, and then drop rows on those sorted pairs: \nCode: df['together'] = [','.join(x) for x in map(sorted, zip(df['term_x'], df['term_y']))]\n\ndf.drop_duplicates(subset=['together'])\nOut[11]: \n   term_x  Intersections     term_y          together\n0  boxers              1     briefs     boxers,briefs\n2  babies              6   costumes   babies,costumes\n4  babies             12    clothes    babies,clothes\n6  babies              1  clothings  babies,clothings\n\nText: Edit: You said time was a huge factor in this problem. Here are some timings comparing mine and Allen's solutions on a dataframe with 200,000 rows: \nCode: while df.shape[0] < 200000:\n    df.append(df)\n\n%timeit df.apply(lambda x: str(sorted([x.term_x,x.term_y])), axis=1)\n1 loop, best of 3: 6.62 s per loop\n\n%timeit [','.join(x) for x in map(sorted, zip(df['term_x'], df['term_y']))]\n10 loops, best of 3: 121 ms per loop\n\nText: As you can see, my approach is more than 98% faster. apply is slow in many instances. \nAPI:\npandas.DataFrame.apply\n","label":[[1029,1034,"Mention"],[1068,1090,"API"]],"Comments":[]}
{"id":60021,"text":"ID:43797255\nPost:\nText: This happens because the .append() method returns a new df: \nText: Pandas Docs (0.19.2): append Returns: appended: DataFrame \nText: Here's a working example so you can see what's happening in each iteration of the loop: \nCode: df1 = pd.DataFrame([[1,2],], columns=['a','b'])\ndf2 = pd.DataFrame()\nfor i in range(0,2):\n    print(df2.append(df1))\n\n>    a  b\n> 0  1  2\n>    a  b\n> 0  1  2\n\nText: If you assign the output of .append() to a df (even the same one) you'll get what you probably expected: \nCode: for i in range(0,2):\n    df2 = df2.append(df1)\nprint(df2)\n\n>    a  b\n> 0  1  2\n> 0  1  2\n\nAPI:\npandas.DataFrame.append\n","label":[[113,119,"Mention"],[623,646,"API"]],"Comments":[]}
{"id":60022,"text":"ID:43812331\nPost:\nText: You can use read_fwf (aka: fixed width format) to do this: \nText: Code: \nCode: df = pd.read_fwf(StringIO(data), header=1, index_col=0)\n\nText: Test code: \nCode: from io import StringIO\nimport pandas as pd\n\ndata = u\"\"\"\n      pclass  survived                                               name\n0          1         1                      Allen, Miss. Elisabeth Walton\n1          1         1                     Allison, Master. Hudson Trevor\n2          1         0                       Allison, Miss. Helen Loraine\n3          1         0               Allison, Mr. Hudson Joshua Creighton\n4          1         0    Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\n5          1         1                                Anderson, Mr. Harry\n6          1         1                  Andrews, Miss. Kornelia Theodosia\n7          1         0                             Andrews, Mr. Thomas Jr\n8          1         1      Appleton, Mrs. Edward Dale (Charlotte Lamson)\n9          1         0                            Artagaveytia, Mr. Ramon\n10         1         0                             Astor, Col. John Jacob\"\"\"\n\ndf = pd.read_fwf(StringIO(data), header=1, index_col=0)\nprint(df)\n\nText: Results: \nCode:     pclass  survived                                             name\n0        1         1                    Allen, Miss. Elisabeth Walton\n1        1         1                   Allison, Master. Hudson Trevor\n2        1         0                     Allison, Miss. Helen Loraine\n3        1         0             Allison, Mr. Hudson Joshua Creighton\n4        1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)\n5        1         1                              Anderson, Mr. Harry\n6        1         1                Andrews, Miss. Kornelia Theodosia\n7        1         0                           Andrews, Mr. Thomas Jr\n8        1         1    Appleton, Mrs. Edward Dale (Charlotte Lamson)\n9        1         0                          Artagaveytia, Mr. Ramon\n10       1         0                           Astor, Col. John Jacob\n\nAPI:\npandas.read_fwf\n","label":[[36,44,"Mention"],[2068,2083,"API"]],"Comments":[]}
{"id":60023,"text":"ID:43944983\nPost:\nText: To get the difference cells from two DataFrame as excel coordinates you can do: \nText: Code: \nCode: def diff_cell_indices(dataframe1, dataframe2):\n    from openpyxl.utils import get_column_letter as column_letter\n\n    x_ofs = dataframe1.columns.nlevels + 1\n    y_ofs = dataframe1.index.nlevels + 1\n    return [column_letter(x + x_ofs) + str(y + y_ofs) for\n            y, x in zip(*np.where(dataframe1 != dataframe2))]\n\nText: Test Code: \nCode: import pandas as pd\ndf1 = pd.read_excel('test.xlsx')\nprint(df1)\n\ndf2 = df.copy()\ndf2.C['R2'] = 1\nprint(df2)\n\nprint(diff_cell_indices(df1, df2))\n\nText: Results: \nCode:     B  C\nR2  2  3\nR3  4  5\n\n    B  C\nR2  2  1\nR3  4  5\n\n['C2']\n\nAPI:\npandas.DataFrame\n","label":[[61,70,"Mention"],[703,719,"API"]],"Comments":[]}
{"id":60024,"text":"ID:44061892\nPost:\nText: You need pd.isnull for check if scalar is NaN: \nCode: df = pd.DataFrame({ 'Col1' : [1,2,3,np.NaN],\n                 'Col2' : [8,9,7,10]})  \n\ndf2 = df.apply(lambda x: x['Col2'] if pd.isnull(x['Col1']) else x['Col1'], axis=1)\n\nprint (df)\n   Col1  Col2\n0   1.0     8\n1   2.0     9\n2   3.0     7\n3   NaN    10\n\nprint (df2)\n0     1.0\n1     2.0\n2     3.0\n3    10.0\ndtype: float64\n\nText: But better is use Series.combine_first: \nCode: df['Col1'] = df['Col1'].combine_first(df['Col2'])\n\nprint (df)\n   Col1  Col2\n0   1.0     8\n1   2.0     9\n2   3.0     7\n3  10.0    10\n\nText: Another solution with Series.update: \nCode: df['Col1'].update(df['Col2'])\nprint (df)\n   Col1  Col2\n0   8.0     8\n1   9.0     9\n2   7.0     7\n3  10.0    10\n\nAPI:\npandas.isnull\n","label":[[33,42,"Mention"],[752,765,"API"]],"Comments":[]}
{"id":60025,"text":"ID:44163987\nPost:\nText: The default behavior of df.drop is that axis=0. This means that the method will try to drop labels in the 0th axis i.e. the DataFrame index. If you want to drop columns you need to specify that the axis to drop from is axis=1. In your example this would look as follows: \nCode:  i.drop(i.columns[2], axis=1, inplace=True)\n\nAPI:\npandas.DataFrame.drop\n","label":[[48,55,"Mention"],[352,373,"API"]],"Comments":[]}
{"id":60026,"text":"ID:44196602\nPost:\nText: Your tags is a list of pd.Series objects. When you build your list from loc-based selection from the data-frame: \nCode: for user in users2:\n   tags.append(user_counters[\"tags\"].loc[user])\n\nText: You'll get a Series. Then you try to make a set out of a list of series, but you can't because series aren't hashable. \nText: So why does TypeError mistake happen since tag is list and set() is for lists? \nText: Huh? set accepts any iterable, and the elements of that iterable are used to construct the resulting set. Your iterable is a list, and the elements are Series objects. That is the problem. \nText: I suspect you have a data-frame indexed by a series of strings representing users... \nCode: >>> df = pd.DataFrame({'tag':[1,2,3, 4], 'c':[1.4,3.9, 2.8, 6.9]}, index=['ted','sara','anne', 'ted'])\n>>> df\n        c  tag\nted   1.4    1\nsara  3.9    2\nanne  2.8    3\nted   6.9    4\n>>>\n\nText: When you do your selection, since your user-index has non-unique data elements, when you do the following selection, you'll get a Series: \nCode: >>> df['tag'].loc['ted']\nuser\nted    1\nted    4\nName: a, dtype: int64\n>>> type(df['a'].loc['ted'])\n<class 'pandas.core.series.Series'>\n\nAPI:\npandas.Series\npandas.Series\n","label":[[47,56,"Mention"],[583,589,"Mention"],[1201,1214,"API"],[1215,1228,"API"]],"Comments":[]}
{"id":60027,"text":"ID:44207031\nPost:\nText: Your error starts here: \nCode: df.Gross.apply(dollarGross)\n\nText: df.Gross is a pd.Series and when you use the apply method, pandas iterates through each member of the series and passes that member to the \"callable\" (also known as a function, more on this in a bit) named dollarGross. The critical thing to understand is what the members of the pd.Series are. In this case, they are integers. So each integer in the series gets passed to dollarGross and gets called like this: \nCode: dollarGross(184)\n\nText: This in turn looks like this: \nCode: float(184[1:-1])\n\nText: Which makes no sense. You are trying to use [1:-1] which is subscripting\/slicing syntax on an integer. And that is what the error is telling you: Hey, you can't subscript an integer! \nText: That is why it's good to tell us what you are trying to do. Because now we can help you do that. Remember I said you can pass a \"callable\" to apply. Well, float is the name of the class of float objects... It's also a \"callable\" because we can do this float(184). So.... \nCode: df.Gross.apply(float)\n\nText: Should get things done. However, it's still probably better to do this \nCode: df.Gross.astype(float)\n\nText: Or, if some of the members of df.Gross cannot be interpreted as a float value, it's probable better to use @MaxU's answer. \nAPI:\npandas.Series\npandas.Series\n","label":[[104,113,"Mention"],[369,378,"Mention"],[1327,1340,"API"],[1341,1354,"API"]],"Comments":[]}
{"id":60028,"text":"ID:44221003\nPost:\nText: A DataFrame will gladly store python objects. \nText: Some test code to demonstrate... \nText: Test Code: \nCode: class MyPoint:\n    def __init__(self, x, y):\n        self._x = x\n        self._y = y\n\n    @property\n    def x(self):\n        return self._x\n\n    @property\n    def y(self):\n        return self._y\n\nmy_list = [MyPoint(1, 1), MyPoint(2, 2)]\nprint(my_list)\n\nplane_pd = pd.DataFrame([[p.x, p.y, p] for p in my_list],\n                        columns=list('XYO'))\nprint(plane_pd.dtypes)\nprint(plane_pd)\n\nText: Results: \nCode: [<__main__.MyPoint object at 0x033D2AF0>, <__main__.MyPoint object at 0x033D2B10>]\n\nX     int64\nY     int64\nO    object\ndtype: object\n\n   X  Y                                        O\n0  1  1  <__main__.MyPoint object at 0x033D2AF0>\n1  2  2  <__main__.MyPoint object at 0x033D2B10>\n\nText: Notes: \nText: Note the two object in the list are the same two objects in the dataframe. Also note the dtype for the O column is object. \nAPI:\npandas.DataFrame\n","label":[[26,35,"Mention"],[985,1001,"API"]],"Comments":[]}
{"id":60029,"text":"ID:44258486\nPost:\nText: You can achieve this by using pd.DataFrame.where - \nCode: df = pd.DataFrame({'id':[1,1,2,2],'window':[2,3,3,2],'Rank':[2,2,1,1],'member':[0,0,0,0]})\n=>\n    Rank  id  member  window\n0     2   1       0       2\n1     2   1       0       3\n2     1   2       0       3\n3     1   2       0       2\n\ndf['member'] = df['Rank'].where(df['window']==3, df['member'])\n\nprint(df)\n=>\n   Rank  id  member  window\n0     2   1       0       2\n1     2   1       2       3\n2     1   2       1       3\n3     1   2       0       2\n\nAPI:\npandas.DataFrame.where\n","label":[[54,72,"Mention"],[541,563,"API"]],"Comments":[]}
{"id":60030,"text":"ID:44357479\nPost:\nText: You can do this very simply using pandas. \nCode: import pandas as pd\n\n# get only the columns you want from the csv file\ndf = pd.read_csv(target_path + target_file, usecols=['Column Name1', 'Column Name2'])\nresult = df.to_dict(orient='records')\n\nText: Sources: \nText: pd.read_csv pd.DataFrame.to_dict \nAPI:\npandas.read_csv\npandas.DataFrame.to_dict\n","label":[[291,302,"Mention"],[303,323,"Mention"],[330,345,"API"],[346,370,"API"]],"Comments":[]}
{"id":60031,"text":"ID:44484751\nPost:\nText: What you might be referring to is the string module. It has multiple string methods and classes. \nText: However, pd.Index is a class and it has the str method as it can be seen from here. This method allows vectorized string functions to be applied to values(index and series) in the dataframe. \nAPI:\npandas.Index\n","label":[[137,145,"Mention"],[325,337,"API"]],"Comments":[]}
{"id":60032,"text":"ID:44497842\nPost:\nText: You can add custom function where first split by whitespace, then get unique values by pd.unique and last join to string back: \nCode: animals[\"detail\"] = animals[\"animal1\"].map(str) + ' ' + \n                    animals[\"animal2\"].map(str) + ' ' +\n                    animals[\"label\"].map(str)\n\nanimals[\"detail\"] = animals[\"detail\"].apply(lambda x: ' '.join(pd.unique(x.split())))\nprint (animals)\n       animal1  animal2  label              detail\n1      cat dog  dolphin     19  cat dog dolphin 19\n2      dog cat      cat     72          dog cat 72\n3  pilchard 26    koala     26   pilchard 26 koala\n4  newt bat 81      bat     81         newt bat 81\n\nText: Also is possible join values in apply: \nCode: animals[\"detail\"] = animals.astype(str)\n                           .apply(lambda x: ' '.join(pd.unique(' '.join(x).split())),axis=1)\nprint (animals)\n       animal1  animal2  label              detail\n1      cat dog  dolphin     19  cat dog dolphin 19\n2      dog cat      cat     72          dog cat 72\n3  pilchard 26    koala     26   pilchard 26 koala\n4  newt bat 81      bat     81         newt bat 81\n\nText: Solution with set, but it change order: \nCode: animals[\"detail\"] = animals.astype(str)\n                           .apply(lambda x: ' '.join(set(' '.join(x).split())), axis=1)\nprint (animals)\n       animal1  animal2  label              detail\n1      cat dog  dolphin     19  cat dolphin 19 dog\n2      dog cat      cat     72          cat dog 72\n3  pilchard 26    koala     26   26 pilchard koala\n4  newt bat 81      bat     81         bat 81 newt\n\nAPI:\npandas.unique\n","label":[[111,120,"Mention"],[1591,1604,"API"]],"Comments":[]}
{"id":60033,"text":"ID:44509865\nPost:\nText: You can use cumsum to make the cumulative column and make a column with sets instead of lists and use df.shift to make \"add\" and \"drop\" columns: \nCode: import pandas as pd\nimport numpy as np\n\n\ndf['cumset'] = df['lists'].cumsum().apply(lambda x: np.unique(x))\ndf['sets'] = df['lists'].apply(lambda x: set(x))\n\nshifted = df['sets'].shift(1).apply(lambda x: x if not pd.isnull(x) else set())\n\ndf['add'] = df['sets'] - shifted\ndf['drop'] = shifted - df['sets']\ndf = df.drop('sets', axis=1)\n\nprint(df)\n#-->Output:\n          lists              cumset     add    drop\n1           [1]                 [1]     {1}      {}\n2     [1, 2, 3]           [1, 2, 3]  {2, 3}      {}\n3  [2, 9, 7, 9]     [1, 2, 3, 7, 9]  {9, 7}  {1, 3}\n4  [2, 7, 3, 5]  [1, 2, 3, 5, 7, 9]  {3, 5}     {9}\n\nAPI:\npandas.DataFrame.cumsum\npandas.DataFrame.shift\n","label":[[36,42,"Mention"],[126,134,"Mention"],[799,822,"API"],[823,845,"API"]],"Comments":[]}
{"id":60034,"text":"ID:44529089\nPost:\nText: Option 1 groupby with size \nCode: df.groupby(['Animal', 'Food']).size().unstack(fill_value=0).astype(bool)\n\nFood    cabbage  carrots  grass    hay\nAnimal                                \ndog       False     True  False  False\nhorse     False    False   True   True\nrabbit     True     True   True  False\n\nCode: s = df.groupby('Animal').Food.apply(list)\npd.DataFrame(s.values.tolist(), s.index).add_prefix('Food').fillna('')\n\n          Food0    Food1    Food2\nAnimal                           \ndog     carrots                  \nhorse     grass      hay         \nrabbit    grass  carrots  cabbage\n\nText: Option 2 groupby with value_counts \nCode: df.groupby('Animal').Food.value_counts().unstack(fill_value=0).astype(bool)\n\nFood    cabbage  carrots  grass    hay\nAnimal                                \ndog       False     True  False  False\nhorse     False    False   True   True\nrabbit     True     True   True  False\n\nText: Option 3 groupby and str.get_dummies \nCode: df.groupby('Animal').Food.apply('|'.join).str.get_dummies().astype(bool)\n\n        cabbage  carrots  grass    hay\nAnimal                                \ndog       False     True  False  False\nhorse     False    False   True   True\nrabbit     True     True   True  False\n\nText: Option 4 pd.factorize with numpy.bincount \nCode: f1, u1 = pd.factorize(df.Animal.values)\nf2, u2 = pd.factorize(df.Food.values)\n\nn = u1.size\nm = u2.size\n\nb = np.bincount(f1 * m + f2, minlength=n * m).reshape(n, m)\n\npd.DataFrame(b.astype(bool), u1, u2)\n\n        grass  carrots  cabbage    hay\nrabbit   True     True     True  False\ndog     False     True    False  False\nhorse    True    False    False   True\n\nText: Option 5 was bored... so came up with more \nCode: f, u = pd.factorize(df.Animal.values)\nn = u.size\n\na = [[] for _ in range(n)]\n[a[i].append(food) for i, food in zip(f, df.Food)];\npd.DataFrame(a, u).rename(columns=lambda x: x+1).add_prefix('Food').fillna('')\n\n          Food1    Food2    Food3\nrabbit    grass  carrots  cabbage\ndog     carrots                  \nhorse     grass      hay         \n\nAPI:\npandas.factorize\n","label":[[1275,1287,"Mention"],[2082,2098,"API"]],"Comments":[]}
{"id":60035,"text":"ID:44555885\nPost:\nText: Consider the dataframe df \nCode: np.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 4), columns=list('ABCD'))\n\n          A         B         C         D\n0  0.444939  0.407554  0.460148  0.465239\n1  0.462691  0.016545  0.850445  0.817744\n2  0.777962  0.757983  0.934829  0.831104\n3  0.879891  0.926879  0.721535  0.117642\n4  0.145906  0.199844  0.437564  0.100702\n5  0.278735  0.609862  0.085823  0.836997\n6  0.739635  0.866059  0.691271  0.377185\n7  0.225146  0.435280  0.700900  0.700946\n8  0.796487  0.018688  0.700566  0.900749\n9  0.764869  0.253200  0.548054  0.778883\n\nText: Option 1 shift \nCode: df.assign(New=(df < df.shift()).all(1).astype(int))\n\n          A         B         C         D  New\n0  0.444939  0.407554  0.460148  0.465239    0\n1  0.462691  0.016545  0.850445  0.817744    0\n2  0.777962  0.757983  0.934829  0.831104    0\n3  0.879891  0.926879  0.721535  0.117642    0\n4  0.145906  0.199844  0.437564  0.100702    1\n5  0.278735  0.609862  0.085823  0.836997    0\n6  0.739635  0.866059  0.691271  0.377185    0\n7  0.225146  0.435280  0.700900  0.700946    0\n8  0.796487  0.018688  0.700566  0.900749    0\n9  0.764869  0.253200  0.548054  0.778883    0\n\nText: Option 2 numpy Same concept as Option 1 \nCode: v = df.values\ndf.assign(New=np.append(False, (v[1:] < v[:-1]).all(1).astype(int)))\n\n          A         B         C         D  New\n0  0.444939  0.407554  0.460148  0.465239    0\n1  0.462691  0.016545  0.850445  0.817744    0\n2  0.777962  0.757983  0.934829  0.831104    0\n3  0.879891  0.926879  0.721535  0.117642    0\n4  0.145906  0.199844  0.437564  0.100702    1\n5  0.278735  0.609862  0.085823  0.836997    0\n6  0.739635  0.866059  0.691271  0.377185    0\n7  0.225146  0.435280  0.700900  0.700946    0\n8  0.796487  0.018688  0.700566  0.900749    0\n9  0.764869  0.253200  0.548054  0.778883    0\n\nText: Option 3 diff Use diff to compare one row with the next and see if it is less than zero. Then use all to determine if its True for the entire row. \nCode: df.assign(New=df.diff().lt(0).all(1).astype(int))\n\n          A         B         C         D  New\n0  0.444939  0.407554  0.460148  0.465239    0\n1  0.462691  0.016545  0.850445  0.817744    0\n2  0.777962  0.757983  0.934829  0.831104    0\n3  0.879891  0.926879  0.721535  0.117642    0\n4  0.145906  0.199844  0.437564  0.100702    1\n5  0.278735  0.609862  0.085823  0.836997    0\n6  0.739635  0.866059  0.691271  0.377185    0\n7  0.225146  0.435280  0.700900  0.700946    0\n8  0.796487  0.018688  0.700566  0.900749    0\n9  0.764869  0.253200  0.548054  0.778883    0\n\nText: Timing \nCode: %timeit df.assign(New=df.diff().lt(0).all(1).astype(int))\n%timeit df.assign(New=(df < df.shift()).all(1).astype(int))\n\n1000 loops, best of 3: 579 s per loop\n1000 loops, best of 3: 1.56 ms per loop\n\n%%timeit\nv = df.values\ndf.assign(New=np.append(False, (v[1:] < v[:-1]).all(1).astype(int)))\n\n1000 loops, best of 3: 322 s per loop\n\nText: How diff Works \nText: check if all the column values in the current row is less than the corresponding column values in previous row \nText: This is what prompted me to use diff. diff by default calculates the diff array for every column. Meaning, for each row, we have the difference of that row relative to the previous row. For the condition the OP presented to be True, we need this difference to be less than zero. \nCode: df.diff()\n\n          A         B         C         D\n0       NaN       NaN       NaN       NaN\n1  0.017752 -0.391009  0.390297  0.352505\n2  0.315271  0.741438  0.084384  0.013360\n3  0.101929  0.168895 -0.213294 -0.713463\n4 -0.733985 -0.727035 -0.283971 -0.016940\n5  0.132829  0.410018 -0.351741  0.736296\n6  0.460900  0.256197  0.605448 -0.459812\n7 -0.514489 -0.430779  0.009629  0.323761\n8  0.571340 -0.416592 -0.000334  0.199803\n9 -0.031618  0.234512 -0.152512 -0.121866\n\nText: Then \nCode: df.diff() < 0\n\n       A      B      C      D\n0  False  False  False  False\n1  False   True  False  False\n2  False  False  False  False\n3  False  False   True   True\n4   True   True   True   True\n5  False  False   True  False\n6  False  False  False   True\n7   True   True  False  False\n8  False   True   True  False\n9   True  False   True   True\n\nText: Then \nCode: (df.diff() < 0).all(1)\n\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\n6    False\n7    False\n8    False\n9    False\ndtype: bool\n\nAPI:\npandas.DataFrame.diff\n","label":[[3128,3132,"Mention"],[4384,4405,"API"]],"Comments":[]}
{"id":60036,"text":"ID:44558877\nPost:\nText: According to the source code, the sorting is defined by _try_sort(data.columns) and cannot be changed by an argument. You can do what Claudiu Creanga suggested. However, in my test, that won't give you a (2, 6) layout. If you really want that layout and what hist does, the following code may be helpful: \nCode: from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ncolumns_all = pd.DataFrame([np.random.randn(1000)] * 7).T\ncolumns_all.columns = ['m1_h', 'm2_h', 'm3_h', 'm4_h', 'm5_h', 'm6_h', 'm6_f']\nplt.clf()\nfig = plt.figure(figsize=(16, 4))\naxarr = []\nfor i, col in enumerate(columns_all.columns):\n    if i \/\/ 6 > 0:\n        sharex = axarr[i % 6]\n        plt.setp(axarr[i % 6].get_xticklabels(), visible=False)\n    else:\n        sharex = None\n    if i % 6 > 0:\n        sharey = axarr[i \/\/ 6]\n    else:\n        sharey = None\n    ax = fig.add_subplot(2, 6, i + 1, sharex=sharex, sharey=sharey)\n    axarr.append(ax)\n    if i % 6 > 0:\n        plt.setp(ax.get_yticklabels(), visible=False)\n    ax.hist(columns_all[col].dropna().values)\n    ax.set_title(col)\n    ax.grid(True)\nfig.subplots_adjust(wspace=0.3, hspace=0.3)\nplt.show()\n\nAPI:\npandas.DataFrame.hist\n","label":[[283,287,"Mention"],[1185,1206,"API"]],"Comments":[]}
{"id":60037,"text":"ID:44569087\nPost:\nText: You can use double dict comprehension with filtering with pd.notnull or pandas.isnull: \nCode: y = {k1:{k:v for k,v in v1.items() if pd.notnull(v)} for k1, v1 in d.items()}\nprint (y)\n{'k': {'c': 1.0, 'b': 1.0}, 'u': {'c': 1.0, 'a': 1.0}, 'y': {'b': 1.0}}\n\nText: Similar solution: \nCode: y = {k1:{k:v for k,v in v1.items() if not pd.isnull(v)} for k1, v1 in d.items()}\nprint (y)\n{'k': {'c': 1.0, 'b': 1.0}, 'u': {'c': 1.0, 'a': 1.0}, 'y': {'b': 1.0}}\n\nAPI:\npandas.notnull\n","label":[[82,92,"Mention"],[479,493,"API"]],"Comments":[]}
{"id":60038,"text":"ID:44571933\nPost:\nText: Question: ...it repeats the headers in rows below. how can i fix that \nText: pandas 0.20.2 documentation: to_lsv DataFrame.to_csv(path_or_buf=None, sep=, , na_rep=, float_format=None, columns=None, header=True, index=True, index_label=None, mode=w, encoding=None, compression=None, quoting=None, quotechar=, line_terminator=\\n, chunksize=None, tupleize_cols=False, date_format=None, doublequote=True, escapechar=None, decimal=.) header : boolean or list of string, default True Write out column names. If a list of string is given it is assumed to be aliases for the column names \nText: Question: ... but the loop only saves the last iteration of the df \nText: Add a mode=a to .to_csv(... \nAPI:\npandas.DataFrame.to_csv\n","label":[[130,136,"Mention"],[733,756,"API"]],"Comments":[]}
{"id":60039,"text":"ID:44575940\nPost:\nText: You can pass ZipFile.open() to pandas.read_csv() to construct a pd.DataFrame from a csv-file packed into a multi-file zip. \nText: Code: \nCode: pd.read_csv(zip_file.open('file3.txt'))\n\nText: Example to read all .csv into a dict: \nCode: from zipfile import ZipFile\n\nzip_file = ZipFile('textfile.zip')\ndfs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n       for text_file in zip_file.infolist()\n       if text_file.filename.endswith('.csv')}\n\nAPI:\npandas.DataFrame\n","label":[[88,100,"Mention"],[494,510,"API"]],"Comments":[]}
{"id":60040,"text":"ID:44602383\nPost:\nText: The simplest way that comes to my mind would be to make a list of all the columns except Name and Job and then iterate to_numeric over them: \nCode: cols=[i for i in df.columns if i not in [\"Name\",\"Job\"]]\nfor col in cols:\n    df[col]=pd.to_numeric(df[col])\n\nText: Edit: \nText: If you absolutely want to use numbers instead of columns names and already know at which indice they are: \nCode: for i in [i for i in list(range(len(df.columns))) if i not in [0,4]]:\n    df.iloc[:,i]=pandas.to_numeric(df.iloc[:,i])\n\nText: That's more complicated than necessary though. \nAPI:\npandas.to_numeric\n","label":[[143,153,"Mention"],[592,609,"API"]],"Comments":[]}
{"id":60041,"text":"ID:44634602\nPost:\nText: Using your current dataframe: df.hist(by='type') \nText: For example: \nCode: # Me recreating your dataframe\npageviews = np.random.randint(200, size=100)\ntypes = np.random.choice(['original','licensed'], size=100)\n\ndf = pd.DataFrame({'pageviews': pageviews,'type':types})\n\n# Code you need to create faceted histogram by type\ndf.hist(by='type')\n\nText: hist documentation \nAPI:\npandas.DataFrame.hist\n","label":[[373,377,"Mention"],[398,419,"API"]],"Comments":[]}
{"id":60042,"text":"ID:44659158\nPost:\nText: cut \nCode: c = pd.cut(\n    df.stack(),\n    [-np.inf, -10, 0, np.inf],\n    labels=['danger', 'warning', 'success']\n)\ndf.join(c.unstack().add_suffix('_cat'))\n\n   x  y   z    x_cat    y_cat   z_cat\n0  2 -7 -30  success  warning  danger\n1  1 -5 -20  success  warning  danger\n\nText: numpy \nCode: v = df.values\ncats = np.array(['danger', 'warning', 'success'])\ncode = np.searchsorted([-10, 0], v.ravel()).reshape(v.shape)\ncdf = pd.DataFrame(cats[code], df.index, df.columns)\ndf.join(cdf.add_suffix('_cat'))\n\n   x  y   z    x_cat    y_cat   z_cat\n0  2 -7 -30  success  warning  danger\n1  1 -5 -20  success  warning  danger\n\nAPI:\npandas.cut\n","label":[[24,27,"Mention"],[646,656,"API"]],"Comments":[]}
{"id":60043,"text":"ID:44661563\nPost:\nText: Just do an outer join: \nText: pd.merge(df1, df2, left_index=True, right_index=True, suffixes=['_df1','_df2'], how='outer') \nText: pd.DataFrame.merge Documentation \nAPI:\npandas.DataFrame.merge\n","label":[[154,172,"Mention"],[193,215,"API"]],"Comments":[]}
{"id":60044,"text":"ID:44703536\nPost:\nText: I provide an alternative solution to using dask here, \nCode: import pandas as pd\nfrom multiprocessing import Pool\ntest = pd.DataFrame({'Address1':['123 Cheese Way','234 Cookie Place','345 Pizza Drive','456 Pretzel Junction'],'city':['X','U','X','U']}) \ntest2 = pd.DataFrame({'Address1':['123 chese wy','234 kookie Pl','345 Pizzza DR','456 Pretzel Junktion'],'city':['X','U','Z','Y'] , 'ID' : ['1','3','4','8']})\n\ntest=test.assign(dataset = 'test')\ntest2=test2.assign(dataset = 'test2')\n\nnewdf=pd.concat([test2,test],keys = ['test2','test'])\ngpd=newdf.groupby('city')\ndef my_func(mygrp):\n    test_data=mygrp.loc['test']\n    test2_data=mygrp.loc['test2']\n    #do something specific\n    #if needed print something\n    return {'Address':test2_data.Address1.values[0],'ID':test2_data.ID.values[0]} #return some other stuff\n\nmypool=Pool(processes=2)\nret_list=mypool.imap(my_func,(group for name, group in gpd))\n\npd.DataFrame(ret_list)\n\nText: returns something like \nCode:     ID  address\n0   3   234 kookie Pl\n1   1   123 chese wy\n2   8   456 Pretzel Junktion\n3   4   345 Pizzza DR\n\nText: PS: In OP's question two similar datasets are compared in a specialized function, the solution here uses concat . One could also imagine a pd.merge depending on the problem. \nAPI:\npandas.concat\n","label":[[1212,1218,"Mention"],[1287,1300,"API"]],"Comments":[]}
{"id":60045,"text":"ID:44747858\nPost:\nText: The pd.read_fwf can have delimiter argument. \nCode: dataframe = pd.read_fwf(\"challenge_dataset.txt\", delimiter=\",\")\n\nText: You can read more in ead_fwf \nText: read_csv is automatically reads with comma separator, although you can change the delimiter argument in read_csv as well. \nAPI:\npandas.read_fwf\npandas.read_fwf\n","label":[[28,39,"Mention"],[168,175,"Mention"],[311,326,"API"],[327,342,"API"]],"Comments":[]}
{"id":60046,"text":"ID:44809828\nPost:\nText: Use get the axes handle from the first plot then use the ax paramater in plot to plot second line on same axes: \nCode: ax = data.plot(x='Data', y='mean_Kincaid',legend=False, title=\"Kincaid score over time\")\ndata.plot(x='Date', y='mean_Score', ax=ax)\n\nAPI:\npandas.DataFrame.plot\n","label":[[97,101,"Mention"],[281,302,"API"]],"Comments":[]}
{"id":60047,"text":"ID:44832489\nPost:\nText: Use the parameter \nText: na_rep : string, default  Missing data representation\" \nText: and set this to \"\" \nText: which you can read here: \nText: df.to_csv \nText: This is the code: \nCode: file=pd.DataFrame({\"one\":[1,2,None,3,4],\"two\":[5,6,7,np.nan,8]})\n\nfile.to_csv(\"xxxxxxx\",na_rep=\"\")\n\nText: Will lead to: \nAPI:\npandas.DataFrame.to_csv\n","label":[[170,179,"Mention"],[338,361,"API"]],"Comments":[]}
{"id":60048,"text":"ID:44839116\nPost:\nText: I think you need concat with DataFrame.unstack: \nCode: df1 = pd.DataFrame({'col3': [1, 3, 2, 1, 5, 0, 3, 4], \n                    'col2': ['a1', 'a2', 'a3', 'a1', 'a2', 'a1', 'a2', 'a3'], \n                    'col1': ['A', 'A', 'A', 'B', 'B', 'C', 'C', 'C']})\ndf1 = df1.set_index(['col1','col2'])\nprint (df1)\n           col3\ncol1 col2      \nA    a1       1\n     a2       3\n     a3       2\nB    a1       1\n     a2       5\nC    a1       0\n     a2       3\n     a3       4\n\ndf2 = pd.DataFrame({'col3': [2, 1, 1, 2, 2, 0], \n                    'col2': ['b1', 'b2', 'b1', 'b2', 'b1', 'b2'], \n                    'col1': ['A', 'A', 'B', 'B', 'C', 'C']})\ndf2 = df2.set_index(['col1','col2'])\nprint (df2)\n           col3\ncol1 col2      \nA    b1       2\n     b2       1\nB    b1       1\n     b2       2\nC    b1       2\n     b2       0\n\nCode: df = pd.concat([df1, df2])['col3'].unstack(fill_value=0)\nprint (df)\ncol2   a1 a2 a3 b1 b2\ncol1                 \nA       1  3  2  2  1\nB       1  5  0  1  2\nC       0  3  4  2  0\n\nText: Last if need remove columns, index names add rename_axis: \nCode: df = pd.concat([df1, df2])['col3'] \\\n       .unstack(fill_value=0) \\\n       .rename_axis(None) \\\n       .rename_axis(None, axis=1)\nprint (df)\n   a1  a2  a3  b1  b2\nA   1   3   2   2   1\nB   1   5   0   1   2\nC   0   3   4   2   0\n\nAPI:\npandas.concat\n","label":[[41,47,"Mention"],[1341,1354,"API"]],"Comments":[]}
{"id":60049,"text":"ID:44851625\nPost:\nText: The Error: \nText: This happens because pd.Series.unique returns an array of the unique values, which agg interprets as an attempt to broadcast different values to different rows and so rejects. You'd get the same error with a function that returns a pandas Series or Index. \nText: Solution: \nText: If you pass the function you use later, pandas.Series.nunique, \nCode: params = {\n  'A': 'sum',\n  'B': 'sum',\n  'C': 'count',\n  'D': 'sum',\n  'F': pd.Series.nunique\n}\n\ndf.groupby('E').agg(params).reset_index()\nOut[69]: \n       E   C  F    A    B    D\n0   1001  10  2  500  463  595\n1   1002  10  2  484  493  348\n2   1003  10  1  507  400  479\n...\n17  1018  10  1  606  454  410\n18  1019  10  2  537  522  724\n19  1020  10  2  541  532  486\n\nText: it should work fine. \nText: If you want the unique values themselves, you can feed a lambda function to agg, as long as it recognizes the return value as an aggregated value\/not a Series, Index, np.ndarray, or a subclass. \nText: Ex: \nCode: params = {\n  'A': 'sum',\n  'B': 'sum',\n  'C': 'count',\n  'D': 'sum',\n  'F': lambda x: ','.join(sorted(pd.Series.unique(x)))\n}\n\ndf.groupby('E').agg(params).reset_index()\nOut[82]: \n       E   C    F    A    B    D\n0   1001  10  c,d  500  463  595\n1   1002  10  a,b  484  493  348\n2   1003  10    b  507  400  479\n...\n17  1018  10    b  606  454  410\n18  1019  10  a,b  537  522  724\n19  1020  10  c,d  541  532  486\n\nText: Or, to be a bit silly: \nCode: params = {\n  'A': 'sum',\n  'B': 'sum',\n  'C': 'count',\n  'D': 'sum',\n  'F': lambda x: pd.DataFrame(pd.Series.unique(x))\n}\n\ndf.groupby('E').agg(params).reset_index()\nOut[92]: \n       E   C     F    A    B    D\n0   1001  10     0\n              0  c\n              1  d  500  463  595\n1   1002  10     0\n              0  b\n              1  a  484  493  348\n2   1003  10     0\n              0  b  507  400  479\n...\n17  1018  10     0\n              0  b  606  454  410\n18  1019  10     0\n              0  a\n              1  b  537  522  724\n19  1020  10     0\n              0  d\n              1  c  541  532  486\n\nAPI:\npandas.Series.unique\n","label":[[63,79,"Mention"],[2073,2093,"API"]],"Comments":[]}
{"id":60050,"text":"ID:44872552\nPost:\nText: According to pd.read_sql_query documentation, \nText: params : list, tuple or dict, optional, default: None List of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249's paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={name : value} \nText: If you then look at PEP 249's paramstyle, you'll see many options. But the PyMySQL's execute documentation states that \nText: If args is a list or tuple, %s can be used as a placeholder in the query. If args is a dict, %(name)s can be used as a placeholder in the query. \nText: So, applying to your case, that would be: \nCode: sql = 'SELECT user_id, user_agent_id, requests ' \\\n      'FROM riskanalysis_user_http_ua_stats ' \\\n      'WHERE since>= %s AND until< %s'\n\ndataframe_records = pd.read_sql_query(sql, engine,\n                                      params=(datetime_object, datetime_object))\n\nText: or \nCode: sql = 'SELECT user_id, user_agent_id, requests ' \\\n      'FROM riskanalysis_user_http_ua_stats ' \\\n      'WHERE since>= %(since)s AND until< %(until)s'\n\ndataframe_records = pd.read_sql_query(sql, engine,\n                                      params={'since':datetime_object,\n                                              'until':datetime_object})\n\nAPI:\npandas.read_sql_query\n","label":[[37,54,"Mention"],[1409,1430,"API"]],"Comments":[]}
{"id":60051,"text":"ID:44909045\nPost:\nCode: df[4] > df[3] + df[4]\n\nText: (that actually is equivalent to df[3] < 0) is a Series of boolean values. When do you want to enter the if statement? When all values are True? then you should use all(). When any of them is True? Then you should use any(). \nText: If instead you want to execute that function for any row when the condition is True, you should do something like \nCode: condition = df[4] > df[3] + df[4]\ntrue_df = df[condition]\ntrue_df[\"account_new\"] = true_df['account']-true_df['account'] *.10\n\nText: but now the columns \"account_new\" exists only in true_df, and not in df. \nText: With something like \nCode: df[\"account_new\"] = true_df[\"account_new\"]\n\nText: now also df has the column \"account_new\", but in the lines where condition is false you have nans... \nAPI:\npandas.Series\n","label":[[101,107,"Mention"],[802,815,"API"]],"Comments":[]}
{"id":60052,"text":"ID:44951908\nPost:\nText: Since there is not inplace parameter in pd.merge i think the most you can do is: \nCode: s1 = pd.merge(s1,s2,how='outer')\n\nText: other than that, i don't think there's much left to do. Hope that was helpful somehow. \nAPI:\npandas.merge\n","label":[[64,72,"Mention"],[245,257,"API"]],"Comments":[]}
{"id":60053,"text":"ID:44973686\nPost:\nText: I had to do two things that solved the issue for me. First, I deleted my table and reuploaded it with the columns as TIMESTAMP types rather than DATETIME types. This made sure that the schema matched when the DataFrame with column type datetime64[ns] was uploaded to using to_gbq, which converts datetime64[ns] to TIMESTAMP type and not to DATETIME type (for now). \nText: The second thing I did was upgrade from pandas 0.19 to pandas 0.20. These two things solved my problem of a schema mismatch. \nAPI:\npandas.DataFrame\n","label":[[233,242,"Mention"],[527,543,"API"]],"Comments":[]}
{"id":60054,"text":"ID:45008181\nPost:\nText: The problem you have here starts at the point where you create your csv file. One line of the file looks like this \nCode: 2-Jan-01,\" 1,283.27 \",\" 1,283.27 \",\" 1,331.00 \",\" 1,299.80 \",\" 1,336.75 \",\" 1,289.25 \",6.50%\n\nText: As you can see, you are using commas (,) as field separators as well as as thousands separators. This makes it necessary that the field values are encapsulated into quote signs (\"). In turn this leads to pandas needing to read the fields as strings and not as numbers. \nText: An option is of course changing the csv format of the file at creation. If this is not possible, you would need to use some of the options that read_csv offers to correctly read in the data. \nCode: sp500 = pd.read_csv('spx1.csv', index_col = 'Date', \n                    parse_dates=True, thousands=r\",\", quotechar='\"')\n\nText: Now comes the second problem: You don't have a \"Date\" column. \"Date\" is the index of the dataframe, not a column (because you use index_col = 'Date'). You would therefore need to use this index as the values for x to be plotted, \nCode: x = sp500.index\n\nText: A complete example: \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import data from csv into a dataframe\nsp500 = pd.read_csv('spx1.csv', index_col = 'Date', parse_dates=True, \n                    thousands=r\",\", quotechar='\"')\n\nx = sp500.index\ny = sp500['Total Return']\n\nplt.plot(x,y)\n\nplt.show()\n\nAPI:\npandas.read_csv\n","label":[[666,674,"Mention"],[1445,1460,"API"]],"Comments":[]}
{"id":60055,"text":"ID:45029667\nPost:\nText: You could use sampel with specifying n=400 combined with loc and then concatenate all the parts using pd.concat like this: \nCode: df = pd.concat([df.loc[df.ID == 'A'].sample(n=400),df.loc[df.ID == 'B'].sample(n=400),df.loc[df.ID == 'C'].sample(n=400)])\n\nText: example test: \nCode: #df:\n#    B ID\n#0  10  A\n#1   9  A\n#2   8  A\n#3   7  A\n#4   6  B\n#5   5  B\n#6   4  B\n#7   3  C\n#8   2  C\n#9   1  C\n\ndf = pd.concat([df.loc[df.ID == 'A'].sample(n=2),df.loc[df.ID == 'B'].sample(n=2),df.loc[df.ID == 'C'].sample(n=2)])\n\nText: output: \nCode:     B ID\n0  10  A\n3   7  A\n6   4  B\n5   5  B\n8   2  C\n7   3  C\n\nText: you could also fix random_state to always have the same random sample. \nText: I believe that's what you asked for. \nAPI:\npandas.DataFrame.sample\npandas.DataFrame.loc\npandas.concat\n","label":[[38,44,"Mention"],[81,84,"Mention"],[126,135,"Mention"],[751,774,"API"],[775,795,"API"],[796,809,"API"]],"Comments":[]}
{"id":60056,"text":"ID:45082855\nPost:\nText: I'm afraid I don't know pandas so I can't give the detail of the answer, but here is a general alogrithm that I think would work fine. It's up to you to match this against pandas API. \nText: For the \"has come\" column: \nText: Sort the entries by deficit (descending) Calculate the cumulative sum of deficit entries in this sorted list Bound this by sum(surplus) i.e. create a column max(cumsum(deficit), sum(surplus)) Now do the difference of each item with the next one (I think this is the pd.Series.diff method?), using 0 as the \"-1\"th entry (maybe you'll have to add a dummy row?). This is your \"has come\" value \nText: For the \"has come\" column (if sum(surplus) >= sum(deficit)): \nText: In this case you just set \"has come\" = \"deficit\" for all rows, which will be faster than the above computation But if you don't check for this case explicitly, the above calculation will still work \nText: For the \"gone\" column: Just do exactly the same stuff as above, reversing \"deficit\" and \"surplus\". \nText: Edit: In your example the gone column is the tricky one, because in that case sum(deficit) < sum(surplus). Here's the above procedure on surplus. \nCode: sum(surplus) = 2200\nsum(deficit) = 1800\n\n+------+---------+---------+-----------------+-----------------------------------+----------------+\n| code | surplus | deficit | cumsum(surplus) | max(cumsum(surplus),sum(deficit)) | diff(prev row) |\n+------+---------+---------+-----------------+-----------------------------------+----------------+\n| NaN  |       0 |       0 |               0 |                                 0 |           NaN  |\n| 0100 |    1000 |       0 |            1000 |                              1000 |           1000 |\n| 0193 |     700 |       0 |            1700 |                              1700 |           700  |\n| 0192 |     500 |       0 |            2200 |                              1800 |           100  |\n| 0191 |       0 |     800 |            2200 |                              1800 |           0    |\n| 0103 |       0 |     100 |            2200 |                              1800 |           0    |\n| 0104 |       0 |     600 |            2200 |                              1800 |           0    |\n| 0190 |       0 |       0 |            2200 |                              1800 |           0    |\n| 0194 |       0 |     300 |            2200 |                              1800 |           0    |\n| 0195 |       0 |       0 |            2200 |                              1800 |           0    |\n+------+---------+---------+-----------------+-----------------------------------+----------------+\n\nText: The final column is the result you want. Note that I added a dummy row to the start so that I could calculate pairwise differences. It turns out the shift() is the key method you need for calculating that column; see this question \nText: Edit 2: I thought it might be worth adding an alternative solution. It's a bit harder to understand, but it might be a bit easier to implement because you don't need to fiddle with an extra dummy row. \nText: As before: Sort the entries by deficit (descending) As before: Calculate the cumulative sum of deficit entries in this sorted list New: Find the index of the first row where the cumulative sum is greater than the sum of the surplus (I don't how easy it is to get this in pandas). Let's call this i (with i=Inf if no such row exists). For all rows before this index (i.e. df[:i]), set \"has come\" = \"deficit\" For all rows after this index (i.e. df[i+1:]), set \"has come\" = 0 For that row (i.e. df[i], if i exists), set \"has come\" to: has come = sum(surplus) - (cumsum(deficit) - deficit) (BTW, (cumsum(deficit) - deficit) is actually equal to cumsum(deficit) of the previous row, or 0 if this is the first row.) \nAPI:\npandas.Series.diff\n","label":[[515,529,"Mention"],[3788,3806,"API"]],"Comments":[]}
{"id":60057,"text":"ID:45093235\nPost:\nText: First, answering your question: \nText: You should use sample to get a sample from your dateframe, and then use regplot, below is a small example using random data: \nCode: import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndates = pd.date_range('20080101', periods=10000, freq=\"D\")\ndf = pd.DataFrame({\"dates\": dates, \"data\": np.random.randn(10000)})\n   \ndfSample = df.sample(1000) # This is the importante line\nxdataSample, ydataSample = dfSample[\"dates\"], dfSample[\"data\"]\n\nsns.regplot(x=mdates.date2num(xdataSample.astype(datetime)), y=ydataSample) \nplt.show()\n\nText: On regplot I perform a convertion in my X data because of datetime's type, notice this definitely should not be necessary depending on your data. \nText: So, instead of something like this: \nText: You'll get something like this: \nText: Now, a suggestion: \nText: Use sns.jointplot, which has a kind parameter, from the docs: \nText: kind : { scatter | reg | resid | kde | hex }, optional Kind of plot to draw. \nText: What we create here is a similar of what matplotlib's hist2d does, it creates something like a heatmap, using your entire dataset. An example using random data: \nCode: dates = pd.date_range('20080101', periods=10000, freq=\"D\")\ndf = pd.DataFrame({\"dates\": dates, \"data\": np.random.randn(10000)})\n\nxdata, ydata = df[\"dates\"], df[\"data\"]\nsns.jointplot(x=mdates.date2num(xdata.astype(datetime)), y=ydata, kind=\"kde\")\n\nplt.show()\n\nText: This results in this image, which is also good for seeing the distributions along your desired axis: \nAPI:\npandas.DataFrame.sample\n","label":[[78,84,"Mention"],[1663,1686,"API"]],"Comments":[]}
{"id":60058,"text":"ID:45114729\nPost:\nText: \"Efficient\" can have different interpretations. Based on your description (especially the mention of generators) I'm guessing you mean memory and computational efficiency (use as little memory as possible and avoid repeated loops over the same data). With that thought, here's one go: \nCode: def df_gen(filename, sheet_names):\n    with xlrd.open_workbook(filename, on_demand=True) as xl_file:\n        for sheet in sheet_names:\n            yield pd.read_excel(\n                xl_file, sheetname=sheet, engine='xlrd').assign(source=sheet)\n            # tell xlrd to let the sheet leave memory\n            xl_file.unload_sheet(sheet)\n\nText: This makes use of xlrd's \"worksheets on demand\" feature to avoid loading the entire Excel document into memory. Sheets are explicitly unloaded from memory after DataFrames are constructed. Because this uses yield it's a generator and how many dataframes are simultaneously created depends on your usage. Here's an example usage passing this generator to pandas.concat: \nCode: df = pd.concat(df_gen('file_name.xlsx', ['sheet1', 'sheet2']), ignore_index=True)\n\nText: Note, though, that concat materializes everything in the generator before doing the concatenation, so this doesn't necessarily end up being any more efficient than your example of building up a list except that my function deliberately manages the resource usage of the xlrd workbook. In this case I think you end up with 1 or 2 copies of the data in memory at once depending on the internals of concat. \nText: If you are really worried about memory you could use the generator to iteratively build a dataframe one sheet at a time: \nCode: # create a generator\ngen = df_gen(str(filename), sheet_names)\n\n# get starting point\ndf = next(gen)\n\n# iterate over the rest of the generator\nfor next_df in gen:\n    df = df.append(next_df, ignore_index=True)\n\nText: I'd expect this to be less computationally efficient than calling concat with the entire set of desired dataframes at once, though I haven't researched whether that's really true. In this case I think you end up with only 1 copy of all the data in memory at once, plus one extra copy of a sheet's data for each loop through the generator. \nText: You know your situation best, but unless these are some truly impressive Excel files I wouldn't put a ton of effort into optimizing memory and computation beyond what seem like clear wins. With that in mind, here's a short function that leverages the ability of pd.read_excel to read multiple sheets at once: \nCode: def sheets_to_df(filename, sheet_names):\n    df_dict = pd.read_excel(filename, sheetname=sheet_names)\n    return pd.concat(\n        (df.assign(source=sheet) for sheet, df in dfs.items()), ignore_index=True)\n\nText: One thing to note that is when passing in a file name read_excel will load the entire Excel document (e.g. does not make use of the \"on demand\" feature of xlrd). So while this is efficient in terms of lines of code, it's definitely not efficient in terms of memory. I think this briefly ends up with all the data in memory 2-3 times: once in df_dict and once in the final concatenated dataframe (and possibly again depending on the internals of concat). But once this function returns you're left with only the one copy in the final dataframe. If you were planning to read most of the sheets anyway this wouldn't be a huge waste (assuming they all fit in memory at least twice), but if you were planning to read only a small subset of the sheets this could be a bit wasteful. \nText: I hope this helps! You can get this as a Jupyter notebook here: https:\/\/gist.github.com\/jiffyclub\/9ab668f63c3d0f9adf3e730dc37cd419 \nAPI:\npandas.read_excel\n","label":[[2490,2503,"Mention"],[3678,3695,"API"]],"Comments":[]}
{"id":60059,"text":"ID:45155307\nPost:\nText: Actually you're not far from the solution, in fact, you're thinking right. But if I had to recommend a method I would recommend this using notnull \nCode: df3 = df2[pd.notnull(df1)]\n\nText: you just pick cells from df2 where df1 is not null and put them in the corresponding cells in df3, all the others will be set to NaN automatically. \nText: If you still wanna use np.where you could do something like this with pandas.isnull: \nCode: df3 = pd.DataFrame(np.where(pd.isnull(df1),np.nan,df2))\n\nText: but then you'll have to use the columns parameter to rename the columns. Also this method is a bit slower so I wouldn't really use it personally. \nText: Hope this was helpful. \nText: output ( I only took your first 2 cols in my sample just to go faster ): \nCode:                      33643   33667\n1998-01-01 10:00:00    NaN     NaN\n1998-01-01 11:00:00    NaN     NaN\n1998-01-01 12:00:00    NaN     NaN\n1998-01-01 13:00:00    NaN     NaN\n1998-01-01 14:00:00  291.1  289.14\n\nAPI:\npandas.notnull\n","label":[[163,170,"Mention"],[1001,1015,"API"]],"Comments":[]}
{"id":60060,"text":"ID:45229430\nPost:\nText: A simple solution will be grouping by year first and then making boxplot: \nCode: import io\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Re-create your sample data\ns = \"\"\"Dates,Hours_played\n2014-11-06,11\n2014-12-06,4\n2015-09-06,5\n2015-07-06,5\"\"\"\ndf = pd.read_table(io.StringIO(s), sep=',', index_col=0, parse_dates=True)\n\n# The following codes are the answer relevant to your question.\ndf.groupby(df.index.year).boxplot()\nplt.show()\n\nText: Your second method ends up with an empty plot because matplotlib fail to recognize df correctly. Try use Numpy-array representation: \nCode: import io\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Re-create your sample data\ns = \"\"\"Dates,Hours_played\n2014-11-06,11\n2014-12-06,4\n2015-09-06,5\n2015-07-06,5\"\"\"\ndf = pd.read_table(io.StringIO(s), sep=',', index_col=0, parse_dates=True)\n\n# The following codes are the answer relevant to your question.    \ndata_2014 = df[df.index.year == 2014].as_matrix()\ndata_2015 = df[df.index.year == 2015].as_matrix()\ndata_to_plot = [data_2014, data_2015]\n\nmpl_fig = plt.figure()\nax = mpl_fig.add_subplot(111)\nax.boxplot(data_to_plot)\n\nplt.show()\n\nText: To use subplots, you will need to plot them one by one: \nCode: import io\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Re-create your sample data\ns = \"\"\"Dates,Hours_played\n2014-11-06,11\n2014-12-06,4\n2015-09-06,5\n2015-07-06,5\"\"\"\ndf = pd.read_table(io.StringIO(s), sep=',', parse_dates=[0])\ndf['Year'] = df.Dates.dt.year\ndf.set_index(['Year', 'Dates'], inplace=True)\n\n# The following codes are the answer relevant to your question.\nmpl_fig = plt.figure()\nax1 = mpl_fig.add_subplot(121)\nax1.boxplot(df.loc[2014]['Hours_played'], labels=[2014])\nax2 = mpl_fig.add_subplot(122)\nax2.boxplot(df.loc[2015]['Hours_played'], labels=[2015])\n\nplt.show()\n\nAPI:\npandas.DataFrame\n","label":[[557,559,"Mention"],[1826,1842,"API"]],"Comments":[]}
{"id":60061,"text":"ID:45245746\nPost:\nText: You can do this with pd.qcut: \nCode: df = pd.DataFrame({'A':np.random.randn(100)})\n\n# pd.qcut(df.A, 10) will bin into deciles\n# you can group by these deciles and take the sums in one step like so:\ndf.groupby(pd.qcut(df.A, 10))['A'].sum()\n# A\n# (-2.662, -1.209]   -16.436286\n# (-1.209, -0.866]   -10.348697\n# (-0.866, -0.612]    -7.133950\n# (-0.612, -0.323]    -4.847695\n# (-0.323, -0.129]    -2.187459\n# (-0.129, 0.0699]    -0.678615\n# (0.0699, 0.368]      2.007176\n# (0.368, 0.795]       5.457153\n# (0.795, 1.386]      11.551413\n# (1.386, 3.664]      20.575449\n\nText: qcut documentation \nAPI:\npandas.qcut\n","label":[[594,598,"Mention"],[619,630,"API"]],"Comments":[]}
{"id":60062,"text":"ID:45257434\nPost:\nText: This task is super easy in Pandas these days. \nText: import pandas as pd \nText: df = pd.read_excel('file_name_here.xlsx', sheet_name='Sheet1') \nText: or \nText: df = pd.read_csv('file_name_here.csv') \nText: This returns a df object which is very powerful for performing operations by column, row, over an entire df, or over individual items with iterrows. Not to mention slicing in different ways. \nAPI:\npandas.DataFrame\n","label":[[245,247,"Mention"],[427,443,"API"]],"Comments":[]}
{"id":60063,"text":"ID:45289182\nPost:\nText: Use the pd.read_excel function to import your excel sheet. It has an optional input argument skiprows that allows you to specify the rows at the top that should be ignored. \nCode: import pandas as pd\n\nfile = 'example.xlsx'\nskiprows_amount = 5\ndf = pd.read_excel(file, skiprows=range(skiprows_amount), dtype=str)\n\nText: Note: this solution has the limitation that you have to know the amount of to-be-skipped rows in advance. \nAPI:\npandas.read_excel\n","label":[[32,45,"Mention"],[455,472,"API"]],"Comments":[]}
{"id":60064,"text":"ID:45290383\nPost:\nText: You should use the native pandas function read_sas it's faster than iterating through the file as you did. \nText: Here is the documentation of the read_sas function. This code sample should be sufficient to load the file: \nCode: df = pandas.read_sas('some_file.sas7bdat')\nprint(df.head())\n\nAPI:\npandas.read_sas\npandas.read_sas\n","label":[[66,74,"Mention"],[171,179,"Mention"],[319,334,"API"],[335,350,"API"]],"Comments":[]}
{"id":60065,"text":"ID:45483373\nPost:\nText: keras expects model inputs to be numpy arrays - not pandas.DataFrames. Try: \nCode: X = train_data.iloc[:, 0:30].as_matrix()\nY = train_data.iloc[:,30].as_matrix()\n\nText: As as_matrix method converts pd.DataFrame to a numpy.array. \nAPI:\npandas.DataFrame\n","label":[[222,234,"Mention"],[259,275,"API"]],"Comments":[]}
{"id":60066,"text":"ID:45493368\nPost:\nCode: df = pd.DataFrame(\n    np.random.randint(10, size=(5, 7)),\n    columns='x1 x2 x3 x4 x5 x6 my_y'.split()\n)\n\ndf\n\n   x1  x2  x3  x4  x5  x6  my_y\n0   0   8   3   2   7   5     8\n1   0   6   2   5   8   4     9\n2   4   7   1   2   6   4     5\n3   8   5   4   0   5   7     4\n4   5   6   0   1   8   7     2\n\nText: Option1 Use the scatter method from the axes elements. \nCode: fig, axes = plt.subplots(2, 3, figsize=(6, 4), sharex=True, sharey=True)\ny = df.my_y.values\nfor i in range(6):\n    axes[i\/\/3, i%3].scatter(df.iloc[:, i].values, y)\n\nfig.tight_layout()\n\nText: Option 2 Use pd.DataFrame.plot \nCode: fig, axes = plt.subplots(2, 3, figsize=(6, 4), sharex=True, sharey=True)\ny = df.my_y.values\nfor i in range(6):\n    df.plot(x='x' + str(i+1),\n            y='my_y',\n            kind='scatter',\n            marker='x',\n            color='black',\n            ylim=[0, 10],\n            ax=axes[i\/\/3, i%3])\n\nfig.tight_layout()\n\nText: Response to Comment Without sharex=True \nCode: fig, axes = plt.subplots(2, 3, figsize=(6, 4), sharey=True)\ny = df.my_y.values\nfor i in range(6):\n    df.plot(x='x' + str(i+1),\n            y='my_y',\n            kind='scatter',\n            marker='x',\n            color='black',\n            ylim=[0, 10],\n            ax=axes[i\/\/3, i%3])\n\nfig.tight_layout()\n\nAPI:\npandas.DataFrame.plot\n","label":[[600,617,"Mention"],[1312,1333,"API"]],"Comments":[]}
{"id":60067,"text":"ID:45596654\nPost:\nText: The docs on replace says you have to provide a nested dictionary: the first level is the column name for which you have to provide a second dictionary with substitution pairs. \nText: So, this should work: \nCode: >>> df=pd.DataFrame({'a': ['NCOLAS','asd'], 'b': [3,4]})\n>>> df\n         a  b\n0  NCOLAS  3\n1     asd  4\n\n>>> df.replace({'a': {'': 'c', '': 'I'}}, regex=True)\n         a  b\n0  NICOLAS  3\n1     asdc  4\n\nText: Edit. Seems pandas also accepts non-nested translation dictionary. In that case, the problem is probably with character encoding, particularly if you use Python 2. Assuming your CSV load function decoded the file characters properly (as true Unicode code-points), then you should take care your translation\/substitution dictionary is also defined with Unicode characters, like this: \nCode: dictionary = {u'': 'i', u'': 'a'}\n\nText: If you have a definition like this (and using Python 2): \nCode: dictionary = {'': 'i', '': 'a'}\n\nText: then the actual keys in that dictionary are multibyte strings. Which bytes (characters) they are depends on the actual source file character encoding used, but presuming you use UTF-8, you'll get: \nCode: dictionary = {'\\xc3\\xa1': 'a', '\\xc3\\xad': 'i'}\n\nText: And that would explain why pandas fails to replace those chars. So, be sure to use Unicode literals in Python 2: u'this is unicode string'. \nText: On the other hand, in Python 3, all strings are Unicode strings, and you don't have to use the u prefix (in fact unicode type from Python 2 is renamed to str in Python 3, and the old str from Python 2 is now bytes in Python 3). \nAPI:\npandas.DataFrame.replace\n","label":[[36,43,"Mention"],[1628,1652,"API"]],"Comments":[]}
{"id":60068,"text":"ID:45603324\nPost:\nText: Consider simply parsing the grid_data node using built-in etree and pass it directly into read_table using StringIO(): \nCode: import pandas as pd\nimport xml.etree.ElementTree as et\nfrom io import StringIO    \nimport requests as rq\n\n# RETRIEVE URL OBJECT\nr = rq.get('https:\/\/earthquake.usgs.gov\/realtime\/product\/shakemap\/us2000a3y4\/us\/1501736303313\/download\/grid.xml')\n\n# BUILD TREE FROM URL CONTENT\ndoc = et.fromstring(r.content)\n\n# PARSE <grid_data> TEXT WITH UNDECLARED PREFIX NAMESPACE\ndata = doc.find('.\/\/{http:\/\/earthquake.usgs.gov\/eqcenter\/shakemap}grid_data').text\n\n# READ SPACE-DELIMITED STRING INTO DATAFRAME\ndf = pd.read_table(StringIO(data), sep=\"\\\\s+\", header=0, \n                   names=['LON','LAT','PGA', 'PGV', 'MMI','PSA03','PSA10','PSA30','STDPGA','URAT','SVEL'])\n\nprint(df.head())\n#         LON      LAT   PGA   PGV   MMI  PSA03  PSA10  PSA30  STDPGA  URAT     SVEL\n# 0 -100.3997  38.1145  0.01  0.01  1.77   0.02   0.01    0.0    0.65   1.0  354.533\n# 1 -100.3831  38.1145  0.01  0.02  1.82   0.02   0.01    0.0    0.65   1.0  310.786\n# 2 -100.3664  38.1145  0.01  0.01  1.77   0.02   0.01    0.0    0.65   1.0  354.545\n# 3 -100.3497  38.1145  0.01  0.01  1.76   0.02   0.01    0.0    0.65   1.0  362.307\n# 4 -100.3331  38.1145  0.01  0.01  1.76   0.02   0.01    0.0    0.65   1.0  360.332\n\nprint(df.tail())\n#             LON      LAT   PGA   PGV   MMI  PSA03  PSA10  PSA30  STDPGA  URAT     SVEL\n# 105767 -94.4831  33.2425  0.01  0.01  1.78   0.02   0.01    0.0    0.65   1.0  337.237\n# 105768 -94.4664  33.2425  0.01  0.02  1.89   0.03   0.01    0.0    0.65   1.0  249.221\n# 105769 -94.4497  33.2425  0.01  0.02  1.83   0.02   0.01    0.0    0.65   1.0  297.622\n# 105770 -94.4331  33.2425  0.01  0.01  1.63   0.02   0.01    0.0    0.65   1.0  500.368\n# 105771 -94.4164  33.2425  0.01  0.01  1.77   0.02   0.01    0.0    0.65   1.0  340.302\n\nAPI:\npandas.read_table\n","label":[[114,124,"Mention"],[1893,1910,"API"]],"Comments":[]}
{"id":60069,"text":"ID:45603707\nPost:\nText: o_csv \nText: When you make the call to the to_csv function, you can supply it the parameter date_format='%Y-%m-%d'. \nAPI:\npandas.DataFrame.to_csv\n","label":[[24,29,"Mention"],[146,169,"API"]],"Comments":[]}
{"id":60070,"text":"ID:45705448\nPost:\nText: 1. \nText: You need set_index with get_dummies: \nCode: df = dfb.set_index('Name').Product.str.get_dummies(',')\nprint (df)\n        Apple  Banana  Orange  Pear\nName                               \nMike        1       0       0     1\nJohn        0       1       1     0\nBob         0       1       0     0\nConnie      0       0       0     1\n\nText: 2. \nText: Solution with gt_dummiens with split for new DataFarme, last groupby by columns, so axis=1 and level=0 and aggregate max: \nCode: dfb = dfb.set_index('Name')\ndf = pd.get_dummies(dfb.Product.str.split(',', expand=True), prefix='', prefix_sep='')\n       .groupby(axis=1, level=0).max()\nprint (df)\n        Apple  Banana  Orange  Pear\nName                               \nMike        1       0       0     1\nJohn        0       1       1     0\nBob         0       1       0     0\nConnie      0       0       0     1\n\nText: 3. \nText: Solution with split and MultiLabelBinarizer: \nCode: from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\n\ndf = pd.DataFrame(mlb.fit_transform(dfb.Product.str.split(',')),\n                  columns=mlb.classes_, \n                  index=dfb.Name)\nprint (df)\n        Apple  Banana  Orange  Pear\nName                               \nMike        1       0       0     1\nJohn        0       1       1     0\nBob         0       1       0     0\nConnie      0       0       0     1\n\nText: If duplicates in column Name: \nCode: df = df.groupby('Name').max()\nprint (df)\n        Apple  Banana  Orange  Pear\nName                               \nBob         0       1       0     0\nConnie      0       0       0     1\nJohn        0       1       1     0\nMike        1       0       0     1\n\nAPI:\npandas.get_dummies\n","label":[[392,403,"Mention"],[1715,1733,"API"]],"Comments":[]}
{"id":60071,"text":"ID:45718128\nPost:\nText: You can use pd.DataFrame.stack + pandas.Series.nlargest, e.g.: \nCode: In [183]: frame.stack().nlargest(3)\nOut[183]:\nTexas   b    1.744812\nUtah    b    1.624345\nOregon  d    1.462108\ndtype: float64\n\nText: or: \nCode: In [184]: frame.stack().nlargest(3).reset_index(drop=True)\nOut[184]:\n0    1.744812\n1    1.624345\n2    1.462108\ndtype: float64\n\nAPI:\npandas.DataFrame.stack\n","label":[[36,54,"Mention"],[371,393,"API"]],"Comments":[]}
{"id":60072,"text":"ID:45798172\nPost:\nText: Use the parse_dates parameter of read_sql to specify that DateVar column values are explicitly converted to datetime on dataframe load. \nText: Updated original code snippet: \nCode: ...\nd2 = pd.read_sql(sql=sql,\n                 con=cnxn,\n                 # explicitly convert DATE type to datetime object\n                 parse_dates=[\"DateVar\"])\n\ncnxn.close()\n\nprint(d2.dtypes)\n\nText: Returns \nCode: DateVar         datetime64[ns]\nDateTimeVar     datetime64[ns]\nDateTime2Var    datetime64[ns]\ndtype: object\n\nText: Tested with pyodbc 4.0.17, pandas 0.20.3, and SQL Server 2014 on Windows. \nAPI:\npandas.read_sql\n","label":[[57,65,"Mention"],[619,634,"API"]],"Comments":[]}
{"id":60073,"text":"ID:45845724\nPost:\nText: Consider using corr in an dataframe apply where you pass each column into a function, here the anonymous lambda, and pair each with the b column: \nText: Random data (seeded to reproduce) \nCode: import pandas as pd\nimport numpy as np\n\nnp.random.seed(50)\n\na = pd.DataFrame({'A':np.random.randn(50),\n                  'B':np.random.randn(50),\n                  'C':np.random.randn(50),\n                  'D':np.random.randn(50),\n                  'E':np.random.randn(50)})\n\nb = pd.DataFrame({'test':np.random.randn(10)})\n\nText: Reproducing Pearson correlation \nCode: pear_result1 = a.ix[:,0:5].corrwith(b.ix[:,0])\nprint(pear_result1)\n# A   -0.073506\n# B   -0.098045\n# C    0.166293\n# D    0.123491\n# E    0.348576\n# dtype: float64\n\npear_result2 = a.apply(lambda col: col.corr(b.ix[:,0], method='pearson'), axis=0)\nprint(pear_result2)\n# A   -0.073506\n# B   -0.098045\n# C    0.166293\n# D    0.123491\n# E    0.348576\n# dtype: float64\n\nprint(pear_result1 == pear_result2)\n# A    True\n# B    True\n# C    True\n# D    True\n# E    True\n# dtype: bool\n\nText: Spearman correlation \nCode: spr_result = a.apply(lambda col: col.corr(b.ix[:,0], method='spearman'), axis=0)\nprint(spr_result)\n# A   -0.018182\n# B   -0.103030\n# C    0.321212\n# D   -0.151515\n# E    0.321212\n# dtype: float64\n\nText: Spearman coefficient with pvalues \nCode: from scipy.stats import spearmanr, pearsonr\n\n# SERIES OF TUPLES (<scipy.stats.stats.SpearmanrResult> class)\nspr_all_result = a.apply(lambda col: spearmanr(col, b.ix[:,0]), axis=0)\n\n# SERIES OF FLOATS\nspr_corr = a.apply(lambda col: spearmanr(col, b.ix[:,0])[0], axis=0)\nspr_pvalues = a.apply(lambda col: spearmanr(col, b.ix[:,0])[1], axis=0)\n\nAPI:\npandas.Series.corr\n","label":[[39,43,"Mention"],[1689,1707,"API"]],"Comments":[]}
{"id":60074,"text":"ID:45889550\nPost:\nText: You need to explicitly specify how to join the table. By default, merge will choose common column name as merge key. For your case, \nCode: c.merge(orders, left_index=True, right_on='CustomID')\n\nText: Also, read the docs of df.merge please. Hope this would be helpful. \nAPI:\npandas.DataFrame.merge\n","label":[[247,255,"Mention"],[298,320,"API"]],"Comments":[]}
{"id":60075,"text":"ID:45910169\nPost:\nText: If you start with numpy arrays, you can use numpy.concatenate: \nCode: pd.np.concatenate([np.repeat([1, 2], 2), np.arange(5, 10, 2), np.random.random_sample(3)])\n#array([ 1.        ,  1.        ,  2.        ,  2.        ,  5.        ,\n#        7.        ,  9.        ,  0.61116272,  0.48863116,  0.84436643])\n\nText: If you start with Series objects, you can append one series to another: \nCode: s1 = pd.Series(np.repeat([1, 2], 2))\ns2 = pd.Series(np.arange(5, 10, 2))\ns3 = pd.Series(np.random.random_sample(3))\n    \ns1.append([s2, s3], ignore_index=True)\n#0    1.000000\n#1    1.000000\n#2    2.000000\n#3    2.000000\n#4    5.000000\n#5    7.000000\n#6    9.000000\n#7    0.766968\n#8    0.730897\n#9    0.196995\n#dtype: float64\n\nText: or use pd.concat method: \nCode: pd.concat([s1, s2, s3], ignore_index=True)\n\nAPI:\npandas.Series\n","label":[[357,363,"Mention"],[833,846,"API"]],"Comments":[]}
{"id":60076,"text":"ID:45910934\nPost:\nText: Assuming this is a df and x is a list object: \nCode: df['count_a'] = df['x'].apply(lambda x: sum('a' in e for e in x))\n\nAPI:\npandas.DataFrame\n","label":[[43,45,"Mention"],[149,165,"API"]],"Comments":[]}
{"id":60077,"text":"ID:46012385\nPost:\nText: You can use DataFrame.merge instead of pd.merge \nText: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.merge.html \nText: Create an empty DataFrame with the columns to prevent the \"key error: Code\" \nCode: df = pd.DataFrame(columns=['Code']) \n\nText: then in the loop, you \nCode: df.merge(my_value, on='Code', how='outer') \n\nText: after my_value is created \nAPI:\npandas.merge\n","label":[[63,71,"Mention"],[408,420,"API"]],"Comments":[]}
{"id":60078,"text":"ID:46062936\nPost:\nText: Check this code and let me know if you are looking for this. iterrows is used to looping through the records and to check the mass flow. \nText: iterrwos --- A generator that iterates over the rows of the frame. \nCode: import pandas as pd\nimport os \nnew_df = pd.read_csv(os.path.join('C:\\Shijo\\Python\\sample.txt'), delimiter = ',')\nnew_df = new_df.drop_duplicates(subset = 'Mass Flow (kg\/hr)')\nnew_df = new_df.sort_values('Mass Flow (kg\/hr)')\nnew_df = new_df.reset_index(drop=True)\ncurrent_mass_flow = new_df.iloc[0]['Mass Flow (kg\/hr)']\n\nindexlst=[1]\n\nfor index, row in new_df.iterrows():\n\n    if row['Mass Flow (kg\/hr)'] > current_mass_flow + 15:\n        print (\"Mathcing index : \",index)\n        indexlst.append(index)\n        current_mass_flow =row['Mass Flow (kg\/hr)']\n\n\nreduced_df= new_df.iloc[indexlst]\nprint (reduced_df ) \n\nText: output \nCode:    TimeStamp (s)   TC 01 (C)   TC 02 (C)   TC 03 (C)  TC 32 (C)  Product Back Pressure (kPa)  Product Mass Flow (kg\/hr)    Semtech Flow (kg\/hr)  Mass Flow (kg\/hr)  Voltage (V)  Angle (degrees)   \n1          7.178  493.132548  296.373478  255.944743  26.405251                     0.281191                 146.212362               30.022240          30.022240     1.634457                 0  \n8        960.629  442.231666  300.542452  264.656445  27.773877                     0.599644                 203.971922               44.775988          44.775988     1.634457                 0  \n\nAPI:\npandas.DataFrame.iterrows\npandas.DataFrame.iterrows\n","label":[[85,93,"Mention"],[168,176,"Mention"],[1469,1494,"API"],[1495,1520,"API"]],"Comments":[]}
{"id":60079,"text":"ID:46082097\nPost:\nText: Apply na_filter =False while using pd.read_csv . This should help you to get away with the problem. \nText: na_filter : boolean, default True Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file \nAPI:\npandas.read_csv\n","label":[[59,70,"Mention"],[345,360,"API"]],"Comments":[]}
{"id":60080,"text":"ID:46083501\nPost:\nText: When calculating the standard deviation it matters whether you are estimating the standard deviation of an entire population with a smaller sample of that population or are you calculating the standard deviation of the entire population. \nText: If it is a smaller sample of a larger population, you need what is called the sample standard deviation. As it turns out, when you divide the sum of squared differences from the mean by the number of observations, you end up with a biased estimator. We correct for that by dividing by one less than the number of observations. We control for this with the argument ddof=1 for sample standard deviation or ddof=0 for population standard deviation. \nText: Truth is, it doesn't matter much if your sample size is large. But you will see small differences. \nText: Use the degrees of freedom argument in your pd.DataFrame.std call: \nCode: import pandas as pd\ndata = pd.read_csv(\"https:\/\/s3.amazonaws.com\/demo-datasets\/wine.csv\")\nnumeric_data = data.drop(\"color\", 1)\nnumeric_data1 = ((numeric_data - numeric_data.mean()) \/\n                 numeric_data.std(ddof=0))  # <<<\nnumeric_data2 = ((numeric_data - np.mean(numeric_data, axis=0)) \/\n                 np.std(numeric_data, axis=0))\n\nnp.isclose(numeric_data1, numeric_data2).all()  # -> True\n\nText: Or in the np.std call: \nCode: import pandas as pd\ndata = pd.read_csv(\"https:\/\/s3.amazonaws.com\/demo-datasets\/wine.csv\")\nnumeric_data = data.drop(\"color\", 1)\nnumeric_data1 = ((numeric_data - numeric_data.mean()) \/\n                 numeric_data.std())\nnumeric_data2 = ((numeric_data - np.mean(numeric_data, axis=0)) \/\n                 np.std(numeric_data, axis=0, ddof=1))  # <<<\n\nnp.isclose(numeric_data1, numeric_data2).all()  # -> True\n\nAPI:\npandas.DataFrame.std\n","label":[[873,889,"Mention"],[1758,1778,"API"]],"Comments":[]}
{"id":60081,"text":"ID:46167095\nPost:\nText: Per the docs of pandas.read_sql_table: \nText: Given a table name and an SQLAlchemy connectable, returns a DataFrame. This function does not support DBAPI connections. \nText: Since pyodbc is a DBAPI, use the query method, read_sql which the con argument does support DBAPI: \nCode: dfTable = pd.read_sql(\"SELECT * FROM TestTable1\", cnxn)\n\nAPI:\npandas.read_sql\n","label":[[245,253,"Mention"],[366,381,"API"]],"Comments":[]}
{"id":60082,"text":"ID:46221397\nPost:\nText: From the documentation for Index \nText: Immutable ndarray implementing an ordered, sliceable set. The basic object storing axis labels for all pandas objects \nText: Having a regular list as an index for a DataFrame could cause issues with unorderable or unhashable objects, evidently - since it is backed by a hash table, the same principles apply as to why lists can't be dictionary keys in regular Python. \nText: At the same time, the Index object being explicit permits us to use different types as an Index, as compared to the implicit integer index that NumPy has for instance, and perform fast lookups. \nText: If you want to retrieve a list of column names, the Index object has a tolist method. \nCode: >>> df.columns.tolist()\n['a', 'b', 'c']\n\nAPI:\npandas.Index\n","label":[[51,56,"Mention"],[779,791,"API"]],"Comments":[]}
{"id":60083,"text":"ID:46223221\nPost:\nText: The problem is qcut chooses the bins so that you have the same number of records in each bin\/quantile, but the same value cannot fall in multiple bins\/quantiles. \nText: Here is a list of solutions. \nAPI:\npandas.qcut\n","label":[[39,43,"Mention"],[228,239,"API"]],"Comments":[]}
{"id":60084,"text":"ID:46253463\nPost:\nText: Let's say you have a csv like that: \nCode: 1,2,3,4,0\n1,2,3,4,1\n1,2,3,4,1\n1,2,3,4,0\n\nText: where the first 4 columns are features and the last one is the label or class you want. You can read the file with read_csv and create a dataframe for you features and one for your labels which you can fit next, to your model. \nCode: import pandas as pd\n\n#CSV localPath\nmypath ='C:\\\\...'\n\n#The names of the columns you want to have in your dataframe\ncolNames = ['Feature1','Feature2','Feature3','Feature4','class']\n\n#Read the data as dataframe\ndf = pd.read_csv(filepath_or_buffer = mypath, \n                 names = colNames , sep  = ',' , header = None)\n\n#Get the first four columns as features\nfeatures = df.ix[:,:4]\n#and last columns as label\nlabels = df['class']\n\nAPI:\npandas.read_csv\n","label":[[229,237,"Mention"],[787,802,"API"]],"Comments":[]}
{"id":60085,"text":"ID:46265380\nPost:\nText: I had a similar problem. It's look like the problem occurs with read_csv with Python 3.6 in a Windows system. \nText: Python 3.6 change Windows filesystem encoding from \"mbcs\" to \"UTF-8\". See Python PEP 529. You can use the command sys.getfilesystemencoding() to get the current file system encoding \nText: I get two solutions around this: \nText: 1.- Use this code to change all the app to works with the prior Python <= 3.5 encoding (\"mbcs\") \nCode: import sys\nsys._enablelegacywindowsfsencoding()\n\nText: 2.- Pass a file pointer to the read_csv \nCode: with open(\"C:\\Users\\MyName\\Desktop\\dumm12\\dm1.csv\", 'r') as fp:\n        dum1 = pd.read_csv(fp, sep = \";\", decimal = \",\", encoding = \"utf-8\")\n\nText: You can see this post: read_csv can't import file with accent mark in path \nAPI:\npandas.read_csv\npandas.read_csv\npandas.read_csv\n","label":[[88,96,"Mention"],[559,567,"Mention"],[747,755,"Mention"],[805,820,"API"],[821,836,"API"],[837,852,"API"]],"Comments":[]}
{"id":60086,"text":"ID:46304463\nPost:\nText: You can do this using cut \nCode: import pandas\n\nbinwidth = 10\ndata = pandas.read_csv('sample.csv', sep=' ', names=['time', 'value'], header=None, comment='#')\n\nmax_bin_edge = int(np.ceil(data['time'].max()\/binwidth)*binwidth) + 1\nbin_edges = list(range(0, max_bin_edge, binwidth))\n\nbins = pd.cut(data['time'], bins=bin_edges, right=False)\n\nbin_counts = bins.groupby(bins).count()\n\nprint(bin_counts)\n\nText: Which will give you the bin edges as well \nCode: time\n[0, 10)     8\n[10, 20)    4\n[20, 30)    4\nName: time, dtype: int64\n\nAPI:\npandas.cut\n","label":[[46,49,"Mention"],[557,567,"API"]],"Comments":[]}
{"id":60087,"text":"ID:46343679\nPost:\nText: You need pd.notnull or pd.isnull for compare with None (or compare with NaN): \nCode: df.apply(lambda row: (row[i[0]] == row[i[1]]) and \n                      pd.notnull(row[i[0]]) and \n                      pd.notnull(row[i[1]), axis=1)\n\nText: But better is compare columns, then it working perfectly, because np.nan != np.nan: \nCode: for i in combinations(Col,2):\n    df[i[0]+' to '+i[1]+' dedication'] = np.where(df[i[0]] == df[i[1]], 'Y', 'N')\n\nText: Sample: \nCode: df = pd.DataFrame({'Key':[1,2,3,4],\n                   'SCANNER A':['AAA1', None, None, 'AAA1'],\n                   'SCANNER B':['AAA1', 'AAA2', None, 'AAA2']})\n\ndf['new'] = np.where(df['SCANNER A'] == df['SCANNER B'], 'Y', 'N')\nprint (df)\n   Key SCANNER A SCANNER B new\n0    1      AAA1      AAA1   Y\n1    2      None      AAA2   N\n2    3      None      None   N\n3    4      AAA1      AAA2   N\n\nAPI:\npandas.notnull\npandas.isnull\n","label":[[33,43,"Mention"],[47,56,"Mention"],[894,908,"API"],[909,922,"API"]],"Comments":[]}
{"id":60088,"text":"ID:46362148\nPost:\nText: If the number of duplicate headers is known and constant, skip those rows: \nText: csv = pd.read_csv('https:\/\/www.dropbox.com\/s\/sl7y5zm0ppqfjn6\/sample_duplicate.csv?dl=1', skiprows=4) \nText: Alternatively, which comes w\/ the bonus of removing all duplicates, based on all columns, do this: \nText: csv = pd.read_csv('https:\/\/www.dropbox.com\/s\/sl7y5zm0ppqfjn6\/sample_duplicate.csv?dl=1') csv = csv.drop_duplicates() \nText: Now you still have a header line in the data, just skip it: csv = csv.iloc[1:] \nText: You certainly can then overwrite the input file with to_csv \nAPI:\npandas.DataFrame.to_csv\n","label":[[583,589,"Mention"],[596,619,"API"]],"Comments":[]}
{"id":60089,"text":"ID:46421682\nPost:\nText: I'm all about @cs's answer and @Zero's linked Q&A... But here is an alternative with numexpr \nCode: import numexpr as ne\n\ns[ne.evaluate('s {} {}'.format(ops[cfg['op']], cfg['threshold']))]\n\n0   -0.308855\n1   -0.031073\n3   -0.547615\nName: A, dtype: float64\n\nText: I reopened this question after having been closed as a dup of How to pass an operator to a python function? \nText: The question and answers are great and I showed my appreciation with up votes. \nText: Asking in the context of a pd.Series opens it up to using answers that include numpy and numexpr. Whereas trying to answer the dup target with this answer would be pure nonsense. \nAPI:\npandas.Series\n","label":[[522,531,"Mention"],[680,693,"API"]],"Comments":[]}
{"id":60090,"text":"ID:46430842\nPost:\nText: You can use factorizz to Encode input values as an enumerated type or categorical variable; To get n2 column, just group by a and factorize b: \nCode: import pandas as pd\n\ndf['n1'] = pd.factorize(df.a)[0] + 1\ndf['n2'] = df.groupby('a').b.transform(lambda x: pd.factorize(x)[0] + 1)\n\ndf\n#   a          b    n1  n2\n#0  A    Alabama    1   1\n#1  A    Alabama    1   1\n#2  A    Antioch    1   2\n#3  B   Brisbane    2   1\n#4  B    Boolean    2   2\n\nAPI:\npandas.factorize\n","label":[[36,45,"Mention"],[472,488,"API"]],"Comments":[]}
{"id":60091,"text":"ID:46475902\nPost:\nText: Simply use pd.DataFrame.plot on the pivot_table object, specifying a line graph. Also, assign Date_String in pivot_table's columns and leave MEASURE_LENGTH for index: \nText: Below includes a data rebuild with pd.read_table() to reproduce your posted data but can be ignored since you source table from MySQL. Also look into pd.read_sql which can read sqlAlchemy objects. \nCode: Data_IR = pandas.read_sql(stmt, con=mysql_engine)\n\nText: Reproduced Data (data slightly adjusted to not result exactly same for all three dates) \nCode: from io import StringIO\nimport pandas as pd\n\ntxt=\"\"\"\nDate_String Experiment  Experiment_Type RESET_FREQUENCY MEASURE_LENGTH  Value   Date_Integer\n28-Sep-16   A   FORWARD_Detector    \"1 Minute\"    1   0.99974 20160928\n28-Sep-16   A   FORWARD_Detector    \"1 Minute\"    7   0.99939 20160928\n28-Sep-16   A   FORWARD_Detector    \"1 Minute\"    14  0.99897 20160928\n28-Sep-16   A   FORWARD_Detector    \"1 Minute\"    21  0.99856 20160928\n28-Sep-16   A   FORWARD_Detector    \"1 Minute\"    30  0.99803 20160928\n28-Sep-16   A   FORWARD_Detector    \"1 Minute\"    60  0.99627 20160928\n28-Sep-16   A   FORWARD_Detector    \"1 Minute\"    90  0.99449 20160928\n28-Sep-16   A   FORWARD_Detector    \"1 Minute\"    120 0.99268 20160928\n29-Sep-16   A   FORWARD_Detector    \"1 Minute\"    1   0.99994 20160929\n29-Sep-16   A   FORWARD_Detector    \"1 Minute\"    7   0.99959 20160929\n29-Sep-16   A   FORWARD_Detector    \"1 Minute\"    14  0.99918 20160929\n29-Sep-16   A   FORWARD_Detector    \"1 Minute\"    21  0.99877 20160929\n29-Sep-16   A   FORWARD_Detector    \"1 Minute\"    30  0.99824 20160929\n29-Sep-16   A   FORWARD_Detector    \"1 Minute\"    60  0.99646 20160929\n29-Sep-16   A   FORWARD_Detector    \"1 Minute\"    90  0.99472 20160929\n29-Sep-16   A   FORWARD_Detector    \"1 Minute\"    120 0.99287 20160929\n30-Sep-16   A   FORWARD_Detector    \"1 Minute\"    1   0.99954 20160930\n30-Sep-16   A   FORWARD_Detector    \"1 Minute\"    7   0.99919 20160930\n30-Sep-16   A   FORWARD_Detector    \"1 Minute\"    14  0.99878 20160930\n30-Sep-16   A   FORWARD_Detector    \"1 Minute\"    21  0.99837 20160930\n30-Sep-16   A   FORWARD_Detector    \"1 Minute\"    30  0.99784 20160930\n30-Sep-16   A   FORWARD_Detector    \"1 Minute\"    60  0.99607 20160930\n30-Sep-16   A   FORWARD_Detector    \"1 Minute\"    90  0.99429 20160930\n30-Sep-16   A   FORWARD_Detector    \"1 Minute\"   120 0.99246 20160930\n\"\"\"\n\nData_IR = pd.read_table(StringIO(txt), sep=\"\\\\s+\")    \n\nText: Plot \nCode: import matplotlib.pyplot as plt\n\ndata_test= pd.pivot_table(Data_IR,index=[\"MEASURE_LENGTH\"], columns=[\"Date_String\"], values=\"Value\")\ndata_test.plot(kind='line')\n\nAPI:\npandas.DataFrame.plot\npandas.read_sql\n","label":[[35,52,"Mention"],[348,359,"Mention"],[2650,2671,"API"],[2672,2687,"API"]],"Comments":[]}
{"id":60092,"text":"ID:46477756\nPost:\nText: This is easiest to achieve by combining Time, grid_latitude and grid_longitude into a uMutliIndex on the DataFrame with set_index() before converting into an xarray Dataset. \nText: For example: \nCode: # note that pandas.DataFrame's to_xarray() method is equivalent to\n# xarray.Dataset.from_dataframe()\nds = df.set_index(['Time', 'grid_latitude', 'grid_longitude']).to_xarray()\n\nAPI:\npandas.MultiIndex\n","label":[[110,121,"Mention"],[407,424,"API"]],"Comments":[]}
{"id":60093,"text":"ID:46497200\nPost:\nText: You can use apply to loop through the List column, and convert each list to a Sreies object with the label as the index; This will result in a data frame with the label as the column headers, and then you can concat with the remaining columns of the data frame to get what you need: \nCode: df1 = pd.concat([\n    df.drop('List', 1), \n    df.List.apply(lambda lst: pd.Series({\n       d['label']: d['value'] for d in lst\n    }))\n], axis=1)\n\ndf1\n# Row   ID  Engagement   Forum Thread Size   Likes and Votes    Unique Commenters\n#0  1   45        NaN                    0                 0                    0\n#1  2   76          1                    1                 0                    1\n#2  3   99        NaN                  NaN               NaN                  NaN\n#3  4   83        NaN                    0                 0                    0\n#4  5   80        NaN                  NaN               NaN                  NaN\n\nAPI:\npandas.Series\n","label":[[102,108,"Mention"],[965,978,"API"]],"Comments":[]}
{"id":60094,"text":"ID:46555047\nPost:\nText: You can use numpy.allclose: \nText: numpy.allclose(a, b, rtol=1e-05, atol=1e-08, equal_nan=False) Returns True if two arrays are element-wise equal within a tolerance. The tolerance values are positive, typically very small numbers. The relative difference (rtol * abs(b)) and the absolute difference atol are added together to compare against the absolute difference between a and b. \nText: numpy works well with Series objects, so if you have two of them - s1 and s2, you can simply do: \nCode: np.allclose(s1, s2, atol=...) \n\nText: Where atol is your tolerance value. \nAPI:\npandas.Series\n","label":[[437,443,"Mention"],[599,612,"API"]],"Comments":[]}
{"id":60095,"text":"ID:46559552\nPost:\nText: It sounds like you're looking for a way to split your customer_id's into exact proportions, and not rely on chance. Here's one way to do that using qcut and np.random.permutation. \nCode: In [228]: df = pd.DataFrame({'customer_id': np.random.normal(size=10000), \n                             'group': np.random.choice(['a', 'b', 'c'], size=10000)})\n\nIn [229]: proportions = {'a':[.5,.5], 'b':[.4,.6], 'c':[.2,.8]}\n\nIn [230]: df.head()\nOut[230]:\n   customer_id group\n0       0.6547     c\n1       1.4190     a\n2       0.4205     a\n3       2.3266     a\n4      -0.5691     b\n\nIn [231]: def assigner(gp):\n     ...:     group = gp['group'].iloc[0]\n     ...:     cut = pd.qcut(\n                  np.arange(gp.shape[0]), \n                  q=np.cumsum([0] + proportions[group]), \n                  labels=range(len(proportions[group]))\n              ).get_values()\n     ...:     return pd.Series(cut[np.random.permutation(gp.shape[0])], index=gp.index, name='assignment')\n     ...:\n\nIn [232]: df['assignment'] = df.groupby('group', group_keys=False).apply(assigner)\n\nIn [233]: df.head()\nOut[233]:\n   customer_id group  assignment\n0       0.6547     c           1\n1       1.4190     a           1\n2       0.4205     a           0\n3       2.3266     a           1\n4      -0.5691     b           0\n\nIn [234]: (df.groupby(['group', 'assignment'])\n             .size()\n             .unstack()\n             .assign(proportion=lambda x: x[0] \/ (x[0] + x[1])))\nOut[234]:\nassignment     0     1  proportion\ngroup\na           1659  1658      0.5002\nb           1335  2003      0.3999\nc            669  2676      0.2000\n\nText: What's going on here? \nText: Within each group we call the function assigner assigner grabs the group name and proportions from the predefined dictionary and calls pd.qcut to split into 0(control) 1(treatment) np.random.permutation then shuffles the the assignments Create this as a new column in the original dataframe \nAPI:\npandas.qcut\n","label":[[172,176,"Mention"],[1957,1968,"API"]],"Comments":[]}
{"id":60096,"text":"ID:46656751\nPost:\nText: It sounds like you want a barplot, not a histogram. A histogram typically acts on a collection of data and plots the frequencies for you. If you want to specify the frequencies, how about something like this (assuming you have a df called df): \nCode: import pandas\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndf = pandas.DataFrame(...)\npositions = np.array(df[['d1','d2']]).flatten()\nfrequencies = np.array(df[['h1','h2']]).flatten()\nplt.bar(positions, frequencies)\nplt.show()\n\nAPI:\npandas.DataFrame\n","label":[[253,255,"Mention"],[520,536,"API"]],"Comments":[]}
{"id":60097,"text":"ID:46733436\nPost:\nText: You don't need to pass an x and a y to savgol_filter. You just need the y values which get passed automatically when you pass graph to it. What you are missing is the window size parameter and the polygon order parameter that define the smoothing. \nCode: from scipy.signal import savgol_filter\nimport pandas as pd\n\n# I passed `graph` but I could've passed `graph.values`\n# It is `graph.values` that will get used in the filtering\npd.Series(savgol_filter(graph, 7, 3), graph.index).plot()\n\nText: To address some other points of misunderstanding \nText: graph is a Series and NOT a pandas.DataFrame. A DataFrame can be thought of as a pd.Series of pandas.Series. So you access the index of the series with graph.index and the values with graph.values. You could have also done import matplotlib.pyplot as plt plt.plot(graph.index, savgol_filter(graph.values, 7, 3)) \nAPI:\npandas.Series\npandas.DataFrame\npandas.Series\n","label":[[586,592,"Mention"],[623,632,"Mention"],[656,665,"Mention"],[893,906,"API"],[907,923,"API"],[924,937,"API"]],"Comments":[]}
{"id":60098,"text":"ID:46761891\nPost:\nText: Use applymap \nCode: df.applymap(addWord)\n\nAPI:\npandas.DataFrame.applymap\n","label":[[28,36,"Mention"],[71,96,"API"]],"Comments":[]}
{"id":60099,"text":"ID:46788110\nPost:\nText: Perhaps pandas can provide you with a more direct way of achieving this, but if you simply want to rely on the csv package from the standard library, the following naive approach should do: \nCode: #!\/usr\/bin\/env python\n# -*- coding: utf-8 -*-\n\nimport csv\n\n\ndata = {\n    'key1': ['value1', 'value2', 'value3', 'n'],\n    'key2': ['value1', 'value2', 'value3', 'n'],\n    'key3': ['value1', 'value2', 'value3', 'n'],\n    'key4': ['value1', 'value2', 'value3', 'n']\n}\n\nheader = data.keys()\nno_rows = len(data[list(header)[0]])\n\nwith open('out.csv', 'w', newline='') as csvfile:\n    csvwriter = csv.writer(csvfile, delimiter=',')\n    csvwriter.writerow(header)\n    for row in range(no_rows):\n        csvwriter.writerow([data[key][row] for key in header])\n\nText: This results in \nCode: key1,key2,key3,key4\nvalue1,value1,value1,value1\nvalue2,value2,value2,value2\nvalue3,value3,value3,value3\nn,n,n,n\n\nText: Hope this helps! \nText: EDIT \nText: Since this is now the accepted answer, I feel that I should add that to_cvs does indeed provide a more direct way of achieving this, as mentioned in other answers: \nCode: import pandas as pd\n\npd.DataFrame(data).to_csv('out.csv', index=False)\n\nText: produces the same output as above (possibly with different line terminators, depending on your system). \nAPI:\npandas.DataFrame.to_csv\n","label":[[1027,1033,"Mention"],[1317,1340,"API"]],"Comments":[]}
{"id":60100,"text":"ID:46794762\nPost:\nText: Here's how I would plot the data in row 31 of a large dataframe, setting row 0 as the x-axis. (updated answer) \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nText: create a random array with 32 rows, and 10 columns \nCode: df = pd.DataFrame(np.random.rand(320).reshape(32,10), columns=range(64,74), index=range(1,33))\ndf.to_excel(r\"D:\\data\\data.xlsx\")\n\nText: Read only the columns and rows that you want using \"parse_cols\" and \"skiprows.\" The first column in this example is the dataframe index. \nCode: # load desired columns and rows into a dataframe\n# in this method, I firse make a list of all skipped_rows\ndesired_cols = [0] + list(range(2,9))\nskipped_rows = list(range(1,33))\nskipped_rows.remove(31)\ndf = pd.read_excel(r\"D:\\data\\data.xlsx\", index_col=0, parse_cols=desired_cols, skiprows=skipped_rows)\n\nText: Currently this yields a dataframe with only one row. \nText: 65 66 67 68 69 70 71 31 0.310933 0.606858 0.12442 0.988441 0.821966 0.213625 0.254897 \nText: isolate only the row that you want to plot, giving a Series with the original column header as the index \nCode: ser = df.loc[31, :]\n\nText: Plot the series. \nCode: fig, ax = plt.subplots()\nser.plot(ax=ax)\nax.set_xlabel(\"year\")\nax.set_ylabel(\"precipitation\")\n\nCode: fig, ax = plt.subplots()\nser.plot(kind=\"bar\", ax=ax)\nax.set_xlabel(\"year\")\nax.set_ylabel(\"precipitation\")\n\nAPI:\npandas.Series\n","label":[[1094,1100,"Mention"],[1417,1430,"API"]],"Comments":[]}
{"id":60101,"text":"ID:46846124\nPost:\nText: The load_dataset functionality of seaborn looks online for it's datasets. \nText: From the docstring \nText: Help on function load_dataset in module seaborn.utils: load_dataset(name, cache=True, data_home=None, **kws) Load a dataset from the online repository (requires internet). Parameters ---------- name : str Name of the dataset (`name`.csv on https:\/\/github.com\/mwaskom\/seaborn-data). You can obtain list of available datasets using :func:`get_dataset_names` cache : boolean, optional If True, then cache data locally and use the cache on subsequent calls data_home : string, optional The directory in which to cache data. By default, uses ~\/seaborn-data\/ kws : dict, optional Passed to read_csv \nText: Since in the defined online repository there is no new_df file it returns a 404 error. \nText: You can just pass your dataframe to seaborn functions (if it is already defined in your code). \nText: So if you df is called new_df. \nCode: f = sns.FacetGrid(new_df, col=\"Year\", col_wrap=4, size=1.5)\n\nText: Should use your dataframe. \nAPI:\npandas.read_csv\n","label":[[715,723,"Mention"],[1065,1080,"API"]],"Comments":[]}
{"id":60102,"text":"ID:46900526\nPost:\nText: For future reference. The coneains has the param regex set to True by default which means we can use Regex expressions. \nText: To find 0 or more of any character we can simply use this (ref. Alan Moore) \nText: .* just means \"0 or more of any character\" It's broken down into two parts: . - a \"dot\" indicates any character * - means \"0 or more instances of the preceding regex token\" \nText: Here is a link to regex101 where you can test regex expressions: \nText: https:\/\/regex101.com\/r\/QNjkch\/1 \nText: And finally we can simplify your code, consider this simple example: \nCode: import pandas as pd\ndf = pd.DataFrame(columns=[\"a1a\",\"a2a\",\"a1b\"])\n\nmask = df.columns.str.contains('a.*a')\n\ndf.loc[:,mask] # selects mask\ndf.loc[:,~mask] # selects inverted (by using ~) mask\n\nAPI:\npandas.Series.str.contains\n","label":[[50,58,"Mention"],[798,824,"API"]],"Comments":[]}
{"id":60103,"text":"ID:47006681\nPost:\nText: you can use DataFrame built-in function max and min to find it \nText: example \nCode: df = pandas.DataFrame(randn(4,4))\ndf.max(axis=0) # will return max value of each column\ndf.max(axis=0)['AAL'] # column AAL's max\ndf.max(axis=1) # will return max value of each row\n\nText: or another way just find that column you want and call max \nCode: df = pandas.DataFrame(randn(4,4))\ndf['AAL'].max()\ndf['AAP'].min()\n\nText: min is the same \nAPI:\npandas.DataFrame\n","label":[[36,45,"Mention"],[457,473,"API"]],"Comments":[]}
{"id":60104,"text":"ID:47030487\nPost:\nText: Use concat with rename columns for align columns - need same columns in both DataFrames: \nCode: df = pd.concat([df1, df2.rename(columns={'Feeds':'Count'})], ignore_index=True)\nprint (df)\n         City    Age  Gender              Source  Count\n0  California  15-24  Female  Amazon Prime Video  14629\n1  California  15-24  Female             Fubo TV   3840\n2  California  15-24  Female                Hulu  54067\n3  California  15-24  Female             Netflix  11713\n4  California  15-24  Female            Sling TV  10642\n5  California  15-24  Female               Blogs    150\n6  California  15-24  Female          Customsite     57\n7  California  15-24  Female         Discussions     28\n8  California  15-24  Female    Facebook Comment    555\n9  California  15-24  Female             Google+     19\n\nText: Alternative with DataFrame.append - not pure python append: \nCode: df = df1.append(df2.rename(columns={'Feeds':'Count'}), ignore_index=True)\nprint (df)\n         City    Age  Gender              Source  Count\n0  California  15-24  Female  Amazon Prime Video  14629\n1  California  15-24  Female             Fubo TV   3840\n2  California  15-24  Female                Hulu  54067\n3  California  15-24  Female             Netflix  11713\n4  California  15-24  Female            Sling TV  10642\n5  California  15-24  Female               Blogs    150\n6  California  15-24  Female          Customsite     57\n7  California  15-24  Female         Discussions     28\n8  California  15-24  Female    Facebook Comment    555\n9  California  15-24  Female             Google+     19\n\nAPI:\npandas.concat\n","label":[[28,34,"Mention"],[1608,1621,"API"]],"Comments":[]}
{"id":60105,"text":"ID:47280249\nPost:\nText: You can use \nCode: df['new'] = df['text'].str.extract(r'Type (\\w+) Capacity')\n\nText: The extract method will only return the captured values (those matched with parenthetical pattern parts). \nText: You may also pass expand=True if you want to make sure a data frame only is returned (or False to get Series\/Index\/DataFrame), and if you have no matches on some rows, .fillna('') may be useful. \nAPI:\npandas.Series.str.extract\n","label":[[113,120,"Mention"],[423,448,"API"]],"Comments":[]}
{"id":60106,"text":"ID:47387088\nPost:\nText: When you are merging data using merge it will use df1 memory, df2 memory and merge_df memory. I believe that it is why you get a memory error. You should export df2 to a csv file and use chunksize option and merge data. \nText: It might be a better way but you can try this. *for large data set you can use chunksize option in pd.read_csv \nCode: df1 = pd.read_csv(\"yourdata.csv\")\ndf2 = pd.read_csv(\"yourdata2.csv\")\ndf2_key = df2.Colname2\n\n# creating a empty bucket to save result\ndf_result = pd.DataFrame(columns=(df1.columns.append(df2.columns)).unique())\ndf_result.to_csv(\"df3.csv\",index_label=False)\n\n# save data which only appear in df1 # sorry I was doing left join here. no need to run below two line.\n# df_result = df1[df1.Colname1.isin(df2.Colname2)!=True]\n# df_result.to_csv(\"df3.csv\",index_label=False, mode=\"a\")\n\n# deleting df2 to save memory\ndel(df2)\n\ndef preprocess(x):\n    df2=pd.merge(df1,x, left_on = \"Colname1\", right_on = \"Colname2\")\n    df2.to_csv(\"df3.csv\",mode=\"a\",header=False,index=False)\n\nreader = pd.read_csv(\"yourdata2.csv\", chunksize=1000) # chunksize depends with you colsize\n\n[preprocess(r) for r in reader]\n\nText: this will save merged data as df3. \nAPI:\npandas.merge\npandas.read_csv\n","label":[[56,61,"Mention"],[350,361,"Mention"],[1208,1220,"API"],[1221,1236,"API"]],"Comments":[]}
{"id":60107,"text":"ID:47402370\nPost:\nText: The objective is to relabel groups defined in the 'cluster' column by the corresponding rank of that group's total value count within the column. We'll break this down into several steps: \nText: Integer factorization. Find an integer representation where each unique value in the column gets its own integer. We'll start with zero. We then need the counts of each of these unique values. We need to rank the unique values by their counts. We assign the ranks back to the positions of the original column. \nText: Approach 1 Using Numpy's numpy.unique + argsort \nText: TL;DR \nCode: u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_counts=True\n)\n(-c).argsort()[i]\n\nText: Turns out, numpy.unique performs the task of integer factorization and counting values in one go. In the process, we get unique values as well, but we don't really need those. Also, the integer factorization isn't obvious. That's because per the numpy.unique function, the return value we're looking for is called the inverse. It's called the inverse because it was intended to act as a way to get back the original array given the array of unique values. So if we let \nCode: u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_couns=True\n)\n\nText: You'll see i looks like: \nCode: array([2, 2, 2, 2, 0, 0, 1, 1, 1, 3, 3, 4, 5])\n\nText: And if we did u[i] we get back the original df.cluster.values \nCode: array([3, 3, 3, 3, 1, 1, 2, 2, 2, 4, 4, 5, 6])\n\nText: But we are going to use it as integer factorization. \nText: Next, we need the counts c \nCode: array([2, 3, 4, 2, 1, 1])\n\nText: I'm going to propose the use of argsort but it's confusing. So I'll try to show it: \nCode: np.row_stack([c, (-c).argsort()])\n\narray([[2, 3, 4, 2, 1, 1],\n       [2, 1, 0, 3, 4, 5]])\n\nText: What argsort does in general is to place the top spot (position 0), the position to draw from in the originating array. \nCode: #            position 2\n#            is best\n#                |\n#                v\n# array([[2, 3, 4, 2, 1, 1],\n#        [2, 1, 0, 3, 4, 5]])\n#         ^\n#         |\n#     top spot\n#     from\n#     position 2\n\n#        position 1\n#        goes to\n#        pen-ultimate spot\n#            |\n#            v\n# array([[2, 3, 4, 2, 1, 1],\n#        [2, 1, 0, 3, 4, 5]])\n#            ^\n#            |\n#        pen-ultimate spot\n#        from\n#        position 1\n\nText: What this allows us to do is to slice this argsort result with our integer factorization to arrive at a remapping of the ranks. \nCode: #     i is\n#        [2 2 2 2 0 0 1 1 1 3 3 4 5]\n\n#     (-c).argsort() is \n#        [2 1 0 3 4 5]\n\n# argsort\n# slice\n#      \\   \/ This is our integer factorization\n#       a i\n#     [[0 2]  <-- 0 is second position in argsort\n#      [0 2]  <-- 0 is second position in argsort\n#      [0 2]  <-- 0 is second position in argsort\n#      [0 2]  <-- 0 is second position in argsort\n#      [2 0]  <-- 2 is zeroth position in argsort\n#      [2 0]  <-- 2 is zeroth position in argsort\n#      [1 1]  <-- 1 is first position in argsort\n#      [1 1]  <-- 1 is first position in argsort\n#      [1 1]  <-- 1 is first position in argsort\n#      [3 3]  <-- 3 is third position in argsort\n#      [3 3]  <-- 3 is third position in argsort\n#      [4 4]  <-- 4 is fourth position in argsort\n#      [5 5]] <-- 5 is fifth position in argsort\n\nText: We can then drop it into the column with pd.DataFrame.assign \nCode: u, i, c = np.unique(\n    df.cluster.values,\n    return_inverse=True,\n    return_counts=True\n)\ndf.assign(cluster=(-c).argsort()[i])\n\n    id  cluster\n0    1        0\n1    2        0\n2    3        0\n3    4        0\n4    5        2\n5    6        2\n6    7        1\n7    8        1\n8    9        1\n9   10        3\n10  11        3\n11  12        4\n12  13        5\n\nText: Approach 2 I'm going to leverage the same concepts. However, I'll use Pandas pd.factorize to get integer factorization with numpy.bincount to count values. The reason to use this approach is because Numpy's unique actually sorts the values in the midst of factorizing and counting. pd.factorize does not. For larger data sets, big oh is our friend as this remains O(n) while the Numpy approach is O(nlogn). \nCode: i, u = pd.factorize(df.cluster.values)\nc = np.bincount(i)\ndf.assign(cluster=(-c).argsort()[i])\n\n    id  cluster\n0    1        0\n1    2        0\n2    3        0\n3    4        0\n4    5        2\n5    6        2\n6    7        1\n7    8        1\n8    9        1\n9   10        3\n10  11        3\n11  12        4\n12  13        5\n\nAPI:\npandas.factorize\npandas.factorize\n","label":[[3880,3892,"Mention"],[4085,4097,"Mention"],[4543,4559,"API"],[4560,4576,"API"]],"Comments":[]}
{"id":60108,"text":"ID:47468205\nPost:\nText: This is how I've done it. I've followed advice found: \nText: subclassing-pandas-data-structures Fix Finalize Issue \nText: The example below only shows the use of constructing new subclasses of pandas.DataFrame. If you follow the advice in my first link, you may consider subclassing Series as well to account for taking single dimensional slices of your DataFrame subclass. \nText: Defining SomeData \nCode: import pandas as pd\nimport numpy as np\n\nclass SomeData(pd.DataFrame):\n    # This class variable tells Pandas the name of the attributes\n    # that are to be ported over to derivative DataFrames.  There\n    # is a method named `__finalize__` that grabs these attributes\n    # and assigns them to newly created `SomeData`\n    _metadata = ['my_attr']\n\n    @property\n    def _constructor(self):\n        \"\"\"This is the key to letting Pandas know how to keep\n        derivative `SomeData` the same type as yours.  It should\n        be enough to return the name of the Class.  However, in\n        some cases, `__finalize__` is not called and `my_attr` is\n        not carried over.  We can fix that by constructing a callable\n        that makes sure to call `__finlaize__` every time.\"\"\"\n        def _c(*args, **kwargs):\n            return SomeData(*args, **kwargs).__finalize__(self)\n        return _c\n\n    def __init__(self, *args, **kwargs):\n        # grab the keyword argument that is supposed to be my_attr\n        self.my_attr = kwargs.pop('my_attr', None)\n        super().__init__(*args, **kwargs)\n\n    def my_method(self, other):\n        return self * np.sign(self - other)\n\nText: Demonstration \nCode: mydata = SomeData(dict(A=[1, 2, 3], B=[4, 5, 6]), my_attr='an attr')\n\nprint(mydata, type(mydata), mydata.my_attr, sep='\\n' * 2)\n\n   A  B\n0  1  4\n1  2  5\n2  3  6\n\n<class '__main__.SomeData'>\n\nan attr\n\nCode: newdata = mydata.mul(2)\n\nprint(newdata, type(newdata), newdata.my_attr, sep='\\n' * 2)\n\n   A   B\n0  2   8\n1  4  10\n2  6  12\n\n<class '__main__.SomeData'>\n\nan attr\n\nCode: newerdata = mydata.my_method(newdata)\n\nprint(newerdata, type(newerdata), newerdata.my_attr, sep='\\n' * 2)\n\n   A  B\n0 -1 -4\n1 -2 -5\n2 -3 -6\n\n<class '__main__.SomeData'>\n\nan attr\n\nText: Gotchas \nText: This borks on the method pd.DataFrame.equals \nCode: newerdata.equals(newdata)  # Should be `False`\n\nText: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-304-866170ab179e> in <module>() ----> 1 newerdata.equals(newdata) ~\/anaconda3\/envs\/3.6.ml\/lib\/python3.6\/site-packages\/pandas\/core\/generic.py in equals(self, other) 1034 the same location are considered equal. 1035 \"\"\" -> 1036 if not isinstance(other, self._constructor): 1037 return False 1038 return self._data.equals(other._data) TypeError: isinstance() arg 2 must be a type or tuple of types \nText: What happens is that this method expected to find an object of type type in the _constructor attribute. Instead, it found my callable that I placed there in order to fix the __finalize__ issue I came across. \nText: Work around \nText: Override the equals method with the following in your class definition. \nCode:     def equals(self, other):\n        try:\n            pd.testing.assert_frame_equal(self, other)\n            return True\n        except AssertionError:\n            return False\n\nnewerdata.equals(newdata)  # Should be `False`\n\nFalse\n\nAPI:\npandas.Series\npandas.DataFrame\n","label":[[307,313,"Mention"],[378,387,"Mention"],[3399,3412,"API"],[3413,3429,"API"]],"Comments":[]}
{"id":60109,"text":"ID:47524662\nPost:\nText: get_dummies will do the required trick. It will convert categorical variable into dummy\/indicator variables. For reference get_dummies \nCode: df1 = pd.get_dummies(df.Events)\ndf\n    rain, thunderstorm  snow\n0                    1     0\n1                    0     1\n\ndf = df[['humidity','temp']].join(df1)\ndf\n    humidity    temp    rain, thunderstorm  snow\n0         40      45                     1     0\n1         35      30                     0     1\n\nAPI:\npandas.get_dummies\npandas.get_dummies\n","label":[[24,35,"Mention"],[147,158,"Mention"],[484,502,"API"],[503,521,"API"]],"Comments":[]}
{"id":60110,"text":"ID:47541073\nPost:\nText: Meaning having a pandas dataframe which I transform to spark with the help of pyarrow. \nText: pyarrow.Table.fromPandas is the function your looking for: \nText: Table.from_pandas(type cls, df, bool timestamps_to_ms=False, Schema schema=None, bool preserve_index=True) Convert df to an Arrow Table \nCode: import pyarrow as pa\n\npdf = ...  # type: pandas.core.frame.DataFrame\nadf = pa.Table.from_pandas(pdf)  # type: pyarrow.lib.Table\n\nText: The result can be written directly to Parquet \/ HDFS without passing data via Spark: \nCode: import pyarrow.parquet as pq\n\nfs  = pa.hdfs.connect()\n\nwith fs.open(path, \"wb\") as fw\n    pq.write_table(adf, fw)\n\nText: See also \nText: @WesMcKinney answer to read a parquet files from HDFS using PyArrow. Reading and Writing the Apache Parquet Format in the pyarrow documentation. Native Hadoop file system (HDFS) connectivity in Python \nText: Spark notes: \nText: Furthermore since Spark 2.3 (current master) Arrow is supported directly in createDataFrame (SPARK-20791 - Use Apache Arrow to Improve Spark createDataFrame from Pandas.DataFrame). It uses SparkContext.defaultParallelism to compute number of chunks so you can easily control the size of individual batches. \nText: Finally defaultParallelism can be used to control number of partitions generated using standard _convert_from_pandas, effectively reducing size of the slices to something more manageable. \nText: Unfortunately these are unlikely to resolve your current memory problems. Both depend on parallelize, therefore store all data in memory of the driver node. Switching to Arrow or adjusting configuration can only speedup the process or address block size limitations. \nText: In practice I don't see any reason to switch to Spark here, as long as you use local Pandas DataFrame as the input. The most severe bottleneck in this scenario is driver's network I\/O and distributing data won't address that. \nAPI:\npandas.DataFrame\n","label":[[299,301,"Mention"],[1934,1950,"API"]],"Comments":[]}
{"id":60111,"text":"ID:47575130\nPost:\nText: In general, if you know the format of a string, you don't need to use dateutil.parser.parse to parse it, because you can use datetime.strptime with a specified string. \nText: In this case, the only slightly unfortunate thing is that you have 2-digit years, some of which are from before 2000. In this case, I'd probably do something like this: \nCode: cent_21_mask =  df['YY'] < 50\ndf.loc[:, 'YY'] = df.loc[:, 'YY'] + 1900\ndf.loc[cent_21_mask, 'YY'] = df.loc[cent_21_mask, 'YY'] + 100\n\nText: Once you've done that, you can use one of the solutions from this question (specifically this one) to convert your individual datetime columns into pandas Timestamps \/ datetimes. \nText: If these are in UTC, you then use pd.Series.tz_localize with 'UTC' to get timezone-aware datetimes. \nText: Putting it all together: \nCode: import pandas as pd\n\ndf = pd.DataFrame(\n    [[98, 12, 5, 11],\n     [98, 12, 5, 10],\n     [4, 12, 5, 00]],\n     columns=['YY', 'MM', 'DD', 'HH'])\n\n# Convert 2-digit years to 4-digit years\ncent_21_mask =  df['YY'] < 50\ndf.loc[:, 'YY'] = df.loc[:, 'YY'] + 1900\ndf.loc[cent_21_mask, 'YY'] = df.loc[cent_21_mask, 'YY'] + 100\n\n\n# Retrieve the date columns and rename them\ncol_renames = {'YY': 'year', 'MM': 'month', 'DD': 'day', 'HH': 'hour'}\ndt_subset = df.loc[:, list(col_renames.keys())].rename(columns=col_renames)\ndt_series = pd.to_datetime(dt_subset)\n\n# Convert to UTC\ndt_series = dt_series.dt.tz_localize('UTC')\n\n# Result:\n# 0   1998-12-05 11:00:00+00:00\n# 1   1998-12-05 10:00:00+00:00\n# 2   2004-12-05 00:00:00+00:00\n# dtype: datetime64[ns, UTC]\n\nText: Also, to clarify two things about this statement: \nText: I've tried reading the detailed documentation of dateutil (https:\/\/labix.org\/python-dateutil) and what I understood is that I've to make a dictionary specifying the timezone as key (UTC in my case) and that might solve the error. \nText: The correct documentation for python-dateutil is now https:\/\/dateutil.readthedocs.io. If you are using parse, in your situation there is no reason to add UTC into a dictionary and pass it to tzinfos. If you know that your datetimes are going to be naive but that they represent times in UTC, parse them as normal to get naive datetimes, then use datetime.replace(dateutil.tz.tzutc()) to get aware datetimes. The tzinfos dictionary is for when the timezone information is actually represented in the string. \nText: An example of what to do when you have strings representing UTC that don't contain timezone information: \nCode: from dateutil.parser import parse\nfrom dateutil import tz\n\ndt = parse('1998-12-05 11:00')\ndt = dt.replace(tzinfo=tz.tzutc())\n\nAPI:\npandas.Series.tz_localize\n","label":[[735,756,"Mention"],[2647,2672,"API"]],"Comments":[]}
{"id":60112,"text":"ID:47644743\nPost:\nText: numpy's mean function first checks whether its input has a mean method, as @EdChum explains in this answer. \nText: When you use df.apply, the input passed to the function is a pandas.Series. Since pd.Series has a mean method, numpy uses that instead of using its own function. And by default, pd.Series.mean ignores NaN. \nText: You can access the underlying numpy array by the values attribute and pass that to the function: \nCode: df.apply(lambda x: np.mean(x.values), axis=1)\n\nText: this will use numpy's version. \nAPI:\npandas.Series\npandas.Series.mean\n","label":[[221,230,"Mention"],[317,331,"Mention"],[546,559,"API"],[560,578,"API"]],"Comments":[]}
{"id":60113,"text":"ID:47795228\nPost:\nText: Look through all of the kwargs in read_csv \nText: There is the lineterminator kwarg: \nCode: lineterminator : str (length 1), default None\n    Character to break file into lines. Only valid with C parser.\n\nText: Note that it requires the use of the C parser (see engine kwarg) \nText: Given that your lines end with \\r, which is the carriage return character I would suggest using that as the lineterminator and doing post-processing to clean up the \\n's left behind. \nText: I would think that setting the lineterminator='\\r' should fix your problem. \nAPI:\npandas.read_csv\n","label":[[58,66,"Mention"],[579,594,"API"]],"Comments":[]}
{"id":60114,"text":"ID:47822490\nPost:\nText: Use to_datetime before you merge them into one DataFrame\/Series. \nAPI:\npandas.to_datetime\n","label":[[28,39,"Mention"],[95,113,"API"]],"Comments":[]}
{"id":60115,"text":"ID:47833036\nPost:\nText: Sure you can do it with pandas. You just need to read first N lines (36 in your case) to use them as header and read rest of the file like normal csv (pandas good at it). Then you can save DataFrame object to csv. \nText: Since your data splitted into adjacent lines, we should split DataFrame we've read on two and stack them one next to other (horizontaly). \nText: Consider the following code: \nCode: import pandas as pd\n\nCOLUMNS_COUNT = 36\n# read first `COLUMNS_COUNT` lines to serve as a header\nwith open('data.data', 'r') as f:\n    columns = [next(f).strip() for line in range(COLUMNS_COUNT)]\n# read rest of the file to temporary DataFrame\ntemp_df = pd.read_csv('data.data', skiprows=COLUMNS_COUNT, header=None, delimiter=';', skip_blank_lines=True)\n# split temp DataFrame on even and odd rows\neven_df = temp_df.iloc[::2].reset_index(drop=True)\nodd_df = temp_df.iloc[1::2].reset_index(drop=True)\n# stack even and odd DataFrames horizontaly\ndf = pd.concat([even_df, odd_df], axis=1)\n# assign column names\ndf.columns = columns\n# save result DataFrame to csv\ndf.to_csv('out.csv', index=False)\n\nText: UPD: code updated to correctly process data splitted onto two lines \nAPI:\npandas.DataFrame\n","label":[[213,222,"Mention"],[1199,1215,"API"]],"Comments":[]}
{"id":60116,"text":"ID:47947335\nPost:\nText: Yes, it seems like np.clip is a lot slower on Series than on numpy.ndarrays. That's correct but it's actually (at least asymptotically) not that bad. 8000 elements is still in the regime where constant factors are major contributors in the runtime. I think this is a very important aspect to the question, so I'm visualizing this (borrowing from another answer): \nCode: # Setup\n\nimport pandas as pd\nimport numpy as np\n\ndef on_series(s):\n    return np.clip(s, a_min=None, a_max=1)\n\ndef on_values_of_series(s):\n    return np.clip(s.values, a_min=None, a_max=1)\n\n# Timing setup\ntimings = {on_series: [], on_values_of_series: []}\nsizes = [2**i for i in range(1, 26, 2)]\n\n# Timing\nfor size in sizes:\n    func_input = pd.Series(np.random.randint(0, 30, size=size))\n    for func in timings:\n        res = %timeit -o func(func_input)\n        timings[func].append(res)\n\n%matplotlib notebook\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor func in timings:\n    ax1.plot(sizes, \n             [time.best for time in timings[func]], \n             label=str(func.__name__))\nax1.set_xscale('log')\nax1.set_yscale('log')\nax1.set_xlabel('size')\nax1.set_ylabel('time [seconds]')\nax1.grid(which='both')\nax1.legend()\n\nbaseline = on_values_of_series # choose one function as baseline\nfor func in timings:\n    ax2.plot(sizes, \n             [time.best \/ ref.best for time, ref in zip(timings[func], timings[baseline])], \n             label=str(func.__name__))\nax2.set_yscale('log')\nax2.set_xscale('log')\nax2.set_xlabel('size')\nax2.set_ylabel('time relative to {}'.format(baseline.__name__))\nax2.grid(which='both')\nax2.legend()\n\nplt.tight_layout()\n\nText: It's a log-log plot because I think this shows the important features more clearly. For example it shows that np.clip on a numpy.ndarray is faster but it also has a much smaller constant factor in that case. The difference for large arrays is only ~3! That's still a big difference but way less than the difference on small arrays. \nText: However, that's still not an answer to the question where the time difference comes from. \nText: The solution is actually quite simple: np.clip delegates to the clip method of the first argument: \nCode: >>> np.clip??\nSource:   \ndef clip(a, a_min, a_max, out=None):\n    \"\"\"\n    ...\n    \"\"\"\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out)\n\n>>> np.core.fromnumeric._wrapfunc??\nSource:   \ndef _wrapfunc(obj, method, *args, **kwds):\n    try:\n        return getattr(obj, method)(*args, **kwds)\n    # ...\n    except (AttributeError, TypeError):\n        return _wrapit(obj, method, *args, **kwds)\n\nText: The getattr line of the _wrapfunc function is the important line here, because np.ndarray.clip and pd.Series.clip are different methods, yes, completely different methods: \nCode: >>> np.ndarray.clip\n<method 'clip' of 'numpy.ndarray' objects>\n>>> pd.Series.clip\n<function pandas.core.generic.NDFrame.clip>\n\nText: Unfortunately is np.ndarray.clip a C-function so it's hard to profile it, however pd.Series.clip is a regular Python function so it's easy to profile. Let's use a Series of 5000 integers here: \nCode: s = pd.Series(np.random.randint(0, 100, 5000))\n\nText: For the np.clip on the values I get the following line-profiling: \nCode: %load_ext line_profiler\n%lprun -f np.clip -f np.core.fromnumeric._wrapfunc np.clip(s.values, a_min=None, a_max=1)\n\nTimer unit: 4.10256e-07 s\n\nTotal time: 2.25641e-05 s\nFile: numpy\\core\\fromnumeric.py\nFunction: clip at line 1673\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  1673                                           def clip(a, a_min, a_max, out=None):\n  1674                                               \"\"\"\n  ...\n  1726                                               \"\"\"\n  1727         1           55     55.0    100.0      return _wrapfunc(a, 'clip', a_min, a_max, out=out)\n\nTotal time: 1.51795e-05 s\nFile: numpy\\core\\fromnumeric.py\nFunction: _wrapfunc at line 55\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    55                                           def _wrapfunc(obj, method, *args, **kwds):\n    56         1            2      2.0      5.4      try:\n    57         1           35     35.0     94.6          return getattr(obj, method)(*args, **kwds)\n    58                                           \n    59                                               # An AttributeError occurs if the object does not have\n    60                                               # such a method in its class.\n    61                                           \n    62                                               # A TypeError occurs if the object does have such a method\n    63                                               # in its class, but its signature is not identical to that\n    64                                               # of NumPy's. This situation has occurred in the case of\n    65                                               # a downstream library like 'pandas'.\n    66                                               except (AttributeError, TypeError):\n    67                                                   return _wrapit(obj, method, *args, **kwds)\n\nText: But for np.clip on the Series I get a totally different profiling result: \nCode: %lprun -f np.clip -f np.core.fromnumeric._wrapfunc -f pd.Series.clip -f pd.Series._clip_with_scalar np.clip(s, a_min=None, a_max=1)\n\nTimer unit: 4.10256e-07 s\n\nTotal time: 0.000823794 s\nFile: numpy\\core\\fromnumeric.py\nFunction: clip at line 1673\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  1673                                           def clip(a, a_min, a_max, out=None):\n  1674                                               \"\"\"\n  ...\n  1726                                               \"\"\"\n  1727         1         2008   2008.0    100.0      return _wrapfunc(a, 'clip', a_min, a_max, out=out)\n\nTotal time: 0.00081846 s\nFile: numpy\\core\\fromnumeric.py\nFunction: _wrapfunc at line 55\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    55                                           def _wrapfunc(obj, method, *args, **kwds):\n    56         1            2      2.0      0.1      try:\n    57         1         1993   1993.0     99.9          return getattr(obj, method)(*args, **kwds)\n    58                                           \n    59                                               # An AttributeError occurs if the object does not have\n    60                                               # such a method in its class.\n    61                                           \n    62                                               # A TypeError occurs if the object does have such a method\n    63                                               # in its class, but its signature is not identical to that\n    64                                               # of NumPy's. This situation has occurred in the case of\n    65                                               # a downstream library like 'pandas'.\n    66                                               except (AttributeError, TypeError):\n    67                                                   return _wrapit(obj, method, *args, **kwds)\n\nTotal time: 0.000804922 s\nFile: pandas\\core\\generic.py\nFunction: clip at line 4969\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  4969                                               def clip(self, lower=None, upper=None, axis=None, inplace=False,\n  4970                                                        *args, **kwargs):\n  4971                                                   \"\"\"\n  ...\n  5021                                                   \"\"\"\n  5022         1           12     12.0      0.6          if isinstance(self, ABCPanel):\n  5023                                                       raise NotImplementedError(\"clip is not supported yet for panels\")\n  5024                                           \n  5025         1           10     10.0      0.5          inplace = validate_bool_kwarg(inplace, 'inplace')\n  5026                                           \n  5027         1           69     69.0      3.5          axis = nv.validate_clip_with_axis(axis, args, kwargs)\n  5028                                           \n  5029                                                   # GH 17276\n  5030                                                   # numpy doesn't like NaN as a clip value\n  5031                                                   # so ignore\n  5032         1          158    158.0      8.1          if np.any(pd.isnull(lower)):\n  5033         1            3      3.0      0.2              lower = None\n  5034         1           26     26.0      1.3          if np.any(pd.isnull(upper)):\n  5035                                                       upper = None\n  5036                                           \n  5037                                                   # GH 2747 (arguments were reversed)\n  5038         1            1      1.0      0.1          if lower is not None and upper is not None:\n  5039                                                       if is_scalar(lower) and is_scalar(upper):\n  5040                                                           lower, upper = min(lower, upper), max(lower, upper)\n  5041                                           \n  5042                                                   # fast-path for scalars\n  5043         1            1      1.0      0.1          if ((lower is None or (is_scalar(lower) and is_number(lower))) and\n  5044         1           28     28.0      1.4                  (upper is None or (is_scalar(upper) and is_number(upper)))):\n  5045         1         1654   1654.0     84.3              return self._clip_with_scalar(lower, upper, inplace=inplace)\n  5046                                           \n  5047                                                   result = self\n  5048                                                   if lower is not None:\n  5049                                                       result = result.clip_lower(lower, axis, inplace=inplace)\n  5050                                                   if upper is not None:\n  5051                                                       if inplace:\n  5052                                                           result = self\n  5053                                                       result = result.clip_upper(upper, axis, inplace=inplace)\n  5054                                           \n  5055                                                   return result\n\nTotal time: 0.000662153 s\nFile: pandas\\core\\generic.py\nFunction: _clip_with_scalar at line 4920\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  4920                                               def _clip_with_scalar(self, lower, upper, inplace=False):\n  4921         1            2      2.0      0.1          if ((lower is not None and np.any(isna(lower))) or\n  4922         1           25     25.0      1.5                  (upper is not None and np.any(isna(upper)))):\n  4923                                                       raise ValueError(\"Cannot use an NA value as a clip threshold\")\n  4924                                           \n  4925         1           22     22.0      1.4          result = self.values\n  4926         1          571    571.0     35.4          mask = isna(result)\n  4927                                           \n  4928         1           95     95.0      5.9          with np.errstate(all='ignore'):\n  4929         1            1      1.0      0.1              if upper is not None:\n  4930         1          141    141.0      8.7                  result = np.where(result >= upper, upper, result)\n  4931         1           33     33.0      2.0              if lower is not None:\n  4932                                                           result = np.where(result <= lower, lower, result)\n  4933         1           73     73.0      4.5          if np.any(mask):\n  4934                                                       result[mask] = np.nan\n  4935                                           \n  4936         1           90     90.0      5.6          axes_dict = self._construct_axes_dict()\n  4937         1          558    558.0     34.6          result = self._constructor(result, **axes_dict).__finalize__(self)\n  4938                                           \n  4939         1            2      2.0      0.1          if inplace:\n  4940                                                       self._update_inplace(result)\n  4941                                                   else:\n  4942         1            1      1.0      0.1              return result\n\nText: I stopped going into the subroutines at that point because it already highlights where the pd.Series.clip does much more work than the np.ndarray.clip. Just compare the total time of the np.clip call on the values (55 timer units) to one of the first checks in the nlip method, the if np.any(pd.isnull(lower)) (158 timer units). At that point the pandas method didn't even start at clipping and it already takes 3 times longer. \nText: However several of these \"overheads\" become insignificant when the array is big: \nCode: s = pd.Series(np.random.randint(0, 100, 1000000))\n\n%lprun -f np.clip -f np.core.fromnumeric._wrapfunc -f pd.Series.clip -f pd.Series._clip_with_scalar np.clip(s, a_min=None, a_max=1)\n\nTimer unit: 4.10256e-07 s\n\nTotal time: 0.00593476 s\nFile: numpy\\core\\fromnumeric.py\nFunction: clip at line 1673\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  1673                                           def clip(a, a_min, a_max, out=None):\n  1674                                               \"\"\"\n  ...\n  1726                                               \"\"\"\n  1727         1        14466  14466.0    100.0      return _wrapfunc(a, 'clip', a_min, a_max, out=out)\n\nTotal time: 0.00592779 s\nFile: numpy\\core\\fromnumeric.py\nFunction: _wrapfunc at line 55\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    55                                           def _wrapfunc(obj, method, *args, **kwds):\n    56         1            1      1.0      0.0      try:\n    57         1        14448  14448.0    100.0          return getattr(obj, method)(*args, **kwds)\n    58                                           \n    59                                               # An AttributeError occurs if the object does not have\n    60                                               # such a method in its class.\n    61                                           \n    62                                               # A TypeError occurs if the object does have such a method\n    63                                               # in its class, but its signature is not identical to that\n    64                                               # of NumPy's. This situation has occurred in the case of\n    65                                               # a downstream library like 'pandas'.\n    66                                               except (AttributeError, TypeError):\n    67                                                   return _wrapit(obj, method, *args, **kwds)\n\nTotal time: 0.00591302 s\nFile: pandas\\core\\generic.py\nFunction: clip at line 4969\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  4969                                               def clip(self, lower=None, upper=None, axis=None, inplace=False,\n  4970                                                        *args, **kwargs):\n  4971                                                   \"\"\"\n  ...\n  5021                                                   \"\"\"\n  5022         1           17     17.0      0.1          if isinstance(self, ABCPanel):\n  5023                                                       raise NotImplementedError(\"clip is not supported yet for panels\")\n  5024                                           \n  5025         1           14     14.0      0.1          inplace = validate_bool_kwarg(inplace, 'inplace')\n  5026                                           \n  5027         1           97     97.0      0.7          axis = nv.validate_clip_with_axis(axis, args, kwargs)\n  5028                                           \n  5029                                                   # GH 17276\n  5030                                                   # numpy doesn't like NaN as a clip value\n  5031                                                   # so ignore\n  5032         1          125    125.0      0.9          if np.any(pd.isnull(lower)):\n  5033         1            2      2.0      0.0              lower = None\n  5034         1           30     30.0      0.2          if np.any(pd.isnull(upper)):\n  5035                                                       upper = None\n  5036                                           \n  5037                                                   # GH 2747 (arguments were reversed)\n  5038         1            2      2.0      0.0          if lower is not None and upper is not None:\n  5039                                                       if is_scalar(lower) and is_scalar(upper):\n  5040                                                           lower, upper = min(lower, upper), max(lower, upper)\n  5041                                           \n  5042                                                   # fast-path for scalars\n  5043         1            2      2.0      0.0          if ((lower is None or (is_scalar(lower) and is_number(lower))) and\n  5044         1           32     32.0      0.2                  (upper is None or (is_scalar(upper) and is_number(upper)))):\n  5045         1        14092  14092.0     97.8              return self._clip_with_scalar(lower, upper, inplace=inplace)\n  5046                                           \n  5047                                                   result = self\n  5048                                                   if lower is not None:\n  5049                                                       result = result.clip_lower(lower, axis, inplace=inplace)\n  5050                                                   if upper is not None:\n  5051                                                       if inplace:\n  5052                                                           result = self\n  5053                                                       result = result.clip_upper(upper, axis, inplace=inplace)\n  5054                                           \n  5055                                                   return result\n\nTotal time: 0.00575753 s\nFile: pandas\\core\\generic.py\nFunction: _clip_with_scalar at line 4920\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  4920                                               def _clip_with_scalar(self, lower, upper, inplace=False):\n  4921         1            2      2.0      0.0          if ((lower is not None and np.any(isna(lower))) or\n  4922         1           28     28.0      0.2                  (upper is not None and np.any(isna(upper)))):\n  4923                                                       raise ValueError(\"Cannot use an NA value as a clip threshold\")\n  4924                                           \n  4925         1          120    120.0      0.9          result = self.values\n  4926         1         3525   3525.0     25.1          mask = isna(result)\n  4927                                           \n  4928         1           86     86.0      0.6          with np.errstate(all='ignore'):\n  4929         1            2      2.0      0.0              if upper is not None:\n  4930         1         9314   9314.0     66.4                  result = np.where(result >= upper, upper, result)\n  4931         1           61     61.0      0.4              if lower is not None:\n  4932                                                           result = np.where(result <= lower, lower, result)\n  4933         1          283    283.0      2.0          if np.any(mask):\n  4934                                                       result[mask] = np.nan\n  4935                                           \n  4936         1           78     78.0      0.6          axes_dict = self._construct_axes_dict()\n  4937         1          532    532.0      3.8          result = self._constructor(result, **axes_dict).__finalize__(self)\n  4938                                           \n  4939         1            2      2.0      0.0          if inplace:\n  4940                                                       self._update_inplace(result)\n  4941                                                   else:\n  4942         1            1      1.0      0.0              return result\n\nText: There are still multiple function calls, for example isna and np.where, that take a significant amount of time, but overall this is at least comparable to the np.ndarray.clip time (that's in the regime where the timing difference is ~3 on my computer). \nText: The takeaway should probably be: \nText: Many NumPy functions just delegate to a method of the object passed in, so there can be huge differences when you pass in different objects. Profiling, especially line-profiling, can be a great tool to find the places where the performance difference comes from. Always make sure to test differently sized objects in such cases. You could be comparing constant factors that probably don't matter except if you process lots of small arrays. \nText: Used versions: \nCode: Python 3.6.3 64-bit on Windows 10\nNumpy 1.13.3\nPandas 0.21.1\n\nAPI:\npandas.Series\npandas.Series.clip\n","label":[[70,76,"Mention"],[13356,13360,"Mention"],[22182,22195,"API"],[22196,22214,"API"]],"Comments":[]}
{"id":60117,"text":"ID:48021260\nPost:\nText: Your data is fine, and pd.eval is buggy, but not in the way you think. There is a hint in the relevant github issue page that urged me to take a closer look at the documentation. \nCode: pandas.eval(expr, parser='pandas', engine=None, truediv=True, local_dict=None,\n            global_dict=None, resolvers=(), level=0, target=None, inplace=False)\n\n    Evaluate a Python expression as a string using various backends.\n\n    Parameters:\n        expr: str or unicode\n            The expression to evaluate. This string cannot contain any Python\n            statements, only Python expressions.\n        [...]\n\nText: As you can see, the documented behaviour is to pass strings to pd.eval, in line with the general (and expected) behaviour of the eval\/exec class of functions. You pass a string, and end up with an arbitrary object. \nText: As I see it, pd.eval is buggy because it doesn't reject the Series input expr up front, leading it to guess in the face of ambiguity. The fact that the default shortening of the Series' __repr__ designed for pretty printing can drastically affect your result is the best proof of this situation. \nText: The solution is then to step back from the XY problem, and use the right tool to convert your data, and preferably stop using evazl for this purpose entirely. Even in the working cases where the Series is small, you can't really be sure that future pandas versions don't break this \"feature\" completely. \nAPI:\npandas.eval\npandas.eval\npandas.eval\n","label":[[47,54,"Mention"],[869,876,"Mention"],[1285,1290,"Mention"],[1469,1480,"API"],[1481,1492,"API"],[1493,1504,"API"]],"Comments":[]}
{"id":60118,"text":"ID:48029072\nPost:\nText: First, group by [\"Start_Date\", \"End_date\"] to save some operations. \nCode: from collections import Counter\nc = Counter()\ndf_g = df.groupby([\"Start_Date\", \"End_date\"]).sum().reset_index()\n\ndef my_counter(row):\n    s, v, e = row.Start_Date, row.Value_to_sum, row.End_date\n    if s == e:\n        c[pd.Timestamp(s, freq=\"D\")] += row.Value_to_sum\n    else:\n         c.update({date: v for date in pd.date_range(s, e)})\n\ndf_g.apply(my_counter, axis=1) \nprint(c)\n\"\"\"\nCounter({Timestamp('2017-12-15 00:00:00', freq='D'): 9,\n     Timestamp('2017-12-14 00:00:00', freq='D'): 7,\n     Timestamp('2017-12-13 00:00:00', freq='D'): 5,\n     Timestamp('2017-12-16 00:00:00', freq='D'): 3})\n\"\"\"\n\nText: Tools used: \nText: Counter.update([iterable-or-mapping]): Elements are counted from an iterable or added-in from another mapping (or counter). Like dict.update() but adds counts instead of replacing them. Also, the iterable is expected to be a sequence of elements, not a sequence of (key, value) pairs. -- Cited from Python 3 Documentation \nText: pd.date_range \nAPI:\npandas.date_range\n","label":[[1055,1068,"Mention"],[1075,1092,"API"]],"Comments":[]}
{"id":60119,"text":"ID:48043339\nPost:\nText: Thanks @Brad Solomon for offering a faster way to capitalize string! \nText: Note 1 @Brad Solomon's answer using pd.categorical should save your resources more than my answer. He showed how to assign order to your categorical data. You should not miss it :P \nText: Alternatively, you can use. \nCode: df = pd.DataFrame([[\"dec\", 12], [\"jan\", 40], [\"mar\", 11], [\"aug\", 21],\n                  [\"aug\", 11], [\"jan\", 11], [\"jan\", 1]], \n                   columns=[\"Month\", \"Price\"])\n# Preprocessing: capitalize `jan`, `dec` to `Jan` and `Dec`\ndf[\"Month\"] = df[\"Month\"].str.capitalize()\n\n# Now the dataset should look like\n#   Month Price\n#   -----------\n#    Dec    XX\n#    Jan    XX\n#    Apr    XX\n\n# make it a datetime so that we can sort it: \n# use %b because the data use the abbreviation of month\ndf[\"Month\"] = pd.to_datetime(df.Month, format='%b', errors='coerce').dt.month\ndf = df.sort_values(by=\"Month\")\n\ntotal = (df.groupby(df['Month'])['Price'].mean())\n\n# total \nMonth\n1     17.333333\n3     11.000000\n8     16.000000\n12    12.000000\n\nText: Note 2 groupby by default will sort group keys for you. Be aware to use the same key to sort and groupby in the df = df.sort_values(by=SAME_KEY) and total = (df.groupby(df[SAME_KEY])['Price'].mean()). Otherwise, one may gets unintended behavior. See Groupby preserve order among groups? In which way? for more information. \nText: Note 3 A more computationally efficient way is first compute mean and then do sorting on months. In this way, you only need to sort on 12 items rather than the whole df. It will reduce the computational cost if one don't need df to be sorted. \nText: Note 4 For people already have month as index, and wonder how to make it categorical, take a look at CategoricalIndex @jezrael has a working example on making categorical index ordered in Pandas series sort by month index \nAPI:\npandas.CategoricalIndex\n","label":[[1747,1763,"Mention"],[1874,1897,"API"]],"Comments":[]}
{"id":60120,"text":"ID:48044606\nPost:\nText: groupby has a sort argument that defaults to True. Try \nCode: total = (df.groupby(df['Month'], sort=False)['Price'].mean())\n\nAPI:\npandas.DataFrame.groupby\n","label":[[24,31,"Mention"],[154,178,"API"]],"Comments":[]}
{"id":60121,"text":"ID:48054452\nPost:\nText: Pandas was made for time-series data so this is it's bread and butter. Try this for performance: \nCode: dt = '2017-12-23 01:49:13'\ndf[\"timedelta\"] = abs(df.index - pd.Timestamp(dt))\ndf.loc[df.groupby(by=\"category\")[\"timedelta\"].idxmin()].drop(\"timedelta\", axis=1)\n\nText: This is creating a new column called timedelta, named after Timedelta class, and then using groupby to combine all the categories, find the smallest timedelta in each and return their index into .loc. Lastly I dropped the column. \nAPI:\npandas.Timedelta\n","label":[[355,364,"Mention"],[531,547,"API"]],"Comments":[]}
{"id":60122,"text":"ID:48081763\nPost:\nText: The reason \nCode: query_str = \"Set @null_val = \\'\\'; \"\\\n    \" select @null_val\"\nerpur_df = pd.read_sql(query_str, con = db)\n\nText: throws that exception is because all you are doing is setting null_value to '' and then selecting that '' - what exactly would you have expected that to give you? EDIT read_sql only seems to execute one query at a time, and as the first query returns no rows it results in that exception. If you split them in to two calls to read_sql then it will in fact return you the value of your @null value in the second call. Due to this behaviour read_sql is clearly not a good way to do this. I strongly suggest you use one of my suggestions below. \nText: Why are you wanting to set the variable in the SQL using '@' anyway? \nText: You could try using the .format style of string formatting. \nText: Like so: \nCode: query_str = \"select ... coalesce(field1, {c}), case(.. then {c})...\".format(c=my_const)\npd.read_sql(query_str)\n\nText: Just remember that if you do it this way and your my_const is a user input then you will need to sanitize it manually to prevent SQL injection. \nText: Another possibility is using a dict of params like so: \nCode: query_str = \"select ... coalesce(field1, %(my_const)s, case(.. then %(my_const)s)...\"\npd.read_sql(query_str, params={'my_const': const_value})\n\nText: However this is dependent on which database driver you use. \nText: From the pd.read_sql docs: \nText: Check your database driver documentation for which of the five syntax styles, described in PEP 249s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={name : value} \nAPI:\npandas.read_sql\n","label":[[1420,1431,"Mention"],[1643,1658,"API"]],"Comments":[]}
{"id":60123,"text":"ID:48122203\nPost:\nText: As docs for pandas.DataFrame.scatter say, we can pass optional keyword arguments to pd.DataFrame.plot which in turn will pass some of them to a corresponding matplotlib plotting method, matplotlib.axes.Axes.scatter in your case. \nText: So, for example, to eliminate the edges from the markers, and to change them to hexagons, you can write: \nCode: df.plot.scatter(x='Supply', \n                y='Demand', \n                alpha=0.2, \n                figsize=(6,6), \n                s=100, \n                linewidths=0.0,\n                marker='h')\n\nAPI:\npandas.DataFrame.plot\n","label":[[108,125,"Mention"],[580,601,"API"]],"Comments":[]}
{"id":60124,"text":"ID:48281270\nPost:\nText: Tested on Python 3.5 and Pandas 0.22 - using slightly different code (pd.melt() instead of df.melt). \nText: Firstly - the reason for the difference. \nText: df1 has values for two different timestamps (01:20:00 and 01:05:00), where df2 both observations that remain are on the same timestamp (01:05:00). \nText: I'm not exactly sure why, but evidently the act of reversing the sort means does mean that the .values call is including the grouper time window on df1 because you're sorting values as descending within a ascending timeindex. The result is different for df2 because the timestamps are identical and so the grouper isn't required. You possibly already deduced that, so apologies for duplication if so. \nText: Suggestion to consistently show your expected result \nText: Normally when working with groupby I would use an aggregation call to manipulate the data (max in the example below but sum, count and others are available). If you're looking to see your values with the reference time of the grouper, this would be the way to approach it: \nText: Here's df1b, with multiple timestamps against the observation values: \nCode: In []: df1b.groupby([pd.Grouper(freq=\"1h\"), df1b.index, \"variable\"])\\\n       .max().sort_values(\"value\", ascending=False)\nOut[]:\n                                                  value\ndtime               dtime               variable\n2017-01-01 01:00:00 2017-01-01 01:20:00 val2       31.0\n                    2017-01-01 01:05:00 val1       11.0    \n\nText: ...and same approach for df2b: \nCode: In []: df2b.groupby([pd.Grouper(freq=\"1h\"), df2b.index, \"variable\"])\\\n       .max().sort_values(\"value\", ascending=False)\nOut[]:\n                                                  value\ndtime               dtime               variable\n2017-01-01 01:00:00 2017-01-01 01:05:00 val2       31.0\n                                        val1       11.0\n\nText: Does this help? Or have I missed the significance of why you were working with the .values method? \nText: Disclaimer: I'm not familiar with working with groupby.value so you might be trying to achieve something I've missed. \nAPI:\npandas.DataFrame.groupby\n","label":[[829,836,"Mention"],[2137,2161,"API"]],"Comments":[]}
{"id":60125,"text":"ID:48426665\nPost:\nText: Since my question was up-voted, I guess, it is still interesting to some people. Having learned quite a bit in Python so far, let me answer it, maybe it is going to be helpful to other users. \nText: First, let us import the required packages \nCode: import pandas as pd\nfrom dfply import *\nfrom os.path import basename, dirname, join\n\nText: and make the required pandas DataFrame \nCode: resultstatsDF = pd.DataFrame({'file': ['\/home\/user\/this\/file1.png', '\/home\/user\/that\/file2.png']})\n\nText: which is \nCode:                         file\n0  \/home\/user\/this\/file1.png\n1  \/home\/user\/that\/file2.png\n\nText: We see that we still get an error (though it changed due to continuous development of dfply): \nCode: resultstatsDF.reset_index() >> \\\nmutate(dirfile = join(basename(dirname(X.file)), basename(X.file)))\n\nText: TypeError: index returned non-int (type Intention) \nText: The reason is, because mutate works on series, but we need a function working on elements. Here we can use the function pd.Series.apply of pandas, which works on series. However, we also need a custom function that we can apply on each element of the series file. Everything put together we end up with the code \nCode: def extract_last_dir_plus_filename(series_element):\n    return join(basename(dirname(series_element)), basename(series_element))\n\nresultstatsDF.reset_index() >> \\\nmutate(dirfile = X.file.apply(extract_last_dir_plus_filename))\n\nText: which outputs \nCode:    index                       file         dirfile\n0      0  \/home\/user\/this\/file1.png  this\/file1.png\n1      1  \/home\/user\/that\/file2.png  that\/file2.png\n\nText: Doing this without dfply's mutate, we could write alternatively \nCode: resultstatsDF['dirfile'] = resultstatsDF.file.apply(extract_last_dir_plus_filename)\n\nAPI:\npandas.Series.apply\n","label":[[1013,1028,"Mention"],[1790,1809,"API"]],"Comments":[]}
{"id":60126,"text":"ID:48466167\nPost:\nText: pd.crosstab has a dropna argument, which by default is set to True, but in your case you can pass False: \nCode: pd.crosstab(df['a'], df['c'], dropna=False)\n# c       do  mi  re\n# a                 \n# first    2   2   0\n# second   0   0   1\n# third    0   0   1\n\nAPI:\npandas.crosstab\n","label":[[24,35,"Mention"],[291,306,"API"]],"Comments":[]}
{"id":60127,"text":"ID:48475758\nPost:\nText: Try concat function: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/merging.html \nCode: dd = pd.concat([df1, df2], axis=1)\nprint(dd)\n\nText: Output: \nCode:           a         b\n0 -0.603074       NaN\n1       NaN -0.021821\n2  0.501050  0.342474\n3 -2.612637 -0.256383\n4  0.095779 -1.423016\n5 -0.644108       NaN\n6       NaN -1.756023\n\nAPI:\npandas.concat\n","label":[[28,34,"Mention"],[359,372,"API"]],"Comments":[]}
{"id":60128,"text":"ID:48694941\nPost:\nText: Looking at the screenshot of your inputted excel file along with the printed dataframe, the problem you're encountering is likely due to the Merged Cells you have in the second and third rows. \nText: I recommend making use of some of the parameters for pd.DataFrame.to_excel that are outlined in docs (Link Here). In particular, header and skiprows should help you. \nText: I've provided an example below, in which I create an excel file (.xlsx) that replicates the issue you have with the merged cells. Then I copy the .xlsx to be a .xls and read it using to_excel with header and skiprows spelled out. \nCode: import pandas as pd\nimport numpy as np\nimport shutil\n\n# Creating a dataframe and saving as test.xlsx in current directory\ndf = pd.DataFrame(np.random.randn(10, 3), columns=list('ABC'))\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\ndf.to_excel(writer, sheet_name='Sheet1', startrow=3, index=False, \nheader=False)\nwb  = writer.book\nws = writer.sheets['Sheet1']\n\nws.merge_range('A1:C1', 'Large Merged Cell in first Row')\nws.merge_range('A2:A3', 'A')\nws.merge_range('B2:B3', 'B')\nws.merge_range('C2:C3', 'C')\n\nwb.close()\n\nprint(df)\n#copying test.xlsx as a .xls file\nshutil.copy(r\"test.xlsx\" , r\"test.xls\")\n\nnew_df = pd.read_excel('test.xls', header = 0, skiprows = [0,2])\nprint(new_df)\n\nText: Expected test.xls file: \nText: print(new_df) should show: \nCode:           A         B         C\n0  1.242498  0.512675 -1.370710\n1  0.060366 -0.467702 -1.420735\n2 -0.198547  0.042364  0.915423\n3  0.340909  0.749019  0.272871\n4  2.633348 -1.343251 -0.248733\n5  0.892257  0.371924  0.023415\n6 -0.809030 -0.633796  0.449373\n7  0.322960  2.073352  1.362657\n8 -0.848093  1.848489  0.813144\n9  2.718069 -0.540174  1.411980 \n\nAPI:\npandas.DataFrame.to_excel\npandas.DataFrame.to_excel\n","label":[[277,298,"Mention"],[580,588,"Mention"],[1759,1784,"API"],[1785,1810,"API"]],"Comments":[]}
{"id":60129,"text":"ID:48775569\nPost:\nText: Try using to_datetime with Python time directives where '%Y' for year, 'm' hard code for the letter m, and '%m' for month: \nCode: pd.to_datetime(df.mydate, format='%Ym%m')\n\nText: Output: \nCode: 0   1985-03-01\n1   1985-04-01\n2   1985-05-01\nName: mydate, dtype: datetime64[ns]\n\nAPI:\npandas.to_datetime\n","label":[[34,45,"Mention"],[305,323,"API"]],"Comments":[]}
{"id":60130,"text":"ID:48801966\nPost:\nText: You can try appending to an empty dataframe using appedn \nCode: df = pd.DataFrame()\ndf = df.append(movie_review_dict, ignore_index=False)\n\nAPI:\npandas.DataFrame.append\n","label":[[74,80,"Mention"],[168,191,"API"]],"Comments":[]}
{"id":60131,"text":"ID:48841443\nPost:\nText: The documentation for rea_csv states: \nText: date_parser : function, default None Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments. \nText: When the array (first column in your case) is passed as an argument, your parser function will be applied to each value in the column. \nText: In other words, only one argument will ever be passed to your function. But your function requires 2 arguments (x, y). \nText: You need to figure out exactly what logic you want to apply to the strings in your column and apply it in the form f(x). \nText: Given the data you provided, this should suffice: \nCode: from datetime import datetime\n\ndef parser(x):\n    return datetime.strptime(x, '%Y-%m-%d')\n\ns = pd.Series(['2002-01-15', '2002-02-15', '2002-03-15'])\ns.apply(parser)\n\n# 0   2002-01-15\n# 1   2002-02-15\n# 2   2002-03-15\n# dtype: datetime64[ns]\n\nAPI:\npandas.read_csv\n","label":[[46,53,"Mention"],[1401,1416,"API"]],"Comments":[]}
{"id":60132,"text":"ID:48871110\nPost:\nText: The answer strongly depends on the String in your tuple. If what you copied is actually whats in the string, you have to convert the string to something pandas can parse, that's why I added the regex substitution. \nCode: import pandas as pd\nimport io\nimport re\na = (('12','22','32'),\n     \"\"\"Column-1    Column-2    Column-3    colum-4 Column-5    Colum-6 Colum-7 Week    ACCT_YEAR   NAME\n12  22  32              51  2016    Name-1\n12  22  32              51  2016    Name-2\n12  22  32              51  2016    Name-3\"\"\")\n# The following substitution is only valid if there are absolutely no spaces in values\nb = re.sub(string=a[1], pattern=' +', repl=',')\ny = pd.read_csv(io.StringIO(b))\ny\n\nText: NB: this answer assumes the first value in tuple a is not part of the data that should be read into the DataFrame. This makes it more into the question how to read data saved in a string into a DataFrame rather than a tuple. \nAPI:\npandas.DataFrame\n","label":[[928,937,"Mention"],[965,981,"API"]],"Comments":[]}
{"id":60133,"text":"ID:48871258\nPost:\nText: The first optimization to this type of code is to replace looping with native numpy \/ pandas functions and using apply \nText: using the definition for actual cover as \nText: the measure (in days) of how much the current stock position would last \nText: one can equivalently say, that the actual cover is \nCode: the first day such that the cumulative sum of sales for all following days exceeds \nthe stock position on a given day\n\nText: using this definition of actual cover the following function returns the actual_cover given a row number \nCode: def actual_cover(rownum, frame):\n    mask = frame.SALES[rownum+1:].cumsum() > frame.STOCK_POS[rownum]\n    not_covered = np.where(mask.values)[0]\n    return np.nan if not_covered.size == 0 else not_covered[0]+1\n\nText: Then, you can apply it to the dataframe and assign the values to a new column \nCode: df['ACTUAL_COVER(days)'] = df.apply(lambda x: actual_cover(x.name, df), axis=1)\n\nText: notes: \nText: i used the name df instead of DF, so you have to change that when you try this code on your dataset \nText: the function uses row index values to determine the number of days. So, for the function to work correctly, there must be a row for each day, even if no sales occur on that day, and the rows must be ordered by the timestamp \nText: the function applied to the dataframe snippet above will return np.nan for the rows where the cumulative sum never exceeds the stock position, i.e. it outputs the following: \nCode: df.apply(lambda x: actual_cover(x.name, df), axis=1)\n# output\n0     9.0\n1     8.0\n2     7.0\n3     9.0\n4     8.0\n5     7.0\n6     6.0\n7     5.0\n8     NaN\n9     NaN\n10    NaN\n11    NaN\n12    NaN\n13    NaN\n\nText: this differs from the sample output you provide because you have truncated rows from the whole dataset in the example \nText: The actual_cover function may be applied in a grouped dataframe, but requires further massaging \nCode: def actual_cover_grouped(grp):\n    return grp.apply(lambda x: actual_cover(x.name, grp), axis=1)\n\ngrouped = df.groupby('MATERIAL_GOODS')\n\ndf['Actual Cover'] = grouped.apply(actual_cover_grouped).values.flatten()\n\nAPI:\npandas.DataFrame.apply\n","label":[[137,142,"Mention"],[2149,2171,"API"]],"Comments":[]}
{"id":60134,"text":"ID:48872376\nPost:\nText: write a function to get the closest index & timestamp in df_short given a timestamp \nCode: def get_closest(n):\n    mask = df_short.mytime_short >= n\n    ids = np.where(mask)[0]\n    if ids.size > 0:\n        return ids[0], df_short.mytime_short[ids[0]]\n    else:\n        return np.nan, np.nan\n\nText: apply this function over df_long.mytime_long, to get a new data frame with the index & timestamp values in a tuple \nCode: df = df_long.mytime_long.apply(get_closest)\ndf\n# output:\n0    (0, 2013-01-10 00:00:02)\n1    (2, 2013-01-10 00:00:06)\n2                  (nan, nan)\n\nText: ilia timofeev's answer reminded me of this merge_asof function which is perfect for this type of join \nCode: df = pd.merge_asof(df_long, \n              df_short.reset_index(), \n              left_on='mytime_long', \n              right_on='mytime_short', \n              direction='forward')[['index', 'mytime_short']]\ndf\n# output:\n   index        mytime_short\n0    0.0 2013-01-10 00:00:02\n1    2.0 2013-01-10 00:00:06\n2    NaN                 NaT\n\nAPI:\npandas.merge_asof\n","label":[[641,651,"Mention"],[1050,1067,"API"]],"Comments":[]}
{"id":60135,"text":"ID:48897292\nPost:\nText: In these situation I like to feed concat a list comprehension. \nCode: from pathlib import Path\nimport pandas\n\ndef _reader(fname):\n    return pandas.read_csv(fname, sep=';', header=None, encoding='latin_1')\n\nfolder = Path(\"candidatos_2014\")\ndf = pandas.concat([\n    _reader(txt)\n    for txt in folder.glob(\"*.txt\")\n])\n\nAPI:\npandas.concat\n","label":[[58,64,"Mention"],[347,360,"API"]],"Comments":[]}
{"id":60136,"text":"ID:48944107\nPost:\nText: Try this: \nCode: import pandas as pd\n\ndef parse_nested_json(json_d):\n    result = {}\n    for key in json_d.keys():\n        if not isinstance(json_d[key], dict):\n            result[key] = json_d[key]\n        else:\n            result.update(parse_nested_json(json_d[key]))\n    return result\n\njson_data = pd.read_json(\"my_json_file.json\")\njson_list = [j[1][0] for j in json_data.iterrows()]\nparsed_list = [parse_nested_json(j) for j in json_list]\nresult = pd.DataFrame(parsed_list)\nresult.to_csv(\"my_csv_file.csv\", index=False)\n\nText: Update(12\/3\/2018): \nText: I read the docs, there is a convenient way: \nCode: from pandas.io.json import json_normalize\ndf = json_normalize(data[\"profile_set\"])\ndf.to_csv(...)\n\nText: Update(11\/7\/2021): \nText: pandas.io.json.json_normalize is deprecated, use pd.json_normalize instead \nCode: from pandas import json_normalize\ndf = json_normalize(data[\"profile_set\"])\ndf.to_csv(...)\n\nAPI:\npandas.json_normalize\n","label":[[813,830,"Mention"],[942,963,"API"]],"Comments":[]}
{"id":60137,"text":"ID:48949152\nPost:\nText: Let's take this example dataframe, whose index is at minute granularity \nCode: import pandas as pd\nimport random\nts_index = pd.date_range('1\/1\/2000', periods=1000, freq='T')\nv1 = [random.random() for i in range(1000)]\nv2 = [random.random() for i in range(1000)]\nv3 = [random.random() for i in range(1000)]\nts_df = pd.DataFrame({'v1':v1,'v2':v2,'v3':v3},index=ts_index)\nts_df.head()\n\n\n\n                         v1           v2        v3\n2000-01-01 00:00:00     0.593039    0.017351    0.742111\n2000-01-01 00:01:00     0.563233    0.837362    0.869767\n2000-01-01 00:02:00     0.453925    0.962600    0.690868\n2000-01-01 00:03:00     0.757895    0.123610    0.622777\n2000-01-01 00:04:00     0.759841    0.906674    0.263902\n\nText: We could use pd.DataFrame.resample to downsample this data to hourly granularity, like shown below \nCode: hourly_mean_df = ts_df.resample('H').mean() # you can use .sum() also\nhourly_mean_df.head()\n\n                          v1          v2         v3\n2000-01-01 00:00:00     0.516001    0.461119    0.467895\n2000-01-01 01:00:00     0.530603    0.458208    0.550892\n2000-01-01 02:00:00     0.472090    0.522278    0.508345\n2000-01-01 03:00:00     0.515713    0.486906    0.541538\n2000-01-01 04:00:00     0.514543    0.478097    0.489217\n\nText: Now you can plot this hourly summary \nCode: hourly_mean_df.plot()\n\nAPI:\npandas.DataFrame.resample\n","label":[[765,786,"Mention"],[1367,1392,"API"]],"Comments":[]}
{"id":60138,"text":"ID:48956650\nPost:\nText: You can use pd.DataFrame.eq to find the location of all the zero values across the entire DataFrame, and then replace this with something else, maybe np.nan. \nCode: import numpy as np\ndf[df.eq(0)] = np.nan\n\nText: Though if you have only one row, you can also replace it with an empty string since you (probably) don't have to worry about the dtype for the column changing: \nCode: df[df.eq(0)] = ''\n\nAPI:\npandas.DataFrame.eq\n","label":[[36,51,"Mention"],[428,447,"API"]],"Comments":[]}
{"id":60139,"text":"ID:48969457\nPost:\nText: Consider parameterizing your query using the params argument of repd_sql and pass the accented character '' with u'' prefix to bind value to the unquoted ? placeholder in SQL query. Do note: params requires a sequence and so below passes a tuple of one item. \nText: Unlike Python 2.x, all strings in Python 3.x are Unicode strings and so accented literals (non-ascii) do not need explicit decoding with u'...'. Hence, why I cannot reproduce your issue in my Python 3.5 running a DB2 SQL query with accented characters. \nCode: import pandas as pd\n...\n\n# Function for creating pandas dataframes from SQL-statements\ndef sqlToFrame(sql): \n    db = ibm_db.connect(connection_string, \"\", \"\")\n    con = ibm_db_dbi.Connection(db)\n\n    return pd.read_sql(sql, con, params = (u'',))\n\n\ndf = sqlToFrame(\"SELECT * FROM DATA_CONFIG WHERE TAG_NAME = ?\")\n\nAPI:\npandas.read_sql\n","label":[[88,96,"Mention"],[871,886,"API"]],"Comments":[]}
{"id":60140,"text":"ID:49027992\nPost:\nText: pd.DataFrame.to_json returns a string (JSON string), not a dictionary. Try to_dict instead: \nCode: >>> df\n   col1  col2\n0     1     3\n1     2     4\n>>> [df.to_dict(orient='index')]\n[{0: {'col1': 1, 'col2': 3}, 1: {'col1': 2, 'col2': 4}}]\n>>> df.to_dict(orient='records')\n[{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}]\n\nAPI:\npandas.DataFrame.to_json\n","label":[[24,44,"Mention"],[350,374,"API"]],"Comments":[]}
{"id":60141,"text":"ID:49073208\nPost:\nText: When using read_csv in threads it appears that the Python process leaks a little memory. I've reduced it to a problem with read_csv and a concurrent.futures.ThreadPoolExecutor. This is raised on the Pandas issue tracker here: https:\/\/github.com\/pandas-dev\/pandas\/issues\/19941 \nCode: # imports\nimport pandas as pd\nimport numpy as np\nimport time\nimport psutil\nfrom concurrent.futures import ThreadPoolExecutor\n\n# prep\nprocess = psutil.Process()\ne = ThreadPoolExecutor(8)\n\n# prepare csv file, only need to run once\npd.DataFrame(np.random.random((100000, 50))).to_csv('large_random.csv')\n\n\n# baseline computation making pandas dataframes with threasds.  This works fine\n\ndef f(_):\n    return pd.DataFrame(np.random.random((1000000, 50)))\n\nprint('before:', process.memory_info().rss \/\/ 1e6, 'MB')\nlist(e.map(f, range(8)))\ntime.sleep(1)  # let things settle\nprint('after:', process.memory_info().rss \/\/ 1e6, 'MB')\n\n# before: 57.0 MB\n# after: 56.0 MB\n\n# example with read_csv, this leaks memory\nprint('before:', process.memory_info().rss \/\/ 1e6, 'MB')\nlist(e.map(pd.read_csv, ['large_random.csv'] * 8))\ntime.sleep(1)  # let things settle\nprint('after:', process.memory_info().rss \/\/ 1e6, 'MB')\n\n# before: 58.0 MB\n# after: 323.0 MB\n\nAPI:\npandas.read_csv\npandas.read_csv\n","label":[[35,43,"Mention"],[147,155,"Mention"],[1254,1269,"API"],[1270,1285,"API"]],"Comments":[]}
{"id":60142,"text":"ID:49089007\nPost:\nText: Have a look at rolling \nCode: df.rolling(window=3).apply(min).dropna()\n\nText: will give the expected result: \nCode: datetime    price\n2017-10-02  08:05:00    12877.0\n2017-10-02  08:06:00    12875.5\n2017-10-02  08:07:00    12875.5\n2017-10-02  08:08:00    12875.5\n2017-10-02  08:09:00    12875.5\n2017-10-02  08:10:00    12878.0\n2017-10-02  08:11:00    12878.0\n2017-10-02  08:12:00    12878.0\n2017-10-02  08:13:00    12881.0\n2017-10-02  08:14:00    12882.0\n2017-10-02  08:15:00    12880.5\n2017-10-02  08:16:00    12880.5\n2017-10-02  08:17:00    12879.0\n2017-10-02  08:18:00    12879.0\n2017-10-02  08:19:00    12879.0\n2017-10-02  08:20:00    12878.5\n\nAPI:\npandas.DataFrame.rolling\n","label":[[39,46,"Mention"],[676,700,"API"]],"Comments":[]}
{"id":60143,"text":"ID:49118741\nPost:\nText: The boxplot documentaton says about the whiskers \nText: whis : float, sequence, or string (default = 1.5) As a float, determines the reach of the whiskers to the beyond the first and third quartiles. In other words, where IQR is the interquartile range (Q3-Q1), the upper whisker will extend to last datum less than Q3 + whisIQR). Similarly, the lower whisker will extend to the first datum greater than Q1 - whisIQR. Beyond the whiskers, data are considered outliers and are plotted as individual points. Set this to an unreasonably high value to force the whiskers to show the min and max values. Alternatively, set this to an ascending sequence of percentile (e.g., [5, 95]) to set the whiskers at specific percentiles of the data. Finally, whis can be the string 'range' to force the whiskers to the min and max of the data. \nText: The only definition from the list from the question which cannot be easily implemented is the \"one standard deviation\", all others are readily set with this argument. The default is the 1.5IQR definition. \nText: The pd.DataFrame.boxplot calls the matplotlib function. Hence they should be identical. \nAPI:\npandas.DataFrame.boxplot\n","label":[[1076,1096,"Mention"],[1166,1190,"API"]],"Comments":[]}
{"id":60144,"text":"ID:49153755\nPost:\nText: You are right, astype(int) does a conversion toward zero: \nText: integer or signed: smallest signed int dtype \nText: from to_numeric documentation (which is linked from astype() for numeric conversions). \nText: If you want to round, you need to do a float round, and then convert to int: \nCode: df.round(0).astype(int)\n\nText: Use other rounding functions, according your needs. \nText: the output is always a bit random as the 'real' value of an integer can be slightly above or below the wanted value \nText: Floats are able to represent whole numbers, making a conversion after round(0) lossless and non-risky, check here for details. \nAPI:\npandas.to_numeric\n","label":[[150,160,"Mention"],[669,686,"API"]],"Comments":[]}
{"id":60145,"text":"ID:49242491\nPost:\nText: Consider the following adjustments: \nText: Move your figure build into the selectXY method so button can run it on click. Pass axes by name into plot which takes x and y as string values. Hence, xList and yList with .loc calls are not needed and values can derive directly out of varX and varY drop downs. Finally, move canvas to the __init__ method to be called once and not create a new canvas each time. \nText: Below is adjustment of Home class: \nCode: class Home(tk.Frame):\n\n    def __init__(self, parent, controller):\n        self.controller = controller\n        tk.Frame.__init__(self,parent)\n        label = tk.Label(self, text=\"Start Page\", font=LARGE_FONT)\n        label.pack(pady=10, padx=10)\n        canvas = FigureCanvasTkAgg(fig, self)\n\n        def selectXY():\n            x = varX.get()            \n            y = varY.get()\n\n            X.config(text=x)\n            Y.config(text=y)\n\n            ax.clear()\n            df.plot.scatter(x, y, ax=ax)\n\n            canvas.draw()\n            canvas.get_tk_widget().pack(side=tk.BOTTOM, fill=tk.BOTH, expand=True)\n\n        # Y axis select\n        varY = tk.StringVar(self)\n        # initial value\n        varY.set('Select Y axis')\n\n        optionY = tk.OptionMenu(self, varY, *Options)\n        optionY.pack()\n\n        # X axis select\n        varX = tk.StringVar(self)\n        # initial value\n        varX.set('Select X axis')\n\n        optionX = tk.OptionMenu(self, varX, *Options)\n        optionX.pack()\n\n        button2 = tk.Button(self, text=\"Plot Axes\", command = selectXY)\n        button2.pack()\n\n        button2 = ttk.Button(self, text=\"Graph\",\n                             command=lambda: controller.show_frame(Graph))\n        button2.pack()\n\n        Y = ttk.Label(self, text=yList)\n        Y.pack()\n\n        X = ttk.Label(self, text=xList)\n        X.pack()\n\nText: For Graph class to replicate same plot as Home requires some extensive adjustment with global variables, visiblity trigger event in show_frame, and updated attributes in Graph. \nAPI:\npandas.DataFrame.plot\n","label":[[169,173,"Mention"],[2038,2059,"API"]],"Comments":[]}
{"id":60146,"text":"ID:49291168\nPost:\nText: The pandas.panel object is deprecated. We use MultiIndex to handle higher dimensional data. \nText: Consider the data frame df \nCode: df = pd.DataFrame(1, list('abc'), list('xyz'))\ndf\n\n   x  y  z\na  1  1  1\nb  1  1  1\nc  1  1  1\n\nText: Add Level \nText: The following are various ways to add a level and dimensionality. \nText: axis=0, level=0 \nCode: pd.concat([df], keys=['A'])\n\n     x  y  z\nA a  1  1  1\n  b  1  1  1\n  c  1  1  1\n\nCode: df.set_index(pd.MultiIndex.from_product([['B'], df.index]))\n\n     x  y  z\nB a  1  1  1\n  b  1  1  1\n  c  1  1  1\n\nText: axis=0, level=1 \nCode: pd.concat([df], keys=['A']).swaplevel(0, 1)\n\n     x  y  z\na A  1  1  1\nb A  1  1  1\nc A  1  1  1\n\nCode: df.set_index(pd.MultiIndex.from_product([df.index, ['B']]))\n\n     x  y  z\na B  1  1  1\nb B  1  1  1\nc B  1  1  1\n\nText: axis=1, level=0 \nCode: pd.concat([df], axis=1, keys=['A'])\n\n   A      \n   x  y  z\na  1  1  1\nb  1  1  1\nc  1  1  1\n\nCode: df.set_axis(pd.MultiIndex.from_product([['B'], df.columns]), axis=1, inplace=False)\n\n   B      \n   x  y  z\na  1  1  1\nb  1  1  1\nc  1  1  1\n\nText: axis=1, level=1 \nCode: pd.concat([df], axis=1, keys=['A']).swaplevel(0, 1, 1)\n\n   x  y  z\n   A  A  A\na  1  1  1\nb  1  1  1\nc  1  1  1\n\nCode: df.set_axis(pd.MultiIndex.from_product([df.columns, ['B']]), axis=1, inplace=False)\n\n   x  y  z\n   B  B  B\na  1  1  1\nb  1  1  1\nc  1  1  1\n\nAPI:\npandas.MultiIndex\n","label":[[70,80,"Mention"],[1383,1400,"API"]],"Comments":[]}
{"id":60147,"text":"ID:49368593\nPost:\nText: You can try something like this: \nCode: df = df.assign(b = False)\n\nText: You can see more details on assign \nAPI:\npandas.DataFrame.assign\n","label":[[125,131,"Mention"],[138,161,"API"]],"Comments":[]}
{"id":60148,"text":"ID:49386840\nPost:\nText: The answer was that the take method takes as argument the position of the row to remove in the current dataframe, and not its index. The confusion comes from the argument name which is indices, but the documentation explicitly states: \nText: An array of ints indicating which positions to take \nText: Let me explain the difference with an example. \nText: Say you have a chunksize of 40000. The first index of your data frame built from your second chunk will then be 40000. However, the position of this row is 0, and that's the position value that take is expecting. \nText: That's why you need to substract the number of rows you already went through (chunksize * (chunk_number - 1)) from your indices. My corresponding line of code is : \nCode: indices_to_keep = [x - (chunk_size * (chunk_number - 1)) for x in indices_to_keep]\n\nText: Now you have a list of the positions of the rows to keep, and you can use the take as expected. \nText: Please let me know if the vocabulary (position and index) is not appropriate so that I can correct it. I am not a native English speaker and the meaning of these words is very important in this problem. \nAPI:\npandas.DataFrame.take\n","label":[[48,52,"Mention"],[1172,1193,"API"]],"Comments":[]}
{"id":60149,"text":"ID:49393472\nPost:\nText: In the first case: \nText: Because the in operator is interpreted as a call to df['name'].__contains__('Adam'). If you look at the implementation of __contains__ in pandas.Series, you will find that it's the following (inhereted from pandas.core.generic.NDFrame) : \nCode: def __contains__(self, key):\n    \"\"\"True if the key is in the info axis\"\"\"\n    return key in self._info_axis\n\nText: so, your first use of in is interpreted as: \nCode: 'Adam' in df['name']._info_axis \n\nText: This gives False, expectedly, because df['name']._info_axis actually contains information about the range\/index and not the data itself: \nCode: In [37]: df['name']._info_axis \nOut[37]: RangeIndex(start=0, stop=3, step=1)\n\nIn [38]: list(df['name']._info_axis) \nOut[38]: [0, 1, 2]\n\nText: In the second case: \nCode: 'Adam' in list(df['name'])\n\nText: The use of list, converts the Series to a list of the values. So, the actual operation is this: \nCode: In [42]: list(df['name'])\nOut[42]: ['Adam', 'Ben', 'Chris']\n\nIn [43]: 'Adam' in ['Adam', 'Ben', 'Chris']\nOut[43]: True\n\nText: Here are few more idiomatic ways to do what you want (with the associated speed): \nCode: In [56]: df.name.str.contains('Adam').any()\nOut[56]: True\n\nIn [57]: timeit df.name.str.contains('Adam').any()\nThe slowest run took 6.25 times longer than the fastest. This could mean that an intermediate result is being cached.\n10000 loops, best of 3: 144 s per loop\n\nIn [58]: df.name.isin(['Adam']).any()\nOut[58]: True\n\nIn [59]: timeit df.name.isin(['Adam']).any()\nThe slowest run took 5.13 times longer than the fastest. This could mean that an intermediate result is being cached.\n10000 loops, best of 3: 191 s per loop\n\nIn [60]: df.name.eq('Adam').any()\nOut[60]: True\n\nIn [61]: timeit df.name.eq('Adam').any()\n10000 loops, best of 3: 178 s per loop\n\nText: Note: the last way is also suggested by @Wen in the comment above \nAPI:\npandas.Series\n","label":[[879,885,"Mention"],[1902,1915,"API"]],"Comments":[]}
{"id":60150,"text":"ID:49393547\nPost:\nText: You have to specify that you're looking for the mean of your dataframe. As it is, you're not referencing your dataframe at all when you call numpy.mean(). \nText: If you dataframe is called df, using pd.Series.mean should work, like this: \nCode: df['longitude'].mean()\ndf['longitude'].std()\n\nText: As it is, you're calling numpy.mean() on a string, which doesn't mean much. If you really wanted to use numpy.mean(), you could use np.mean(df['longitude']) \nAPI:\npandas.Series.mean\n","label":[[223,237,"Mention"],[484,502,"API"]],"Comments":[]}
{"id":60151,"text":"ID:49435999\nPost:\nText: Pandas NaT behaves like a floating-point NaN, in that it's not equal to itself. Instead, you can use pandas.isnull: \nCode: In [21]: pandas.isnull(pandas.NaT)\nOut[21]: True\n\nText: This also returns True for None and NaN. \nText: Technically, you could also check for Pandas NaT with x != x, following a common pattern used for floating-point NaN. However, this is likely to cause issues with NumPy NaTs, which look very similar and represent the same concept, but are actually a different type with different behavior: \nCode: In [29]: x = pandas.NaT\n\nIn [30]: y = numpy.datetime64('NaT')\n\nIn [31]: x != x\nOut[31]: True\n\nIn [32]: y != y\n\/home\/i850228\/.local\/lib\/python3.6\/site-packages\/IPython\/__main__.py:1: FutureWarning: In the future, NAT != NAT will be True rather than False.\n  # encoding: utf-8\nOut[32]: False\n\nText: numpy.isnat, the function to check for NumPy NaT, also fails with a Pandas NaT: \nCode: In [33]: numpy.isnat(pandas.NaT)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-33-39a66bbf6513> in <module>()\n----> 1 numpy.isnat(pandas.NaT)\n\nTypeError: ufunc 'isnat' is only defined for datetime and timedelta.\n\nText: isnull works for both Pandas and NumPy NaTs, so it's probably the way to go: \nCode: In [34]: pandas.isnull(pandas.NaT)\nOut[34]: True\n\nIn [35]: pandas.isnull(numpy.datetime64('NaT'))\nOut[35]: True\n\nAPI:\npandas.isnull\n","label":[[1272,1278,"Mention"],[1474,1487,"API"]],"Comments":[]}
{"id":60152,"text":"ID:49460945\nPost:\nText: Since you have to modify an external object, and append does not allow in-place modification, you have to either make all function and df available as members of some class, so you would be able to write self.df = self.df.append(msg), or use global: \nCode: import json\n\nimport pandas as pd\nimport websocket\n\ndf = pd.DataFrame(columns=['foreignNotional', 'grossValue', 'homeNotional', 'price', 'side',\n                           'size', 'symbol', 'tickDirection', 'timestamp', 'trdMatchID'])\n\n\ndef on_message(ws, message):\n    msg = json.loads(message)\n    print(msg)\n    global df\n    # `ignore_index=True` has to be provided, otherwise you'll get\n    # \"Can only append a Series if ignore_index=True or if the Series has a name\" errors\n    df = df.append(msg, ignore_index=True)\n\n\ndef on_error(ws, error):\n    print(error)\n\n\ndef on_close(ws):\n    print(\"### closed ###\")\n\n\ndef on_open(ws):\n    return\n\n\nif __name__ == \"__main__\":\n    ws = websocket.WebSocketApp(\"wss:\/\/www.bitmex.com\/realtime?subscribe=trade:XBTUSD\",\n                                on_open=on_open, on_message=on_message, on_error=on_error, on_close=on_close)\n    ws.run_forever()\n\nAPI:\npandas.DataFrame.append\n","label":[[73,79,"Mention"],[1180,1203,"API"]],"Comments":[]}
{"id":60153,"text":"ID:49534143\nPost:\nText: This is not possible. \nText: Underlying DataFrame objects are numpy arrays, which do not group data in the way you suggest. Therefore, an arbitrary column cannot be displayed as grouped data. \nText: Option 1 \nText: It is possible to partially replicate your desired output by using MultiIndex: \nCode: import pandas as pd\n\ndf = pd.DataFrame([['AAA', 8, 2, 'BBB'],\n                   ['AAA', 9, 5, 'BBB'],\n                   ['AAA', 10, 6, 'BBB']],\n                  columns=['Name', 'Score1', 'Score2', 'PM'])\n\nres = df.set_index(['Name', 'PM'])\n\nText: Result: \nCode:           Score1  Score2\nName PM                 \nAAA  BBB       8       2\n     BBB       9       5\n     BBB      10       6\n\nText: Option 2 \nText: Or you can add a dummy column and set_index on 3 columns: \nCode: df['dummy'] = 0\nres = df.set_index(['Name', 'PM', 'dummy'])\n\nText: Result: \nCode:                 Score1  Score2\nName PM  dummy                \nAAA  BBB 0           8       2\n         0           9       5\n         0          10       6\n\nAPI:\npandas.DataFrame\n","label":[[64,73,"Mention"],[1047,1063,"API"]],"Comments":[]}
{"id":60154,"text":"ID:49584927\nPost:\nText: Use concat or DataFrame.append for join together and then DataFrame.sort_values by column Date, last for default indices DataFrame.reset_index with parameter drop=True: \nCode: df3 = pd.concat([df1, df2]).sort_values('Date').reset_index(drop=True)\n\nText: Alternative: \nCode: df3 = df1.append(df2).sort_values('Date').reset_index(drop=True)\n\nCode: print (df3)\n         Date   Price\n0  2010-01-01  1800.0\n1  2010-01-02  2000.0\n2  2010-01-03  2200.0\n3  2010-01-04  1500.0\n4  2010-01-05  2010.0\n5  2010-01-07  2100.0\n6  2010-01-08  1600.0\n7  2010-01-09  1400.0\n8  2010-01-10  2110.0\n\nText: EDIT: \nText: If TimeSeries then solution is simplify: \nCode: s3= pd.concat([s1, s2]).sort_index()\n\nAPI:\npandas.concat\n","label":[[28,34,"Mention"],[713,726,"API"]],"Comments":[]}
{"id":60155,"text":"ID:49593647\nPost:\nText: If you're okay with pandas, you can do: \nCode: import pandas as pd\n\nfilename = 'path\/to\/your.csv'\ndf = pd.read_csv(filename, skiprows=30, usecols=[2, 4, 8])\n\nText: skiprows can be an integer (the number of rows to skip, from the top), a list of rows to exclude, or a boolean callable that gets called on each row index. \nText: usecols can be a list of indices, a list of column names, or a boolean callable that gets called on each column name. \nText: Check out the documentation for read_csv \nAPI:\npandas.read_csv\n","label":[[508,516,"Mention"],[523,538,"API"]],"Comments":[]}
{"id":60156,"text":"ID:49620539\nPost:\nText: In the process of answering this question for myself, I learned many things, and I wanted to put together a catalog of examples and some explanation. \nText: The specific answer to the point of the levels argument will come towards the end. \nText: pandas.concat: The Missing Manual \nText: Link To Current Documentation \nText: Imports and defining objects \nCode: import pandas as pd\n\nd1 = pd.DataFrame(dict(A=.1, B=.2, C=.3), index=[2, 3])\nd2 = pd.DataFrame(dict(B=.4, C=.5, D=.6), index=[1, 2])\nd3 = pd.DataFrame(dict(A=.7, B=.8, D=.9), index=[1, 3])\n\ns1 = pd.Series([1, 2], index=[2, 3])\ns2 = pd.Series([3, 4], index=[1, 2])\ns3 = pd.Series([5, 6], index=[1, 3])\n\nText: Arguments \nText: objs \nText: The first argument we come across is objs: \nText: objs: a sequence or mapping of Series, DataFrame, or Panel objects If a dict is passed, the sorted keys will be used as the keys argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised \nText: We typically see this used with a list of Series or DataFrame objects. I'll show that dict can be very useful as well. Generators may also be used and can be useful when using map as in map(f, list_of_df) \nText: For now, we'll stick with a list of some of the DataFrame and Series objects defined above. I'll show how dictionaries can be leveraged to give very useful MultiIndex results later. \nCode: pd.concat([d1, d2])\n\n     A    B    C    D\n2  0.1  0.2  0.3  NaN\n3  0.1  0.2  0.3  NaN\n1  NaN  0.4  0.5  0.6\n2  NaN  0.4  0.5  0.6\n\nText: axis \nText: The second argument we encounter is axis whose default value is 0: \nText: axis: {0\/index, 1\/columns}, default 0 The axis to concatenate along. \nText: Two DataFrames with axis=0 (stacked) \nText: For values of 0 or index we mean to say: \"Align along the columns and add to the index\". \nText: As shown above where we used axis=0, because 0 is the default value, and we see that the index of d2 extends the index of d1 despite there being overlap of the value 2: \nCode: pd.concat([d1, d2], axis=0)\n\n     A    B    C    D\n2  0.1  0.2  0.3  NaN\n3  0.1  0.2  0.3  NaN\n1  NaN  0.4  0.5  0.6\n2  NaN  0.4  0.5  0.6\n\nText: Two DataFrames with axis=1 (side by side) \nText: For values 1 or columns we mean to say: \"Align along the index and add to the columns\", \nCode: pd.concat([d1, d2], axis=1)\n\n     A    B    C    B    C    D\n1  NaN  NaN  NaN  0.4  0.5  0.6\n2  0.1  0.2  0.3  0.4  0.5  0.6\n3  0.1  0.2  0.3  NaN  NaN  NaN\n\nText: We can see that the resulting index is the union of indices and the resulting columns are the extension of columns from d1 by the columns of d2. \nText: Two (or Three) Series with axis=0 (stacked) \nText: When combining pd.Series along axis=0, we get back a pandas.Series. The name of the resulting Series will be None unless all Series being combined have the same name. Pay attention to the 'Name: A' when we print out the resulting Series. When it isn't present, we can assume the Series name is None. \nCode:                |                       |                        |  pd.concat(\n               |  pd.concat(           |  pd.concat(            |      [s1.rename('A'),\n pd.concat(    |      [s1.rename('A'), |      [s1.rename('A'),  |       s2.rename('B'),\n     [s1, s2]) |       s2])            |       s2.rename('A')]) |       s3.rename('A')])\n-------------- | --------------------- | ---------------------- | ----------------------\n2    1         | 2    1                | 2    1                 | 2    1\n3    2         | 3    2                | 3    2                 | 3    2\n1    3         | 1    3                | 1    3                 | 1    3\n2    4         | 2    4                | 2    4                 | 2    4\ndtype: int64   | dtype: int64          | Name: A, dtype: int64  | 1    5\n               |                       |                        | 3    6\n               |                       |                        | dtype: int64\n\nText: Two (or Three) Series with axis=1 (side by side) \nText: When combining pd.Series along axis=1, it is the name attribute that we refer to in order to infer a column name in the resulting pandas.DataFrame. \nCode:                        |                       |  pd.concat(\n                       |  pd.concat(           |      [s1.rename('X'),\n pd.concat(            |      [s1.rename('X'), |       s2.rename('Y'),\n     [s1, s2], axis=1) |       s2], axis=1)    |       s3.rename('Z')], axis=1)\n---------------------- | --------------------- | ------------------------------\n     0    1            |      X    0           |      X    Y    Z\n1  NaN  3.0            | 1  NaN  3.0           | 1  NaN  3.0  5.0\n2  1.0  4.0            | 2  1.0  4.0           | 2  1.0  4.0  NaN\n3  2.0  NaN            | 3  2.0  NaN           | 3  2.0  NaN  6.0\n\nText: Mixed Series and DataFrame with axis=0 (stacked) \nText: When performing a concatenation of a Series and DataFrame along axis=0, we convert all Series to single column DataFrames. \nText: Take special note that this is a concatenation along axis=0; that means extending the index (rows) while aligning the columns. In the examples below, we see the index becomes [2, 3, 2, 3] which is an indiscriminate appending of indices. The columns do not overlap unless I force the naming of the Series column with the argument to to_frame: \nCode:  pd.concat(               |\n     [s1.to_frame(), d1]) |  pd.concat([s1, d1])\n------------------------- | ---------------------\n     0    A    B    C     |      0    A    B    C\n2  1.0  NaN  NaN  NaN     | 2  1.0  NaN  NaN  NaN\n3  2.0  NaN  NaN  NaN     | 3  2.0  NaN  NaN  NaN\n2  NaN  0.1  0.2  0.3     | 2  NaN  0.1  0.2  0.3\n3  NaN  0.1  0.2  0.3     | 3  NaN  0.1  0.2  0.3\n\nText: You can see the results of pd.concat([s1, d1]) are the same as if I had perfromed the to_frame myself. \nText: However, I can control the name of the resulting column with a parameter to to_frame. Renaming the Series with the rename method does not control the column name in the resulting DataFrame. \nCode:  # Effectively renames       |                            |\n # `s1` but does not align   |  # Does not rename.  So    |  # Renames to something\n # with columns in `d1`      |  # Pandas defaults to `0`  |  # that does align with `d1`\n pd.concat(                  |  pd.concat(                |  pd.concat(\n     [s1.to_frame('X'), d1]) |      [s1.rename('X'), d1]) |      [s1.to_frame('B'), d1])\n---------------------------- | -------------------------- | ----------------------------\n     A    B    C    X        |      0    A    B    C      |      A    B    C\n2  NaN  NaN  NaN  1.0        | 2  1.0  NaN  NaN  NaN      | 2  NaN  1.0  NaN\n3  NaN  NaN  NaN  2.0        | 3  2.0  NaN  NaN  NaN      | 3  NaN  2.0  NaN\n2  0.1  0.2  0.3  NaN        | 2  NaN  0.1  0.2  0.3      | 2  0.1  0.2  0.3\n3  0.1  0.2  0.3  NaN        | 3  NaN  0.1  0.2  0.3      | 3  0.1  0.2  0.3\n\nText: Mixed Series and DataFrame with axis=1 (side by side) \nText: This is fairly intuitive. Series column name defaults to an enumeration of such Series objects when a name attribute is not available. \nCode:                     |  pd.concat(\n pd.concat(         |      [s1.rename('X'),\n     [s1, d1],      |       s2, s3, d1],\n     axis=1)        |      axis=1)\n------------------- | -------------------------------\n   0    A    B    C |      X    0    1    A    B    C\n2  1  0.1  0.2  0.3 | 1  NaN  3.0  5.0  NaN  NaN  NaN\n3  2  0.1  0.2  0.3 | 2  1.0  4.0  NaN  0.1  0.2  0.3\n                    | 3  2.0  NaN  6.0  0.1  0.2  0.3\n\nText: join \nText: The third argument is join that describes whether the resulting merge should be an outer merge (default) or an inner merge. \nText: join: {inner, outer}, default outer How to handle indexes on other axis(es). \nText: It turns out, there is no left or right option as pd.concat can handle more than just two objects to merge. \nText: In the case of d1 and d2, the options look like: \nText: outer \nCode: pd.concat([d1, d2], axis=1, join='outer')\n\n     A    B    C    B    C    D\n1  NaN  NaN  NaN  0.4  0.5  0.6\n2  0.1  0.2  0.3  0.4  0.5  0.6\n3  0.1  0.2  0.3  NaN  NaN  NaN\n\nText: inner \nCode: pd.concat([d1, d2], axis=1, join='inner')\n\n     A    B    C    B    C    D\n2  0.1  0.2  0.3  0.4  0.5  0.6\n\nText: join_axes \nText: Fourth argument is the thing that allows us to do our left merge and more. \nText: join_axes: list of Index objects Specific indexes to use for the other n - 1 axes instead of performing inner\/outer set logic. \nText: Left Merge \nCode: pd.concat([d1, d2, d3], axis=1, join_axes=[d1.index])\n\n     A    B    C    B    C    D    A    B    D\n2  0.1  0.2  0.3  0.4  0.5  0.6  NaN  NaN  NaN\n3  0.1  0.2  0.3  NaN  NaN  NaN  0.7  0.8  0.9\n\nText: Right Merge \nCode: pd.concat([d1, d2, d3], axis=1, join_axes=[d3.index])\n\n     A    B    C    B    C    D    A    B    D\n1  NaN  NaN  NaN  0.4  0.5  0.6  0.7  0.8  0.9\n3  0.1  0.2  0.3  NaN  NaN  NaN  0.7  0.8  0.9\n\nText: ignore_index \nText: ignore_index: boolean, default False If True, do not use the index values along the concatenation axis. The resulting axis will be labeled 0, ..., n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join. \nText: Like when I stack d1 on top of d2, if I don't care about the index values, I could reset them or ignore them. \nCode:                       |  pd.concat(             |  pd.concat(\n                      |      [d1, d2],          |      [d1, d2]\n pd.concat([d1, d2])  |      ignore_index=True) |  ).reset_index(drop=True)\n--------------------- | ----------------------- | -------------------------\n     A    B    C    D |      A    B    C    D   |      A    B    C    D\n2  0.1  0.2  0.3  NaN | 0  0.1  0.2  0.3  NaN   | 0  0.1  0.2  0.3  NaN\n3  0.1  0.2  0.3  NaN | 1  0.1  0.2  0.3  NaN   | 1  0.1  0.2  0.3  NaN\n1  NaN  0.4  0.5  0.6 | 2  NaN  0.4  0.5  0.6   | 2  NaN  0.4  0.5  0.6\n2  NaN  0.4  0.5  0.6 | 3  NaN  0.4  0.5  0.6   | 3  NaN  0.4  0.5  0.6\n\nText: And when using axis=1: \nCode:                                    |     pd.concat(\n                                   |         [d1, d2], axis=1,\n pd.concat([d1, d2], axis=1)       |         ignore_index=True)\n-------------------------------    |    -------------------------------\n     A    B    C    B    C    D    |         0    1    2    3    4    5\n1  NaN  NaN  NaN  0.4  0.5  0.6    |    1  NaN  NaN  NaN  0.4  0.5  0.6\n2  0.1  0.2  0.3  0.4  0.5  0.6    |    2  0.1  0.2  0.3  0.4  0.5  0.6\n3  0.1  0.2  0.3  NaN  NaN  NaN    |    3  0.1  0.2  0.3  NaN  NaN  NaN\n\nText: keys \nText: We can pass a list of scalar values or tuples in order to assign tuple or scalar values to corresponding MultiIndex. The length of the passed list must be the same length as the number of items being concatenated. \nText: keys: sequence, default None If multiple levels passed, should contain tuples. Construct hierarchical index using the passed keys as the outermost level \nText: axis=0 \nText: When concatenating Series objects along axis=0 (extending the index). \nText: Those keys, become a new initial level of a MultiIndex object in the index attribute. \nCode:  #           length 3             length 3           #         length 2        length 2\n #          \/--------\\         \/-----------\\         #          \/----\\         \/------\\\n pd.concat([s1, s2, s3], keys=['A', 'B', 'C'])       pd.concat([s1, s2], keys=['A', 'B'])\n----------------------------------------------      -------------------------------------\nA  2    1                                           A  2    1\n   3    2                                              3    2\nB  1    3                                           B  1    3\n   2    4                                              2    4\nC  1    5                                           dtype: int64\n   3    6\ndtype: int64\n\nText: However, we can use more than scalar values in the keys argument to create an even deeper MultiIndex. Here we pass tuples of length 2 the prepend two new levels of a MultiIndex: \nCode:  pd.concat(\n     [s1, s2, s3],\n     keys=[('A', 'X'), ('A', 'Y'), ('B', 'X')])\n-----------------------------------------------\nA  X  2    1\n      3    2\n   Y  1    3\n      2    4\nB  X  1    5\n      3    6\ndtype: int64\n\nText: axis=1 \nText: It's a bit different when extending along columns. When we used axis=0 (see above) our keys acted as MultiIndex levels in addition to the existing index. For axis=1, we are referring to an axis that Series objects don't have, namely the columns attribute. \nText: Variations of Two \nText: Series \nText: wtih \nText: axis=1 \nText: Notice that naming the s1 and s2 matters so long as no keys are passed, but it gets overridden if keys are passed. \nCode:                |                       |                        |  pd.concat(\n               |  pd.concat(           |  pd.concat(            |      [s1.rename('U'),\n pd.concat(    |      [s1, s2],        |      [s1.rename('U'),  |       s2.rename('V')],\n     [s1, s2], |      axis=1,          |       s2.rename('V')], |       axis=1,\n     axis=1)   |      keys=['X', 'Y']) |       axis=1)          |       keys=['X', 'Y'])\n-------------- | --------------------- | ---------------------- | ----------------------\n     0    1    |      X    Y           |      U    V            |      X    Y\n1  NaN  3.0    | 1  NaN  3.0           | 1  NaN  3.0            | 1  NaN  3.0\n2  1.0  4.0    | 2  1.0  4.0           | 2  1.0  4.0            | 2  1.0  4.0\n3  2.0  NaN    | 3  2.0  NaN           | 3  2.0  NaN            | 3  2.0  NaN\n\nText: MultiIndex \nText: with \nText: Series \nText: and \nText: axis=1 \nCode:  pd.concat(\n     [s1, s2],\n     axis=1,\n     keys=[('W', 'X'), ('W', 'Y')])\n-----------------------------------\n     W\n     X    Y\n1  NaN  3.0\n2  1.0  4.0\n3  2.0  NaN\n\nText: Two \nText: DataFrame \nText: with \nText: axis=1 \nText: As with the axis=0 examples, keys add levels to a MultiIndex, but this time to the object stored in the columns attribute. \nCode:  pd.concat(                     |  pd.concat(\n     [d1, d2],                  |      [d1, d2],\n     axis=1,                    |      axis=1,\n     keys=['X', 'Y'])           |      keys=[('First', 'X'), ('Second', 'X')])\n------------------------------- | --------------------------------------------\n     X              Y           |   First           Second\n     A    B    C    B    C    D |       X                X\n1  NaN  NaN  NaN  0.4  0.5  0.6 |       A    B    C      B    C    D\n2  0.1  0.2  0.3  0.4  0.5  0.6 | 1   NaN  NaN  NaN    0.4  0.5  0.6\n3  0.1  0.2  0.3  NaN  NaN  NaN | 2   0.1  0.2  0.3    0.4  0.5  0.6\n                                | 3   0.1  0.2  0.3    NaN  NaN  NaN\n\nText: Series \nText: and \nText: DataFrame \nText: with \nText: axis=1 \nText: This is tricky. In this case, a scalar key value cannot act as the only level of index for the Series object when it becomes a column while also acting as the first level of a MultiIndex for the DataFrame. So Pandas will again use the name attribute of the Series object as the source of the column name. \nCode:  pd.concat(           |  pd.concat(\n     [s1, d1],        |      [s1.rename('Z'), d1],\n     axis=1,          |      axis=1,\n     keys=['X', 'Y']) |      keys=['X', 'Y'])\n--------------------- | --------------------------\n   X    Y             |    X    Y\n   0    A    B    C   |    Z    A    B    C\n2  1  0.1  0.2  0.3   | 2  1  0.1  0.2  0.3\n3  2  0.1  0.2  0.3   | 3  2  0.1  0.2  0.3\n\nText: Limitations of \nText: keys \nText: and \nText: MultiIndex \nText: inferrence. \nText: Pandas only seems to infer column names from Series name, but it will not fill in the blanks when doing an analogous concatenation among data frames with a different number of column levels. \nCode: d1_ = pd.concat(\n    [d1], axis=1,\n    keys=['One'])\nd1_\n\n   One\n     A    B    C\n2  0.1  0.2  0.3\n3  0.1  0.2  0.3\n\nText: Then concatenate this with another data frame with only one level in the columns object and Pandas will refuse to try and make tuples of the MultiIndex object and combine all data frames as if a single level of objects, scalars and tuples. \nCode: pd.concat([d1_, d2], axis=1)\n\n   (One, A)  (One, B)  (One, C)    B    C    D\n1       NaN       NaN       NaN  0.4  0.5  0.6\n2       0.1       0.2       0.3  0.4  0.5  0.6\n3       0.1       0.2       0.3  NaN  NaN  NaN\n\nText: Passing a dict instead of a list \nText: When passing a dictionary, pd.concat will use the keys from the dictionary as the keys parameter. \nCode:  # axis=0               |  # axis=1\n pd.concat(             |  pd.concat(\n     {0: d1, 1: d2})    |      {0: d1, 1: d2}, axis=1)\n----------------------- | -------------------------------\n       A    B    C    D |      0              1\n0 2  0.1  0.2  0.3  NaN |      A    B    C    B    C    D\n  3  0.1  0.2  0.3  NaN | 1  NaN  NaN  NaN  0.4  0.5  0.6\n1 1  NaN  0.4  0.5  0.6 | 2  0.1  0.2  0.3  0.4  0.5  0.6\n  2  NaN  0.4  0.5  0.6 | 3  0.1  0.2  0.3  NaN  NaN  NaN\n\nText: levels \nText: This is used in conjunction with the keys argument.When levels is left as its default value of None, Pandas will take the unique values of each level of the resulting MultiIndex and use that as the object used in the resulting index.levels attribute. \nText: levels: list of sequences, default None Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys. \nText: If Pandas already infers what these levels should be, what advantage is there to specify it ourselves? I'll show one example and leave it up to you to think up other reasons why this might be useful. \nText: Example \nText: Per the documentation, the levels argument is a list of sequences. This means that we can use another Index as one of those sequences. \nText: Consider the data frame df that is the concatenation of d1, d2 and d3: \nCode: df = pd.concat(\n    [d1, d2, d3], axis=1,\n    keys=['First', 'Second', 'Fourth'])\n\ndf\n\n  First           Second           Fourth\n      A    B    C      B    C    D      A    B    D\n1   NaN  NaN  NaN    0.4  0.5  0.6    0.7  0.8  0.9\n2   0.1  0.2  0.3    0.4  0.5  0.6    NaN  NaN  NaN\n3   0.1  0.2  0.3    NaN  NaN  NaN    0.7  0.8  0.9\n\nText: The levels of the columns object are: \nCode: print(df, *df.columns.levels, sep='\\n')\n\nIndex(['First', 'Second', 'Fourth'], dtype='object')\nIndex(['A', 'B', 'C', 'D'], dtype='object')\n\nText: If we use sum within a groupby we get: \nCode: df.groupby(axis=1, level=0).sum()\n\n   First  Fourth  Second\n1    0.0     2.4     1.5\n2    0.6     0.0     1.5\n3    0.6     2.4     0.0\n\nText: But what if instead of ['First', 'Second', 'Fourth'] there were another missing categories named Third and Fifth? And I wanted them included in the results of a groupby aggregation? We can do this if we had a pandas.CategoricalIndex. And we can specify that ahead of time with the levels argument. \nText: So instead, let's define df as: \nCode: cats = ['First', 'Second', 'Third', 'Fourth', 'Fifth']\nlvl = pd.CategoricalIndex(cats, categories=cats, ordered=True)\n\ndf = pd.concat(\n    [d1, d2, d3], axis=1,\n    keys=['First', 'Second', 'Fourth'],\n    levels=[lvl]\n)\n\ndf\n\n   First  Fourth  Second\n1    0.0     2.4     1.5\n2    0.6     0.0     1.5\n3    0.6     2.4     0.0\n\nText: But the first level of the columns object is: \nCode: df.columns.levels[0]\n\nCategoricalIndex(\n    ['First', 'Second', 'Third', 'Fourth', 'Fifth'],\n    categories=['First', 'Second', 'Third', 'Fourth', 'Fifth'],\n    ordered=True, dtype='category')\n\nText: And our groupby summation looks like: \nCode: df.groupby(axis=1, level=0).sum()\n\n   First  Second  Third  Fourth  Fifth\n1    0.0     1.5    0.0     2.4    0.0\n2    0.6     1.5    0.0     0.0    0.0\n3    0.6     0.0    0.0     2.4    0.0\n\nText: names \nText: This is used to name the levels of a resulting MultiIndex. The length of the names list should match the number of levels in the resulting MultiIndex. \nText: names: list, default None Names for the levels in the resulting hierarchical index \nCode:  # axis=0                     |  # axis=1\n pd.concat(                   |  pd.concat(\n     [d1, d2],                |      [d1, d2],\n     keys=[0, 1],             |      axis=1, keys=[0, 1],\n     names=['lvl0', 'lvl1'])  |      names=['lvl0', 'lvl1'])\n----------------------------- | ----------------------------------\n             A    B    C    D | lvl0    0              1\nlvl0 lvl1                     | lvl1    A    B    C    B    C    D\n0    2     0.1  0.2  0.3  NaN | 1     NaN  NaN  NaN  0.4  0.5  0.6\n     3     0.1  0.2  0.3  NaN | 2     0.1  0.2  0.3  0.4  0.5  0.6\n1    1     NaN  0.4  0.5  0.6 | 3     0.1  0.2  0.3  NaN  NaN  NaN\n     2     NaN  0.4  0.5  0.6 |\n\nText: verify_integrity \nText: Self explanatory documentation \nText: verify_integrity: boolean, default False Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation. \nText: Because the resulting index from concatenating d1 and d2 is not unique, it would fail the integrity check. \nCode: pd.concat([d1, d2])\n\n     A    B    C    D\n2  0.1  0.2  0.3  NaN\n3  0.1  0.2  0.3  NaN\n1  NaN  0.4  0.5  0.6\n2  NaN  0.4  0.5  0.6\n\nText: And \nCode: pd.concat([d1, d2], verify_integrity=True)\n\nText: > ValueError: Indexes have overlapping values: [2] \nAPI:\npandas.Series\npandas.Series\npandas.concat\npandas.Index\n","label":[[2796,2805,"Mention"],[4115,4124,"Mention"],[16564,16573,"Mention"],[17876,17881,"Mention"],[21443,21456,"API"],[21457,21470,"API"],[21471,21484,"API"],[21485,21497,"API"]],"Comments":[]}
{"id":60157,"text":"ID:49647489\nPost:\nText: Use merge_asof with a forward direction \nCode: pd.merge_asof(\n    df1, df2.loc[df2.col03 == 0, ['timestamp2']],\n    left_on='timestamp1', right_on='timestamp2', direction='forward'\n).rename(columns=dict(timestamp2='colnew'))\n\n   col01  timestamp1     colnew\n0      1       88148  5629500.0\n1      2     5617900  5629500.0\n2      3     5622548  5629500.0\n3      4     5645748  6578800.0\n4      5     6603950        NaN\n5      6     6666502        NaN\n\nAPI:\npandas.merge_asof\n","label":[[28,38,"Mention"],[480,497,"API"]],"Comments":[]}
{"id":60158,"text":"ID:49696443\nPost:\nText: When reading a workbook with multiple sheets, pd.read_excel returns a dictionary of DataFrames where the keys of the dictionary are the names of the sheets. \nText: It seems like you want to add a column code to each DataFrame based on the values in a list. \nText: Your code: \nCode: for i in codes:\n    for key in df.keys():\n        df['Sheet1']['Code'] = i\n\nText: Has two issues. Firstly, inside the loop you are not using the key at all. You're always accessing \"Sheet1\". Secondly, this is a double for loop which means it will iterate over every sheet for every code. \nText: What you want instead is to loop over the values in parallel. Basically you want to do the following: \nCode: for i in range(len(codes)):\n    code = codes[i]\n    key = df.keys()[i]\n    df[key]['Code'] = code\n\nText: This is exactly what zip() does. So, you can write the above loop more compactly as: \nCode: for code, key in zip(codes, df.keys()):\n    df[key]['Code'] = code\n\nText: Not this assumes that the length of codes is equal to the number of keys in the dictionary df. \nText: Afterwards, you can concatenate all of the DataFrames using pandas.concat: \nCode: combined = pd.concat(df)\n\nText: Which works because concat: \nText: takes a sequence or mapping of Series, DataFrame ... If a dict is passed, the sorted keys will be used as the keys argument, unless > it is passed, in which case the values will be selected (see below). \nAPI:\npandas.read_excel\n","label":[[70,83,"Mention"],[1441,1458,"API"]],"Comments":[]}
{"id":60159,"text":"ID:49700235\nPost:\nText: Assuming that is a Series object \nText: Option 1 \nText: Full list \nCode: np.concatenate(s).tolist()\n\nText: Option 1.1 \nText: Unique list \nCode: np.unique(np.concatenate(s)).tolist()\n\nText: Option 2 \nText: Works if elements are lists. Doesn't work if they are numpy arrays. Full list \nCode: s.sum()\n\nText: Option 2.1 \nText: Unique list \nCode: pd.unique(s.sum()).tolist()\n\nText: Option 3 \nText: Full list \nCode: [x for y in s for x in y]\n\nText: Option 3.1 \nText: Unique list (Thanks @pault) \nCode: list({x for y in s for x in y})\n\nText: @Wen's Option \nCode: list(set.union(*map(set, s)))\n\nText: Setup \nCode: s = pd.Series([\n    ['ID01'],\n    ['ID02'],\n    ['ID05', 'ID08'],\n    ['ID09', 'ID56', 'ID32'],\n    ['ID03']\n])\n\ns\n\n0                [ID01]\n1                [ID02]\n2          [ID05, ID08]\n3    [ID09, ID56, ID32]\n4                [ID03]\ndtype: object\n\nAPI:\npandas.Series\n","label":[[43,49,"Mention"],[886,899,"API"]],"Comments":[]}
{"id":60160,"text":"ID:49754621\nPost:\nText: There are 2 ways you can do this. \nText: Ignore columns when reading the data \nText: pd.read_csv has the argument usecols, which accepts an integer list. \nText: So you can try: \nCode: # work out required columns\ndf = pd.read_csv('file.csv', header=0)\ncols = [0] + list(range(1, len(df.columns), 2))\n\n# use column integer list\ndf = pd.read_csv('file.csv', usecols=cols)\n\nText: Remove columns from dataframe \nText: You can use similar logic with pd.DataFrame.iloc to remove unwanted columns. \nCode: # cols as defined in previous example\n\ndf = df.iloc[:, cols]\n\nAPI:\npandas.read_csv\n","label":[[109,120,"Mention"],[588,603,"API"]],"Comments":[]}
{"id":60161,"text":"ID:49798782\nPost:\nText: DataFrame accepts a list of dictionaries directly. \nText: You can create a list via lst = list(result) and build the dataframe as below. \nCode: from collections import OrderedDict\nimport pandas as pd\n\nlst = [OrderedDict([('H', '123'), ('U', 'aaa@global-bilgi.entp'), ('S', 'motv:SMP_SESSION_ID\/1523537360524\/-86840158')]),\n       OrderedDict([('H', '456'), ('U', 'sss@global-bilgi.entp'), ('S', 'motv:SMP_SESSION_ID\/1523537367876\/-765151654')]),\n       OrderedDict([('H', '145'), ('U', 'ddd@global-bilgi.entp'), ('S', 'motv:SMP_SESSION_ID\/1523537367571\/540003017')]),\n       OrderedDict([('H', '111'), ('U', 'asd@global-bilgi.entp'), ('S', 'motv:SMP_SESSION_ID\/1523537376045\/540216322')]),\n       OrderedDict([('H', '222'), ('U', 'asd@global-bilgi.entp'), ('S', 'motv:SMP_SESSION_ID\/1523537383484\/-86104258')])]\n\ndf = pd.DataFrame(lst)\n\nprint(df)\n\n#      H                      U                                             S\n# 0  123  aaa@global-bilgi.entp   motv:SMP_SESSION_ID\/1523537360524\/-86840158\n# 1  456  sss@global-bilgi.entp  motv:SMP_SESSION_ID\/1523537367876\/-765151654\n# 2  145  ddd@global-bilgi.entp   motv:SMP_SESSION_ID\/1523537367571\/540003017\n# 3  111  asd@global-bilgi.entp   motv:SMP_SESSION_ID\/1523537376045\/540216322\n# 4  222  asd@global-bilgi.entp   motv:SMP_SESSION_ID\/1523537383484\/-86104258\n\nAPI:\npandas.DataFrame\n","label":[[24,33,"Mention"],[1346,1362,"API"]],"Comments":[]}
{"id":60162,"text":"ID:49823695\nPost:\nText: Use replace with regex=True \nCode: df.replace('^.*:\\s*(.*)', r'\\1', regex=True)\n\nText: Notice that my pattern uses parentheses to capture the part after the ':' and uses a raw string r'\\1' to reference that capture group. \nText: MCVE \nCode: df = pd.DataFrame([\n    [np.nan, 'thing1: hello'],\n    ['thing2: world', np.nan]\n], columns=['extdkey1', 'extdkey2'])\n\ndf\n\n        extdkey1       extdkey2\n0            NaN  thing1: hello\n1  thing2: world            NaN\n\nCode: df.replace('^.*:\\s*(.*)', r'\\1', regex=True)\n\n  extdkey1 extdkey2\n0      NaN    hello\n1    world      NaN\n\nAPI:\npandas.DataFrame.replace\n","label":[[28,35,"Mention"],[603,627,"API"]],"Comments":[]}
{"id":60163,"text":"ID:49834955\nPost:\nText: You can use concat to concatenate dataframes: \nCode: import pandas as pd\n\nurls = ['http:\/\/www.url1.com',\n        'http:\/\/www.url2.com',\n        'http:\/\/www.url3.com']\n\ndf = pd.concat([pd.concat(pd.read_html(url, header=0), axis=0) for url in urls], axis=0)\n\ndf.to_csv('file.csv')\n\nText: Explanation \nText: pd.concat concatenates a list of dataframes. pd.read_html returns a list of dataframes. Therefore, to concatenates a list of lists of dataframes, i.e. a list of pd.read_html output, you need to use nested pd.concat. \nAPI:\npandas.concat\n","label":[[36,42,"Mention"],[552,565,"API"]],"Comments":[]}
{"id":60164,"text":"ID:49846698\nPost:\nText: This is because numpy.std (resp. numpy.var) tries to delegate to the first argument's std (resp. var) method if it isn't an ndarray (from source code here): \nCode: def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue):\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n\n    if type(a) is not mu.ndarray:\n        try:\n            std = a.std\n        except AttributeError:\n            pass\n        else:\n            return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)\n\nText: So really, you're just calling std (with 0 degrees of freedom). And in the Pandas library, all of the descriptive stats functions handle missing values (from docs see Calculations with missing data). \nText: The takeaway here is that it is a lot more clear to use the Pandas data type methods instead of the NumPy free functions in the first place, given you have a Pandas Series. \nText: Comments \nText: This behavior is what NumPy does for many functions with an array-like object as a first argument - try to use the same method on the object should it exist, and if not use some fallback. It isn't always the case though - for instance \nCode: >>> a = np.random.randint(0, 100, 5)\n\n>>> a\narray([49, 68, 93, 51, 94])\n\n>>> np.sort(a) # not in-place\narray([49, 51, 68, 93, 94])\n\n>>> a\narray([49, 68, 93, 51, 94])\n\n>>> a.sort() # in-place\n\n>>> a\narray([49, 51, 68, 93, 94])\n\nText: Also, in most cases the NaN handling functions in nanfunctions.py first call _replace_nan, which casts your type to an ndarray, and replaces the NaN values in your ndarray with a value that won't affect whatever calculation they are performing (i.e. np.nansum replaces NaNs with 0, np.nanprod replaces NaNs with 1). They then call their non-NaN counterparts to perform the actual calculation. (ex: np.nansum) \nCode: def nansum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue):\n    a, mask = _replace_nan(a, 0)\n    return np.sum(a, axis=axis, dtype=dtype, out=out, keepdims=keepdims)\n\nText: So calling np.nansum on a Pandas series for instance, you don't actually end up using sum because the Series is cast to an ndarray first inside _replace_nan. So don't (I'm not sure why you would) assume or rely on the sum method of your Series being called. \nCode: # a silly example\n\n>>> s = pd.Series([1, 2, 3, np.nan])\n\n>>> s.sum = lambda *args, **kwargs: \"instance sum\"\n\n>>> s.sum()\n'instance sum'\n\n>>> np.sum(s)\n'instance sum'\n\n>>> np.nansum(s)\n6\n\nAPI:\npandas.Series.std\npandas.Series.sum\n","label":[[710,713,"Mention"],[2242,2245,"Mention"],[2613,2630,"API"],[2631,2648,"API"]],"Comments":[]}
{"id":60165,"text":"ID:49860849\nPost:\nText: Use iloc for filtering by position. \nText: For example, to extract rows 0 and 2: \nCode: res = stock_prices.iloc[[0, 2], :]\n\nText: The first indexing parameter filters by row, the second by column. \nText: The pandas documentation describes in detail various ways of indexing and selecting data. \nAPI:\npandas.DataFrame.iloc\n","label":[[28,32,"Mention"],[324,345,"API"]],"Comments":[]}
{"id":60166,"text":"ID:49862669\nPost:\nText: You should not continually append to an existing pd.DataFrame, as this will be extremely inefficient. \nText: You should use conca with a list of dataframes. \nText: This can be facilitated by a list comprehension: \nCode: df = pd.concat([pd.read_excel(f, skiprows=range(10)) for f in files], axis=0)\n\nText: Columns will automatically align, assuming that headers are present in each Excel worksheet in row 11. \nAPI:\npandas.concat\n","label":[[148,153,"Mention"],[438,451,"API"]],"Comments":[]}
{"id":60167,"text":"ID:49889150\nPost:\nText: You can use apply (source): \nCode: import operator\n\ndef transform(df, op):\n    lengths = [len(s) for s in df['first'].replace(',', ' ').split()]\n    return [f for f in lengths if op(f, len(df.second))] or [0]\n\ndf['longer']  = df.apply(transform, axis=1, args=[operator.gt])\ndf['shorter'] = df.apply(transform, axis=1, args=[operator.lt])\n\nText: This should work for any amount of strings, assuming any space or comma indicates a new string. \nText: Here is the output: \nCode:    numeric  numericvals                     first second     longer shorter\n0       42           25         beneficiary, duke  abcde       [11]     [4]\n1       42           25                   compose  abcde        [7]     [0]\n2       42           25              herd primary  abcde        [7]     [4]\n3       42           25                     stall  abcde        [0]     [0]\n4       42           25                      deep  abcde        [0]     [4]\n5       42           25  regular summary classify  abcde  [7, 7, 8]     [0]\n6       42           25                    timber  abcde        [6]     [0]\n7       42           25                  property  abcde        [8]     [0]\n\nText: I tried my best. Hope this helps! \nAPI:\npandas.DataFrame.apply\n","label":[[36,41,"Mention"],[1230,1252,"API"]],"Comments":[]}
{"id":60168,"text":"ID:49908024\nPost:\nText: This occures because read_csv doesn't recognize complex types like list and reads them as strings: \nCode: type(interactionsCSV.at[0, 'Sequence1'])\n# <class 'str'>\n\nText: One possible work around is to use pd.eval function: \nCode: interactionsCSV['Sequence1'] = pd.eval(interactionsCSV['Sequence1'])\ntype(interactionsCSV.at[0, 'Sequence1'])\n# <class 'list'>\nmax([len(s) for s in interactionsCSV.get('Sequence1')])\n# 847 \n\nAPI:\npandas.eval\n","label":[[229,236,"Mention"],[450,461,"API"]],"Comments":[]}
{"id":60169,"text":"ID:49930086\nPost:\nText: I think you can use the skiprows and nrows arguments in read_csv to pick out individual rows to read in. \nText: With skiprows, you can provide it a long list (0 indexed) of rows not to import , e.g. [0,5,6,10]. That might end up being a huge list though. If you provide it a single integer, it will skip that number of rows and start reading. Set nrows to whatever to pick up the number of rows you want at the point where you have it start. \nText: If I've misunderstood the issue, let me know. \nAPI:\npandas.read_csv\n","label":[[80,88,"Mention"],[525,540,"API"]],"Comments":[]}
{"id":60170,"text":"ID:50005681\nPost:\nText: row['PIC_1'] returns a Pandas series so you need to access the values before you can slice them. You're looking to slice a string so you can use str vectorised string methods: \nCode: row['PIC_1'].str[PIC_loc:PIC_loc+15]\n\nAPI:\npandas.Series.str\n","label":[[169,172,"Mention"],[250,267,"API"]],"Comments":[]}
{"id":60171,"text":"ID:50051329\nPost:\nText: you could use dask.dataframe for this task \nCode: import dask.dataframe as dd\ndf = dd.from_pandas(df)\nresult = df.groupby('id').max().reset_index().compute()\n\nText: All you need to do is convert your df into a dask.dataframe. Dask is a python out-of-core parallelization framework that offers various parallelized container types, one of which is the dataframe. It let's you perform most common DataFrame operations in parallel and\/or distributed with data that is too large to fit in memory. The core of dask is a set of schedulers and an API for building computation graphs, hence we have to call .compute() at the end in order for any computation to actually take place. The library is easy to install because it is written in pure python for the most part. \nAPI:\npandas.DataFrame\npandas.DataFrame\n","label":[[224,226,"Mention"],[419,428,"Mention"],[791,807,"API"],[808,824,"API"]],"Comments":[]}
{"id":60172,"text":"ID:50051458\nPost:\nText: Use item: \nText: item Series.item() return the first element of the underlying data as a python scalar \nCode: df.loc[(df['A'] == 'foo') & (df['B'] == 'car'), 'C'].item()\n\nText: Output: \nCode: 'green'\n\nAPI:\npandas.Series.item\n","label":[[41,45,"Mention"],[230,248,"API"]],"Comments":[]}
{"id":60173,"text":"ID:50090272\nPost:\nText: Answer to get result: \nCode: import requests\nimport pandas as pd\nURL ='http:\/\/tools.morningstar.it\/api\/rest.svc\/timeseries_price\/jbyiq3rhyf?currencyId=EURtype=Morningstar&frequency=daily&startDate=2008-04-01&priceType=&outputType=COMPACTJSON&id=F00000YU62]2]0]FOITA$$ALL&applyTrackRecordExtension=true'\nr = requests.get(URL)\n# a= eval(r.content) Never user eval for online texts\ndf = pd.DataFrame(r.json())\n\nText: Answer to understand whats going on \nText: In my answer, I use a little trick that is not recommended all the times. First, I used request to get data from URL and then evaluate it using python eval function, as you can see its a nested list. But its a better idea to use r.json() \nText: df is a method that converts data to data frame using different method for example you can use nested list or json like data(like dictionaries) to create a Dataframe. \nText: But In most case results from web can become a pandas Dataframe using pd.read_csv it parse data using sep and lineterminator. \nAPI:\npandas.DataFrame\n","label":[[726,728,"Mention"],[1032,1048,"API"]],"Comments":[]}
{"id":60174,"text":"ID:50161242\nPost:\nText: TLDR: Regex capturing groups can be used for the suffix parameter. \nText: The suffix parameter tells wide_to_long which columns it should include in the transformation based on the suffix after the stub. \nText: The default behavior of wide to long assumes that your columns are labeled with numbers so for instance columns A1, A2, A3, A4 will work fine without specifying the suffix parameter, while Aone, Atwo, Athree, Afour will fail. \nText: As explained, it also has various other uses in the rare cases that your columns may be A1, A2, A3, A4, A100, and you don't want to actually include A100 because it isn't actually related to the other A# columns. \nText: Here are some illustrative examples. \nCode: import pandas as pd\ndf = pd.DataFrame({'id': [1,2], 'A_1': ['a', 'b'],\n                  'A_2': ['aa', 'bb'], 'A_3': ['aaa', 'bbb'],\n                  'A_person': ['Mike', 'Amy']})\n\npd.wide_to_long(df, stubnames='A_', i='id', j='num')\n#       A_person   A_\n#id num              \n#1  1       Mike    a\n#2  1        Amy    b\n#1  2       Mike   aa\n#2  2        Amy   bb\n#1  3       Mike  aaa\n#2  3        Amy  bbb\n\nText: Because the default behavior is to only consider numbers, 'A_person' was ignored. If you wanted to add that to the conversion, then you would use the suffix parameter. Let's tell it we want either numbers or words. \nCode: pd.wide_to_long(df, stubnames='A_', i='id', j='suffix', suffix='(\\d+|\\w+)')\n#             A_\n#id suffix         \n#1  1          a\n#2  1          b\n#1  2         aa\n#2  2         bb\n#1  3        aaa\n#2  3        bbb\n#1  person  Mike\n#2  person   Amy\n\nText: Now if your df starts without numeric suffixes, you can take care of that with the suffix parameter too. The default call will fail because it expects numbers, but telling it to look for words gives you what you want. \nCode: df = pd.DataFrame({'id': [1,2], 'A_one': ['a', 'b'],\n                  'A_two': ['aa', 'bb'], 'A_three': ['aaa', 'bbb'],\n                  'A_person': ['Mike', 'Amy']})\n\npd.wide_to_long(df, stubnames='A_', i='id', j='num')\n#Empty DataFrame\n#Columns: [A_three, A_person, A_one, A_two, A_]\n#Index: []\n\npd.wide_to_long(df, stubnames='A_', i='id', j='suffix', suffix='\\w+')\n#             A_\n#id suffix         \n#1  one        a\n#2  one        b\n#1  person  Mike\n#2  person   Amy\n#1  three    aaa\n#2  three    bbb\n#1  two       aa\n#2  two       bb\n\nText: And if you don't want to include A_person you can tell the suffix parameter to only include certain stubs. \nCode: pd.wide_to_long(df, stubnames='A_', i='id', j='num', suffix='(one|two|three)')\n#         A_person   A_\n#id num                \n#1  one       Mike    a\n#2  one        Amy    b\n#1  three     Mike  aaa\n#2  three      Amy  bbb\n#1  two       Mike   aa\n#2  two        Amy   bb\n\nText: Basically, if you can capture it with regex, you can pass it to suffix to use only the columns you want. \nAPI:\npandas.wide_to_long\n","label":[[125,137,"Mention"],[2906,2925,"API"]],"Comments":[]}
{"id":60175,"text":"ID:50203789\nPost:\nText: Index objects, including dataframe columns, have useful set-like methods, such as intersection and difference. \nText: For example, given dataframes train and test: \nCode: train_cols = train.columns\ntest_cols = test.columns\n\ncommon_cols = train_cols.intersection(test_cols)\ntrain_not_test = train_cols.difference(test_cols)\n\nAPI:\npandas.Index\n","label":[[24,29,"Mention"],[353,365,"API"]],"Comments":[]}
{"id":60176,"text":"ID:50254160\nPost:\nText: Use stack + get_dummies \nCode: df.assign(\n    D=df.stack().str.get_dummies(';').sum(level=0).gt(1).any(1).astype(int)\n)\n\n               A                B              C  D\n0   mom;dad;son;      sister;son;  yes;no;maybe;  1\n1           dad;  daughter;niece;       no;snow;  0\n2       son;dad;     cat;son;dad;  tree;dad;son;  1\n3  daughter;mom;           niece;       referee;  0\n4  dad;daughter;             cat;           dad;  1\n\nText: Details \nText: Notice that when we stack and get dummies, the interim result looks like this: \nCode:      cat  dad  daughter  maybe  mom  niece  no  referee  sister  snow  son  tree  yes\n0 A    0    1         0      0    1      0   0        0       0     0    1     0    0\n  B    0    0         0      0    0      0   0        0       1     0    1     0    0\n  C    0    0         0      1    0      0   1        0       0     0    0     0    1\n1 A    0    1         0      0    0      0   0        0       0     0    0     0    0\n  B    0    0         1      0    0      1   0        0       0     0    0     0    0\n  C    0    0         0      0    0      0   1        0       0     1    0     0    0\n2 A    0    1         0      0    0      0   0        0       0     0    1     0    0\n  B    1    1         0      0    0      0   0        0       0     0    1     0    0\n  C    0    1         0      0    0      0   0        0       0     0    1     1    0\n3 A    0    0         1      0    1      0   0        0       0     0    0     0    0\n  B    0    0         0      0    0      1   0        0       0     0    0     0    0\n  C    0    0         0      0    0      0   0        1       0     0    0     0    0\n4 A    0    1         1      0    0      0   0        0       0     0    0     0    0\n  B    1    0         0      0    0      0   0        0       0     0    0     0    0\n  C    0    1         0      0    0      0   0        0       0     0    0     0    0\n\nText: Where the prior columns are embedded in the second level of the index. So I want to sum over the first level in order to see how many times that word appears. \nText: That summation interim looks like: \nCode:    cat  dad  daughter  maybe  mom  niece  no  referee  sister  snow  son  tree  yes\n0    0    1         0      1    1      0   1        0       1     0    2     0    1\n1    0    1         1      0    0      1   1        0       0     1    0     0    0\n2    1    3         0      0    0      0   0        0       0     0    3     1    0\n3    0    0         1      0    1      1   0        1       0     0    0     0    0\n4    1    2         1      0    0      0   0        0       0     0    0     0    0\n\nText: Notice that we catch 'son' in row 1, 'dad' and 'son' in row 3 and so on. \nText: If it appears in more than 1 column (hence gt(1)) then I want to count it as a 1 (hence any(1).astype(int)). \nAPI:\npandas.Series.str.get_dummies\n","label":[[36,47,"Mention"],[2862,2891,"API"]],"Comments":[]}
{"id":60177,"text":"ID:50254564\nPost:\nText: The code section below contains two functions. df_sample() creates a dataframe of a desired size, a starting point and column names. The function multiJoin() takes a pre-defined list of dataframes and joins them using any method available for pandas Join. With that setup, all you need to do is run multiJoin(dfs = [df1, df2, df3], method = 'outer', names = ['Apple', 'Amazon', 'SomeOther']) to obtain your desired result for the sample dataframes. And I've added a a function newNames(df, sep, name1, name2) to take care of the hierarchical column names: \nCode: Apple                           Amazon\nOpen  High  Low  Close  Volume  Open  High  Low  Close  Volume\n\n# imports\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1234)\n\n# Function for reproducible data sample\ndef df_sample(start, rows, names):\n    ''' Function to create data sample with random returns\n\n    Parameters\n    ==========\n    rows : number of rows in the dataframe\n    names: list of names to represent assets\n\n    Example\n    =======\n\n    >>> returns(rows = 2, names = ['A', 'B'])\n\n                  A       B\n    2017-01-01  0.0027  0.0075\n    2017-01-02 -0.0050 -0.0024\n    '''\n    listVars= names\n    rng = pd.date_range(start, periods=rows, freq='D')\n    df_temp = pd.DataFrame(np.random.randint(-100,200,size=(rows, len(listVars))), columns=listVars) \n    df_temp = df_temp.set_index(rng)\n    #df_temp = df_temp \/ 10000\n\n    return df_temp\n\ncolNames = ['Open', 'High', 'Low', 'Close']\n\n# Reproducible dataframes\ndf1 = df_sample('1\/1\/2017', 150,colNames)\ndf2 = df_sample('2\/1\/2017', 150,colNames)\ndf3 = df_sample('3\/1\/2017', 150,colNames)\n\n#%%\n\ndef multiJoin(dfs, method, names):\n    \"\"\" Takes a pre-defined list of pandas dataframes and joins them\n        by the method specified and available in df.join().\n        This is a specific case for joining a bunch og OHLCV tables,\n        so column names will overlap. You should therefore specify \n        a list for each dataframe to provide unique column names.\n\n        Joining dataframes with different indexes will result in\n        omitted and \/ or missing data.\n\n        Using method = 'outer' will display missing values for mismatching dates.\n\n        Using method = 'inner' will keep only dates where all dataframes have values and omit\n                        all other.\n\n    \"\"\"\n\n    # Isolate a df to join all other dfs on\n    df_left = dfs[0]\n    df_left.columns = [names[0]+ '_' + col for col in df_left.columns]\n    df_other = dfs[1:]\n\n    # Manage names\n    names_other = names[1:]\n\n    # Loop through list of dataframes to join on the first one,\n    # and rename columns\n    counter = 0\n    for df in df_other:\n        df.columns = [names_other[counter] + '_' + col for col in df.columns]\n        df_left = df_left.join(df, how = method)\n        counter = counter + 1\n\n    return df_left\n\ndfJoined_outer = multiJoin(dfs = [df1, df2, df3], method = 'outer', names = ['Apple', 'Amazon', 'SomeOther'])\n\nText: Output: \nText: If you run dfJoined_inner = multiJoin(dfs = [df1, df2, df3], method = 'inner', names = ['Apple', 'Amazon', 'SomeOther']), you'll get: \nText: Addition after taking OP's comment into consideration: \nText: I've added a function that builds on pd.MultiIndex.from_arrays that will give you hierarchical column names that should make the dataframes look just like you requested. Simply run df_multi = newNames(df = dfJoined_inner, sep = '_'). \nCode: def newNames(df, sep, name1, name2):\n    \"\"\" Takes a single column index from a pandas dataframe,\n        splits the original titles by a specified separator,\n        and replaces the single column index with a \n        multi index. You can also assign names to levels of your new index\n    \"\"\"\n\n    df_temp = dfJoined_inner\n    sep = '_'\n\n    single = pd.Series(list(df_temp))\n    multi= single.str.split(sep, expand = True)\n\n    multiIndex = pd.MultiIndex.from_arrays((multi[0], multi[1]), names = (name1, name2))\n\n\n    df_new = pd.DataFrame(df_temp.values, index = df_temp.index, columns = multiIndex)\n\n    return(df_new)\n\n\ndf_multi = newNames(df = dfJoined_inner, sep = '_', name1 = 'Stock', name2 = 'Category')\n\nText: I'm using Spyder, so a screenshot of the dataframe in the variable explorer looks like this (notice the parenthesis in the column headers): \nText: But if you run print(df_multi.tail()), you'll see that the column headers look just like you requested: \nCode: #Output\nStock       Apple                 Amazon                    SomeOther            \nCategory    Open High Low Close   Open High  Low Close      Open High  Low  Close   \n2017-05-26   -92  140  47   -53    -73  -50  -94   -72        16  115   96     74\n2017-05-27   169  -34 -78   120     46  195   28   186        -9  102  -13    141\n2017-05-28   -98  -10  57   151    169  -17  148   150       -26  -43  -53     63\n2017-05-29     1   87  38     0     28   71   52   -57         6   86  179     -6\n2017-05-30   -31   52  33    63     46  149  -71   -30       -20  188  -34    -60\n\nAPI:\npandas.MultiIndex.from_arrays\n","label":[[3236,3261,"Mention"],[5012,5041,"API"]],"Comments":[]}
{"id":60178,"text":"ID:50256962\nPost:\nText: You can do this: \nCode: # Sorting indices so it's easier to read \ncounts.sort_index(inplace=True)\n\nsns.barplot(x = counts.index, y = counts)\nplt.ylabel('counts')\n\nText: Note that using pd.Series.plot gives a very similar plot: counts.plot('bar') or counts.plot.bar() \nAPI:\npandas.Series.plot\n","label":[[209,223,"Mention"],[297,315,"API"]],"Comments":[]}
{"id":60179,"text":"ID:50263847\nPost:\nText: You can use pd.melt to unpivot your dataframe. In this particular case, you need to elevate your index to a column: \nCode: res = pd.melt(df.reset_index(), id_vars=['index'])\n\nText: Result: \nCode: print(res)\n\n    index      variable         value\n0   count  TradesBefore  31540.000000\n1    mean  TradesBefore     39.151712\n2     std  TradesBefore    130.948917\n3     min  TradesBefore  -1611.000000\n4     25%  TradesBefore     29.000000\n5     50%  TradesBefore     74.000000\n6     75%  TradesBefore     99.000000\n7     max  TradesBefore    184.000000\n8   count   TradesAfter   1000.000000\n9    mean   TradesAfter     42.216000\n10    std   TradesAfter    143.153156\n11    min   TradesAfter  -1371.000000\n12    25%   TradesAfter     34.000000\n13    50%   TradesAfter     79.000000\n14    75%   TradesAfter    109.000000\n15    max   TradesAfter    179.000000\n16  count     Predicted   1000.000000\n17   mean     Predicted     90.144811\n18    std     Predicted      1.345089\n19    min     Predicted     88.234987\n20    25%     Predicted     89.052902\n21    50%     Predicted     89.979200\n22    75%     Predicted     91.127657\n23    max     Predicted     93.915568\n24  count        Change     -0.968294\n25   mean        Change      0.078267\n26    std        Change      0.093198\n27    min        Change     -0.148976\n28    25%        Change      0.172414\n29    50%        Change      0.067568\n30    75%        Change      0.101010\n31    max        Change     -0.027174\n\nAPI:\npandas.melt\n","label":[[36,43,"Mention"],[1492,1503,"API"]],"Comments":[]}
{"id":60180,"text":"ID:50273481\nPost:\nText: Credit to zipa for getting the dates correct. I've edited my post to correct my mistake. \nText: Set the index then use fromproduct to produce the Cartesian product of values. I also use fill_value=0 to fill in those missing values. \nCode: d = df.set_index(['date', 'group'])\nmidx = pd.MultiIndex.from_product(\n    [pd.date_range(df.date.min(), df.date.max()), df.group.unique()],\n    names=d.index.names\n)\nd.reindex(midx, fill_value=0).reset_index()\n\n         date  group  value\n0  2010-01-01      1      1\n1  2010-01-01      2      5\n2  2010-01-02      1      2\n3  2010-01-02      2      0\n4  2010-01-03      1      3\n5  2010-01-03      2      6\n6  2010-01-04      1      0\n7  2010-01-04      2      0\n8  2010-01-05      1      0\n9  2010-01-05      2      0\n10 2010-01-06      1      4\n11 2010-01-06      2      0\n\nText: Or \nCode: d = df.set_index(['date', 'group'])\nmidx = pd.MultiIndex.from_product(\n    [pd.date_range(df.date.min(), df.date.max()), df.group.unique()],\n    names=d.index.names\n)\nd.reindex(midx).reset_index()\n\n         date  group  value\n0  2010-01-01      1    1.0\n1  2010-01-01      2    5.0\n2  2010-01-02      1    2.0\n3  2010-01-02      2    NaN\n4  2010-01-03      1    3.0\n5  2010-01-03      2    6.0\n6  2010-01-04      1    NaN\n7  2010-01-04      2    NaN\n8  2010-01-05      1    NaN\n9  2010-01-05      2    NaN\n10 2010-01-06      1    4.0\n11 2010-01-06      2    NaN\n\nText: Another dance we could do is a cleaned up version of OP's attempt. Again I use fill_value=0 to fill in missing values. We could leave that out to produce the NaN. \nCode: df.set_index(['date', 'group']) \\\n  .unstack(fill_value=0) \\\n  .asfreq('D', fill_value=0) \\\n  .stack().reset_index()\n\n         date  group  value\n0  2010-01-01      1      1\n1  2010-01-01      2      5\n2  2010-01-02      1      2\n3  2010-01-02      2      0\n4  2010-01-03      1      3\n5  2010-01-03      2      6\n6  2010-01-04      1      0\n7  2010-01-04      2      0\n8  2010-01-05      1      0\n9  2010-01-05      2      0\n10 2010-01-06      1      4\n11 2010-01-06      2      0\n\nText: Or \nCode: df.set_index(['date', 'group']) \\\n  .unstack() \\\n  .asfreq('D') \\\n  .stack(dropna=False).reset_index()\n\n         date  group  value\n0  2010-01-01      1    1.0\n1  2010-01-01      2    5.0\n2  2010-01-02      1    2.0\n3  2010-01-02      2    NaN\n4  2010-01-03      1    3.0\n5  2010-01-03      2    6.0\n6  2010-01-04      1    NaN\n7  2010-01-04      2    NaN\n8  2010-01-05      1    NaN\n9  2010-01-05      2    NaN\n10 2010-01-06      1    4.0\n11 2010-01-06      2    NaN\n\nAPI:\npandas.MultiIndex.from_product\n","label":[[143,154,"Mention"],[2568,2598,"API"]],"Comments":[]}
{"id":60181,"text":"ID:50278310\nPost:\nText: get_dummies to the rescue. \nCode: pd.get_dummies(df.set_index(['A', 'B'])).reset_index()\n\n        A   B  C_003  C_012  C_016  C_020  C_053\n0  000001  1A      1      0      0      0      0\n1  000001  1A      0      1      0      0      0\n2  000001  1C      0      0      1      0      0\n3  000001  1D      1      0      0      0      0\n4  000002  1A      0      0      0      1      0\n5  000002  1A      0      1      0      0      0\n6  000003  1D      0      0      0      0      1\n\nText: Or... (thanks Wen for the reminder) \nCode: pd.get_dummies(df.set_index(['A', 'B'])).sum(level=[0, 1]).reset_index()\n\n        A   B  C_003  C_012  C_016  C_020  C_053\n0  000001  1A      1      1      0      0      0\n1  000001  1C      0      0      1      0      0\n2  000001  1D      1      0      0      0      0\n3  000002  1A      0      1      0      1      0\n4  000003  1D      0      0      0      0      1\n\nAPI:\npandas.get_dummies\n","label":[[24,35,"Mention"],[930,948,"API"]],"Comments":[]}
{"id":60182,"text":"ID:50307309\nPost:\nText: Instead of the for loop, you can use fillna with the shifted Series for the close price. \nCode: price3['open price'].fillna(price3['close price'].shift(1), inplace=True)\n\nText: This is vectorized and so should be far faster than your for loop. \nText: Note I am assuming that price2 and price3 have the same length and you may as well be iterating over price3 in your loop. \nAPI:\npandas.Series.fillna\n","label":[[61,67,"Mention"],[403,423,"API"]],"Comments":[]}
{"id":60183,"text":"ID:50321211\nPost:\nText: You can basically achieve the second DataFrame with groupby \nCode: df2 = df1.groupby(['id', 'revenue']).id_name.apply(list).reset_index()\n\n  id  revenue           id_name\n0  a       65  [name_a, name_b]\n1  a       70  [name_a, name_b]\n2  a      121  [name_a, name_b]\n\nText: For the third DataFrame you can just apply pd.Series to the lists you created above. Here's a solution where you don't need to know how many columns you'll wind up with in the end. It will rename up to 10 properly. \nCode: import pandas as pd\nimport numpy as np\n\ndf3 = pd.concat([df2[['id', 'revenue']], df2['id_name'].apply(pd.Series)], axis=1)\ndf3.rename(columns=dict((item, 'id_name'+str(item+1)) for item in np.arange(0,10,1)), inplace=True)\n\n  id  revenue id_name1 id_name2\n0  a       65   name_a   name_b\n1  a       70   name_a   name_b\n2  a      121   name_a   name_b\n\nAPI:\npandas.Series\n","label":[[341,350,"Mention"],[878,891,"API"]],"Comments":[]}
{"id":60184,"text":"ID:50321762\nPost:\nText: You can create a list of series and then use concat to combine them into a single dataframe. \nText: The solution is functionally identical to @DyZ, but laid out differently. \nCode: series_list = [df.set_index('Time'+str(i))['Sensor'+str(i)].dropna() \\\n               for i in range(1, int(len(df.columns)\/2) + 1)]\n\nres = pd.concat(series_list, axis=1)\\\n        .rename_axis('Time').reset_index()\n\nText: Setup \nCode: df = pd.DataFrame({'Time1': [0, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, np.nan],\n                   'Sensor1': ['x', 'x', 'x', 'x', 'x', 'x', np.nan, np.nan, np.nan, np.nan, np.nan],\n                   'Time2': [0, 2, 4, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n                   'Sensor2': ['y', 'y', 'y', np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n                   'Time3': [0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5],\n                   'Sensor3': ['z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']})\n\nText: Result \nCode: print(res)\n\n    Time Sensor1 Sensor2 Sensor3\n0    0.0       x       y       z\n1    0.5     NaN     NaN       z\n2    1.0       x     NaN       z\n3    1.5     NaN     NaN       z\n4    2.0       x       y       z\n5    2.5     NaN     NaN       z\n6    3.0       x     NaN       z\n7    3.5     NaN     NaN       z\n8    4.0       x       y       z\n9    4.5     NaN     NaN       z\n10   5.0       x     NaN       z\n\nAPI:\npandas.concat\n","label":[[69,75,"Mention"],[1440,1453,"API"]],"Comments":[]}
{"id":60185,"text":"ID:50330014\nPost:\nText: Assuming you have a pandas dataframe, one option is to use crosstab to return another dataframe: \nCode: import pandas as pd\n\ndf = pd.read_csv('file.csv')\nres = pd.crosstab(df['X'], df['Y'])\n\nprint(res)\n\nY  0  1\nX      \n0  3  7\n1  1  3\n\nText: A collections.Counter solution is also possible if a dictionary result is required: \nCode: res = Counter(zip(df['X'].values, df['Y'].values))\n\nAPI:\npandas.crosstab\n","label":[[83,91,"Mention"],[414,429,"API"]],"Comments":[]}
{"id":60186,"text":"ID:50416180\nPost:\nText: Can you pass in the format argument to pandas.to_datetime()? Something like: pandas.to_datetime(date_column, format='%d%b%Y') \nText: If you can get the dates as strings, then any date-parsing function that takes strptime formatting should work with the pattern %d%b%Y: \nCode: >>> datetime.datetime.strptime('12FEB1993', '%d%b%Y')\ndatetime.datetime(1993, 2, 12, 0, 0)\n\nText: EDIT: \nText: It looks like pandas.to_datetime() results in Timeostamp objects, which, due to resolution limits are limited to ~584 years, or a max year of 2262. \nText: Because you have date-string that extend beyond that, you could read the column in as a string, and then call apply to convert the values into date objects: \nCode: import datetime\nmy_df['date'] = my_df['date_text'].apply(lambda x: datetime.datetime.strptime(x, '%d%b%Y').date())\n\nAPI:\npandas.Timestamp\n","label":[[457,467,"Mention"],[851,867,"API"]],"Comments":[]}
{"id":60187,"text":"ID:50448429\nPost:\nText: you can: \nText: read all files into a Series using pathlib.Path('\/path\/to\/data_dir').glob('*.csv') group that series by first part of filename read all CSV files belonging to each group concatenate them save concatenated DF to CSV file \nCode: from pathlib import Path\n\np = Path(r'\/path\/to\/data\/directory')\n\nfiles = pd.Series([f.name for f in p.glob('*.csv')])\n\n(files.groupby(files.str.split('-').str[0])\n      .apply(lambda g: pd.concat([pd.read_csv(p \/ f) for f in g], ignore_index=True)\n                         .to_csv(p \/ (g.name.split('-')[0] + '-ALL.csv'), index=False)))\n\nAPI:\npandas.Series\n","label":[[62,68,"Mention"],[609,622,"API"]],"Comments":[]}
{"id":60188,"text":"ID:50453636\nPost:\nText: str \nCode: df.Information.str[2:3]\n\n0    [C]\n1    [C]\n2    [H]\n3    [H]\n4    [H]\n5    [H]\nName: Information, dtype: object\n\nText: With assign \nCode: df.assign(Information=df.Information.str[2:3])\n\n   Index Information\n0      1         [C]\n1      2         [C]\n2      3         [H]\n3      4         [H]\n4      5         [H]\n5      6         [H]\n\nText: comprehension per @coldspeed \nCode: df.assign(Information=[l[2:3] for l in df.Information.tolist()])\n\n   Index Information\n0      1         [C]\n1      2         [C]\n2      3         [H]\n3      4         [H]\n4      5         [H]\n5      6         [H]\n\nAPI:\npandas.Series.str\n","label":[[24,27,"Mention"],[630,647,"API"]],"Comments":[]}
{"id":60189,"text":"ID:50457906\nPost:\nText: This is one solution. You can also use pd.merge to add 2 new columns to data and perform the equivalent mapping. \nCode: # create series mappings from info\ns_lat = info.set_index('station')['latitude']\ns_lon = info.set_index('station')['latitude']\n\n# calculate Boolean mask on year\nmask = data['year'] == '2018'\n\n# apply mappings, if no map found use fillna to retrieve original data\ndata.loc[mask, 'latitude'] = data.loc[mask, 'station'].map(s_lat)\\\n                                 .fillna(data.loc[mask, 'latitude'])\n\ndata.loc[mask, 'longitude'] = data.loc[mask, 'station'].map(s_lon)\\\n                                  .fillna(data.loc[mask, 'longitude'])\n\nAPI:\npandas.merge\n","label":[[63,71,"Mention"],[689,701,"API"]],"Comments":[]}
{"id":60190,"text":"ID:50468140\nPost:\nText: Option 1 \nText: You can let Python's max do the work and use Series to hold the results \nCode: readings = [allReadings1, allReadings2, allReadings3, allReadings4, allReadings5,\n            allReadings6, allReadings7, allReadings8, allReadings9, allReadings10]\n\ns = pd.Series(dict(zip(allFeatures, map(max, zip(*readings)))))\ns[aoiFeatures]\n\n181    0.29\n843    0.30\n849    0.29\ndtype: float64\n\nText: Option 2 \nText: Or leverage Numpy \nCode: readings = [allReadings1, allReadings2, allReadings3, allReadings4, allReadings5,\n            allReadings6, allReadings7, allReadings8, allReadings9, allReadings10]\n\ns = pd.Series(np.max(readings, 0), allFeatures)\ns[aoiFeatures]\n\n181    0.29\n843    0.30\n849    0.29\ndtype: float64\n\nText: If you needed to update the array of maximums with a new reading \nCode: allReadings11 =[0.13, 0.03, 0.30, 0.13, 0.30, 0.30, 0.28, 0.12, 0.19, 0.22]\ns[:] = np.maximum(s, allReadings11)\n\ns[aoiFeatures]\n\n181    0.29\n843    0.30\n849    0.29\ndtype: float64\n\nAPI:\npandas.Series\n","label":[[85,91,"Mention"],[1010,1023,"API"]],"Comments":[]}
{"id":60191,"text":"ID:50473175\nPost:\nText: rename \nText: pass a callable that gets applied to each index value \nCode: df.rename('{}_x'.format)\n\n     x\n1_x  1\n2_x  2\n3_x  3\n\nText: set_index \nCode: df.set_index(df.index.astype(str) + '_x')\n\n     x\n1_x  1\n2_x  2\n3_x  3\n\nAPI:\npandas.DataFrame.rename\n","label":[[24,30,"Mention"],[254,277,"API"]],"Comments":[]}
{"id":60192,"text":"ID:50501889\nPost:\nText: tl;dr: \nText: concat and append currently sort the non-concatenation index (e.g. columns if you're adding rows) if the columns don't match. In pandas 0.23 this started generating a warning; pass the parameter sort=True to silence it. In the future the default will change to not sort, so it's best to specify either sort=True or False now, or better yet ensure that your non-concatenation indices match. \nText: The warning is new in pandas 0.23.0: \nText: In a future version of pandas pandas.concat() and DataFrame.append() will no longer sort the non-concatenation axis when it is not already aligned. The current behavior is the same as the previous (sorting), but now a warning is issued when sort is not specified and the non-concatenation axis is not aligned, link. \nText: More information from linked very old github issue, comment by smcinerney : \nText: When concat'ing DataFrames, the column names get alphanumerically sorted if there are any differences between them. If they're identical across DataFrames, they don't get sorted. This sort is undocumented and unwanted. Certainly the default behavior should be no-sort. \nText: After some time the parameter sort was implemented in pd.concat and DataFrame.append: \nText: sort : boolean, default None Sort non-concatenation axis if it is not already aligned when join is 'outer'. The current default of sorting is deprecated and will change to not-sorting in a future version of pandas. Explicitly pass sort=True to silence the warning and sort. Explicitly pass sort=False to silence the warning and not sort. This has no effect when join='inner', which already preserves the order of the non-concatenation axis. \nText: So if both DataFrames have the same columns in the same order, there is no warning and no sorting: \nCode: df1 = pd.DataFrame({\"a\": [1, 2], \"b\": [0, 8]}, columns=['a', 'b'])\ndf2 = pd.DataFrame({\"a\": [4, 5], \"b\": [7, 3]}, columns=['a', 'b'])\n\nprint (pd.concat([df1, df2]))\n   a  b\n0  1  0\n1  2  8\n0  4  7\n1  5  3\n\ndf1 = pd.DataFrame({\"a\": [1, 2], \"b\": [0, 8]}, columns=['b', 'a'])\ndf2 = pd.DataFrame({\"a\": [4, 5], \"b\": [7, 3]}, columns=['b', 'a'])\n\nprint (pd.concat([df1, df2]))\n   b  a\n0  0  1\n1  8  2\n0  7  4\n1  3  5\n\nText: But if the DataFrames have different columns, or the same columns in a different order, pandas returns a warning if no parameter sort is explicitly set (sort=None is the default value): \nCode: df1 = pd.DataFrame({\"a\": [1, 2], \"b\": [0, 8]}, columns=['b', 'a'])\ndf2 = pd.DataFrame({\"a\": [4, 5], \"b\": [7, 3]}, columns=['a', 'b'])\n\nprint (pd.concat([df1, df2]))\n\nText: FutureWarning: Sorting because non-concatenation axis is not aligned. \nCode:    a  b\n0  1  0\n1  2  8\n0  4  7\n1  5  3\n\nprint (pd.concat([df1, df2], sort=True))\n   a  b\n0  1  0\n1  2  8\n0  4  7\n1  5  3\n\nprint (pd.concat([df1, df2], sort=False))\n   b  a\n0  0  1\n1  8  2\n0  7  4\n1  3  5\n\nText: If the DataFrames have different columns, but the first columns are aligned - they will be correctly assigned to each other (columns a and b from df1 with a and b from df2 in the example below) because they exist in both. For other columns that exist in one but not both DataFrames, missing values are created. \nText: Lastly, if you pass sort=True, columns are sorted alphanumerically. If sort=False and the second DafaFrame has columns that are not in the first, they are appended to the end with no sorting: \nCode: df1 = pd.DataFrame({\"a\": [1, 2], \"b\": [0, 8], 'e':[5, 0]}, \n                    columns=['b', 'a','e'])\ndf2 = pd.DataFrame({\"a\": [4, 5], \"b\": [7, 3], 'c':[2, 8], 'd':[7, 0]}, \n                    columns=['c','b','a','d'])\n\nprint (pd.concat([df1, df2]))\n\nText: FutureWarning: Sorting because non-concatenation axis is not aligned. \nCode:    a  b    c    d    e\n0  1  0  NaN  NaN  5.0\n1  2  8  NaN  NaN  0.0\n0  4  7  2.0  7.0  NaN\n1  5  3  8.0  0.0  NaN\n\nprint (pd.concat([df1, df2], sort=True))\n   a  b    c    d    e\n0  1  0  NaN  NaN  5.0\n1  2  8  NaN  NaN  0.0\n0  4  7  2.0  7.0  NaN\n1  5  3  8.0  0.0  NaN\n\nprint (pd.concat([df1, df2], sort=False))\n\n   b  a    e    c    d\n0  0  1  5.0  NaN  NaN\n1  8  2  0.0  NaN  NaN\n0  7  4  NaN  2.0  7.0\n1  3  5  NaN  8.0  0.0\n\nText: In your code: \nCode: placement_by_video_summary = placement_by_video_summary.drop(placement_by_video_summary_new.index)\n                                                       .append(placement_by_video_summary_new, sort=True)\n                                                       .sort_index()\n\nAPI:\npandas.concat\n","label":[[1215,1224,"Mention"],[4474,4487,"API"]],"Comments":[]}
{"id":60193,"text":"ID:50513447\nPost:\nText: I think you're overly complicating things. You can just groupby and rank to the vals columns. This returns a Series of the same length of your original df so you can just set the column to this. \nCode: df['vals'] = df.groupby(['date', 'category']).vals.rank()\n\n  category       date  info1  info2  vals\n0        A 2015-01-01  BJWYE    990   2.0\n1        A 2015-01-01  ISQES    475   1.0\n2        A 2015-01-01  KDEKE    214   3.0\n3        B 2015-01-01  TFOXR    203   2.0\n4        B 2015-01-01  HKTNF    992   1.0\n\nAPI:\npandas.Series\n","label":[[133,139,"Mention"],[543,556,"API"]],"Comments":[]}
{"id":60194,"text":"ID:50605964\nPost:\nText: You can use searchsorted \nText: Copy \nCode: labels = np.array(list('NLMH'))\nbreaks = np.array([1, 10, 50])\npd.DataFrame(\n    labels[breaks.searchsorted(df.values)].reshape(df.shape),\n    df.index, df.columns)\n\n   A  B  C\nA  N  L  M\nB  N  H  L\nC  L  M  N\n\nText: In Place \nCode: labels = np.array(list('NLMH'))\nbreaks = np.array([1, 10, 50])\ndf[:] = labels[breaks.searchsorted(df.values)].reshape(df.shape)\ndf\n\n   A  B  C\nA  N  L  M\nB  N  H  L\nC  L  M  N\n\nText: Chained pure Pandas approach with pd.DataFrame.mask \nText: Deprecated since version 0.21 \nCode: df.mask(df.lt(1), 'N').mask(df.gt(1) & df.lt(10), 'L') \\\n  .mask(df.gt(10) & df.lt(50), 'M').mask(df.gt(50), 'H')\n\n   A  B  C\nA  N  L  M\nB  N  H  L\nC  L  M  N\n\nAPI:\npandas.DataFrame.mask\n","label":[[518,535,"Mention"],[745,766,"API"]],"Comments":[]}
{"id":60195,"text":"ID:50663595\nPost:\nText: Setup \nCode: df = pd.DataFrame(dict(A=[1, 2, 3], B=list('XYZ')))\ndf.A = df.A.astype(np.int16)\ndf.B = pd.Categorical(df.B)\n\ndf\n\n   A  B\n0  1  X\n1  2  Y\n2  3  Z\n\nCode: df.dtypes\n\nA       int16\nB    category\ndtype: object\n\nText: You can use tow_hdf \nText: Save to hdf Use format='table' because NotImplementedError from categorical \nCode: df.to_hdf('small.h5', 'this_df', format='table')\n\nText: Read back in \nCode: df1 = pd.read_hdf('small.h5', 'this_df')\n\ndf1\n\n   A  B\n0  1  X\n1  2  Y\n2  3  Z\n\nText: Check dtypes \nCode: df.dtypes\n\nA       int16\nB    category\ndtype: object\n\nText: Check equvivalence \nCode: df1.equals(df)\n\nTrue\n\nText: Use feather \nText: You might need to install the feather-format \nCode: conda install feather-format -c conda-forge\n\nText: Or \nCode: pip install -U feather-format\n\nText: Then \nCode: df.to_feather('small.feather')\n\ndf1 = pd.read_feather('small.feather')\n\ndf1.equals(df)\n\nTrue\n\nText: The advantages of feather are that you should also be able to read them in R and reading and writing should be very fast. \nText: Crude time comparison \nCode: %timeit pd.read_feather('small.feather')\n%timeit pd.read_hdf('small.h5', 'this_df')\n\n842 s  11.1 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n23.2 ms  479 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n\nAPI:\npandas.DataFrame.to_hdf\n","label":[[262,269,"Mention"],[1328,1351,"API"]],"Comments":[]}
{"id":60196,"text":"ID:50713209\nPost:\nText: This error is due to the fact that the FeatureUnion expects a 2-d array from each of its parts. \nText: Now the first two parts of your FeatureUnion:- 'numericals' and 'categoricals' are correctly sending 2-d data of shape (n_samples, n_features). \nText: n_samples = 3 in your example data. n_features will depend on individual parts (like OneHotEncoder will change them in 2nd part, but will be 1 in first part). \nText: But the third part 'word2vec' returns a pd.Series object which have the 1-d shape (3,). FeatureUnion takes this a shape (1, 3) by default and hence the complains that it does not match other blocks. \nText: So you need to correct that shape. \nText: Now even if you simply do a reshape() at the end and change it to shape (3,1), your code will not run, because the internal contents of that array are lists from your word2vec dict, which are not transformed correctly to a 2-d array. Instead it will become a array of lists. \nText: Change the w2vTransformer to correct the error: \nCode: class w2vTransformer(TransformerMixin):\n    ...\n    ...\n    def transform(self, X, y=None):\n        return np.array([np.array(vv) for vv in X['word'].apply(self.wv)])\n\nText: And after that the pipeline will work. \nAPI:\npandas.Series\n","label":[[484,493,"Mention"],[1248,1261,"API"]],"Comments":[]}
{"id":60197,"text":"ID:50747014\nPost:\nText: Unfortunately, query uses eval (which supports arithmetic operations, so these operators are not allowed in column names), so there isn't a workaround for your desired outcome. \nText: You have a couple options. First, you can simply replace the dashes with something else, like an underscore: \nCode: df.columns = [i.replace('-', '_') for i in df.columns]\ndf.query('SWISS_PROT_ID.notnull() & idx_filter')\n\nText: Or you could simply index based on your condition: \nCode: df.loc[df['SWISS-PROT-ID'].notnull() & df.idx_filter]\n\nText: Both produce (although one has renamed columns): \nCode:       BLATTNER-ID  NAME SWISS-PROT-ID  idx_filter\nG6335       b0608  ybdR        P77316        True\nG6866       b1615  uidC        Q47706        True\n\nAPI:\npandas.eval\n","label":[[50,54,"Mention"],[766,777,"API"]],"Comments":[]}
{"id":60198,"text":"ID:50751667\nPost:\nText: You can do this in 2 steps: \nText: Expand \/ flatten your dataframe with a series of comma separated strings. Use pd.crosstab to tabulate your counts. \nText: Here's an example assuming you have performed your merge and the result is df: \nCode: import numpy as np\nfrom itertools import chain\n\n# split by comma to form series of lists\ntag_split = df['tags'].str.split(',')\n\n# create expanded dataframe\ndf_full = pd.DataFrame({'customer': np.repeat(df['customer'], tag_split.map(len)),\n                        'tags': list(chain.from_iterable(tag_split))})\n\n# use pd.crosstab for result\nres = pd.crosstab(df_full['customer'], df_full['tags'])\n\nprint(res)\n\ntags       filled   sprinkles  chocolate  glazed\ncustomer                                        \nA               1           2          3       0\nB               2           0          2       0\nC               0           1          1       2\n\nAPI:\npandas.crosstab\n","label":[[137,148,"Mention"],[927,942,"API"]],"Comments":[]}
{"id":60199,"text":"ID:50829378\nPost:\nText: You are calling the Series that is returned from the mean method. This doesnt make any sense and since Series doesn't implement __call__ it fails with an error. \nText: What you probably want to do is remove the parens and just return the Series itself. \nCode: def CalculateEMA(window):\n\n    sma = Close.rolling(window, min_periods=window).mean()[:window]\n    rest = Close[window:]\n    EMA_window=pd.concat([sma, rest]).ewm(span=window, adjust=False).mean()\n\n    return EMA_window # <-- changed here\nCalculateEMA(60)\n\nAPI:\npandas.Series\n","label":[[44,50,"Mention"],[546,559,"API"]],"Comments":[]}
{"id":60200,"text":"ID:50843253\nPost:\nText: You are looking for pd.merge_asof \nText: It allows you to join two DataFrames on keys that are not exact. In this case, you want to use it with direciton = nearest to match based on the two closest timestamps. \nCode: import pandas as pd\n\npd.merge_asof(df_obs, df_cont[['lat', 'long', 'alt', 'time']], \n              left_on='ts', right_on='time', direction='nearest')\n\nText: Outputs: \nCode:    counter  depth  latdeg   latmin     latdec  londeg   lonmin                  ts        lat        long     alt                time\n0   100001  21.11      72  18.5412  72.309020    -148 -47.0710 2018-03-20 16:21:49  72.310603 -148.790624 -8.3746 2018-03-20 16:21:45\n1   100002  22.14      72  18.5448  72.309080    -148 -47.0785 2018-03-20 16:22:07  72.310599 -148.790625 -7.5989 2018-03-20 16:22:00\n2   100003  45.30      72  18.5396  72.308993    -148 -47.0936 2018-03-20 16:34:38  72.310599 -148.790625 -7.5989 2018-03-20 16:22:00\n3   100004  45.31      72  18.5360  72.308933    -148 -47.0974 2018-03-20 16:36:31  72.310599 -148.790625 -7.5989 2018-03-20 16:22:00\n\nText: You can then drop the time column if you don't want it, I just left it in to make it clear how the merge worked. \nAPI:\npandas.merge_asof\n","label":[[44,57,"Mention"],[1211,1228,"API"]],"Comments":[]}
{"id":60201,"text":"ID:50861110\nPost:\nText: I think all you need is a call to pd.DataFrame.swaplevel after the initial pivot, followed by sorting the columns to group the top level (level=0): \nCode: # Assuming df holds the result of the pivot\ndf.swaplevel(0, 1, axis=1).sort_index(axis=1)\n\nAPI:\npandas.DataFrame.swaplevel\n","label":[[58,80,"Mention"],[275,301,"API"]],"Comments":[]}
{"id":60202,"text":"ID:50944337\nPost:\nText: Using totd followed by some messy string formatting to convert days into hours: \nCode: def formatter(x):\n    x = str(x)\n    return str(int(x[-8:-6])+int(x.split('days')[0])*24).zfill(2) + x[-6:]\n\ndf['TD'] = pd.to_timedelta(df['X'].fillna(0).astype(int), unit='s')\\\n             .apply(formatter)\n\nprint(df)\n\n        X   Y    Z        TD\n0   21000   0  0.0  05:50:00\n1     NaN   2  2.0  00:00:00\n2   40000  yy  NaN  11:06:40\n3     NaN   3  3.0  00:00:00\n4   49000  yy  4.0  13:36:40\n5  100000  yy  NaN  27:46:40\n\nAPI:\npandas.to_timedelta\n","label":[[30,34,"Mention"],[541,560,"API"]],"Comments":[]}
{"id":60203,"text":"ID:50949823\nPost:\nText: You can use pd.melt after elevating your index to a series. You may then sort and rename columns \/ rows if necessary. \nCode: res = pd.melt(df.assign(index=df.index), id_vars=['index'])\n\nprint(res)\n\n   index variable value\n0      1        a     l\n1      2        a     n\n2      1        b     m\n3      2        b     o\n\nText: A more verbose, but equivalent, version: \nCode: df = df.reset_index()    \nres = pd.melt(df, id_vars=['index'])\n\nAPI:\npandas.melt\n","label":[[36,43,"Mention"],[466,477,"API"]],"Comments":[]}
{"id":60204,"text":"ID:50952973\nPost:\nText: Using pd.melt after elevating your index to a series: \nCode: res = pd.melt(df.assign(index=df.index), id_vars='index',\n              value_name='stat', var_name='type_stat')\\\n        .set_index('index')\n\nprint(res)\n\n      type_stat   stat\nindex                 \nid1       click     55\nid2       click     64\nid3       click     85\nid4       click      9\nid1      length    112\nid2      length    214\nid3      length     52\nid4      length     88\nid1       views  10000\nid2       views  50000\nid3       views  25000\nid4       views   5000\n\nAPI:\npandas.melt\n","label":[[30,37,"Mention"],[568,579,"API"]],"Comments":[]}
{"id":60205,"text":"ID:50953338\nPost:\nText: This is not a bug. What's happening is ipvot_table is calculating the Cartesian product of grouper categories. \nText: This is a known intended behaviour. In Pandas v0.23.0, we saw the introduction of the observed argument for pandas.groupby. Setting observed=True only includes observed combinations; it is False by default. This argument has not yet now been rolled out to related methods such as pandas.pivot_table. In my opinion, it should be. \nText: But now let's see what this means. We can use an example dataframe and see what happens when we print the result. \nText: Setup \nText: We make the dataframe substantially smaller: \nCode: import pandas as pd\n\nn = 10\n\ndf = pd.DataFrame({'t1': [\"a\",\"b\",\"c\"]*n, 't2': [\"x\",\"y\",\"z\"]*n,\n                   'i1': list(range(int(n\/2)))*6, 'i2': list(range(int(n\/2)))*6,\n                   'dummy':0})\n\nText: Without categories \nText: This is likely what you are looking for. Unobserved combinations of categories are not represented in your pivot table. \nCode: piv = df.pivot_table(values='dummy', index=['i1','i2'], columns=['t1','t2'])\nprint(piv)\n\nt1     a  b  c\nt2     x  y  z\ni1 i2         \n0  0   0  0  0\n1  1   0  0  0\n2  2   0  0  0\n3  3   0  0  0\n4  4   0  0  0\n\nText: With categories \nText: With categories, all combinations of categories, even unobserved combinations, are accounted for in the result. This is expensive computationally and memory-hungry. Moreover, the dataframe is dominated by NaN from unobserved combinations. It's probably not what you want. \nText: Update: you can now set the observed parameter to True to only show observed values for categorical groupers. \nCode: d2 = df.copy()\nd2.t1 = d2.t1.astype('category')\nd2.t2 = d2.t2.astype('category')\n\npiv2 = d2.pivot_table(values='dummy', index=['i1','i2'], columns=['t1','t2'])\nprint(piv2)\n\nt1       a           b            c         \nt2       x   y   z   x    y   z   x   y    z\ni1 i2                                       \n0  0   0.0 NaN NaN NaN  0.0 NaN NaN NaN  0.0\n   1   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   2   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   3   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   4   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n1  0   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   1   0.0 NaN NaN NaN  0.0 NaN NaN NaN  0.0\n   2   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   3   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   4   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n2  0   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   1   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   2   0.0 NaN NaN NaN  0.0 NaN NaN NaN  0.0\n   3   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   4   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n3  0   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   1   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   2   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   3   0.0 NaN NaN NaN  0.0 NaN NaN NaN  0.0\n   4   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n4  0   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   1   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   2   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   3   NaN NaN NaN NaN  NaN NaN NaN NaN  NaN\n   4   0.0 NaN NaN NaN  0.0 NaN NaN NaN  0.0\n\nAPI:\npandas.pivot_table\n","label":[[63,74,"Mention"],[3104,3122,"API"]],"Comments":[]}
{"id":60206,"text":"ID:50967405\nPost:\nText: To adjust the colours of your boxes in pandas.boxplot, you have to adjust your code slightly. First of all, you have to tell boxplot to actually fill the boxes with a colour. You do this by specifying patch_artist = True, as is documented here. However, it appears that you cannot specify a colour (default is blue) -- please anybody correct me if I'm wrong. This means you have to change the colour afterwards. Luckily pandas.boxplot offers an easy option to get the artists in the boxplot as return value by specifying return_type = 'both' see here for an explanation. What you get is a Series with keys according to your DataFrame columns and values that are tuples containing the Axes instances on which the boxplots are drawn and the actual elements of the boxplots in a dictionary. I think the code is pretty self-explanatory: \nCode: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import PathPatch\n\ndf = pd.DataFrame(np.random.rand(140, 4), columns=['A', 'B', 'C', 'D'])\n\ndf['models'] = pd.Series(np.repeat(['model1','model2', 'model3', 'model4',     'model5', 'model6', 'model7'], 20))\n\nbp_dict = df.boxplot(\n    by=\"models\",layout=(4,1),figsize=(6,8),\n    return_type='both',\n    patch_artist = True,\n)\n\ncolors = ['b', 'y', 'm', 'c', 'g', 'b', 'r', 'k', ]\nfor row_key, (ax,row) in bp_dict.iteritems():\n    ax.set_xlabel('')\n    for i,box in enumerate(row['boxes']):\n        box.set_facecolor(colors[i])\n\nplt.show()\n\nText: The resulting plot looks like this: \nText: Hope this helps. \nAPI:\npandas.Series\n","label":[[613,619,"Mention"],[1568,1581,"API"]],"Comments":[]}
{"id":60207,"text":"ID:50977465\nPost:\nText: Default mode for merge is 'inner' which will take the intersection of the two lists. So your script work and it will continue if one file had a date entry that the other does not but if you want your result file to have all data then you should use 'outer' mode. \nText: More info here : https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.merge.html \nText: For example with PV.csv like : \nCode: date,PVkW\n2018\/03\/05 11:00,887.4\n2018\/03\/05 12:00,940.9\n2018\/03\/05 13:00,927.2\n2018\/03\/05 14:00,845.9\n2018\/03\/05 15:00,683.0\n2018\/03\/05 16:00,423.1\n2018\/03\/05 17:00,186.4\n2018\/03\/05 18:00,186.4\n\nText: TBL.csv like : \nCode: date,TBLkW\n2018\/03\/05 11:00,277.01953\n2018\/03\/05 12:00,285.30783\n2018\/03\/05 13:00,236.8461\n2018\/03\/05 14:00,243.26564\n2018\/03\/05 15:00,274.98438\n2018\/03\/05 16:00,255.20079\n2018\/03\/05 17:00,262.28046\n\nText: And this script : \nCode: import pandas as pd\n\na = pd.read_csv('PV.csv')\nb = pd.read_csv('TBL.csv')\nb = b.dropna(axis=1)\nmerged = a.merge(b, how='outer', on='date')\nmerged.to_csv(\"combined.csv\", index=False)\n\nText: You get : \nCode: date,PVkW,TBLkW\n2018\/03\/05 11:00,887.4,277.01953\n2018\/03\/05 12:00,940.9,285.30782999999997\n2018\/03\/05 13:00,927.2,236.8461\n2018\/03\/05 14:00,845.9,243.26564\n2018\/03\/05 15:00,683.0,274.98438\n2018\/03\/05 16:00,423.1,255.20078999999998\n2018\/03\/05 17:00,186.4,262.28046\n2018\/03\/05 18:00,186.4,\n\nAPI:\npandas.merge\n","label":[[41,46,"Mention"],[1393,1405,"API"]],"Comments":[]}
{"id":60208,"text":"ID:51006642\nPost:\nText: Another simple way is to use the pivot function to format the data. \nText: Use plot to plot. Providing the colors in the 'color' column exist in matplotlib: List of named colors, they can be passed to the color parameter. \nCode: # sample data\ndf = pd.DataFrame([['red', 0, 0], ['red', 1, 1], ['red', 2, 2], ['red', 3, 3], ['red', 4, 4], ['red', 5, 5], ['red', 6, 6], ['red', 7, 7], ['red', 8, 8], ['red', 9, 9], ['blue', 0, 0], ['blue', 1, 1], ['blue', 2, 4], ['blue', 3, 9], ['blue', 4, 16], ['blue', 5, 25], ['blue', 6, 36], ['blue', 7, 49], ['blue', 8, 64], ['blue', 9, 81]],\n                  columns=['color', 'x', 'y'])\n\n# pivot the data into the correct shape\ndf = df.pivot(index='x', columns='color', values='y')\n\n# display(df)\ncolor  blue  red\nx               \n0         0    0\n1         1    1\n2         4    2\n3         9    3\n4        16    4\n5        25    5\n6        36    6\n7        49    7\n8        64    8\n9        81    9\n\n# plot the pivoted dataframe; if the column names aren't colors, remove color=df.columns\ndf.plot(color=df.columns, figsize=(5, 3))\n\nAPI:\npandas.DataFrame.pivot\npandas.DataFrame.plot\n","label":[[57,62,"Mention"],[103,107,"Mention"],[1102,1124,"API"],[1125,1146,"API"]],"Comments":[]}
{"id":60209,"text":"ID:51027533\nPost:\nText: If would like to get the sum of the number of occurrences of the name, you can use 'groupby': \nCode: df.groupby('Name')['Count'].sum()\n\nText: Please check out the documentation on pd.Series.groupby \nText: In case you only want to remove the duplicate names, you could also use 'drop_duplicates' (specifying the column): \nCode: df.drop_duplicates('Name')\n\nText: Documentation: drop_duplicates \nAPI:\npandas.Series.groupby\npandas.DataFrame.drop_duplicates\n","label":[[204,221,"Mention"],[400,415,"Mention"],[422,443,"API"],[444,476,"API"]],"Comments":[]}
{"id":60210,"text":"ID:51045263\nPost:\nText: stack \nText: If you select just the things that might have 'ball' which are columns that are of dtype object, then you can stack the resulting dataframe into a series object. At that point you can perform containns and unstack the results back into a dataframe. \nCode: df.select_dtypes(include=[object]).stack().str.contains('ball').unstack()\n\n     ids    id2\n0   True   True\n1   True   True\n2  False  False\n3   True   True\n\nAPI:\npandas.Series.str.contains\n","label":[[229,238,"Mention"],[454,480,"API"]],"Comments":[]}
{"id":60211,"text":"ID:51088659\nPost:\nText: The following should work: \nCode: latitude = latitude.values[0]\n\nText: .values accesses the numpy representation of a DataFrame Assuming your code latitude = df['latitude'] really results in a DataFrame of shape (1,1), then the above should work. \nAPI:\npandas.DataFrame\n","label":[[142,151,"Mention"],[277,293,"API"]],"Comments":[]}
{"id":60212,"text":"ID:51090659\nPost:\nText: From the read_html documentation: Read HTML tables into a list of DataFrame objects. \nText: So, df is a list of dataframes. If you expect to only have one, it may be safe for you to use df[0].to_excel... \nAPI:\npandas.read_html\n","label":[[33,42,"Mention"],[234,250,"API"]],"Comments":[]}
{"id":60213,"text":"ID:51094225\nPost:\nText: Your function isn't set up to take pd.Series as inputs. Use a different way. \nCode: df['Winner'] = [\n    winner(*t) for t in zip(df.Team1, df.Team2, df.Score1, df.Score2, df.P1, df.P2)]\n\ndf\n\n  Team1  Score1 Team2  Score2   Match  Match_no    P1    P2 Winner\n0     A       1     U       2  A Vs U         1  0.80  0.75      U\n1     B       2     V       2  B Vs V         2  0.70  0.75      V\n2     C       3     W       2  C Vs W         3  0.60  0.65      C\n3     D       1     X       2  D Vs X         4  0.90  0.78      X\n4     E       2     Y       3  E Vs Y         5  0.75  0.79      Y\n5     F       4     Z       3  F Vs Z         6  0.77  0.85      F\n\nText: Another way to go about it \nCode: def winner(T1,T2,S1,S2,PS1,PS2):\n    ninit = 5\n    Ts1 = np.random.rand(5).sum() * PS1\n    Ts2 = np.random.rand(5).sum() * PS2\n    a = np.select(\n        [S1 > S2, S2 > S1, Ts1 > Ts2, Ts2 > Ts1],\n        [T1, T2, T1, T2], 'DRAW')\n    return a\n\ndf.assign(Winner=winner(df.Team1, df.Team2, df.Score1, df.Score2, df.P1, df.P2))\n\n  Team1  Score1 Team2  Score2   Match  Match_no    P1    P2 Winner\n0     A       1     U       2  A Vs U         1  0.80  0.75      U\n1     B       2     V       2  B Vs V         2  0.70  0.75      B\n2     C       3     W       2  C Vs W         3  0.60  0.65      C\n3     D       1     X       2  D Vs X         4  0.90  0.78      X\n4     E       2     Y       3  E Vs Y         5  0.75  0.79      Y\n5     F       4     Z       3  F Vs Z         6  0.77  0.85      F\n\nAPI:\npandas.Series\n","label":[[59,68,"Mention"],[1526,1539,"API"]],"Comments":[]}
{"id":60214,"text":"ID:51111207\nPost:\nText: datetime.timdelta objects store days, seconds and microseconds as attributes. We can access them in a DataFrame with dt: \nText: Setting up some dummy data \nCode: import datetime as dt\nimport pandas as pd\n\ndf = pd.DataFrame(\n    data=(\n        dt.timedelta(days=17711, hours=i, minutes=i, seconds=i) for i in range(0, 10)\n    ), \n    columns=['Duration']\n)\n\nprint(df['Duration'])\n\n             Duration\n0 17711 days 00:00:00\n1 17711 days 01:01:01\n2 17711 days 02:02:02\n3 17711 days 03:03:03\n4 17711 days 04:04:04\n5 17711 days 05:05:05\n6 17711 days 06:06:06\n7 17711 days 07:07:07\n8 17711 days 08:08:08\n9 17711 days 09:09:09\nName: Duration, dtype: timedelta64[ns]\n\nText: Accesing seconds and turning them into hours \nCode: print(df['Duration'].dt.seconds \/ 3600)\n\n0    0.000000\n1    1.016944\n2    2.033889\n3    3.050833\n4    4.067778\n5    5.084722\n6    6.101667\n7    7.118611\n8    8.135556\n9    9.152500\nName: Duration, dtype: float64    \n\nText: Only hours \nCode: print(df['Duration'].dt.seconds \/\/ 3600)\n\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\nName: Duration, dtype: int64\n\nAPI:\npandas.DataFrame\n","label":[[126,135,"Mention"],[1132,1148,"API"]],"Comments":[]}
{"id":60215,"text":"ID:51150310\nPost:\nText: This is because you are calling factorize as a member function of a Serie object (in your case you call it from mydata.col1). In that case the first argument of the function call is the sort option, while you pass it another pandas.Series. For the detailed signature of the function see \nText: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.Series.factorize.html \nText: What I suspect you were trying to do is to call the factorize function of the pandas module. In this case the first argument of the call is a Series and the second is the sort option \nText: https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.22\/generated\/pandas.factorize.html \nText: Then the last line of your code would look something like \nCode: pandas.factorize(mydata.col1, sort=True)\n\nText: P.S I suspect that you want to access the first element of the above as \nCode: pandas.factorize(mydata.col1, sort=True)[0]\n\nAPI:\npandas.Series\npandas.Series\n","label":[[92,97,"Mention"],[551,557,"Mention"],[931,944,"API"],[945,958,"API"]],"Comments":[]}
{"id":60216,"text":"ID:51163610\nPost:\nText: I found the reason is that the way I used to create the Series is wrong. Instead of \nCode: vp_series = pd.Series(data=raw_df.Count, index=raw_df.Timestamp)\n\nText: I should be using \nCode: vp_series = pd.Series(data=raw_df.Count.values, index=raw_df.Timestamp)\n\nText: The first way is causing my series to contain a lot of missing values (NaN) which are not plotted. The reason is well explained in here. \nText: I know I didn't ask my question properly and I appreciate all the comments. \nAPI:\npandas.Series\n","label":[[80,86,"Mention"],[517,530,"API"]],"Comments":[]}
{"id":60217,"text":"ID:51171744\nPost:\nText: The keys are df.groupby to group with nodes, agg to aggregate (sum) by the grouped nodes, and cename to rename your columns. \nCode: import pandas as pd\ndf = pd.DataFrame({\"from_node\": [1907, 2343, 2050, 2050], \"to_node\": [2343, 2344, 2051, 2344], \"0\": [0.1, 0.2, 0.1, 0.4]})\nnew_df1 = df.groupby(\"from_node\").agg({\"0\": \"sum\"}).rename(columns={\"0\": \"node_loss\"})\nnew_df2 = df.groupby(\"to_node\").agg({\"0\": \"sum\"}).rename(columns={\"0\": \"node_add\"})\n\nText: Now new_df1 and new_df2 are what you want. \nText: BTW, since you are tagging Python 2.7, please also note that pandas is dropping Python 2.7. \nAPI:\npandas.DataFrame.groupby\npandas.core.groupby.DataFrameGroupBy.agg\npandas.DataFrame.rename\n","label":[[37,47,"Mention"],[69,72,"Mention"],[118,124,"Mention"],[625,649,"API"],[650,690,"API"],[691,714,"API"]],"Comments":[]}
{"id":60218,"text":"ID:51217322\nPost:\nText: If I understand your question correctly, you have a dictionary of similar DataFrames, such as this: \nCode: df1 = pd.DataFrame(data={\"col1\":[1,2,3], \"col2\":[\"A\",\"B\",\"C\"]})\ndf2 = pd.DataFrame(data={\"col1\":[4,5,6], \"col2\":[\"D\",\"E\",\"F\"]})\n\ndct = dict({\"DataFrame1\":df1, \"DataFrame2\":df2})\n\nText: If so then you just use concat and list comprehension to merge the DataFrames together, like so: \nCode: pd.concat([dct[k] for k in dct])\n\nAPI:\npandas.concat\n","label":[[340,346,"Mention"],[459,472,"API"]],"Comments":[]}
{"id":60219,"text":"ID:51219375\nPost:\nText: If you need to apply a function that doesn't support your argument directly, you can apply it element wise using dummy function lambda. \nText: Also, you need to assign back to your original panda series to overwrite it, use: \nCode: ms['close_time'] = ms['close_time'].apply( lambda x: datetime.datetime.fromtimestamp(x\/1000) )\n\nText: If you want to use pd.to_datetime directly. use: \nCode: pd.to_datetime(ms['close_time'], unit = 'ms')         \n\nText: PS. There might be difference in datetime obtained from these two methods \nAPI:\npandas.to_datetime\n","label":[[377,391,"Mention"],[556,574,"API"]],"Comments":[]}
{"id":60220,"text":"ID:51221331\nPost:\nText: You can use merge for this: \nCode: import pandas as pd\n\n# frames from the question\ndf1 = pd.DataFrame(data={\n  'team': ['ATL', 'BOS', 'BRK', 'CHI'],\n  'first': ['2017-10-18', '2017-10-17', '2017-10-18', '2017-10-19'],\n  'last': ['2018-04-10', '2018-04-11', '2018-04-11', '2018-04-11']\n}).set_index('team')\n\ndf2 = pd.DataFrame(data={\n  'date': ['2017-10-18', '2017-10-17', '2017-10-18', '2017-10-19', '2018-04-10'],\n  'team': ['ATL', 'BOS', 'BRK', 'CHI', 'ATL'],\n  'ELO_before': [1648.0, 1761.0, 1427.0, 1458.0, 1406.0],\n  'ELO_after': [1650.308911, 1753.884111, 1439.104231, 1464.397752, 1411.729285]\n})\n\n# merge on first and last\ndf1.reset_index(inplace=True)\ndf3 = df1.merge(df2.drop('ELO_after', axis=1), how='left', left_on=['team', 'first'], right_on=['team', 'date']).drop(['date'], axis=1)\ndf3 = df3.merge(df2.drop('ELO_before', axis=1), how='left', left_on=['team', 'last'], right_on=['team', 'date']).drop(['date'], axis=1)\n\n# calculate the differences\ndf3['ELO_difference'] = df3['ELO_after'] - df3['ELO_before']\ndf3.set_index('team', inplace=True)\n\nAPI:\npandas.DataFrame.merge\n","label":[[36,41,"Mention"],[1089,1111,"API"]],"Comments":[]}
{"id":60221,"text":"ID:51241870\nPost:\nText: As indicated by @FlorianGD, this can be achieved using pd.merge_asof with the direction='forward' argument: \nCode: pd.merge_asof(left=df, right=c_index, on='Date', suffixes=('_Raw', '_Total'), direction='forward')\n\n         Date  Count_Raw  Count_Total  Index_Col\n0  2008-11-30          1          3.0          1\n1  2008-11-30          1          3.0          1\n2  2008-11-30          1          3.0          1\n3  2009-01-17          1          1.0          2\n4  2009-02-08          1          1.0          3\n5  2009-03-08          1          2.0          4\n6  2009-03-08          1          2.0          4\n7  2009-07-30          1          1.0          5\n8  2009-08-30          1          1.0          6\n9  2009-09-04          1          1.0          7\n10 2009-09-13          1          1.0          8\n11 2009-10-19          1          1.0          9\n12 2009-10-25          1          1.0         10\n13 2009-10-28          1          1.0         11\n14 2009-12-12          1          2.0         12\n15 2009-12-15          1          2.0         12\n16 2009-12-30          1          1.0         13\n17 2010-01-24          1          1.0         14\n18 2010-01-28          1          3.0         15\n19 2010-01-31          1          3.0         15\n20 2010-01-31          1          3.0         15\n21 2010-02-21          1          1.0         16\n22 2010-03-19          1          1.0         17\n23 2010-04-09          1          1.0         18\n24 2010-06-18          1          1.0         19\n\nAPI:\npandas.merge_asof\n","label":[[79,92,"Mention"],[1519,1536,"API"]],"Comments":[]}
{"id":60222,"text":"ID:51332266\nPost:\nText: I don't think this is going to be possible with win32com. The Outlook Attachments.Add method requires a file or an outlook item as the Source argument: \nText: The source of the attachment. This can be a file (represented by the full file system path with a file name) or an Outlook item that constitutes the attachment. \nText: And the ExcelWriter also requires path although this other similar question (and answer) suggest that it may be possible by wrapping ExcelWriter in a BytesIO to send via email, just not via Outlook: \nCode: def export_excel(df):\n  with io.BytesIO() as buffer:\n    writer = pd.ExcelWriter(buffer)\n    df.to_excel(writer)\n    writer.save()\n    return buffer.getvalue()\n\nText: But this does not seem work with Outlook API (tested just now): \nCode: >>> attachment  = export_excel(df)\n>>> mail.Attachments.Add(attachment)\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<COMObject <unknown>>\", line 3, in Add\npywintypes.com_error: (-2147024809, 'The parameter is incorrect.', None, None)\n\nAPI:\npandas.ExcelWriter\n","label":[[359,370,"Mention"],[1073,1091,"API"]],"Comments":[]}
{"id":60223,"text":"ID:51345329\nPost:\nText: In pandas is best avoid list comprehension if exist vectorized solutions because performance and no support NaNs. \nText: I think need replace by \\s+ : one or more whitespaces with to_datetime for converting to datetimes and last for dates add date: \nCode: data['date'] = (pd.to_datetime(data['date'].str.replace('\\s+', ''), format='%Y%m%d')\n                  .dt.date)\n\nText: Performance: \nText: The plot was created with perfplot: \nCode: def list_compr(df):\n    df['date1'] = [datetime.datetime.strptime(d.replace(\" \", \"\"), '%Y%m%d').date() for d in df['date']]\n    return df\n\ndef vector(df):\n    df['date2'] = (pd.to_datetime(df['date'].str.replace('\\s+', ''), format='%Y%m%d').dt.date)\n    return df\n\ndef make_df(n):\n    df = pd.DataFrame({'date':['2016 4 1','20161010']}) \n    df = pd.concat([df] * n, ignore_index=True)\n    return df\n\nperfplot.show(\n    setup=make_df,\n    kernels=[list_compr, vector],\n    n_range=[2**k for k in range(2, 13)],\n    logx=True,\n    logy=True,\n    equality_check=False,  # rows may appear in different order\n    xlabel='len(df)')\n\nAPI:\npandas.to_datetime\n","label":[[204,215,"Mention"],[1111,1129,"API"]],"Comments":[]}
{"id":60224,"text":"ID:51348266\nPost:\nText: you want to use apply on column A \nCode: df['B'] = df['A'].apply(function)\n\nText: this does the function on each value in A. \nText: essentially you are using the apply method of the series object, more info: \nText: apply \nAPI:\npandas.Series.apply\n","label":[[239,244,"Mention"],[251,270,"API"]],"Comments":[]}
{"id":60225,"text":"ID:51434359\nPost:\nText: You should make use of datetime objects, which makes working with date and time objects very simple. In fact, pd.to_datetime is built to parse columns exactly as you have. (You need at least a year, month and day column for this parsing to work) \nCode: import pandas as pd\ndf = pd.DataFrame({'year': [2001, 2012],\n                   'month': [1, 12],\n                   'day': [16, 19],\n                   'hour': [1, 23],\n                   'minutes': [5, 35],\n                   'val1': [1.23, 1.151]})\n\ndf['Date'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minutes']])\nprint(df)\n#   year  month  day  hour  minutes   val1                Date\n#0  2001      1   16     1        5  1.230 2001-01-16 01:05:00\n#1  2012     12   19    23       35  1.151 2012-12-19 23:35:00\n\nText: Then, if you are willing to set it as your index you can use the built in functionality of between_time. \nCode: df.set_index('Date').between_time('19:00', '23:40')\n#                     year  month  day  hour  minutes   val1\n#Date                                                       \n#2012-12-19 23:35:00  2012     12   19    23       35  1.151\n\nAPI:\npandas.to_datetime\n","label":[[134,148,"Mention"],[1166,1184,"API"]],"Comments":[]}
{"id":60226,"text":"ID:51469517\nPost:\nText: Use merge in order to join both dataframes. If the other DataFrame is called demographic_df and assuming it has a column Country as well then you can use an inner join: \nCode: election_and_demographic_df = strong_Trump.merge(right=demographic_df, how='inner', left_on='Country', right_on='Country')\n\nText: If both have the index set to the country column then you can use left_index=True and right_index=True instead of left_on and right_on. \nAPI:\npandas.DataFrame.merge\n","label":[[28,33,"Mention"],[472,494,"API"]],"Comments":[]}
{"id":60227,"text":"ID:51484273\nPost:\nText: Consider several ways to specify column ordering: \nCode: # COLUMNS ARGUMENT OF DATAFRAME CONSTRUCTOR\ndf = pd.DataFrame(my_dict, columns=['x','y','z','Area'])   \n\n# SPECIFYING COLUMNS AFTER DATAFRAME BUILD\ndf = pd.DataFrame.from_dict(my_dict, orient='index')[['x','y','z','Area']]\n\n# REINDEXING\ndf = pd.DataFrame.from_dict(my_dict, orient='index').reindex(['x','y','z','Area'], axis='columns')\n\nText: Aside - dictionary construction from dict() is usually slower than defining keys and values inside {} as this answer shows. And df constructor can directly receive many data structures (dict, list, tuple, set, pandas.Series, numpy.array) as first argument. \nAPI:\npandas.DataFrame\n","label":[[552,554,"Mention"],[687,703,"API"]],"Comments":[]}
{"id":60228,"text":"ID:51485972\nPost:\nText: I took the liberty to change your data a bit to let it span across all possible ranges and labels. df: \nCode:    product ID  fees % fees quantity % quantity avg. price\/item label\n0        ABB    40     6%      651         5%             100     2\n1        AXX     2     5%      425         4%             110     1\n2        ACC  2000     5%      538         4%              90     3\n3        ADD   150     4%      217         3%              80     2\n4        AEE  1300     4%      192         3%             120     3\n\nText: To label the data, you need to use pd.cut \nCode: df['label'] = pd.cut(df['fees'], [1, 10, 1000, np.inf], labels=[1,2,3])\n\nText: Output: \nCode:    product ID  fees % fees quantity % quantity avg. price\/item label\n0        ABB    40     6%      651         5%             100     2\n1        AXX     2     5%      425         4%             110     1\n2        ACC  2000     5%      538         4%              90     3\n3        ADD   150     4%      217         3%              80     2\n4        AEE  1300     4%      192         3%             120     3\n\nText: Then, as you have mentioned, you can simply groupby data with labels and perform the statistic with groupby. \nText: Note that [1, 10, 1000, np.inf] defines the bins while, [1,2,3] are labels for the bins. \nAPI:\npandas.cut\n","label":[[585,591,"Mention"],[1320,1330,"API"]],"Comments":[]}
{"id":60229,"text":"ID:51521907\nPost:\nText: You can use concat along axis=1. This will also work for mismatched lengths. \nCode: grouper = df.groupby('category')\ndf = pd.concat([pd.Series(v['value'].tolist(), name=k) for k, v in grouper], axis=1)\n\nprint(df)\n\n    Topic1 Topic2  Topic3\n0    hello    hey      hi\n1  valuess   name  python\n\nAPI:\npandas.concat\n","label":[[36,42,"Mention"],[322,335,"API"]],"Comments":[]}
{"id":60230,"text":"ID:51525125\nPost:\nText: Use pd.Series.str.extract and then drop null rows: \nCode: df.assign(A=df.A.str.extract(r'[a-z]{2}\\_(\\d+)\\_[a-z]')).dropna()\n\n   A  B\n0  1  9\n1  2  2\n3  4  5\n\nText: You may need to change the regular expression, here is an explanation of the one I chose: \nCode: [a-z]{2}              # matches two lowercase characters a-z\n\\_                    # matches an underscore\n(\\d+)                 # matches one or more digits\n\\_                    # matches an underscore\n[a-z]                 # matches a single character a-z\n\nText: Here is an intermediate visualization of the series after using str.extract: \nCode: df.A.str.extract(r'[a-z]{2}\\_(\\d+)\\_[a-z]')\n\n     0 \n0    1 \n1    2 \n2  NaN \n3    4 \n\nText: Any rows where a match is not found result in NaN, which we then drop in the final step. \nAPI:\npandas.Series.str.extract\n","label":[[28,49,"Mention"],[822,847,"API"]],"Comments":[]}
{"id":60231,"text":"ID:51554478\nPost:\nText: Use cdt for new column or Series: \nCode: df['bins'] = pd.cut(df['number'], bins=[0,15,40, np.inf], right=False, include_lowest=True)\nprint (df)\n  name  number          bins\n0    A       2   [0.0, 15.0)\n1    B      10   [0.0, 15.0)\n2    C      25  [15.0, 40.0)\n3    D      35  [15.0, 40.0)\n4    E      45   [40.0, inf)\n5    F      55   [40.0, inf)\n\ns = pd.cut(df['number'], bins=[0,15,40, np.inf], right=False, include_lowest=True)\n\nText: And then use groupby with aggregation like: \nCode: df1 = df.groupby('bins').sum()\nprint (df1)\n\nbins                \n[0.0, 15.0)       12\n[15.0, 40.0)      60\n[40.0, inf)      100\n\nText: Or use Series: \nCode: df1 = df.groupby(s).sum()\n\nText: If want tuples: \nCode: s = pd.cut(df['number'], bins=[0,15,40, np.inf], right=False, include_lowest=True)\n\nout = [tuple(x) for x in df.groupby(s)['name'].apply(list)]\nprint (out)\n[('A', 'B'), ('C', 'D'), ('E', 'F')]\n\nAPI:\npandas.cut\n","label":[[28,31,"Mention"],[925,935,"API"]],"Comments":[]}
{"id":60232,"text":"ID:51571646\nPost:\nText: The docs for read_csv suggest why: \nText: infer_datetime_format : boolean, default False If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x. \nText: Essentially, Pandas deduces the format of your datetime from the first element(s) and then assumes all other elements in the series will use the same format. This means Pandas does not need to check multiple formats when attempting to convert a string to datetime. \nText: Remember, CSV files can only hold textual data, so a conversion to datetime (essentially a numeric type) will always be required. \nText: Here's a demonstration: \nCode: from dateutil import parser\nfrom datetime import datetime\n\nL = ['2018-01-05', '2018-12-20', '2018-03-30', '2018-04-15']*5000\n\n%timeit [parser.parse(i) for i in L]                   # 1.57 s\n%timeit [datetime.strptime(i, '%Y-%m-%d') for i in L]  # 338 ms\n\nAPI:\npandas.read_csv\n","label":[[37,45,"Mention"],[1066,1081,"API"]],"Comments":[]}
{"id":60233,"text":"ID:51577101\nPost:\nText: Did you try writing directly from your Pandas dataframe instead of going through Numpy? \nText: Try DF.to_csv(output.txt, sep=\\t, float_format=%g) \nText: For more details see df.to_csv \nAPI:\npandas.DataFrame.to_csv\n","label":[[204,213,"Mention"],[220,243,"API"]],"Comments":[]}
{"id":60234,"text":"ID:51584963\nPost:\nText: I think Mephy is right that this should probably go to StackOverflow. \nText: You're going to have a shape incompatibility because there will be fewer entries in the grouped result than in the original table. You'll need to do the equivalent of an SQL left outer join with the original table and the results, and you'll have the total length show up multiple times in the new column -- every time you have an equal (account_id, location_id) pair, you'll have the same value in the new column. (There's nothing necessarily wrong with this, but it could cause an issue if people are trying to sum up the new column, for example) \nText: Check out join (you can also use merge). You'll want to join the old table with the results, on (account_id, location_id), as a left (or outer) join. \nAPI:\npandas.DataFrame.join\n","label":[[667,671,"Mention"],[813,834,"API"]],"Comments":[]}
{"id":60235,"text":"ID:51702409\nPost:\nText: The correct way to do it will be using the pd.DataFrame.assign method \nCode: df.assign(url=unique_id)\n\nText: This will give you a new column in the DataFrame with name url and values the values from the numpy array. As far as I know column assignment like df['url] = unique_id is deprecated. You can read more here. \nAPI:\npandas.DataFrame.assign\n","label":[[67,86,"Mention"],[346,369,"API"]],"Comments":[]}
{"id":60236,"text":"ID:51713163\nPost:\nText: TA-Lib is expecting floating point data, whereas yours is integral. \nText: As such, when constructing your dataframe you need to coerce the input data by specifying dtype=numpy.float64: \nCode: import pandas\nimport numpy\nimport talib\n\nd = {'security1': [1,2,8,9,8,5], 'security2': [3,8,5,4,3,5]}\ndf = pandas.DataFrame(data=d, dtype=numpy.float64)         # note numpy.float64 here\n\nText: TA-Lib expects 1D arrays, which means it can operate on pd.Series but not pandas.DataFrame. \nText: You can, however, use pd.DataFrame.apply to apply a function on each column of your dataframe \nCode: df.apply(lambda c: talib.EMA(c, 2))\n\n    security1   security2\n0         NaN         NaN\n1    1.500000    5.500000\n2    5.833333    5.166667\n3    7.944444    4.388889\n4    7.981481    3.462963\n5    5.993827    4.487654\n\nAPI:\npandas.Series\npandas.DataFrame.apply\n","label":[[467,476,"Mention"],[532,550,"Mention"],[836,849,"API"],[850,872,"API"]],"Comments":[]}
{"id":60237,"text":"ID:51714409\nPost:\nText: You can pass a dictionary to uastype \nCode: d = {k: v.astype({'col1': float}) for k, v in d.items()}\n\nAPI:\npandas.DataFrame.astype\n","label":[[53,60,"Mention"],[131,154,"API"]],"Comments":[]}
{"id":60238,"text":"ID:51721088\nPost:\nText: Although I still couldn't make df.to_parquet approach to work with S3, I did find different solution which seems to work: \nCode: import s3fs\nfrom fastparquet import write\ns3 = s3fs.S3FileSystem()\nmyopen = s3.open\nwrite('s3:\/\/bucketname\/test.parquet', dftest, compression='GZIP', open_with=myopen)\n\nAPI:\npandas.DataFrame.to_parquet\n","label":[[55,68,"Mention"],[327,354,"API"]],"Comments":[]}
{"id":60239,"text":"ID:51757170\nPost:\nText: I tried the following: \nText: File authors.xlsx: \nText: authors \nText: Note that I used LibreOffice Calc for editing the table, but I saved the document as .xlsx \nText: Then I used read_excel for reading it: \nText: import pandas as pd df = pd.read_excel('authors.xlsx') \nText: You can then query the dataframe by columns, which correctly return the values: \nText: In: df['Name'] \nText: Out: 0 Jia-Ren Lin 1 Benjamin Izar 2 Daniel Treacy Name: Name, dtype: object \nAPI:\npandas.read_excel\n","label":[[205,215,"Mention"],[493,510,"API"]],"Comments":[]}
{"id":60240,"text":"ID:51758000\nPost:\nText: The behavior you are searching for is implemented by numpy.insert, however, this will not play very well with DataFrame objects, but no-matter, pd.DataFrame objects have a numpy.ndarray inside of them (sort of, depending on various factors, it may be multiple arrays, but you can think of them as on array accessible via the .values parameter). \nText: You will simply have to reconstruct the columns of your data-frame, but otherwise, I suspect this is the easiest and fastest way: \nCode: In [1]: import pandas as pd, numpy as np\n\nIn [2]: df = pd.DataFrame({'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':\n   ...: [28,34,29,42]})\n\nIn [3]: df1 = pd.DataFrame({'Name':['Anna', 'Susie'],'Age':[20,50]})\n\nIn [4]: np.insert(df.values, (0,2), df1.values, axis=0)\nOut[4]:\narray([['Anna', 20],\n       ['Tom', 28],\n       ['Jack', 34],\n       ['Susie', 50],\n       ['Steve', 29],\n       ['Ricky', 42]], dtype=object)\n\nText: So this returns an array, but this array is exactly what you need to make a data-frame! And you have the other elements, i.e. the columns already on the original data-frames, so you can just do: \nCode: In [5]: pd.DataFrame(np.insert(df.values, (0,2), df1.values, axis=0), columns=df.columns)\nOut[5]:\n    Name Age\n0   Anna  20\n1    Tom  28\n2   Jack  34\n3  Susie  50\n4  Steve  29\n5  Ricky  42\n\nText: So that single line is all you need. \nAPI:\npandas.DataFrame\npandas.DataFrame\n","label":[[134,143,"Mention"],[168,180,"Mention"],[1380,1396,"API"],[1397,1413,"API"]],"Comments":[]}
{"id":60241,"text":"ID:51776576\nPost:\nText: If you are using python3 (this won't work in python2), you can use translte as follows: \nCode: import pandas as pd\nreps = {'+' : '', 'E' : '0'}\nseries = pd.Series(['+1', 'E', '+5', '-1'])\n\nprint(series)\n#0    +1\n#1     E\n#2    +5\n#3    -1\n#dtype: object\n\nprint(series.str.translate(str.maketrans(reps)))\n#0     1\n#1     0\n#2     5\n#3    -1\n#dtype: object\n\nText: A better way to verify that it's doing what you expect: \nCode: print(series.str.translate(str.maketrans(reps)).values)\n#array(['1', '0', '5', '-1'], dtype=object)\n\nAPI:\npandas.Series.str.translate\n","label":[[91,99,"Mention"],[555,582,"API"]],"Comments":[]}
{"id":60242,"text":"ID:51792018\nPost:\nText: Use factorize for filtered values by mask: \nCode: m = df['cluster'].ne(df['cluster'].shift()).cumsum().duplicated(keep=False)\ndf.loc[m, 'new_cluster'] =  pd.factorize(df.loc[m, 'cluster'])[0] + 1\nprint (df)\n   measurement  cluster  new_cluster\n0           M1        6          1.0\n1           M2        6          1.0\n2           M3        6          1.0\n3           M4       12          2.0\n4           M5       12          2.0\n5           M6       12          2.0\n6           M7        2          NaN\n7           M8        9          3.0\n8           M9        9          3.0\n9          M10        9          3.0\n10         M11        9          3.0\n\nText: If want replace NaN to x: \nCode: df['new_cluster'] = df['new_cluster'].fillna('x')\nprint (df)\n   measurement  cluster new_cluster\n0           M1        6           1\n1           M2        6           1\n2           M3        6           1\n3           M4       12           2\n4           M5       12           2\n5           M6       12           2\n6           M7        2           x\n7           M8        9           3\n8           M9        9           3\n9          M10        9           3\n10         M11        9           3\n\nText: Details for boolean mask - first create helper Series for consecutive values and then mask by duplicated with keep='False' for return all dupes: \nCode: print (df['cluster'].ne(df['cluster'].shift()).cumsum())\n0     1\n1     1\n2     1\n3     2\n4     2\n5     2\n6     3\n7     4\n8     4\n9     4\n10    4\nName: cluster, dtype: int32\n\nprint (m)\n0      True\n1      True\n2      True\n3      True\n4      True\n5      True\n6     False\n7      True\n8      True\n9      True\n10     True\nName: cluster, dtype: bool\n\nAPI:\npandas.factorize\n","label":[[28,37,"Mention"],[1716,1732,"API"]],"Comments":[]}
{"id":60243,"text":"ID:51795032\nPost:\nText: Use pandas.Series.dt.day_name. \nCode: df.date.dt.day_name()\n\n0    Monday\nName: date, dtype: object\n\nText: While not necessary here, it also takes locale as a parameter \nCode: df.date.dt.day_name(locale='es')\n\n0    Lunes\nName: date, dtype: object\n\nText: pd.Series.dt.day_name is new in pandas 0.23.0, if you are using a previous version, use weekday_name instead: \nCode: df.date.dt.weekday_name\n\n0    Monday\nName: date, dtype: object\n\nText: Using weekday_name has been deprecated in 0.23.0 in favor of day_name() \nAPI:\npandas.Series.dt.day_name\n","label":[[277,298,"Mention"],[542,567,"API"]],"Comments":[]}
{"id":60244,"text":"ID:51827157\nPost:\nText: You can use \nCode: pd.Timestamp('today')\n\nText: or \nCode: pd.to_datetime('today')\n\nText: But both of those give the date and time for 'now'. \nText: Try this instead: \nCode: pd.Timestamp('today').floor('D')\n\nText: or \nCode: pd.to_datetime('today').floor('D')\n\nText: You could have also passed the datetime object to todattime but I like the other option mroe. \nCode: pd.to_datetime(datetime.datetime.today()).floor('D')\n\nText: Pandas also has a Timedelta object \nCode: pd.Timestamp('now').floor('D') + pd.Timedelta(-3, unit='D')\n\nText: Or you can use the offsets module \nCode: pd.Timestamp('now').floor('D') + pd.offsets.Day(-3)\n\nText: To check for membership, try one of these \nCode: cur_date in df['date'].tolist()\n\nText: Or \nCode: df['date'].eq(cur_date).any()\n\nAPI:\npandas.to_datetime\n","label":[[339,348,"Mention"],[793,811,"API"]],"Comments":[]}
{"id":60245,"text":"ID:51835928\nPost:\nText: Opening the excel in this way, the content variable is a list of tuples. \nText: Having a look on those tuples there is a TimeZoneInfo that localizes all the dates in a kind of time zone, in my case \"GMT Standard Time\". \nText: So once converted to a dataframe, when doing df.dtypes the result is not only \"datetime64\" but \"datetime64 (UTC+0:00) Dublin, Edimburg, ...\" \nText: This time zone setting only happens when opening the excel file through win32com.client. If you removed the password, you can open it with pd.read_excel and discover that there is no timezone set for those datetimes and the mentioned warning does not appear. \nText: Don't know exactly the reason it happens, but I have a solution for the original example. The warning dissapears setting a timezone recognized by tz database as \"UTC\" or simply None. Something like: \nCode: df[\"col_name\"]=df[\"col_name\"].dt.tz_convert(None)\n\nAPI:\npandas.read_excel\n","label":[[537,550,"Mention"],[926,943,"API"]],"Comments":[]}
{"id":60246,"text":"ID:51846599\nPost:\nText: Setup \nCode: np.random.seed([3, 1415])\ndf = pd.DataFrame(\n    np.random.randint(10, size=(10, 3)),\n    columns=list('ABC')\n)\n\nText: muask \nText: Pandas only and intuitive \nCode: is_small = df < df.quantile(.25)\nis_large = df > df.quantile(.75)\nis_medium = ~(is_small | is_large)\n\ndf.mask(is_small, 'small').mask(is_large, 'large').mask(is_medium, 'medium')\n\n        A       B       C\n0   small   small  medium\n1  medium   large  medium\n2   small   large   large\n3  medium   small   small\n4   small  medium   large\n5   large  medium   small\n6  medium  medium  medium\n7  medium   large  medium\n8  medium  medium  medium\n9   large  medium   large\n\nText: Nested numpy.where \nCode: is_small = df < df.quantile(.25)\nis_large = df > df.quantile(.75)\n\npd.DataFrame(\n    np.where(is_small, 'small', np.where(is_large, 'large', 'medium')),\n    df.index, df.columns\n)\n\n        A       B       C\n0   small   small  medium\n1  medium   large  medium\n2   small   large   large\n3  medium   small   small\n4   small  medium   large\n5   large  medium   small\n6  medium  medium  medium\n7  medium   large  medium\n8  medium  medium  medium\n9   large  medium   large\n\nAPI:\npandas.DataFrame.mask\n","label":[[156,161,"Mention"],[1174,1195,"API"]],"Comments":[]}
{"id":60247,"text":"ID:51850988\nPost:\nText: That's an interesting question. I've done some digging around and did my best to explain some of this, although one thing i still don't get is why we get pandas throwing an error instead of numpy when we do b<a. \nText: Regards to your question: \nText: If a can be compared to b, I thought we should be able to compare the other way around? \nText: That's not necesserily true. It just depends on the implementation of the comparison operators. \nText: Take this test class for example: \nCode: class TestCom(int):\n    def __init__(self, a):\n    self.value = a\n\n    def __gt__(self, other):\n    print('TestComp __gt__ called')\n    return True\n\n    def __eq__(self, other):\n    return self.a == other\n\nText: Here I have defined my __gt__ (<) method to always return true no matter what the other value is. While __eq__ (==) left the same. \nText: Now check the following comparisons out: \nCode: a = TestCom(9)\nprint(a)\n# Output: 9\n\n# my def of __ge__\na > 100\n\n# Ouput: TestComp __gt__ called\n# True\n\na > '100'\n# Ouput: TestComp __gt__ called\n# True\n\n'100' < a\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-486-8aee1b1d2500> in <module>()\n      1 # this will not use my def of __ge__\n----> 2 '100' > a\n\nTypeError: '>' not supported between instances of 'str' and 'TestCom'\n\nText: So going back to your case. Looking at the timestamps_sourceCode the only thing i can think of is Timestmap does some type checking and conversion if possible. \nText: When we're comparing a with b (pd.Timestamp against np.datetime64), Timestamp.__richcmp__ function does the comparison, if it is of type np.datetime64 then it converts it to pd.Timestamp type and does the comparison. \nCode: # we can do the following to have a comparison of say b > a\n# this converts a to np.datetime64 - .asm8 is equivalent to .to_datetime64()\nb > a.asm8\n\n# or we can confert b to datetime64[ms]\nb.astype('datetime64[ms]') > a\n\n# or convert to timestamp\npd.to_datetime(b) > a\n\nText: What i found surprising was, as i thought the issue is with nanoseconds not in Timestamp, is that even if you do the following the comparison between np.datetime64 with pd.Timestamp fails. \nCode: a = pd.Timestamp('2013-03-24 05:32:00.00000001')\na.nanosecond   # returns 10\n# doing the comparison again where they're both ns still fails\nb < a\n\nText: Looking at the source code it seems like we can use == and != operators. But even they dont work as expected. Take a look at the following for an example: \nCode: a = pd.Timestamp('2013-03-24 05:32:00.00000000')\nb = np.datetime64('2013-03-24 05:32:00.00000000', 'ns')\n\nb == a  # returns False\n\na == b  # returns True\n\nText: I think this is the result of lines 149-152 or 163-166. Where they return False if your using == and True for !=, without actually comparing the values. \nText: Edit: The nanosecond feature was added in version 0.23.0. So you can do something like pd.Timestamp('2013-03-23T05:33:00.000000022', unit='ns'). So yes when you compare np.datetime64 it will be converted to pd.Timestamp with ns precision. \nText: Just note that pd.Timestamp is supposed to be a replacement for python`s datetime: \nText: Timestamp is the pandas equivalent of python's Datetime and is interchangeable with it in most cases. \nText: But python's datetime doesn't support nanoseconds - good answer here explaining why SO_Datetime.pd.Timestamp have support for comparison between the two even if your Timestamp has nanoseconds in it. When you compare a datetime object agains pd.Timestamp object with ns they have _compare_outside_nanorange that will do the comparison. \nText: Going back to np.datetime64, one thing to note here as explained nicely in this post SO is that it's a wrapper on an int64 type. So not suprising if i do the following: \nCode: 1 > a\na > 1\n\nText: Both will though an error Cannot compare type 'Timestamp' with type 'int'. \nText: So under the hood when you do b > a the comparison most be done on an int level, this comparison will be done by np.greater() function np.greater - also take a look at ufunc_docs. \nText: Note: I'm unable to confirm this, the numpy docs are too complex to go through. If any numpy experts can comment on this, that'll be helpful. \nText: If this is the case, if the comparison of np.datetime64 is based on int, then the example above with a == b and b == a makes sense. Since when we do b == a we compare the int value of b against pd.Timestamp this will always return Flase for == and True for !=. \nText: Its the same as doing say 123 == '123', this operation will not fail, it will just return False. \nAPI:\npandas.Timestamp\n","label":[[1517,1526,"Mention"],[4689,4705,"API"]],"Comments":[]}
{"id":60248,"text":"ID:51866628\nPost:\nText: First you need to open your file with xlrd, then pass the result to pd.read_excel with the parameter engine=\"xlrd\" \nCode: import pandas as pd\nimport xlrd\n\nbook = xlrd.open_workbook(file_contents=myfile)\ndf = pd.read_excel(book,engine='xlrd')\nprint(df.head())\n\nText: here \"myfile\" is your in memory file \nAPI:\npandas.read_excel\n","label":[[92,105,"Mention"],[333,350,"API"]],"Comments":[]}
{"id":60249,"text":"ID:51880087\nPost:\nText: The problem is that your outliers in each column may happen for varying rows(records). I'd advise you be happy with substituting np.nan \nText: Setup \nCode: np.random.seed([3, 1415])\ndf = pd.DataFrame(\n    np.random.normal(size=(20, 8)),\n    columns=list('ABCDEFGH')\n)\n\ndf\n\n           A         B         C         D         E         F         G         H\n0  -2.129724 -1.268466 -1.970500 -2.259055 -0.349286 -0.026955  0.316236  0.348782\n1   0.715364  0.770763 -0.608208  0.352390 -0.352521 -0.415869 -0.911575 -0.142538\n2   0.746839 -1.504157  0.611362  0.400219 -0.959443  1.494226 -0.346508 -1.471558\n3   1.063243  1.062997  0.591860  0.296212 -0.774732  0.831452  1.486976  0.256220\n4  -0.899906  0.375085 -0.519501  0.050101  0.949959 -1.033773  0.948247  0.733776\n5   1.236118  0.155475 -1.341267  0.162864  1.258253  0.778040  1.341599 -1.636039\n6  -0.195368  0.131820  2.069013  0.048729 -1.500564  0.907342  0.029326  0.066119\n7  -0.728821 -2.137846  1.402702 -0.017209 -0.071309 -0.533061  1.273899  0.348510\n8  -0.920391  0.348579 -0.835074 -0.225377  0.206295 -0.582825 -1.511850  1.633570\n9   0.403321  0.992765  0.025249 -1.664999 -1.558044 -0.361630 -1.784971 -0.318569\n10 -0.326400 -0.688203  0.506420 -0.386706 -0.368351 -0.293383 -2.086973 -0.807873\n11  0.068855 -0.525141  0.745524  0.911930 -0.277785 -0.866313  1.155518  1.421480\n12  1.416653 -0.120607  1.367540 -0.811585 -0.205071 -0.450472 -0.993868 -0.084107\n13  2.222507  0.668158  0.463331 -0.302869  0.226355 -0.966131  1.015160 -0.329008\n14 -1.070002  0.525867  0.616915  0.399136 -0.233075 -0.482919 -1.018142 -1.673869\n15  0.058956  0.242391 -0.660237 -0.081101  1.690625  0.296406 -0.938197  0.225710\n16 -0.352254  0.170126 -0.943541  0.627847 -0.948773  0.126131  1.162792 -0.492266\n17 -0.444413 -0.028003 -0.286051  0.895515 -0.234507  1.005886 -1.350465 -0.959034\n18  0.992524 -1.471428  0.270001 -1.197004 -0.324760 -1.383568  0.838075 -1.125205\n19  0.024837  0.238895  0.350742 -0.541868 -0.730284  0.113695  0.068872 -0.032520\n\nText: mask \nCode: df.mask((df - df.mean()).abs() > 2 * df.std())\n\n           A         B         C         D         E         F         G         H\n0        NaN -1.268466       NaN       NaN -0.349286 -0.026955  0.316236  0.348782\n1   0.715364  0.770763 -0.608208  0.352390 -0.352521 -0.415869 -0.911575 -0.142538\n2   0.746839 -1.504157  0.611362  0.400219 -0.959443       NaN -0.346508 -1.471558\n3   1.063243  1.062997  0.591860  0.296212 -0.774732  0.831452  1.486976  0.256220\n4  -0.899906  0.375085 -0.519501  0.050101  0.949959 -1.033773  0.948247  0.733776\n5   1.236118  0.155475 -1.341267  0.162864  1.258253  0.778040  1.341599 -1.636039\n6  -0.195368  0.131820  2.069013  0.048729 -1.500564  0.907342  0.029326  0.066119\n7  -0.728821       NaN  1.402702 -0.017209 -0.071309 -0.533061  1.273899  0.348510\n8  -0.920391  0.348579 -0.835074 -0.225377  0.206295 -0.582825 -1.511850       NaN\n9   0.403321  0.992765  0.025249 -1.664999 -1.558044 -0.361630 -1.784971 -0.318569\n10 -0.326400 -0.688203  0.506420 -0.386706 -0.368351 -0.293383 -2.086973 -0.807873\n11  0.068855 -0.525141  0.745524  0.911930 -0.277785 -0.866313  1.155518  1.421480\n12  1.416653 -0.120607  1.367540 -0.811585 -0.205071 -0.450472 -0.993868 -0.084107\n13       NaN  0.668158  0.463331 -0.302869  0.226355 -0.966131  1.015160 -0.329008\n14 -1.070002  0.525867  0.616915  0.399136 -0.233075 -0.482919 -1.018142 -1.673869\n15  0.058956  0.242391 -0.660237 -0.081101       NaN  0.296406 -0.938197  0.225710\n16 -0.352254  0.170126 -0.943541  0.627847 -0.948773  0.126131  1.162792 -0.492266\n17 -0.444413 -0.028003 -0.286051  0.895515 -0.234507  1.005886 -1.350465 -0.959034\n18  0.992524 -1.471428  0.270001 -1.197004 -0.324760 -1.383568  0.838075 -1.125205\n19  0.024837  0.238895  0.350742 -0.541868 -0.730284  0.113695  0.068872 -0.032520\n\nText: + dropna \nText: If you only want rows for which no outliers exist for any column, you could follow up the above with dropna \nCode: df.mask((df - df.mean()).abs() > 2 * df.std()).dropna()\n\n\n\n      A         B         C         D         E         F         G         H\n1   0.715364  0.770763 -0.608208  0.352390 -0.352521 -0.415869 -0.911575 -0.142538\n3   1.063243  1.062997  0.591860  0.296212 -0.774732  0.831452  1.486976  0.256220\n4  -0.899906  0.375085 -0.519501  0.050101  0.949959 -1.033773  0.948247  0.733776\n5   1.236118  0.155475 -1.341267  0.162864  1.258253  0.778040  1.341599 -1.636039\n6  -0.195368  0.131820  2.069013  0.048729 -1.500564  0.907342  0.029326  0.066119\n9   0.403321  0.992765  0.025249 -1.664999 -1.558044 -0.361630 -1.784971 -0.318569\n10 -0.326400 -0.688203  0.506420 -0.386706 -0.368351 -0.293383 -2.086973 -0.807873\n11  0.068855 -0.525141  0.745524  0.911930 -0.277785 -0.866313  1.155518  1.421480\n12  1.416653 -0.120607  1.367540 -0.811585 -0.205071 -0.450472 -0.993868 -0.084107\n14 -1.070002  0.525867  0.616915  0.399136 -0.233075 -0.482919 -1.018142 -1.673869\n16 -0.352254  0.170126 -0.943541  0.627847 -0.948773  0.126131  1.162792 -0.492266\n17 -0.444413 -0.028003 -0.286051  0.895515 -0.234507  1.005886 -1.350465 -0.959034\n18  0.992524 -1.471428  0.270001 -1.197004 -0.324760 -1.383568  0.838075 -1.125205\n19  0.024837  0.238895  0.350742 -0.541868 -0.730284  0.113695  0.068872 -0.032520\n\nAPI:\npandas.DataFrame.mask\n","label":[[2047,2051,"Mention"],[5293,5314,"API"]],"Comments":[]}
{"id":60250,"text":"ID:51881060\nPost:\nText: There are a lot of options for creating a pdf in python. Some of these options are ReportLab, pydf2, pdfdocument and FPDF. \nText: The FPDF library is fairly stragihtforward to use and is what I've used in this example. FPDF Documentation can be found here. \nText: It's perhaps also good to think about what python modules you might want to use to create graphs and tables. In my example, I use matplotlib (link to docs) and I also use Pandas to create a dataframe using pandas.dataframe(). \nText: I've posted a rather lengthy but fully reproducible example below, using pandas, matplotlib and fpdf. The data are a subset of what the OP provided in the question. I loop through the dataframe in my example to create the table, but there are alternative and perhaps more efficient ways to do this. \nCode: import pandas as pd\nimport matplotlib\nfrom pylab import title, figure, xlabel, ylabel, xticks, bar, legend, axis, savefig\nfrom fpdf import FPDF\n\n\ndf = pd.DataFrame()\ndf['Question'] = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\ndf['Charles'] = [3, 4, 5, 3]\ndf['Mike'] = [3, 3, 4, 4]\n\ntitle(\"Professor Criss's Ratings by Users\")\nxlabel('Question Number')\nylabel('Score')\n\nc = [2.0, 4.0, 6.0, 8.0]\nm = [x - 0.5 for x in c]\n\nxticks(c, df['Question'])\n\nbar(m, df['Mike'], width=0.5, color=\"#91eb87\", label=\"Mike\")\nbar(c, df['Charles'], width=0.5, color=\"#eb879c\", label=\"Charles\")\n\nlegend()\naxis([0, 10, 0, 8])\nsavefig('barchart.png')\n\npdf = FPDF()\npdf.add_page()\npdf.set_xy(0, 0)\npdf.set_font('arial', 'B', 12)\npdf.cell(60)\npdf.cell(75, 10, \"A Tabular and Graphical Report of Professor Criss's Ratings by Users Charles and Mike\", 0, 2, 'C')\npdf.cell(90, 10, \" \", 0, 2, 'C')\npdf.cell(-40)\npdf.cell(50, 10, 'Question', 1, 0, 'C')\npdf.cell(40, 10, 'Charles', 1, 0, 'C')\npdf.cell(40, 10, 'Mike', 1, 2, 'C')\npdf.cell(-90)\npdf.set_font('arial', '', 12)\nfor i in range(0, len(df)):\n    pdf.cell(50, 10, '%s' % (df['Question'].iloc[i]), 1, 0, 'C')\n    pdf.cell(40, 10, '%s' % (str(df.Mike.iloc[i])), 1, 0, 'C')\n    pdf.cell(40, 10, '%s' % (str(df.Charles.iloc[i])), 1, 2, 'C')\n    pdf.cell(-90)\npdf.cell(90, 10, \" \", 0, 2, 'C')\npdf.cell(-30)\npdf.image('barchart.png', x = None, y = None, w = 0, h = 0, type = '', link = '')\npdf.output('test.pdf', 'F')\n\nText: Expected test.pdf: \nText: Update (April 2020): I made an edit to the original answer in April 2020 to replace use of pandas.DataFrame.ix() since this is deprecated. In my example I was able to replace it's use with iloc and the output is the same as before. \nAPI:\npandas.DataFrame.iloc\n","label":[[2470,2474,"Mention"],[2519,2540,"API"]],"Comments":[]}
{"id":60251,"text":"ID:51898231\nPost:\nText: You can use a column coding which night it is using date of pandas datetime series. And then use this column for a groupby to compute your rolling mean per week: \nCode: df.Time = pd.to_datetime(df.Time)\ndf['night'] = (df.Time.dt.hour>22) | (df.Time.dt.hour < 6)\ndf['date'] = df.Time.dt.date\nidx = (df.night) & (df.Time.dt.hour>22)\ndf.loc[idx,'date'] = df.loc[idx,'date'].values + pd.DateOffset(1)\ndf.date = pd.to_datetime(df.date)\ndf['rolling_mean'] = np.nan\ndf.loc[df.night,'rolling_mean'] = df.loc[df.night,'date'].apply(lambda x : df.loc[df.night & df.date.between(x-pd.DateOffset(3),x+pd.DateOffset(3)),'Value'].mean())\ndf.drop(['night','date'],1,inplace = True)\n\n\n    Time                        Value   rolling_mean\n1   2016-02-16 08:40:14.133000  12      NaN\n2   2016-02-16 11:25:14.133000  4       NaN\n3   2016-02-16 23:45:14.133000  8       8.000000\n4   2016-03-16 08:40:14.002700  17      NaN\n5   2016-03-16 23:45:14.133000  2       5.666667\n6   2016-03-16 23:50:14.133000  6       5.666667\n7   2016-03-16 23:55:14.133000  9       5.666667\n8   2016-04-16 08:40:14.133000  10      NaN\n9   2016-04-16 11:20:14.133000  2       NaN\n10  2016-04-16 12:40:14.133000  7       NaN\n11  2016-04-16 23:45:14.133000  5       5.000000\n12  2016-05-16 08:40:14.002700  11      NaN\n13  2016-05-16 23:40:14.133000  3       3.500000\n14  2016-05-16 23:50:14.133000  4       3.500000\n15  2016-06-16 08:40:14.002700  11      NaN\n16  2016-06-16 10:30:14.002700  27      NaN\n17  2016-06-16 23:25:14.133000  3       4.000000\n18  2016-06-16 23:30:14.133000  5       4.000000\n19  2016-07-16 08:40:14.002700  7       NaN\n20  2016-07-16 11:15:14.002700  9       NaN\n21  2016-07-16 23:45:14.133000  18      18.000000\n\nText: It is a heavy way to do this and there is probably a more elegant and pythonic way to do it, but at least you have your output! \nText: EDIT: \nText: There must definitely be a better way to do it using df.rolling on a subdataframe only containing the night rows. See This question for example. \nText: Here is a suboptimal example: \nCode: df['night'] = (df.Time.dt.hour>22) | (df.Time.dt.hour < 6)\ndf['date'] = df.Time.dt.date\nidx = (df.night) & (df.Time.dt.hour>22)\ndf.loc[idx,'date'] = df.loc[idx,'date'].values + pd.DateOffset(1)\ndf.date = pd.to_datetime(df.date)\ndf = df.set_index('date').join(df.loc[df.night].set_index('date').resample(\"1d\").Value.mean().rolling(window=3, min_periods=1).mean(),rsuffix=\"_rolling_mean\").reset_index()\ndf.drop(['night','date'],1,inplace = True)\n\nAPI:\npandas.DataFrame.rolling\n","label":[[1929,1939,"Mention"],[2515,2539,"API"]],"Comments":[]}
{"id":60252,"text":"ID:51953106\nPost:\nText: If train_arg is a dataframe, then train_arg['accomodates'] is a series whereas train_arg[['accomodate']] is a dataframe (containing only one column). \nText: Since the data used in fit and predict is supposed to have multiple columns, the functions will on a pd.DataFrame but not on a pandas.Series. \nText: To prevent this error from happening, make sure your data (first argument in fit, and only argument in predict) is of DataFrame or numpy.ndarray type. \nAPI:\npandas.DataFrame\npandas.DataFrame\n","label":[[282,294,"Mention"],[448,457,"Mention"],[487,503,"API"],[504,520,"API"]],"Comments":[]}
{"id":60253,"text":"ID:51968305\nPost:\nText: pd.factorize \nCode: df.assign(client_id=df.client_id.factorize()[0] + 1)\n\n  action  client_id        date\n0  test1          1  2018-08-20\n1  test2          2  2018-08-22\n2  test3          1  2018-08-21\n3  test4          2  2018-08-21\n4  test5          3  2018-08-18\n5  test6          2  2018-08-20\n6  test7          1  2018-08-18\n7  test8          3  2018-08-19\n\nText: numpy.unique \nCode: df.assign(client_id=np.unique(df.client_id, return_inverse=True)[1] + 1)\n\n  action  client_id        date\n0  test1          1  2018-08-20\n1  test2          2  2018-08-22\n2  test3          1  2018-08-21\n3  test4          2  2018-08-21\n4  test5          3  2018-08-18\n5  test6          2  2018-08-20\n6  test7          1  2018-08-18\n7  test8          3  2018-08-19\n\nAPI:\npandas.factorize\n","label":[[24,36,"Mention"],[781,797,"API"]],"Comments":[]}
{"id":60254,"text":"ID:51970953\nPost:\nText: You want to add a level to frame1 with empty strings \nText: from_tuples \nCode: idx = pd.MultiIndex.from_tuples([(c, '') for c in frame1])\nf1 = frame1.set_axis(idx, axis=1, inplace=False)\nframe3 = pd.concat([f1, frame2], axis=1)\n\nframe3.reindex([0, 1])\n\n     A    B    C   T1             T2     \n                   d1   d2   d3   d4   d5\n0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n\nText: conca \nCode: frame3 = pd.concat([\n    pd.concat([frame1], keys=[''], axis=1).swaplevel(0, 1, 1),\n    frame2], axis=1)\n\nframe3.reindex([0, 1])\n\n     A    B    C   T1             T2     \n                   d1   d2   d3   d4   d5\n0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n\nAPI:\npandas.MultiIndex.from_tuples\npandas.concat\n","label":[[84,95,"Mention"],[452,457,"Mention"],[769,798,"API"],[799,812,"API"]],"Comments":[]}
{"id":60255,"text":"ID:51975680\nPost:\nText: Numpy's numpy.add.at and pd.factorize \nText: This is intended to be fast. However, I tried to organize it to be readable as well. \nCode: i, r = pd.factorize(df.name)\nj, c = pd.factorize(df.color)\nn, m = len(r), len(c)\n\nb = np.zeros((n, m), dtype=np.int64)\n\nnp.add.at(b, (i, j), 1)\npd.Series(c[b.argmax(1)], r)\n\nJohn     White\nTom       Blue\nJerry    Black\ndtype: object\n\nText: groupby, size, and idxmax \nCode: df.groupby(['name', 'color']).size().unstack().idxmax(1)\n\nname\nJerry    Black\nJohn     White\nTom       Blue\ndtype: object\n\nname\nJerry    Black\nJohn     White\nTom       Blue\nName: color, dtype: object\n\nText: Counter \nText: \\_()_\/ \nCode: from collections import Counter\n\ndf.groupby('name').color.apply(lambda c: Counter(c).most_common(1)[0][0])\n\nname\nJerry    Black\nJohn     White\nTom       Blue\nName: color, dtype: object\n\nAPI:\npandas.factorize\n","label":[[49,61,"Mention"],[864,880,"API"]],"Comments":[]}
{"id":60256,"text":"ID:51987922\nPost:\nText: pd.Series.str.extract \nCode: df.assign(New_Variable=df.Category.str.extract('(Bonds|Stocks)'))\n\n         Category New_Variable\n0  Foreign Stocks       Stocks\n1    Stocks China       Stocks\n2          Stocks       Stocks\n3           Bonds        Bonds\n4       Bonds USA        Bonds\n5     Bonds India        Bonds\n\nAPI:\npandas.Series.str.extract\n","label":[[24,45,"Mention"],[343,368,"API"]],"Comments":[]}
{"id":60257,"text":"ID:51991985\nPost:\nText: You can use loc and pass a boolean array to it. \nCode: bi = pd.cut(df[0], bins, labels=False)\nx = df.loc[bi == 4]\n\nAPI:\npandas.DataFrame.loc\n","label":[[36,39,"Mention"],[144,164,"API"]],"Comments":[]}
{"id":60258,"text":"ID:51993139\nPost:\nText: idxmax \nText: Is the method that tells you the index value where the maximum occurs. \nText: Then use that to get at the value of the other column. \nCode: df.at[df['sequence'].idxmax(), 'people']\n\n'Bob'\n\nText: I like the solution @user3483203 provided in the comments. The reason I provided a different one is to show that the same think can be done with fewer objects created. \nText: In this case, df['sequence'] is accessing an internally stored object and subsequently calling the idxmax method on it. At that point we are accessing a specific cell in the dataframe df with the at accessor. \nText: We can see that we are accessing the internally stored object because we can access it in two different ways and validate that it is the same object. \nCode: df['sequence'] is df.sequence\n\nTrue\n\nText: While \nCode: df['sequence'] is df.sequence.copy()\n\nFalse\n\nText: On the other hand, df.set_index('people') creates a new object and that is expensive. \nText: Clearly this is over a ridiculously small data set but: \nCode: %timeit df.loc[df['sequence'].idxmax(), 'people']\n%timeit df.at[df['sequence'].idxmax(), 'people']\n%timeit df.set_index('people').sequence.idxmax()\n\n10000 loops, best of 3: 65.1 s per loop\n10000 loops, best of 3: 62.6 s per loop\n1000 loops, best of 3: 556 s per loop\n\nText: Over a much larger data set: \nCode: df = pd.DataFrame(dict(\n    people=range(10000),\n    sequence=np.random.permutation(range(10000))\n))\n\n%timeit df.loc[df['sequence'].idxmax(), 'people']\n%timeit df.at[df['sequence'].idxmax(), 'people']\n%timeit df.set_index('people').sequence.idxmax()\n\n10000 loops, best of 3: 107 s per loop\n10000 loops, best of 3: 101 s per loop\n1000 loops, best of 3: 816 s per loop\n\nText: The relative difference is consistent. \nAPI:\npandas.Series.idxmax\n","label":[[24,30,"Mention"],[1779,1799,"API"]],"Comments":[]}
{"id":60259,"text":"ID:52069371\nPost:\nText: eval \nText: We can use some dynamic string interpolation and eval REQUIRES Python 3.6 \nCode: fbase = '(((({0:}) ** 2) * ({1:})) + ((({1:}) ** 2) * ({2:}))) ** .5'.format\n\ndf.eval(f\"\"\"\\\nX_A = {fbase('1.1 * A', 'B', 'C')}\nX_B = {fbase('A', '1.1 * B', 'C')}\nX_C = {fbase('A', 'B', '1.1 * C')}\n\"\"\")\n\n    A   B   C         X_A         X_B         X_C\n0  10  20  30  120.083304  129.305839  123.288280\n1  20  30  40  224.766546  238.243573  227.156334\n2  30  40  50  351.511024  369.323706  352.136337\n3  40  50  60  496.789694  519.133894  494.974747\n\nText: Lest dynamic but doesn't require Python 3.6 \nCode: df.eval(\"\"\"\\\nX_A = ((((1.1 * A) ** 2) * (B)) + (((B) ** 2) * (C))) ** .5\nX_B = '((((A) ** 2) * (1.1 * B)) + (((1.1 * B) ** 2) * (C))) ** .5'\nX_C = ((((A) ** 2) * (B)) + (((B) ** 2) * (1.1 * C))) ** .5\n\"\"\")\n\n    A   B   C         X_A         X_B         X_C\n0  10  20  30  120.083304  129.305839  123.288280\n1  20  30  40  224.766546  238.243573  227.156334\n2  30  40  50  351.511024  369.323706  352.136337\n3  40  50  60  496.789694  519.133894  494.974747\n\nText: Numpy-ish \nCode: def f(m):\n  A, B, C = m.T\n  return (((A ** 2) * B) + ((B ** 2) * C)) ** .5\n\nv = df.values\nm = np.eye(3) * .1 + np.ones((3, 3))\n\nr = f((v * m[:, None]).reshape(-1, 3)).reshape(3, -1)\n\ndf.assign(**dict(zip('X_A X_B X_C'.split(), r)))\n\n    A   B   C         X_A         X_B         X_C\n0  10  20  30  120.083304  129.305839  123.288280\n1  20  30  40  224.766546  238.243573  227.156334\n2  30  40  50  351.511024  369.323706  352.136337\n3  40  50  60  496.789694  519.133894  494.974747\n\nAPI:\npandas.DataFrame.eval\n","label":[[85,89,"Mention"],[1598,1619,"API"]],"Comments":[]}
{"id":60260,"text":"ID:52078751\nPost:\nText: itertuples \nText: to avoid forcing your ints to floats \nCode: pd.Series([*df.itertuples(index=False)])\n\n0    (1, 2, 3.0)\n1    (3, 4, 5.0)\ndtype: object\n\nText: zip, map, splat... magic \nCode: pd.Series([*zip(*map(df.get, df))])\n\n0    (1, 2, 3.0)\n1    (3, 4, 5.0)\ndtype: object\n\nAPI:\npandas.DataFrame.itertuples\n","label":[[24,34,"Mention"],[306,333,"API"]],"Comments":[]}
{"id":60261,"text":"ID:52116923\nPost:\nText: pd.to_numeric \nCode: df['colname'] = pd.to_numeric(df['colname'], errors='coerce')\n\nText: This will produce np.nan for any thing it can't convert to a number. After this, you can fill in with any value you'd like with fillna \nCode: df['colname'] = df['colname'].fillna(0)\n\nText: All in one go \nCode: df['colname'] = pd.to_numeric(df['colname'], errors='coerce').fillna(0)\n\nAPI:\npandas.to_numeric\n","label":[[24,37,"Mention"],[402,419,"API"]],"Comments":[]}
{"id":60262,"text":"ID:52143874\nPost:\nText: In pandas loops are not recommended because slow if exist some vectorized solution. \nText: Notice: In function apply are loops under the hood too. \nText: So use get_dummies and DataFrame.add_prefix and join for add to original df: \nCode: df = df.join(pd.get_dummies(df['Hour'].astype(str)).add_prefix('HE'))\nprint (df)\n         Date  Hour  HE1  HE2  HE3  HE4\n0  2005-01-01     1    1    0    0    0\n1  2005-01-01     2    0    1    0    0\n2  2005-01-01     3    0    0    1    0\n3  2005-01-01     4    0    0    0    1\n\nText: Similar function have different performance: \nCode: df = pd.concat([df] * 1000, ignore_index=True)\n\nIn [62]: %timeit df.join(pd.get_dummies(df['Hour'].astype(str)).add_prefix('HE'))\n3.54 ms  277 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n\n#U9-Forward solution\nIn [63]: %timeit df.join(df['Hour'].astype(str).str.get_dummies().add_prefix('HE'))\n61.6 ms  297 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n\nAPI:\npandas.get_dummies\n","label":[[185,196,"Mention"],[985,1003,"API"]],"Comments":[]}
{"id":60263,"text":"ID:23364415\nPost:\nText: When the first argument of dia_matrix has the form (data, offsets), data is expected to be a 2-d array, with each row of data holding a diagonal of the matrix. Since data is a rectangular matrix, some of the elements in data are ignored. Subdiagonals are \"left aligned\", and superdiagonals are \"right aligned\". (Specifically, the mapping between data and the sparse matrix A is data[i,j] == A[j - offsets[i], j].) For example, consider the following that will be used to create a 5x5 matrix: \nCode: In [28]: data\nOut[28]: \narray([[ 1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10],\n       [11, 12, 13, 14, 15]])\n\nIn [29]: offsets\nOut[29]: [0, 1, -1]\n\nText: data contains three diagonals. The since offset[0] is 0, row 0 of data contains the main diagonal. All 5 elements in this row are used in the matrix. offset[1] is 1, so the data in data[1] becomes the first superdiagonal. Only the values [7, 8, 9, 10] will be used; the first value, 6, is ignored. Similarly, the third row of data gives the first subdiagonal, and only the values [11, 12, 13, 14] are used. \nCode: In [30]: a = dia_matrix((data, offsets), shape=(5, 5))\n\nIn [31]: a.A\nOut[31]: \narray([[ 1,  7,  0,  0,  0],\n       [11,  2,  8,  0,  0],\n       [ 0, 12,  3,  9,  0],\n       [ 0,  0, 13,  4, 10],\n       [ 0,  0,  0, 14,  5]])\n\nText: Your example can be rewritten as follows: \nCode: In [32]: N = 5\n\nIn [33]: data = np.ones((3, 5))\n\nIn [34]: A = dia_matrix((data, offsets), shape=(N, N), dtype=float)\n\nIn [35]: A.A\nOut[35]: \narray([[ 1.,  1.,  0.,  0.,  0.],\n       [ 1.,  1.,  1.,  0.,  0.],\n       [ 0.,  1.,  1.,  1.,  0.],\n       [ 0.,  0.,  1.,  1.,  1.],\n       [ 0.,  0.,  0.,  1.,  1.]])\n\nText: The dia_matrix docstring has another example. \nText: Alternatively, you can use sps.diags to create the matrix. This is useful if you already have code that generates the \"correctly\" sized diagonals. With diags, you don't have to create the rectangular data matrix. For example, \nCode: In [104]: from scipy.sparse import diags\n\nIn [105]: d0 = ones(n)\n\nIn [106]: dp1 = np.ones(N - 1)\n\nIn [107]: dm1 = np.ones(N - 1)\n\nIn [108]: d = [d0, dp1, dm1]\n\nIn [109]: B = diags(d, offsets, dtype=float)\n\nIn [110]: B.A\nOut[110]: \narray([[ 1.,  1.,  0.,  0.,  0.],\n       [ 1.,  1.,  1.,  0.,  0.],\n       [ 0.,  1.,  1.,  1.,  0.],\n       [ 0.,  0.,  1.,  1.,  1.],\n       [ 0.,  0.,  0.,  1.,  1.]])\n\nAPI:\nscipy.sparse.diags\n","label":[[1774,1783,"Mention"],[2388,2406,"API"]],"Comments":[]}
{"id":60264,"text":"ID:23687451\nPost:\nText: I used ndi and this gist as a template. This gets you almost there, you'll have to find a reasonable way to parameterize the curve from the mostly skeletonized image. \nCode: from scipy.misc import imread\nimport scipy.ndimage as ndimage\n\n# Load the image\nraw = imread(\"bG2W9mM.png\")\n\n# Convert the image to greyscale, using the red channel\ngrey = raw[:,:,0]\n\n# Simple thresholding of the image\nthreshold = grey>200\n\nradius = 10\ndistance_img = ndimage.distance_transform_edt(threshold)\nmorph_laplace_img = ndimage.morphological_laplace(distance_img, \n                                                  (radius, radius))\nskeleton = morph_laplace_img < morph_laplace_img.min()\/2\n\nimport matplotlib.cm as cm\nfrom pylab import *\nsubplot(221); imshow(raw)\nsubplot(222); imshow(grey, cmap=cm.Greys_r)\nsubplot(223); imshow(threshold, cmap=cm.Greys_r)\nsubplot(224); imshow(skeleton, cmap=cm.Greys_r)\nshow()\n\nText: You may find other answers that reference skeletonization useful, an example of that is here: \nText: Problems during Skeletonization image for extracting contours \nAPI:\nscipy.ndimage\n","label":[[31,34,"Mention"],[1096,1109,"API"]],"Comments":[]}
{"id":60265,"text":"ID:23819461\nPost:\nText: Generalized linear models, GLM, in statsmodels currently does not estimate the extra parameter of the Negative Binomial distribution. Negative Binomial belongs to the exponential family of distributions only for fixed shape parameter. \nText: However, statsmodels also has Negative Binomial as a Maximum Likelihood Model in discrete_model which estimates all parameters. \nText: The parameterization of the Negative Binomial for count regression is in terms of the mean or expected value, which is different from the parameterization in scipy.stats.nbinom. Actually, there are two different commonly used parameterization for the Negative Binomial count regression, usually called nb1 and nb2 \nText: Here is a quickly written script that recovers the nbinom parameters, n=size and p=prob from the estimated parameters. Once you have the parameters for the scipy.stats.distribution you can use all the available method, rvs, pmf, and so on. \nText: Something like this should be made available in statsmodels. \nText: In a few example runs, I got results like this \nCode: data generating parameters 50 0.25\nestimated params           51.7167511571 0.256814610633\nestimated params           50.0985814878 0.249989725917\n\nText: Aside, because of the underlying exponential reparameterization, the scipy optimizers have sometimes problems to converge. In those cases, either providing better starting values or using Nelder-Mead as optimization method usually helps. \nCode: import numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\n\n# generate some data to check\nnobs = 1000\nn, p = 50, 0.25\ndist0 = stats.nbinom(n, p)\ny = dist0.rvs(size=nobs)\nx = np.ones(nobs)\n\nloglike_method = 'nb1'  # or use 'nb2'\nres = sm.NegativeBinomial(y, x, loglike_method=loglike_method).fit(start_params=[0.1, 0.1])\n\nprint dist0.mean()\nprint res.params\n\nmu = res.predict()   # use this for mean if not constant\nmu = np.exp(res.params[0])   # shortcut, we just regress on a constant\nalpha = res.params[1]\n\nif loglike_method == 'nb1':\n    Q = 1\nelif loglike_method == 'nb2':    \n    Q = 0\n\nsize = 1. \/ alpha * mu**Q\nprob = size \/ (size + mu)\n\nprint 'data generating parameters', n, p\nprint 'estimated params          ', size, prob\n\n#estimated distribution\ndist_est = stats.nbinom(size, prob)\n\nText: BTW: I ran into this before but didn't have time to look at it https:\/\/github.com\/statsmodels\/statsmodels\/issues\/106 \nAPI:\nscipy.stats.nbinom\n","label":[[773,779,"Mention"],[2427,2445,"API"]],"Comments":[]}
{"id":60266,"text":"ID:23840407\nPost:\nText: Although it isn't said in the maxwell subset of st it is described in the rv_continuous as the scale parameter. \nAPI:\nscipy.stats\nscipy.stats.rv_continuous\n","label":[[72,74,"Mention"],[98,111,"Mention"],[142,153,"API"],[154,179,"API"]],"Comments":[]}
{"id":60267,"text":"ID:23879726\nPost:\nText: Yes, n-1 is the degrees of freedom in that example. \nText: Given a t-value and a degrees of freedom, you can use the \"survival function\" sf of t (aka the complementary CDF) to compute the one-sided p-value. The first argument is the T value, and the second is the degrees of freedom. \nText: For example, the first entry of the table on this page says that for 1 degree of freedom, the critical T value for p=0.1 is 3.078. Here's how you can verify that with t.sf: \nCode: In [7]: from scipy.stats import t\n\nIn [8]: t.sf(3.078, 1)\nOut[8]: 0.09999038172554342   # Approximately 0.1, as expected.\n\nText: For the two-sided p-value, just double the one-sided p-value. \nAPI:\nscipy.stats.t\n","label":[[167,168,"Mention"],[692,705,"API"]],"Comments":[]}
{"id":60268,"text":"ID:24382501\nPost:\nText: You could try using the fmni_obyla function, I don't know the numerical details so you should check it with values for which you know the expected answer and see if it works for your needs, play with the tolerance arguments rhoend and rhobeg and see if you get an expected answer, a sample program could be something like: \nCode: import numpy as np\nimport scipy.optimize\n\nA = \\\nnp.array([[ 1,  2,  3,  4,  5],\n          [ 2,  1,  3,  4,  5],\n          [-1,  2,  3,  0,  0],\n          [ 0,  0,  0,  0,  0],\n          [ 0,  0,  0,  0,  0]])\n\nB = \\\nnp.array([[0],\n          [2],\n          [3],\n          [0],\n          [0]])\n\ndef objfun(x):\n    H = 0.1*np.ones([5,5])\n    f = np.dot(np.transpose(x),np.dot(H,x))\n    return f\n\ndef constr1(x):\n    \"\"\" The constraint is satisfied when return value >=0 \"\"\"\n    sol = A*x\n    if np.allclose(sol, B):\n        return 0.01\n    else:\n        # Return the negative distance between the expected solution\n        # and the actual solution for a somehow meaningful value\n        return -np.linalg.norm(B-sol)\n\nscipy.optimize.fmin_cobyla(objfun, [0.0, 0.0, 0.0, 0.0,0.0], [constr1])\n#np.linalg.solve(A, b)\n\nText: Please note that this given example doesn't have a solution, try it with something that does. I am not completely sure that the constraint function is properly defined, try to find something that works well for you. You should try to provide an initial guess that it's an actual solution instead of [0.0, 0.0, 0.0, 0.0,0.0] for better results. \nText: Check the oficial documentation for more details: http:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.optimize.fmin_cobyla.html#scipy.optimize.fmin_cobyla \nText: Edit: Also depending what kind of solution you are looking for you could probably form a better constrain function, maybe allowing values that are around a certain tolerance distance from the expected solution even if not completely exact, and returning a value higher than 0 the closer they are to the tolerance instead of always 0.1, etc... \nAPI:\nscipy.optimize.fmin_cobyla\n","label":[[48,58,"Mention"],[2040,2066,"API"]],"Comments":[]}
{"id":60269,"text":"ID:24470571\nPost:\nText: The problem seems to be in the function's behaviour near zero. If the function is plotted, it looks smooth: \nText: However, quad complains about round-off errors, which is very strange with this beautiful curve. However, the function is not defined at 0 (of course, you are dividing by zero!), hence the integration does not go well. \nText: You may use a simpler integration method or do something about your function. You may also be able to integrate it to very close to zero from both sides. However, with these numbers the integral does not look right when looking at your results. \nText: However, I think I have a hunch of what your problem is. As far as I remember, the integral you have shown is actually the intensity (power\/area) of Fraunhofer diffraction as a function of distance from the center. If you want to integrate the total power within some radius, you will have to do it in two dimensions. \nText: By simple area integration rules you should multiply your function by 2 pi r before integrating (or x instead of r in your case). Then it becomes: \nCode: f = lambda(r): r*(sp.j1(r)\/r)**2\n\nText: or \nCode: f = lambda(r): sp.j1(r)**2\/r\n\nText: or even better: \nCode: f = lambda(r): r * (sp.j0(r) + sp.jn(2,r))\n\nText: The last form is best as it does not suffer from any singularities. It is based on Jaime's comment to the original answer (see the comment below this answer!). \nText: (Note that I omitted a couple of constants.) Now you can integrate it from zero to infinity (no negative radii): \nCode: fullpower = quad(f, 1e-9, np.inf)[0]\n\nText: Then you can integrate from some other radius and normalize by the full intensity: \nCode: pwr = quad(f, 1e-9, 3.8317)[0] \/ fullpower\n\nText: And you get 0.839 (which is quite close to 84 %). If you try the farther radius (13.33): \nCode: pwr = quad(f, 1e-9, 13.33)\n\nText: which gives 0.954. \nText: It should be noted that we introduce a small error by starting the integration from 1e-9 instead of 0. The magnitude of the error can be estimated by trying different values for the starting point. The integration result changes very little between 1e-9 and 1e-12, so they seem to be safe. Of course, you could use, e.g., 1e-30, but then there may be numerical instability in the division. (In this case there isn't, but in general singularities are numerically evil.) \nText: Let us do one thing still: \nCode: import matplotlib.pyplot as plt\nimport numpy as np\n\nx = linspace(0.01, 20, 1000)\nintg = np.array([ quad(f, 1e-9, xx)[0] for xx in x])\n\nplt.plot(x, intg\/fullpower)\nplt.grid('on')\nplt.show()\n\nText: And this is what we get: \nText: At least this looks right, the dark fringes of the Airy disk are clearly visible. \nText: What comes to the last part of the question: I0 defines the maximum intensity (the units may be, e.g. W\/m2), whereas the integral gives total power (if the intensity is in W\/m2, the total power is in W). Setting the maximum intensity to 100 does not guarantee anything about the total power. That is why it is important to calculate the total power. \nText: There actually exists a closed form equation for the total power radiated onto a circular area: \nText: P(x) = P0 ( 1 - J0(x)^2 - J1(x)^2 ), \nText: where P0 is the total power. \nAPI:\nscipy.integrate.quad\n","label":[[148,152,"Mention"],[3248,3268,"API"]],"Comments":[]}
{"id":60270,"text":"ID:24769222\nPost:\nText: As Lukas Graf hints, you are looking for cross-correlation. It works well, if: \nText: The scale of your images does not change considerably. There is no rotation change in the images. There is no significant illumination change in the images. \nText: For plain translations cross-correlation is very good. \nText: The simplest cross-correlation tool is scipy.signal.correlate. However, it uses the trivial method for cross-correlation, which is O(n^4) for a two-dimensional image with side length n. In practice, with your images it'll take very long. \nText: The better too is sig.fftconvolve as convolution and correlation are closely related. \nText: Something like this: \nCode: import numpy as np\nimport scipy.signal\n\ndef cross_image(im1, im2):\n   # get rid of the color channels by performing a grayscale transform\n   # the type cast into 'float' is to avoid overflows\n   im1_gray = np.sum(im1.astype('float'), axis=2)\n   im2_gray = np.sum(im2.astype('float'), axis=2)\n\n   # get rid of the averages, otherwise the results are not good\n   im1_gray -= np.mean(im1_gray)\n   im2_gray -= np.mean(im2_gray)\n\n   # calculate the correlation image; note the flipping of onw of the images\n   return scipy.signal.fftconvolve(im1_gray, im2_gray[::-1,::-1], mode='same')\n\nText: The funny-looking indexing of im2_gray[::-1,::-1] rotates it by 180 (mirrors both horizontally and vertically). This is the difference between convolution and correlation, correlation is a convolution with the second signal mirrored. \nText: Now if we just correlate the first (topmost) image with itself, we get: \nText: This gives a measure of self-similarity of the image. The brightest spot is at (201, 200), which is in the center for the (402, 400) image. \nText: The brightest spot coordinates can be found: \nCode: np.unravel_index(np.argmax(corr_img), corr_img.shape)\n\nText: The linear position of the brightest pixel is returned by argmax, but it has to be converted back into the 2D coordinates with unravel_index. \nText: Next, we try the same by correlating the first image with the second image: \nText: The correlation image looks similar, but the best correlation has moved to (149,200), i.e. 52 pixels upwards in the image. This is the offset between the two images. \nText: This seems to work with these simple images. However, there may be false correlation peaks, as well, and any of the problems outlined in the beginning of this answer may ruin the results. \nText: In any case you should consider using a windowing function. The choice of the function is not that important, as long as something is used. Also, if you have problems with small rotation or scale changes, try correlating several small areas agains the surrounding image. That will give you different displacements at different positions of the image. \nAPI:\nscipy.signal.fftconvolve\n","label":[[599,614,"Mention"],[2828,2852,"API"]],"Comments":[]}
{"id":60271,"text":"ID:24771340\nPost:\nText: in qaud there's a lower-level call to a difference function that is iterated over, and it's iterated over divmax times. In your case, the default for divmax=20. There are some functions that you can override this default -- for example scipy.integrate.quadrature.romberg allows you to set divmax (default here is 10) as a keyword. The warning is thrown when the tolerances aren't met for the difference function. Setting divmax to a higher value will run longer, and hopefully meet the tolerance requirements. \nAPI:\nscipy.integrate.quad\n","label":[[27,31,"Mention"],[540,560,"API"]],"Comments":[]}
{"id":60272,"text":"ID:24811169\nPost:\nText: Put all of the groups into one list (be it two arrays or 4 arrays in this example), and pass that to sp.stats.anderson_ksamp \nCode: In [12]:\n\nimport scipy.stats as ss\n#data from From the example given by Scholz and Stephens (1987, p.922)\nx1=[38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0]\nx2=[39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8]\nx3=[34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0]\nx4=[34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8]\nss.anderson_ksamp([x1,x2,x3,x4])\nOut[12]:\n(4.4797806271353506,\n array([ 0.49854918,  1.3236709 ,  1.91577682,  2.49304213,  3.24593219]),\n 0.0020491057074350956)\n\nText: It returns 3 values, 1: Normalized k-sample Anderson-Darling test statistic., 2: The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%., 3: the p-values. \nText: In this example, p values is 0.002, we conclude the samples are drawn from different populations. \nAPI:\nscipy.stats.anderson_ksamp\n","label":[[125,148,"Mention"],[945,971,"API"]],"Comments":[]}
{"id":60273,"text":"ID:24852786\nPost:\nText: In addition to np.polyfit and linregress as suggested by @user2589273, a low-level way to do linear regression is to solve for the matrix of coefficients using np.linalg.lstsq. Although this approach is a bit more work than using one of the pre-packaged functions for doing linear regression, it's very useful to understand how this works at a basic level, in particular when you start dealing with multivariate data. \nText: For example: \nCode: import numpy as np\n\n# a simple linear relationship: y = mx + c with m=0.5 and c=2\nx = np.arange(50)\ny = x * 0.5 + 2\ny += np.random.randn(50) * 5    # add some noise\n\n# we can rewrite the line equation as y = Ap, where A=[[x, 1]] and p=[[m], [c]]\nA = np.c_[x, np.ones(50)]\n\n# solving for p gives us the slope and intercept\np, residuals, rank, svals = np.linalg.lstsq(A, y)\n\nText: Plotting the fit: \nCode: from matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\nax.hold(True)\nax.plot(x, y, 'ob', label='data')\nax.plot(x, A.dot(p), '-k', lw=2, label='linear fit')\nax.legend()\n\nAPI:\nscipy.stats.linregress\n","label":[[54,64,"Mention"],[1059,1081,"API"]],"Comments":[]}
{"id":60274,"text":"ID:24860625\nPost:\nText: sp.stats.rv_discrete has you covered. It'll help you make a random distribution class from your data. The result will have a whole slew of handy methods. The .stats method will give you the first four moments. If you don't specify, it'll just return mean (m) and variance (v). \nCode: b2=bin[:-1]\nprint mean(a), var(a), scipy.stats.skew(a)\ndist = scipy.stats.rv_discrete(values=(b2,n))\nprint dist.stats(moments='mvsk')\n\nText: The above should be compatible with your code. Just reorganize to make use of the output. \nAPI:\nscipy.stats.rv_discrete\n","label":[[24,44,"Mention"],[545,568,"API"]],"Comments":[]}
{"id":60275,"text":"ID:25039338\nPost:\nText: It is normal for the odeint solver to evaluate your function at time values past the last requested time. Most ODE solvers work this way--they take internal time steps with sizes determined by their error control algorithm, and then use their own interpolation to evaluate the solution at the times requested by the user. Some solvers (e.g. the CVODE solver in the Sundials library) allow you to specify a hard bound on the time, beyond which the solver is not allowed to evaluate your equations, but odeint does not have such an option. \nText: If you don't mind switching from odein to scipy.integrate.ode, it looks like the \"dopri5\" and \"dop853\" solvers don't evaluate your function at times beyond the requested time. Two caveats: \nText: The ode solvers use a different convention for the order of the arguments that define the differential equation. In the ode solvers, t is the first argument. (Yeah, I know, grumble, grumble...) The \"dopri5\" and \"dop853\" solvers are for non-stiff systems. If your problem is stiff, they should still give correct answers, but they will do a lot more work than a stiff solver would do. \nText: Here's a script that shows how to solve your example. To emphasize the change in the arguments, I renamed func to rhs. \nCode: import numpy as np\nfrom scipy.integrate import ode\nfrom scipy.interpolate import interp1d\n\n\nt = np.linspace(0, 3, 4)\ndata = [1, 2, 3, 4]\nlinear_interpolation = interp1d(t, data)\n\ndef rhs(t, y):\n    \"\"\"The \"right-hand side\" of the differential equation.\"\"\"\n    #print 't', t\n    return -2*y + linear_interpolation(t)\n\n\n# Initial condition\ny0 = 1\n\nsolver = ode(rhs).set_integrator(\"dop853\")\nsolver.set_initial_value(y0)\n\nk = 0\nsoln = [y0]\nwhile solver.successful() and solver.t < t[-1]:\n    k += 1\n    solver.integrate(t[k])\n    soln.append(solver.y)\n\n# Convert the list to a numpy array.\nsoln = np.array(soln)\n\nText: The rest of this answer looks at how you could continue to use odeint. \nText: If you are only interested in linear interpolation, you could simply extend your data linearly, using the last two points of the data. A simple way to extend the data array is to append the value 2*data[-1] - data[-2] to the end of the array, and do the same for the t array. If the last time step in t is small, this might not be a sufficiently long extension to avoid the problem, so in the following, I've used a more general extension. \nText: Example: \nCode: import numpy as np\nfrom scipy.integrate import odeint\nfrom scipy.interpolate import interp1d\n\nt = np.linspace(0, 3, 4)\n\ndata = [1, 2, 3, 4]\n\n# Slope of the last segment.\nm = (data[-1] - data[-2]) \/ (t[-1] - t[-2])\n# Amount of time by which to extend the interpolation.\ndt = 3.0\n# Extended final time.\nt_ext = t[-1] + dt\n# Extended final data value.\ndata_ext = data[-1] + m*dt\n\n# Extended arrays.\nextended_t = np.append(t, t_ext)\nextended_data = np.append(data, data_ext)\n\nlinear_interpolation = interp1d(extended_t, extended_data)\n\ndef func(y, t0):\n    print 't0', t0\n    return -2*y + linear_interpolation(t0)\n\nsoln = odeint(func, 1, t)\n\nText: If simply using the last two data points to extend the interpolator linearly is too crude, then you'll have to use some other method to extrapolate a little beyond the final t value given to odeint. \nText: Another alternative is to include the final t value as an argument to func, and explicitly handle t values larger than it in func. Something like this, where extrapolation is something you'll have to figure out: \nCode: def func(y, t0, tmax):\n    if t0 > tmax:\n        f = -2*y + extrapolation(t0)\n    else:\n        f = -2*y + linear_interpolation(t0)\n    return f\n\nsoln = odeint(func, 1, t, args=(t[-1],))\n\nAPI:\nscipy.integrate.odeint\n","label":[[602,607,"Mention"],[3702,3724,"API"]],"Comments":[]}
{"id":60276,"text":"ID:25099152\nPost:\nText: BLAS is just used for dense floating-point matrices. Matrix multiplication of a csr_matrix is done using pure C++ functions that don't make any calls to external BLAS libraries. \nText: For example, matrix-matrix multiplication is implemented here, in csr_matmat_pass_1 and csr_matmat_pass_2. \nText: Optimised BLAS libraries are highly tuned to make efficient use of CPU caches by decomposing the dense input matrices into smaller block matrices in order to achieve better locality-of-reference. My understanding is that this strategy can't be easily applied to sparse matrices, where the non-zero elements may be arbitrarily distributed within the matrix. \nAPI:\nscipy.sparse.csr_matrix\n","label":[[104,114,"Mention"],[686,709,"API"]],"Comments":[]}
{"id":60277,"text":"ID:25110947\nPost:\nText: There are many ways to approach this. The first thing that comes to mind is to numerically differentiate the data, and look for the jump in the slope from 0 to 0.5. But (as you observed) noisy data can prevent this from working well. If you google for \"numerical differentiation of noisy data\", you'll find a lot of research on this topic, but I don't know of any off-the-shelf libraries in python. You might be able to make some progress using a Savitzky-Golay filter: scipy.signal.savgol_filter. \nText: However, that approach is probably overkill, since your signal has a very simple and specific expected structure: a constant interval followed by a ramp and then another constant. You might find that curve_fit works fine for this. Here's an example: \nCode: from __future__ import division\n\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n\ndef ramp(t, temp_init, temp_final, t0, t1):\n    slope = (temp_final - temp_init) \/ (t1 - t0)\n    y = temp_init + np.minimum(slope * np.maximum(t - t0, 0.0), temp_final - temp_init)\n    return y\n\nnp.random.seed(123)\n\ndt = 0.5\nt = np.arange(0, 100, dt)\n\n# Generate a sample to work with.\ntemp_init = 20.0 + np.random.randn()\ntemp_final = 40.0 + np.random.randn()\nt0 = 25.0\nt1 = t0 + 40\ny = ramp(t, temp_init, temp_final, t0, t1)\ny += 0.25*np.random.randn(*t.shape)  # Add noise.\n\n# Create an initial guess for the four parameters and use curve_fit\n# to fit the ramp function to the sample.\nT = t[-1] - t[0]\np0 = (20, 40, t[0] + 0.333*T, t[0] + 0.667*T)\npopt, pcov = curve_fit(ramp, t, y, p0=p0)\nfit_temp_init, fit_temp_final, fit_t0, fit_t1 = popt\n\n\nprint \"             Input    Fit\"\nprint \"temp_init   %6.2f  %6.2f\" % (temp_init, fit_temp_init)\nprint \"temp_final  %6.2f  %6.2f\" % (temp_final, fit_temp_final)\nprint \"t0          %6.2f  %6.2f\" % (t0, fit_t0)\nprint \"t1          %6.2f  %6.2f\" % (t1, fit_t1)\n\nplt.plot(t, y, 'ro', alpha=0.15)\nplt.plot(t, ramp(t, popt[0], popt[1], popt[2], popt[3]), 'k-', linewidth=1.5)\nplt.grid(True)\nplt.xlabel('t', fontsize=12)\nplt.show()\n\nText: This generates the output: \nCode:              Input    Fit\ntemp_init    18.91   18.91\ntemp_final   41.00   40.99\nt0           25.00   24.85\nt1           65.00   65.09\n\nText: and the plot: \nAPI:\nscipy.optimize.curve_fit\n","label":[[729,738,"Mention"],[2282,2306,"API"]],"Comments":[]}
{"id":60278,"text":"ID:25192640\nPost:\nText: A few comments: \nText: The Nyquist frequency is half the sampling rate. You are working with regularly sampled data, so you want a digital filter, not an analog filter. This means you should not use analog=True in the call to butter, and you should use sp.signal.freqz (not freqs) to generate the frequency response. One goal of those short utility functions is to allow you to leave all your frequencies expressed in Hz. You shouldn't have to convert to rad\/sec. As long as you express your frequencies with consistent units, the fs parameter of the SciPy functions will take care of the scaling for you. \nText: Here's my modified version of your script, followed by the plot that it generates. \nCode: import numpy as np\nfrom scipy.signal import butter, lfilter, freqz\nimport matplotlib.pyplot as plt\n\n\ndef butter_lowpass(cutoff, fs, order=5):\n    return butter(order, cutoff, fs=fs, btype='low', analog=False)\n\ndef butter_lowpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n\n\n# Filter requirements.\norder = 6\nfs = 30.0       # sample rate, Hz\ncutoff = 3.667  # desired cutoff frequency of the filter, Hz\n\n# Get the filter coefficients so we can check its frequency response.\nb, a = butter_lowpass(cutoff, fs, order)\n\n# Plot the frequency response.\nw, h = freqz(b, a, fs=fs, worN=8000)\nplt.subplot(2, 1, 1)\nplt.plot(w, np.abs(h), 'b')\nplt.plot(cutoff, 0.5*np.sqrt(2), 'ko')\nplt.axvline(cutoff, color='k')\nplt.xlim(0, 0.5*fs)\nplt.title(\"Lowpass Filter Frequency Response\")\nplt.xlabel('Frequency [Hz]')\nplt.grid()\n\n\n# Demonstrate the use of the filter.\n# First make some data to be filtered.\nT = 5.0         # seconds\nn = int(T * fs) # total number of samples\nt = np.linspace(0, T, n, endpoint=False)\n# \"Noisy\" data.  We want to recover the 1.2 Hz signal from this.\ndata = np.sin(1.2*2*np.pi*t) + 1.5*np.cos(9*2*np.pi*t) + 0.5*np.sin(12.0*2*np.pi*t)\n\n# Filter the data, and plot both the original and filtered signals.\ny = butter_lowpass_filter(data, cutoff, fs, order)\n\nplt.subplot(2, 1, 2)\nplt.plot(t, data, 'b-', label='data')\nplt.plot(t, y, 'g-', linewidth=2, label='filtered data')\nplt.xlabel('Time [sec]')\nplt.grid()\nplt.legend()\n\nplt.subplots_adjust(hspace=0.35)\nplt.show()\n\nAPI:\nscipy.signal.freqz\n","label":[[277,292,"Mention"],[2293,2311,"API"]],"Comments":[]}
{"id":60279,"text":"ID:25230202\nPost:\nText: The following code \nText: import numpy as np from scipy import linalg as la import scipy print np.__version__ print scipy.__version__ def build_laplacian(n): lap=np.zeros([n,n]) for j in range(n-1): lap[j+1][j]=1 lap[j][j+1]=1 lap[n-1][n-2]=1 lap[n-2][n-1]=1 return lap def evolve(s, lap): wave=la.expm(-1j*s*lap).dot([1]+[0]*(lap.shape[0]-1)) for i in range(len(wave)): wave[i]=la.norm(wave[i])**2 return wave r = evolve(2, build_laplacian(500)) print np.min(abs(r)) print r[59] \nText: prints \nText: 1.8.1 0.14.0 0 (2.77560227344e-101+0j) \nText: for me, with OpenBLAS 0.2.8-6ubuntu1. \nText: So it appears your problem is not immediately reproduced. Your code examples above are not runnable as-is (typos). \nText: As mentioned in sp.linalg.expm documentation, the algorithm is from Al-Mohy and Higham (2009), which is different from the simpler scale-and-square-Pade in Octave. \nText: As a consequence, the results also I get from Octave are slightly different, although the results are eps-close in matrix norms (1,2,inf). MATLAB uses the Pade approach from Higham (2005), which seems to give the same results as Scipy above. \nAPI:\nscipy.linalg.expm\n","label":[[754,768,"Mention"],[1157,1174,"API"]],"Comments":[]}
{"id":60280,"text":"ID:25264697\nPost:\nText: pdf is a Python function meaning that it is callable just like any other function. \nCode: from scipy.stats import uniform\nimport numpy as np\n\nx = np.linspace(0,1,10)\n\npdf = uniform.pdf(x) # Create a uniform pdf over the range [0,1].\n\nprint(pdf)\n# [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n\nText: You can see the arguments that uniform.pdf() takes at the bottom of this docs page. \nAPI:\nscipy.stats.uniform.pdf\n","label":[[24,27,"Mention"],[410,433,"API"]],"Comments":[]}
{"id":60281,"text":"ID:25493294\nPost:\nText: np.interp performs 1D linear interpolation: \nCode: newx = np.linspace(minX, maxX, 1000)\nnewy = np.interp(newx, x, y)\n\nText: Or, using inp.interp1d you can interpolate with splines. For example, kind='cubic' gives you third order spline interpolation: \nCode: import scipy.interpolate as interpolate\nnewx = np.linspace(minX, maxX, 1000)\nnewy = interpolate.interp1d(x, y, kind='cubic')(newx)\n\nAPI:\nscipy.interpolate.interp1d\n","label":[[158,170,"Mention"],[419,445,"API"]],"Comments":[]}
{"id":60282,"text":"ID:25650142\nPost:\nText: To directly answer your question, you are correct - Degrees of freedom being 0 is invalid\/useless. cheisquare parameter isn't degrees of freedom but an adjustment to degrees of freedom. Degrees of freedom defaults to k - 1 when ddof = 0. k can be directly determined from the data you pass to the chisquare function. \nText: From the Scipy chiqsuare documentation \nText: ddof : int, optional Delta degrees of freedom: adjustment to the degrees of freedom for the p-value. The p-value is computed using a chi-squared distribution with k - 1 - ddof degrees of freedom, where k is the number of observed frequencies. The default value of ddof is 0. \nText: ddof is the delta not the absolute value for degrees of freedom. So degrees of freedom is k - 1 and ddof is an adjustment subtracted from k - 1. So when ddof = 0, degrees of freedom = k - 1 - 0 or k - 1 . When ddof = 1, degrees of freedom = k - 1 - 1 or k - 2 . k is the number of observed frequencies and the chisquare function can determine that from the data you pass \nAPI:\nscipy.stats.chisquare\n","label":[[123,133,"Mention"],[1055,1076,"API"]],"Comments":[]}
{"id":60283,"text":"ID:25664700\nPost:\nText: I use st.gaussian_kde to distinguish the locus from the scatter points. I can determine the outliers by excluding objects in low density regions with the criteria that their density should be less than mean_kde divided with 20, e.g. in my case. I wrote a function that can deal with too many similar feature in different two-dimensional space and exclude all the points with similar problem from all of them at once. I also illustrated the excluded scatter points with objects with empty black circle around. \nCode: import numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport pandas as pd\nimport pylab as plt\n\ndef refineLocus(Colorx,Colory, ratio):\n    \"\"\"\n    Remove objects whose density is below 1\/ratio of the mean density.\n    The point density is estimated by Gaussian kernel convolution,\n    with automatic bandwidth determination.\n    Inputs: the color pair Colorx and Colory (the x- and y-color)\n            the threshold ratio for the cut\n    Output: object index array\n    \"\"\"\n    for j in range(Colorx.shape[1]):\n        X=np.array([[Colorx[i,j],Colory[i,j]] for i in range(Colorx.shape[0])])\n        data = pd.DataFrame(X, columns=[\"X\", \"Y\"])\n        kernel = stats.gaussian_kde(X.T)\n        kernel.set_bandwidth(bw_method='scott')\n        KdeEval=np.zeros(Colorx.shape[0],float)\n        for i in range(Colorx.shape[0]):\n            KdeEval[i]=kernel.evaluate(X[i,:])\n        ex=np.where(KdeEval<KdeEval.mean()\/ratio)\n        if (j==0):\n           exarr=ex[0]\n        else:\n           if len(ex[0])!=0:\n              exarr=np.unique(np.concatenate((exarr,ex[0])))\n    for j in range(Colorx.shape[1]):\n        X=np.array([[Colorx[i,j],Colory[i,j]] for i in range(Colorx.shape[0])])\n        data = pd.DataFrame(X, columns=[\"X\", \"Y\"])\n        sns.kdeplot(data.X,data.Y,bw='scott',shade=True, cmap=\"Purples\") \n        plt.scatter(Colorx[:,j],Colory[:,j],marker='.',s=2,color='red')\n        plt.scatter(Colorx[exarr,j],Colory[exarr,j],s=10,facecolors='none', edgecolors='k')\n        figname = 'refineLocus.kde.%d.pdf' % (j, )\n        plt.savefig(figname)\n        plt.close()\n    return exarr\n\nif __name__ == \"__main__\":\n    Cy=np.loadtxt('Colory.asc')\n    Cx=np.loadtxt('Colorx.asc')\n    arr=refineLocus(Cx,Cy, ratio=25)\n    r=arange(Cx.shape[0])\n    new_r=setxor1d(r,arr)\n    nCX=Cx[new_r]\n    nCY=Cy[new_r]\n\nAPI:\nscipy.stats.gaussian_kde\n","label":[[30,45,"Mention"],[2359,2383,"API"]],"Comments":[]}
{"id":60284,"text":"ID:25729040\nPost:\nText: The savemat saves sparse matrices in a MATLAB compatible format: \nCode: In [1]: from scipy.io import savemat, loadmat\nIn [2]: from scipy import sparse\nIn [3]: M = sparse.csr_matrix(np.arange(12).reshape(3,4))\nIn [4]: savemat('temp', {'M':M})\n\nIn [8]: x=loadmat('temp.mat')\nIn [9]: x\nOut[9]: \n{'M': <3x4 sparse matrix of type '<type 'numpy.int32'>'\n    with 11 stored elements in Compressed Sparse Column format>,\n '__globals__': [],\n '__header__': 'MATLAB 5.0 MAT-file Platform: posix, Created on: Mon Sep  8 09:34:54 2014',\n '__version__': '1.0'}\n\nIn [10]: x['M'].A\nOut[10]: \narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\nText: Note that savemat converted it to csc. It also transparently takes care of the index starting point difference. \nText: And in Octave: \nCode: octave:4> load temp.mat\noctave:5> M\nM =\nCompressed Column Sparse (rows = 3, cols = 4, nnz = 11 [92%])\n  (2, 1) ->  4\n  (3, 1) ->  8\n  (1, 2) ->  1\n  (2, 2) ->  5\n  ...\n\noctave:8> full(M)\nans =    \n    0    1    2    3\n    4    5    6    7\n    8    9   10   11\n\nAPI:\nscipy.io.savemat\n","label":[[28,35,"Mention"],[1091,1107,"API"]],"Comments":[]}
{"id":60285,"text":"ID:26007299\nPost:\nText: So far the best solution I found is to do it through matplotlib.mlab using griddata with linear interpolation. \nCode: import matplotlib.mlab as ml\nzi = ml.griddata(x,y,z,xi,yi,interp='linear') \n\nText: Update: Two additional improvements. I realized that matplotlib.mlab griddata is not really same as sp.interpolate griddata and the second is better for my case. Also my coordinates are in millions which for big grids is causing big troubles with presicision (for underlying Qhull library) so it is better to shift all coordinates close to origin and after calculation shift back. \nCode: from scipy.interpolate import griddata\nx -= shift_x\ny -= shift_y\nzi = griddata((x,y),z,(xi,yi),method='linear')\nxi += shift_x\nyi += shift_y\n\nAPI:\nscipy.interpolate\n","label":[[325,339,"Mention"],[759,776,"API"]],"Comments":[]}
{"id":60286,"text":"ID:26012533\nPost:\nText: There is no relationship because you are doing two different things. \nText: With scipy.ndimage.filters.gaussian_filter, you are filtering a 2D variable (an image) with a kernel, and that kernel happens to be a gaussian. It is essentially smoothing the image. \nText: With gaussian_kde you try to estimate the probability density function of your 2D-variable. The bandwidth (or smoothing parameter) is your integration step, and should be as small as the data allows. \nText: The two images look the same because your uniform distribution, from which you drew the samples, is not that different from a normal distribution. Obviously you'd get a better estimate with a normal kernel function. \nText: You can read about Kernel density estimation. \nText: Edit: In Kernel Density Estimation (KDE), the kernels are scaled such that the bandwidth is the standard deviation of the smoothing kernel. Which bandwidth to use is not obvious as it depends on the data. There exists an optimal choice for univariate data, called the Silverman's rule of thumb. \nText: To summarize, there is no relationship between the standard deviation of a gaussian filter, and the bandwidth of a KDE, because we're talking oranges and apples. However, talking about KDE only, there is a relationship between the KDE bandwidth and the standard deviation of the same KDE kernel. They are equal! Well in truth implementation details differ, and there may be a scaling that depends on the size of the kernel. You could read your specific package gaussian_kde.py \nAPI:\nscipy.stats.gaussian_kde\n","label":[[295,307,"Mention"],[1558,1582,"API"]],"Comments":[]}
{"id":60287,"text":"ID:26080894\nPost:\nText: As Warren Weckesser suggested, I can simply follow the Scipy cookbook for the coupled mass-spring system. First, I need to write my \"right side\" equations as: \nCode: x'  = vx\ny'  = vy\nz'  = vz\nvx' = Ac*x\/r\nvy' = Ac*y\/r + q*E\/m\nvz' = Ac*z\/r \n\nText: where Ac=keq^2\/(mr^2) is the magnitude of the acceleration due to the Coulomb potential and E is the time-dependent electric field of the laser. Then, I can use ODEint to find the solutions. This is faster and more reliable than the method that I was using previously. \nText: Here is what the electron trajectories look like with odeint. Now none of them fly away crazily: \nText: And here is the code: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.integrate\n\nq  = 1.602e-19    #Coulombs   Charge of electron\nc  = 3.0e8        #m\/s        Speed of light\neo = 8.8541e-12   #C^2\/(Nm^2) Permittivity of vacuum\nme = 9.109e-31    #kg         Mass of electron\nke = 8.985551e9   #N m^2 C-2  Coulomb's constant\n\ndef tunnel_position(tb,intensity,wavelength,pulseFWHM,Ip):\n    Ip = 15.13 * q  \n    Eb = E_laser(tb,intensity,wavelength,pulseFWHM) \n    return -Ip \/ (Eb*q) \n\ndef E_laser(t,intensity,wavelength,pulseFWHM):\n    w = c\/wavelength * 2. * np.pi #Angular frequency of the laser\n    Eo = np.sqrt(2*intensity*10**4\/(c*8.85e-12)) # Electric field in V\/m\n    return Eo*np.sin(w*t) * np.exp(-t**2\/(2*(pulseFWHM \/ 2.35482)**2))\n\ndef vectorfield(variables,t,params):\n    x,y,z,vx,vy,vz = variables\n    intensity,wavelength,pulseFWHM,tb = params\n    r = np.sqrt(x**2+y**2+z**2)\n    Ac = -ke*q**2\/(r**2*me)\n    return [vx,vy,vz,\n         Ac*x\/r,\n         Ac*y\/r + q\/me * E_laser((t-tb),intensity,wavelength,pulseFWHM),\n         Ac*z\/r]\n\nIp  = 15.13   # Ionization potential of Argon eV\nintensity  = 2e14\nwavelength = 800e-9\npulseFWHM  = 40e-15\n\nperiod  = wavelength\/c\nt = np.linspace(0,20*period,10000)\n\nbirth_times = np.linspace(0.01*period,0.999*period,50)\nmax_field   = np.max(np.abs(E_laser(birth_times,intensity,wavelength,pulseFWHM)))\n\nfor tb in birth_times:\n    x0  = 0 \n    y0  = tunnel_position(tb,intensity,wavelength,pulseFWHM,Ip)\n    z0  = 0\n    vx0 = 2e4\n    vy0 = 0\n    vz0 = 0\n\n    p = [intensity,wavelength,pulseFWHM,tb]\n    w0 = [x0,y0,z0,vx0,vy0,vz0]\n\n    solution,info = scipy.integrate.odeint(vectorfield,w0,t, args=(p,),full_output=True)\n    print 'Tb: %.2f fs - smallest step : %.05f attosec'%((tb*1e15),np.min(info['hu'])*1e18)\n\n    y = solution[:,1]\n\n    importance = (np.abs(E_laser(tb,intensity,wavelength,pulseFWHM))\/max_field)\n    plt.plot(t,y,alpha=importance*0.8,lw=1)\n\nplt.xlabel('Time (sec)')\nplt.ylabel('Y-distance (meters)')\n\nplt.show()\n\nAPI:\nscipy.integrate.odeint\n","label":[[433,439,"Mention"],[2662,2684,"API"]],"Comments":[]}
{"id":60288,"text":"ID:26288975\nPost:\nText: Directly using scipy.optimize.minimize() the code below solves this problem. Note that with more points yn you will tend to get the same result as x_true, otherwise more than one solution exists. You can minimize the effect of the ill-constrained optimization by adding boundaries (see the bounds parameter used below). \nCode: import numpy as np\nfrom scipy.optimize import minimize\n\ndef residual(x, a, y):\n    s = ((y - a.dot(x**2))**2).sum()\n    return s\n\ndef main():\n    M = 3\n    N = 5\n    a = np.random.random((M, N))\n    x_true = np.array([10, 13, 5, 8, 40])\n    y = a.dot(x_true**2)\n\n    x0 = np.array([2, 3, 1, 4, 20])\n    bounds = [[0, None] for x in x0]\n    out = minimize(residual, x0=x0, args=(a, y), method='L-BFGS-B', bounds=bounds)\n    print(out.x)\n\nText: If M>=N you could also use opt.leastsq for this task: \nCode: import numpy as np\nfrom scipy.optimize import leastsq\n\ndef residual(x, a, y):\n    return y - a.dot(x**2)\n\ndef main():\n    M = 5\n    N = 5\n    a = np.random.random((M, N))\n    x_true = np.array([10, 13, 5, 8, 40])\n    y = a.dot(x_true**2)\n\n    x0 = np.array([2, 3, 1, 4, 20])\n    out = leastsq(residual, x0=x0, args=(a, y))\n    print(out[0])\n\nAPI:\nscipy.optimize.leastsq\n","label":[[821,832,"Mention"],[1202,1224,"API"]],"Comments":[]}
{"id":60289,"text":"ID:26317065\nPost:\nText: X0 is not a regular floating point value; it is an mpf-type from mpmath (which is included as part of sympy). k0 doesn't know what that is. Change this: \nCode: G = sp.k0(X0)\n\nText: to this: \nCode: G = sp.k0(float(X0))\n\nAPI:\nscipy.special.k0\n","label":[[134,136,"Mention"],[248,264,"API"]],"Comments":[]}
{"id":60290,"text":"ID:26747232\nPost:\nText: I have created a wrapper of odeint called odeintw that can handle complex matrix equations such as this. See How to plot the Eigenvalues when solving matrix coupled differential equations in PYTHON? for another question involving a matrix differential equation. \nText: Here's a simplified version of your code that shows how you could use it. (For simplicity, I got rid of most of the constants from your example). \nCode: import numpy as np\nfrom odeintw import odeintw\n\n\ndef right_part(rho, t, w_p):\n    hamiltonian = (1. \/ 2) * np.array(\n        [[0.1, 0.01, 1.0 \/ 2.0 * np.sin(t * w_p)],\n        [0.01, 0.0, 0.0],\n        [1.0 \/ 2.0 * np.sin(t * w_p), 0.0, 0.0]],\n        dtype=np.complex128)\n    return (np.dot(hamiltonian, rho) - np.dot(rho, hamiltonian)) \/ (1j)\n\n\npsi_init = np.array([[1.0, 0.0, 0.0],\n                     [0.0, 0.0, 0.0],\n                     [0.0, 0.0, 0.0]], dtype=np.complex128)\n\n\nt = np.linspace(0, 10, 101)\nsol = odeintw(right_part, psi_init, t, args=(0.25,))\n\nText: sol will be a complex numpy array with shape (101, 3, 3), holding the solution rho(t). The first index is the time index, and the other two indices are the 3x3 matrix. \nAPI:\nscipy.integrate.odeint\n","label":[[52,58,"Mention"],[1193,1215,"API"]],"Comments":[]}
{"id":60291,"text":"ID:26846796\nPost:\nText: What you seem to be looking for is the mean over blocks of 4, which is not obtainable with zoom, since zoom uses interpolation (see its docstring) \nText: To obtain what you show, try the following \nCode: import numpy as np\nx = np.arange(16).reshape(4, 4)\n\nxx = x.reshape(len(x) \/\/ 2, 2, x.shape[1] \/\/ 2, 2).transpose(0, 2, 1, 3).reshape(len(x) \/\/ 2, x.shape[1] \/\/ 2, -1).mean(-1)\n\nprint xx\n\nText: This yields \nCode: [[  2.5   4.5]\n [ 10.5  12.5]]\n\nText: Alternatively, this can be done using sklearn.feature_extraction.image.extract_patches \nCode: from sklearn.feature_extraction.image import extract_patches\n\npatches = extract_patches(x, patch_shape=(2, 2), extraction_step=(2, 2))\n\nxx = patches.mean(-1).mean(-1)\n\nprint xx\n\nText: However, if your goal is to subsample an image in a graceful way, then taking the mean over blocks of the image is not the right way to do it: It is likely to cause aliasing effects. What you should do in this case is smooth the image ever so slightly using ndi.gaussian_filter (e.g. sigma=0.35 * subsample_factor) and then subsample simply by indexing [::2, ::2] \nAPI:\nscipy.ndimage.gaussian_filter\n","label":[[1014,1033,"Mention"],[1126,1155,"API"]],"Comments":[]}
{"id":60292,"text":"ID:26868541\nPost:\nText: Your data format is almost the internal structure of a sps.lil_matrix (list of lists). You should first generate one of those, and then call .tocsr() on it to obtain the desired csr matrix. \nText: A small example on how to populate these: \nCode: from scipy.sparse import lil_matrix\n\npositions = [[1, 2, 10], [], [5, 6, 2]]\ndata = [[1, 1, 1], [], [1, 1, 1]]\n\nl = lil_matrix((3, 11))\nl.rows = positions\nl.data = data\n\nc = l.tocsr()\n\nText: where data is just a list of lists of ones mirroring the structure of positions and positions would correspond to your feature indices. As you can see, the attributes l.rows and l.data are real lists here, so you can append data as it comes. In that case you need to be careful with the shape, though. When scipy generates these lil_matrix from other data, then it will put arrays of dtype object, but those are almost lists, too. \nAPI:\nscipy.sparse.lil_matrix\n","label":[[79,93,"Mention"],[898,921,"API"]],"Comments":[]}
{"id":60293,"text":"ID:26888164\nPost:\nText: I found that a particular sparse matrix constructor can achieve the desired result very efficiently. It's a bit obscure but we can abuse it for this purpose. The function below can be used in nearly the same way as binned_statistic but can be orders of magnitude faster \nCode: import numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef binned_statistic(x, values, func, nbins, range):\n    '''The usage is nearly the same as scipy.stats.binned_statistic''' \n\n    N = len(values)\n    r0, r1 = range\n\n    digitized = (float(nbins)\/(r1 - r0)*(x - r0)).astype(int)\n    S = csr_matrix((values, [digitized, np.arange(N)]), shape=(nbins, N))\n\n    return [func(group) for group in np.split(S.data, S.indptr[1:-1])]\n\nText: I avoided np.digitize because it doesn't use the fact that all bins are equal width and hence is slow, but the method I used instead may not handle all edge cases perfectly. \nAPI:\nscipy.stats.binned_statistic\n","label":[[239,255,"Mention"],[918,946,"API"]],"Comments":[]}
{"id":60294,"text":"ID:27300799\nPost:\nText: When the data is 1-dimensional, griddata defers to interpolate.interp1d: \nCode: if ndim == 1 and method in ('nearest', 'linear', 'cubic'):\n    from .interpolate import interp1d\n    points = points.ravel()\n    ...\n    ip = interp1d(points, values, kind=method, axis=0, bounds_error=False,\n                  fill_value=fill_value)\n    return ip(xi)\n\nText: So even though method='nearest' griddata will not extrapolate since interp1d behaves this way. \nText: However, there are other tools, such as vq (vector quantization), which you could use to find the nearest value. For example, \nCode: import numpy as np\nimport scipy.cluster.vq as vq\nimport matplotlib.pyplot as plt\n\ntarget_points = np.array([1.,2.,3.,4.,5.,6.,7.])\npoints = (np.random.rand(50)*2*np.pi)\nvalues = np.sin(points)\ncode, dist = vq.vq(target_points, points)\ninterp = values[code]\n\nplt.plot(points,values,'o')\nplt.plot(target_points,interp,'ro')\nprint interp\nplt.show()\n\nAPI:\nscipy.cluster.vq\n","label":[[520,522,"Mention"],[965,981,"API"]],"Comments":[]}
{"id":60295,"text":"ID:27472941\nPost:\nText: If you use the function minimize_scalar you get the expected result: \nCode: results = minimize_scalar(error_p, tol=0.00001)\nprint results['x'], results['fun']\n>>> 1.88536329298 0.000820148069544\n\nText: Why does minigize not work? My guess is that your function error_p is malformed from a numpy perspective. Try this: \nCode: MU = np.linspace(0,20,100)\nerror_p(MU)\n\nText: and you'll see that it fails. Your function isn't tailored to take in an array of inputs and spit out an array of outputs, which I think is what minimize is looking for. \nAPI:\nscipy.optimize.minimize_scalar\nscipy.optimize.minimize\n","label":[[48,63,"Mention"],[235,243,"Mention"],[571,601,"API"],[602,625,"API"]],"Comments":[]}
{"id":60296,"text":"ID:27485178\nPost:\nText: You can 'convert' a datasets (arrays) to continuous functions by means of interpolation. sp.interpolate.interp1d is a factory that provides you with the resulting function, which you could then use with your root finding algorithm. --edit-- an example for computing an intersection of sin and cos from 20 samples (I've used cubic spline interpolation, as piecewise linear gives warnings about the smoothness): \nCode: >>> import numpy, scipy.optimize, scipy.interpolate\n>>> x = numpy.linspace(0,2*numpy.pi, 20)\n>>> x\narray([ 0.        ,  0.33069396,  0.66138793,  0.99208189,  1.32277585,\n    1.65346982,  1.98416378,  2.31485774,  2.64555171,  2.97624567,\n    3.30693964,  3.6376336 ,  3.96832756,  4.29902153,  4.62971549,\n    4.96040945,  5.29110342,  5.62179738,  5.95249134,  6.28318531])\n>>> y1sampled = numpy.sin(x)\n>>> y2sampled = numpy.cos(x)\n>>> y1int = scipy.interpolate.interp1d(x,y1sampled,kind='cubic')\n>>> y2int = scipy.interpolate.interp1d(x,y2sampled,kind='cubic')\n>>> scipy.optimize.fsolve(lambda x: y1int(x) - y2int(x), numpy.pi)\narray([ 3.9269884])\n>>> scipy.optimize.fsolve(lambda x: numpy.sin(x) - numpy.cos(x), numpy.pi)\narray([ 3.92699082])\n\nText: Note that interpolation will give you 'guesses' about what data should be between the sampling points. No way to tell how good these guesses are. (but for my example, you can see it's a pretty good estimation) \nAPI:\nscipy.interpolate.interp1d\n","label":[[113,136,"Mention"],[1411,1437,"API"]],"Comments":[]}
{"id":60297,"text":"ID:27529176\nPost:\nText: If you are looking for all points close within a distance of a single point, use spsp.KDTree.query_ball_point not query_ball_tree. The latter when you need to compare sets of points against each other. \nCode: import numpy as np\nfrom scipy.spatial import KDTree\n\npts = np.array([(1, 1), (2, 1), (3, 1), (4, 1), (1, 2), (2, 2), (3, 2), (4, 2), (1, 3), (2, 3), (3, 3), (4, 3), (1, 4), (2, 4), (3, 4), (4, 4)])\n\nT = KDTree(pts)\nidx = T.query_ball_point([1,1],r=2)\nprint pts[idx]\n\nText: This returns \nCode: [[1 1]\n [2 1]\n [1 2]\n [2 2]\n [1 3]\n [3 1]]\n\nText: Note that your output must include the point (1,1) as well since that is a distance of zero from your target. \nAPI:\nscipy.spatial.KDTree.query_ball_point\n","label":[[105,133,"Mention"],[692,729,"API"]],"Comments":[]}
{"id":60298,"text":"ID:27564568\nPost:\nText: Use the callback keyword argument. \nText: minimize can take a keyword argument callback. This should be a function that accepts, as input, the current vector of parameters. This function is called after every iteration. \nText: For instance, \nCode: from scipy.optimize import minimize\n\ndef objective_function(xs):\n    \"\"\" Function to optimize. \"\"\"\n    x, y = xs\n    return (x-1)**2 + (y-2)**4\n\ndef print_callback(xs):\n    \"\"\"\n    Callback called after every iteration.\n\n    xs is the estimated location of the optimum.\n    \"\"\"\n    print xs\n\nminimize(objective_function, x0 = (0., 0.), callback=print_callback)\n\nText: Often, one wants to retain information between different calls to the callback, such as, for instance, the iteration number. One way to do this is to use a closure: \nCode: def generate_print_callback():\n    \"\"\"\n    Generate a callback that prints \n\n        iteration number | parameter values | objective function\n\n    every tenth iteration.\n    \"\"\"\n    saved_params = { \"iteration_number\" : 0 }\n    def print_callback(xs):\n        if saved_params[\"iteration_number\"] % 10 == 0:\n            print \"{:3} | {} | {}\".format(\n                saved_params[\"iteration_number\"], xs, objective_function(xs))\n        saved_params[\"iteration_number\"] += 1\n    return print_callback\n\nText: Call the minimize function with: \nCode: minimize(objective_function, x0 = (0., 0.), callback=generate_print_callback())\n\nAPI:\nscipy.optimize.minimize\n","label":[[66,74,"Mention"],[1445,1468,"API"]],"Comments":[]}
{"id":60299,"text":"ID:27623920\nPost:\nText: Neither sklearn.neighbors.KernelDensity nor statsmodels.nonparametric seem to support weighted samples. I modified gaussian_kde to allow for heterogeneous sampling weights and thought the results might be useful for others. An example is shown below. \nText: An ipython notebook can be found here: http:\/\/nbviewer.ipython.org\/gist\/tillahoffmann\/f844bce2ec264c1c8cb5 \nText: Implementation details \nText: The weighted arithmetic mean is \nText: The unbiased data covariance matrix is then given by \nText: The bandwidth can be chosen by scott or silverman rules as in scipy. However, the number of samples used to calculate the bandwidth is Kish's approximation for the effective sample size. \nAPI:\nscipy.stats.gaussian_kde\n","label":[[139,151,"Mention"],[718,742,"API"]],"Comments":[]}
{"id":60300,"text":"ID:27736563\nPost:\nText: Typically, you'd use a library for this, rather than implementing it yourself. \nText: I'm going to use sp.ndimage for this instead of scipy.signal. If you've had a signal processing class, you'd probably find the scipy.signal approach more intuitive, but if you haven't it will likely seem confusing. sp.ndimage provides a straight-forward, one-function-call gaussian_filter, as opposed to having to understand a few more signal processing conventions. \nText: Here's a quick example, using the data you posted in your question. This assumes that your data is regularly sampled (it is: every 2 units in time). \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage\n\ntime, _, shrinkage = np.loadtxt('discrete_data.txt', skiprows=1).T\n\nfig, ax = plt.subplots()\nax.plot(time, shrinkage, 'ro')\nax.plot(time, scipy.ndimage.gaussian_filter(shrinkage, 3))\nplt.show()\n\nText: Most of this is fairly straight-forward, but you might notice the \"magical\" value of 3 that I've specified in scipy.ndimage.gaussian_filter(shrinkage, 3). That's the sigma parameter of the gaussian function in samples. Because your data is sampled every 2 units in time, that's a sigma of 6 units. \nText: The sigma parameter is exactly analogous to the standard deviation in a \"bell curve\" normal distribution. The larger you make it, the broader the gaussian function will be, and the smoother your curve will be. By trial and error, a value of 3 seems about right for this particular dataset, but you should experiment and see what you think looks best. \nText: One more final note: There are a lot of different ways to approach this problem. A gaussian filter is a reasonable solution, but there are many, many others. If the exact result is very important, you should probably compare several methods and see which works best for your particular dataset. \nText: In your comment, you asked about saving the smoothed data to a file instead of plotting it. Here's a quick example of one way you might go about that: \nCode: import numpy as np\nimport scipy.ndimage\n\ntime, _, shrinkage = np.loadtxt('discrete_data.txt', skiprows=1).T\nsmoothed = scipy.ndimage.gaussian_filter(shrinkage, 3)\n\nnp.savetxt('smoothed_data.txt', np.c_[time, smoothed])\n\nAPI:\nscipy.ndimage\nscipy.ndimage\n","label":[[127,137,"Mention"],[325,335,"Mention"],[2262,2275,"API"],[2276,2289,"API"]],"Comments":[]}
{"id":60301,"text":"ID:27818014\nPost:\nText: In the given code mniimize iteratively minimizes function given it's derivative (Jacobi's matrix). According to the documentation, use can specify callback argument to a function that will be called after each iteration  this will let you measure performance, though I'm not sure if it'll let you halt the optimization process. \nText: All parameters you listed are hyperparameters, it's hard to optimize them directly: \nText: Number of neurons in the hidden layer is a discrete valued parameters, and, thus, is not optimizable via gradient techniques. Moreover, it affects NeuralNet architecture, so you can't optimize it while training the net. What you can do, though, is to use some higher-level routine to search for possible options, like exhaustive grid search with cross-validation (for example look at GridSearchCV) or other tools for hyperparameter search (hyperopt, spearmint, MOE, etc). \nText: Learning rate does not seem to be customizable for most of the optimization methods available. But, actually, learning rate in gradient descent is just a Newton's method with Hessian \"approximated\" by 1 \/ eta I  diagonal matrix with inverted learning rates on the major diagonal. So you can try hessian-based methods with this heuristic. \nText: Momentum is completely unrelated to regularization. It's an optimization technique, and, since you use scipy for optimization, is unavailable for you. \nAPI:\nscipy.optimize.minimize\n","label":[[42,50,"Mention"],[1433,1456,"API"]],"Comments":[]}
{"id":60302,"text":"ID:27842406\nPost:\nText: If you just want correlation through a Gaussian Copula (*), then it can be calculated in a few steps with numpy and scipy. \nText: create multivariate random variables with desired covariance, numpy.random.multivariate_normal, and creating a (nobs by k_variables) array apply scipy.stats.norm.cdf to transform normal to uniform random variables, for each column\/variable to get uniform marginal distributions apply dist.ppf to transform uniform margin to the desired distribution, where dist can be one of the distributions in stats \nText: (*) Gaussian copula is only one choice and it is not the best when we are interested in tail behavior, but it is the easiest to work with for example http:\/\/archive.wired.com\/techbiz\/it\/magazine\/17-03\/wp_quant?currentPage=all \nText: two references \nText: https:\/\/stats.stackexchange.com\/questions\/37424\/how-to-simulate-from-a-gaussian-copula \nText: http:\/\/www.mathworks.com\/products\/demos\/statistics\/copulademo.html \nText: (I might have done this a while ago in python, but don't have any scripts or function right now.) \nAPI:\nscipy.stats\n","label":[[550,555,"Mention"],[1090,1101,"API"]],"Comments":[]}
{"id":60303,"text":"ID:27923511\nPost:\nText: You can give the Delaunay triangulation to sp.interpolate.LinearNDInterpolator together with the set of Z-values, and it should do the job for you. \nText: If you really want to do the interpolation yourself, you can build it up from find_simplex and transform. \nAPI:\nscipy.interpolate.LinearNDInterpolator\n","label":[[67,102,"Mention"],[291,329,"API"]],"Comments":[]}
{"id":60304,"text":"ID:27961447\nPost:\nText: See sps.linalg.lgmres and check out the papers linked in the references for more information. \nText: Googling gives PDFs for the papers: https:\/\/www.google.com\/search?q=A%20Technique%20for%20Accelerating%20the%20Convergence%20of%20Restarted%20GMRES \nText: The brief explanation is that the algorithm keeps k \"representative\" vectors of the previous Krylov subspace when doing GMRES restart, when inverting the implicit Jacobian matrix. \nAPI:\nscipy.sparse.linalg.lgmres\n","label":[[28,45,"Mention"],[466,492,"API"]],"Comments":[]}
{"id":60305,"text":"ID:28002050\nPost:\nText: Method 1 doesn't work because you can't pickle bound instance methods with pickle. Method 2 doesn't work because stats is doing something \"tricky\" something that the dill and pathos author (me) doesn't quite know what it is without first investigating. \nText: You can see the issue is not that sp.stats is using a bound method (not a problem for dill or pathos), but it's doing some renaming magic which is why you when you look in the traceback from your pathos call, you see _locate_function failing (it fails and finds None) and this is actually why Method 2 doesn't work. \nCode: >>> import scipy.stats as ss\n>>>        \n>>> ss.lognorm\n<scipy.stats._continuous_distns.lognorm_gen object at 0x10932d6d0>\n\nText: The workaround is simple. Let the method be found easier by making a function that knows where it is. \nCode: >>> import pathos.multiprocessing as mp\n>>> p = mp.ProcessingPool()\n>>>        \n>>> def doit(x):\n...   return ss.lognorm.fit(x)\n... \n>>> p.map(doit, range(5))\n[(1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (1.0, 2.0, 1.0), (1.0, 3.0, 1.0), (1.0, 4.0, 1.0)]\n\nAPI:\nscipy.stats\nscipy.stats\n","label":[[137,142,"Mention"],[319,327,"Mention"],[1100,1111,"API"],[1112,1123,"API"]],"Comments":[]}
{"id":60306,"text":"ID:28415242\nPost:\nText: You can use dist.squareform to convert between a full m x n distance matrix and the upper triangle: \nCode: import numpy as np\nfrom scipy.spatial import distance\n\nm = 100\nn = 200\nX = np.random.randn(m, n)\n\nd = distance.pdist(X, metric='jaccard')\nprint(d.shape)\n# (4950,)\n\nD = distance.squareform(d)\nprint D.shape\n# (100, 100)\n\nAPI:\nscipy.spatial.distance.squareform\n","label":[[36,51,"Mention"],[355,388,"API"]],"Comments":[]}
{"id":60307,"text":"ID:28541805\nPost:\nText: Here are a couple suggestions. \nText: First, try the lowess function from statsmodels with it=0, and tweak the frac argument a bit: \nCode: In [328]: from statsmodels.nonparametric.smoothers_lowess import lowess\n\nIn [329]: filtered = lowess(pressure, time, is_sorted=True, frac=0.025, it=0)\n\nIn [330]: plot(time, pressure, 'r')\nOut[330]: [<matplotlib.lines.Line2D at 0x1178d0668>]\n\nIn [331]: plot(filtered[:,0], filtered[:,1], 'b')\nOut[331]: [<matplotlib.lines.Line2D at 0x1173d4550>]\n\nText: A second suggestion is to use filtfilt instead of lfilter to apply the Butterworth filter. filtfilt is the forward-backward filter. It applies the filter twice, once forward and once backward, resulting in zero phase delay. \nText: Here's a modified version of your script. The significant changes are the use of filtfilt instead of lfilter, and the change of cutoff from 3000 to 1500. You might want to experiment with that parameter--higher values result in better tracking of the onset of the pressure increase, but a value that is too high doesn't filter out the 3kHz (roughly) oscillations after the pressure increase. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter, filtfilt\n\ndef butter_lowpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff \/ nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\ndef butter_lowpass_filtfilt(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = filtfilt(b, a, data)\n    return y\n\ndata = np.loadtxt('data.dat', skiprows=2, delimiter=',', unpack=True).transpose()\ntime = data[:,0]\npressure = data[:,1]\ncutoff = 1500\nfs = 50000\npressure_smooth = butter_lowpass_filtfilt(pressure, cutoff, fs)\n\nfigure_pressure_trace = plt.figure()\nfigure_pressure_trace.clf()\nplot_P_vs_t = plt.subplot(111)\nplot_P_vs_t.plot(time, pressure, 'r', linewidth=1.0)\nplot_P_vs_t.plot(time, pressure_smooth, 'b', linewidth=1.0)\nplt.show()\nplt.close()\n\nText: Here's the plot of the result. Note the deviation in the filtered signal at the right edge. To handle that, you can experiment with the padtype and padlen parameters of filtfilt, or, if you know you have enough data, you can discard the edges of the filtered signal. \nAPI:\nscipy.signal.filtfilt\n","label":[[545,553,"Mention"],[2288,2309,"API"]],"Comments":[]}
{"id":60308,"text":"ID:28577101\nPost:\nText: I think your example input\/output does not correspond to typical ways of calculating percentile. If you calculate the percentile as \"proportion of data points strictly less than this value\", then the top value should be 0.8 (since 4 of 5 values are less than the largest one). If you calculate it as \"percent of data points less than or equal to this value\", then the bottom value should be 0.2 (since 1 of 5 values equals the smallest one). Thus the percentiles would be [0, 0.2, 0.4, 0.6, 0.8] or [0.2, 0.4, 0.6, 0.8, 1]. Your definition seems to be \"the number of data points strictly less than this value, considered as a proportion of the number of data points not equal to this value\", but in my experience this is not a common definition (see for instance wikipedia). \nText: With the typical percentile definitions, the percentile of a data point is equal to its rank divided by the number of data points. (See for instance this question on Stats SE asking how to do the same thing in R.) Differences in how to compute the percentile amount to differences in how to compute the rank (for instance, how to rank tied values). The percewntileofscore function provides four ways of computing percentiles: \nCode: >>> x = [1, 1, 2, 2, 17]\n>>> [stats.percentileofscore(x, a, 'rank') for a in x]\n[30.0, 30.0, 70.0, 70.0, 100.0]\n>>> [stats.percentileofscore(x, a, 'weak') for a in x]\n[40.0, 40.0, 80.0, 80.0, 100.0]\n>>> [stats.percentileofscore(x, a, 'strict') for a in x]\n[0.0, 0.0, 40.0, 40.0, 80.0]\n>>> [stats.percentileofscore(x, a, 'mean') for a in x]\n[20.0, 20.0, 60.0, 60.0, 90.0]\n\nText: (I used a dataset containing ties to illustrate what happens in such cases.) \nText: The \"rank\" method assigns tied groups a rank equal to the average of the ranks they would cover (i.e., a three-way tie for 2nd place gets a rank of 3 because it \"takes up\" ranks 2, 3 and 4). The \"weak\" method assigns a percentile based on the proportion of data points less than or equal to a given point; \"strict\" is the same but counts proportion of points strictly less than the given point. The \"mean\" method is the average of the latter two. \nText: As Kevin H. Lin noted, calling percentileofscore in a loop is inefficient since it has to recompute the ranks on every pass. However, these percentile calculations can be easily replicated using different ranking methods provided by scipy.stats.rankdata, letting you calculate all the percentiles at once: \nCode: >>> from scipy import stats\n>>> stats.rankdata(x, \"average\")\/len(x)\narray([ 0.3,  0.3,  0.7,  0.7,  1. ])\n>>> stats.rankdata(x, 'max')\/len(x)\narray([ 0.4,  0.4,  0.8,  0.8,  1. ])\n>>> (stats.rankdata(x, 'min')-1)\/len(x)\narray([ 0. ,  0. ,  0.4,  0.4,  0.8])\n\nText: In the last case the ranks are adjusted down by one to make them start from 0 instead of 1. (I've omitted \"mean\", but it could easily be obtained by averaging the results of the latter two methods.) \nText: I did some timings. With small data such as that in your example, using rankdata is somewhat slower than Kevin H. Lin's solution (presumably due to the overhead scipy incurs in converting things to numpy arrays under the hood) but faster than calling percentileofscore in a loop as in reptilicus's answer: \nCode: In [11]: %timeit [stats.percentileofscore(x, i) for i in x]\n1000 loops, best of 3: 414 s per loop\n\nIn [12]: %timeit list_to_percentiles(x)\n100000 loops, best of 3: 11.1 s per loop\n\nIn [13]: %timeit stats.rankdata(x, \"average\")\/len(x)\n10000 loops, best of 3: 39.3 s per loop\n\nText: With a large dataset, however, the performance advantage of numpy takes effect and using rankdata is 10 times faster than Kevin's list_to_percentiles: \nCode: In [18]: x = np.random.randint(0, 10000, 1000)\n\nIn [19]: %timeit [stats.percentileofscore(x, i) for i in x]\n1 loops, best of 3: 437 ms per loop\n\nIn [20]: %timeit list_to_percentiles(x)\n100 loops, best of 3: 1.08 ms per loop\n\nIn [21]: %timeit stats.rankdata(x, \"average\")\/len(x)\n10000 loops, best of 3: 102 s per loop\n\nText: This advantage will only become more pronounced on larger and larger datasets. \nAPI:\nscipy.stats.percentileofscore\n","label":[[1159,1177,"Mention"],[4104,4133,"API"]],"Comments":[]}
{"id":60309,"text":"ID:28745720\nPost:\nText: It helps to first be familiar with np.unravel_index. It converts a \"flat index\" (i.e. binnumber!) to a tuple of coordinates. You can think of the flat index as the index into arr.ravel(), and the tuple of coordinates as the index into arr. For example, if in the diagram below we think of the numbers 0,1,2,3,4,5 as bin numbers: \nCode:    | 0 | 1 | 2 |\n---+---+---+---|\n 0 | 0 | 1 | 2 |\n 1 | 3 | 4 | 5 |\n   +---+---+---|\n\nText: then np.unravel_index(4, (2,3)) \nCode: In [65]: np.unravel_index(4, (2,3))\nOut[65]: (1, 1)\n\nText: equals (1,1) because the 4th bin number in an array of shape (2,3) has coordinate (1,1). \nText: Okay then. Next, we need to know that internally sp.stats.binned_statistic_dd adds two edges to the given bin edges to handle outliers: \nCode: bin_edges = [np.r_[-np.inf, edge, np.inf] for edge in bin_edges]\n\nText: So the edge coordinates corresponding to the bin numbers is given by \nCode: edge_index = np.unravel_index(binnumber, [len(edge)-1 for edge in bin_edges])\n\nText: (We use len(edge)-1 because the shape of the array axis is one less than the number of edges.) \nText: For example: \nCode: import itertools as IT\nimport numpy as np\nimport scipy.stats as stats\n\nsample = np.array(list(IT.product(np.arange(5)-0.5, \n                                  np.arange(5)*10-5, \n                                  np.arange(5)*100-50)))\nbins = [np.arange(4),\n        np.arange(4)*10,\n        np.arange(4)*100]\n\nstatistic, bin_edges, binnumber = stats.binned_statistic_dd(\n    sample=sample, values=sample, statistic='count', \n    bins=bins, \n    range=[(0,100)]*3)\n\nbin_edges = [np.r_[-np.inf, edge, np.inf] for edge in bin_edges]\nedge_index = np.unravel_index(binnumber, [len(edge)-1 for edge in bin_edges])\n\n\nfor samp, idx in zip(sample, zip(*edge_index)):\n    vert = [edge[i] for i, edge in zip(idx, bin_edges)]\n    print('{} goes in bin with left-most corner: {}'.format(samp, vert))\n\nText: yields \nCode: [ -0.5  -5.  -50. ] goes in bin with left-most corner: [-inf, -inf, -inf]\n[ -0.5  -5.   50. ] goes in bin with left-most corner: [-inf, -inf, 0.0]\n[  -0.5   -5.   150. ] goes in bin with left-most corner: [-inf, -inf, 100.0]\n[  -0.5   -5.   250. ] goes in bin with left-most corner: [-inf, -inf, 200.0]\n...\n\nAPI:\nscipy.stats.binned_statistic_dd\n","label":[[695,723,"Mention"],[2264,2295,"API"]],"Comments":[]}
{"id":60310,"text":"ID:28858633\nPost:\nText: In scipy 0.14 or later, there is a new function sp.interpolate.RegularGridInterpolator which closely resembles interp3. \nText: The MATLAB command Vi = interp3(x,y,z,V,xi,yi,zi) would translate to something like: \nCode: from numpy import array\nfrom scipy.interpolate import RegularGridInterpolator as rgi\nmy_interpolating_function = rgi((x,y,z), V)\nVi = my_interpolating_function(array([xi,yi,zi]).T)\n\nText: Here is a full example demonstrating both; it will help you understand the exact differences... \nText: MATLAB CODE: \nCode: x = linspace(1,4,11);\ny = linspace(4,7,22);\nz = linspace(7,9,33);\nV = zeros(22,11,33);\nfor i=1:11\n    for j=1:22\n        for k=1:33\n            V(j,i,k) = 100*x(i) + 10*y(j) + z(k);\n        end\n    end\nend\nxq = [2,3];\nyq = [6,5];\nzq = [8,7];\nVi = interp3(x,y,z,V,xq,yq,zq);\n\nText: The result is Vi=[268 357] which is indeed the value at those two points (2,6,8) and (3,5,7). \nText: SCIPY CODE: \nCode: from scipy.interpolate import RegularGridInterpolator\nfrom numpy import linspace, zeros, array\nx = linspace(1,4,11)\ny = linspace(4,7,22)\nz = linspace(7,9,33)\nV = zeros((11,22,33))\nfor i in range(11):\n    for j in range(22):\n        for k in range(33):\n            V[i,j,k] = 100*x[i] + 10*y[j] + z[k]\nfn = RegularGridInterpolator((x,y,z), V)\npts = array([[2,6,8],[3,5,7]])\nprint(fn(pts))\n\nText: Again it's [268,357]. So you see some slight differences: Scipy uses x,y,z index order while MATLAB uses y,x,z (strangely); In Scipy you define a function in a separate step and when you call it, the coordinates are grouped like (x1,y1,z1),(x2,y2,z2),... while matlab uses (x1,x2,...),(y1,y2,...),(z1,z2,...). \nText: Other than that, the two are similar and equally easy to use. \nAPI:\nscipy.interpolate.RegularGridInterpolator\n","label":[[72,110,"Mention"],[1735,1776,"API"]],"Comments":[]}
{"id":60311,"text":"ID:28946216\nPost:\nText: It's not that the correlation operator is slow, but rather that your problem is very large. \nText: The direct correlation (or convolution) of a 3D array of size N by a kernel of size n involves, roughlyN**3*(2*n**3) floating point operations. So with a fairly recent CPU at 10 GFLOP per core, the problem of this size will take at least 2.4 hours, even without accounting for the memory copy overhead. \nText: Other factors can speed up the calculations though, such as multi-threading, and if sparse kernels are used. In the later case, the complexity can be reduced from O(N**3*n**3) to O(N**3) , which would explain the difference in execution time between step 1 and step 2 (as pointed out by the author of the question). \nText: For the step 2, an FFT based approach with fftconvolve (the kernel would need to be flipped to perform a cross-correlation), might be faster, particularly if the problem size N can be made equal to a power of 2 (e.g. 1024). \nAPI:\nscipy.signal.fftconvolve\n","label":[[799,810,"Mention"],[986,1010,"API"]],"Comments":[]}
{"id":60312,"text":"ID:29048167\nPost:\nText: The integral of exp(-x*x) is a scaled version of the error function, so you can use sp.special.erf to compute the integral. Given scalars a and b, the integral of your function from a to b is 0.5*np.sqrt(np.pi)*(erf(b) - erf(a)). \nText: erf is a \"ufunc\", which means it handles array arguments. Given a_list and b_list, your calculation can be written as \nCode: total = 0.5*np.sqrt(np.pi)*(erf(b_list) - erf(a_list)).sum()\n\nText: The function rho can also be handled with erf, by using the appropriate scaling: \nCode: g = np.sqrt(2)*H\ntotal = g*rho0*0.5*np.sqrt(np.pi)*(erf(b_list\/g) - erf(a_list\/g)).sum()\n\nText: Check this against your slow solution before relying on it. For some values, the subtraction of the erf functions can result in a significant loss of precision. \nAPI:\nscipy.special.erf\n","label":[[108,122,"Mention"],[805,822,"API"]],"Comments":[]}
{"id":60313,"text":"ID:29103996\nPost:\nText: For this interpolation, you should rather use interp1d with the argument kind='cubic' (see a related SO question ) \nText: I have yet to find a use case where InterpolatedUnivariateSpline can be used in practice (or maybe I just don't understand its purpose). With your code I get, \nText: So the interpolation works but shows extremely strong oscillations, making it unusable, which is typically the result I was getting with this interpolation method in the past. With a lower order spline (e.g. k=1) that works better, but then you lose the advantage of cubic interpolation. \nAPI:\nscipy.interpolate.interp1d\n","label":[[70,78,"Mention"],[606,632,"API"]],"Comments":[]}
{"id":60314,"text":"ID:29312332\nPost:\nText: This appears to be a bug in spec.btdtri which is supposed to compute quantiles for the beta distribution. Maybe you can file a bug report. \nCode: >>> from scipy import special\n>>> special.btdtri (betaparams[0],betaparams[1], 1-1e-6)\n0.00068501413697504238\n>>> special.btdtri (betaparams[0],betaparams[1], 1-1e-7)\n0.99999966996999767\n\nText: I can't figure out where btdtri is defined. \nText: EDIT: For the record, here is the SciPy bug report: https:\/\/github.com\/scipy\/scipy\/issues\/4677 \nAPI:\nscipy.special.btdtri\n","label":[[52,63,"Mention"],[516,536,"API"]],"Comments":[]}
{"id":60315,"text":"ID:29756651\nPost:\nText: If you simply want P_n(x), then you can create a suitable object representing the P_n polynomial using legendre and call it with your values of x: \nCode: In [1]: from scipy.special import legendre\nIn [2]: n = 3\nIn [3]: Pn = legendre(n)\nIn [4]: Pn(2.5)\nOut[4]: 35.3125        # P_3(2.5)\n\nText: The object Pn is, in a sense, the \"output\" of the Rodrigues formula: it is a polynomial of the required order, which can be evaluated at a provided value of x. If you want a single function that takes n and x, you can use eval_legendre: \nCode: In [5]: from scipy.special import eval_legendre\nIn [6]: eval_legendre(3, 2.5)\nOut[6]: 35.3125\n\nText: As noted in the docs, this is the recommended way to do it for large-ish n (e.g. n > 20), instead of creating a polynomial object with all the coefficients which does not handle rounding errors and numerical stability as well. \nText: EDIT: Both approaches work with arrays (at least for the x argument). For example: \nCode: In [7]: x = np.array([0, 1, 2, 5, 10])\nIn [8]: Pn(x)\nOut[8]: \narray([  0.00000000e+00,   1.00000000e+00,   1.70000000e+01,\n     3.05000000e+02,   2.48500000e+03])\n\nAPI:\nscipy.special.legendre\n","label":[[127,135,"Mention"],[1155,1177,"API"]],"Comments":[]}
{"id":60316,"text":"ID:29862365\nPost:\nText: You can extrapolate data with sp.interpolate.UnivariateSpline as illustrated in this answer. \nText: Although, since your data has a nice quadratic behavior, a better solution would be to fit it with a global polynomial, which is simpler and would yield more predictable results, \nCode: poly = np.polyfit(x[:, i], y[:, i], deg=3)\ny_int  = np.polyval(poly, x_val)\n\nAPI:\nscipy.interpolate.UnivariateSpline\n","label":[[54,85,"Mention"],[392,426,"API"]],"Comments":[]}
{"id":60317,"text":"ID:30237034\nPost:\nText: According to the documentation of the function lstsq http:\/\/docs.scipy.org\/doc\/scipy-0.15.1\/reference\/generated\/scipy.linalg.lstsq.html the estimated coefficients should be stored in your variable C (the order corresponding to columns in A). \nText: To print your equation with estimated coefficients showing 2 digits after decimal point: \nCode: print 'f(x,y) = {:.2f}x^2+{:.2f}y^2+{:.2f}xy+{:.2f}x+{:.2f}y+{:.2f}'.format(C[4],C[5],C[3],C[1],C[2],C[0])\n\nText: or: \nCode: print 'f(x,y) = {4:.2f}x^2+{5:.2f}y^2+{3:.2f}xy+{1:.2f}x+{2:.2f}y+{0:.2f}'.format(*C)\n\nText: By the way, libraries pandas and statsmodels can be very helpful for this kind of task (e.g. check Run an OLS regression with Pandas Data Frame ) \nAPI:\nscipy.linalg.lstsq\n","label":[[71,76,"Mention"],[741,759,"API"]],"Comments":[]}
{"id":60318,"text":"ID:30338008\nPost:\nText: This is very well within reach of opt.curve_fit (or just scipy.optimize.leastsqr). The fact that a sum is involved does not matter at all, nor that you have arrays of parameters. The only thing to note is that curve_fit wants to give your fit function the parameters as individual arguments, while leastsqr gives a single vector. \nText: Here's a solution: \nCode: import numpy as np\nfrom scipy.optimize import curve_fit, leastsq\n\ndef f(x,r,s):\n    \"\"\" The fit function, applied to every x_k for the vectors r_i and s_i. \"\"\"\n    x = x[...,np.newaxis]  # add an axis for the summation\n    # by virtue of numpy's fantastic broadcasting rules,\n    # the following will be evaluated for every combination of k and i.\n    x2s2 = (x*s)**2\n    return np.sum(r * x2s2 \/ (1 + x2s2), axis=-1)\n\n# fit using curve_fit\npopt,pcov = curve_fit(\n    lambda x,*params: f(x,params[:N],params[N:]),\n    X,Y,\n    np.r_[R0,S0],\n)\nR = popt[:N]\nS = popt[N:]\n\n# fit using leastsq\npopt,ier = leastsq(\n    lambda params: f(X,params[:N],params[N:]) - Y,\n    np.r_[R0,S0],\n)\nR = popt[:N]\nS = popt[N:]\n\nText: A few things to note: \nText: Upon start, we need the 1d arrays X and Y of measurements to fit to, the 1d arrays R0 and S0 as initial guesses and Nthe length of those two arrays. I separated the implementation of the actual model f from the objective functions supplied to the fitters. Those I implemented using lambda functions. Of course, one could also have ordinary def ... functions and combine them into one. The model function f uses numpy's broadcasting to simultaneously sum over a set of parameters (along the last axis), and calculate in parallel for many x (along any axes before the last, though both fit functions would complain if there is more than one... .ravel() to help there) We concatenate the fit parameters R and S into a single parameter vector using numpy's shorthand np.r_[R,S]. curve_fit supplies every single parameter as a distinct parameter to the objective function. We want them as a vector, so we use *params: It catches all remaining parameters in a single list. leastsq gives a single params vector. However, it neither supplies x, nor does it compare it to y. Those are directly bound into the objective function. \nAPI:\nscipy.optimize.curve_fit\n","label":[[58,71,"Mention"],[2256,2280,"API"]],"Comments":[]}
{"id":60319,"text":"ID:30392142\nPost:\nText: What you are looking for is called constrained Delaunay triangulation, and unfortunately the spsp implementation does not support it. \nText: As you pointed out, triangle does have that feature -- why not use it instead? \nAPI:\nscipy.spatial\n","label":[[117,121,"Mention"],[250,263,"API"]],"Comments":[]}
{"id":60320,"text":"ID:30420294\nPost:\nText: You're on the right track with converting the data into a table like the one on the linked page (a redundant distance matrix). According to the documentation, you should be able to pass that directly into lnkage or a related function, such as sp.cluster.hierarchy.single or scipy.cluster.hierarchy.complete. The related functions explicitly specify how distance between clusters should be calculated. inkage lets you specify whichever method you want, but defaults to single link (i.e. the distance between two clusters is the distance between their closest points). All of these methods will return a multidimensional array representing the agglomerative clustering. You can then use the rest of the scipy.cluster.hierarchy module to perform various actions on this clustering, such as visualizing or flattening it. \nText: However, there's a catch. As of the time this question was written, you couldn't actually use a redundant distance matrix, despite the fact that the documentation says you can. Based on the fact that the github issue is still open, I don't think this has been resolved yet. As pointed out in the answers to the linked question, you can get around this issue by passing the complete distance matrix into the dist.squareform function, which will convert it into the format which is actually accepted (a flat array containing the upper-triangular portion of the distance matrix, called a condensed distance matrix). You can then pass the result to one of the scipy.cluster.hierarchy functions. \nAPI:\nscipy.cluster.hierarchy.linkage\nscipy.cluster.hierarchy.single\nscipy.cluster.hierarchy.linkage\nscipy.spatial.distance.squareform\n","label":[[229,235,"Mention"],[267,294,"Mention"],[425,431,"Mention"],[1255,1270,"Mention"],[1545,1576,"API"],[1577,1607,"API"],[1608,1639,"API"],[1640,1673,"API"]],"Comments":[]}
{"id":60321,"text":"ID:30637521\nPost:\nText: fsolve is a wrapper of MINPACK's hybrd, which requires the function's argument and output have the same number of elements. You can try other algorithms from the more general root that do not have this restriction (e.g. lm): \nCode: from scipy.optimize import fsolve, root\n\ndef fsolve_function(arguments):\n    x = arguments[0]\n    y = arguments[1]\n    z = arguments[2]\n\n    out = [(35.85 - x)**2 + (93.23 - y)**2 + (-39.50 - z)**2 - 15**2]\n    out.append((42.1 - x)**2 + (81.68 - y)**2 + (-14.64 - z)**2 - 27**2)\n    out.append((-70.90 - x)**2 + (-55.94 - y)**2 + (-68.62 - z)**2 - 170**2)\n    out.append((-118.69 - x)**2 + (-159.80 - y)**2 + (-39.29 - z)**2 - 277**2)\n\n    return out\n\ninitialGuess = [35, 93, -39]\nresult = root(fsolve_function, initialGuess, method='lm')\nprint(result.x)\n\nText: Incidentally, it cannot find the actual zero --- is there supposed to be one at all? \nText: You can also force fsolve to use your function if you supply it an initial guess with a \"bogus\" fourth variable: \nCode: initialGuess = [35, 93, -39, 0]\n\nText: but I'm not sure how reliable are the results in this case. \nAPI:\nscipy.optimize.root\n","label":[[199,203,"Mention"],[1136,1155,"API"]],"Comments":[]}
{"id":60322,"text":"ID:30757875\nPost:\nText: Looking at your graphs shows that the signal filtered with filtfilt has a peak magnitude of 4.43x107 in the frequency domain compared with 4.56x107 for the signal filtered with lfilter. In other words, the signal filtered with filtfilt has an peak magnitude that is 0.97 that when filtering with \nText: Now we should note that sp.signal.filtfilt applies the filter twice, whereas lfilter only applies it once. As a result, input signals get attenuated twice as much. To confirm this we can have a look at the frequency response of the Butterworth filter you have used (obtained with iirfilter) around the input tone's normalized frequency of 100\/2500 = 0.04: \nText: which indeed shows an that the application of this filter does causes an attenuation of ~0.97 at a frequency of 0.04. \nAPI:\nscipy.signal.filtfilt\nscipy.signal.lfilter\n","label":[[351,369,"Mention"],[404,411,"Mention"],[814,835,"API"],[836,856,"API"]],"Comments":[]}
{"id":60323,"text":"ID:30860739\nPost:\nText: Distances between labeled regions of an image can be calculated with the following code, \nCode: import itertools\nfrom scipy.spatial.distance import cdist\n\n# making sure that IDs are integer\nexample_array = np.asarray(example_array, dtype=np.int) \n# we assume that IDs start from 1, so we have n-1 unique IDs between 1 and n\nn = example_array.max()\n\nindexes = []\nfor k in range(1, n):\n    tmp = np.nonzero(example_array == k)\n    tmp = np.asarray(tmp).T\n    indexes.append(tmp)\n\n# calculating the distance matrix\ndistance_matrix = np.zeros((n-1, n-1), dtype=np.float)   \nfor i, j in itertools.combinations(range(n-1), 2):\n    # use squared Euclidean distance (more efficient), and take the square root only of the single element we are interested in.\n    d2 = cdist(indexes[i], indexes[j], metric='sqeuclidean') \n    distance_matrix[i, j] = distance_matrix[j, i] = d2.min()**0.5\n\n# mapping the distance matrix to labeled IDs (could be improved\/extended)\nlabels_i, labels_j = np.meshgrid( range(1, n), range(1, n))  \nresults = np.dstack((labels_i, labels_j, distance_matrix)).reshape((-1, 3))\n\nprint(distance_matrix)\nprint(results)\n\nText: This assumes integer IDs, and would need to be extended if that is not the case. For instance, with the test data above, the calculated distance matrix is, \nCode: # From  1             2         3            4              5         # To\n[[  0.           4.12310563   4.           9.05538514   5.        ]   # 1\n [  4.12310563   0.           3.16227766  10.81665383   8.24621125]   # 2\n [  4.           3.16227766   0.           4.24264069   2.        ]   # 3 \n [  9.05538514  10.81665383   4.24264069   0.           3.16227766]   # 4\n [  5.           8.24621125   2.           3.16227766   0.        ]]  # 5\n\nText: while the full output can be found here. Note that this takes the Eucledian distance from the center of each pixel. For instance, the distance between zones 1 and 3 is 2.0, while they are separated by 1 pixel. \nText: This is a brute-force approach, where we calculate all the pairwise distances between pixels of different regions. This should be sufficient for most applications. Still, if you need better performance, have a look at cKDTree which would be more efficient in computing the minimum distance between two regions, when compared to cdist. \nAPI:\nscipy.spatial.cKDTree\n","label":[[2212,2219,"Mention"],[2335,2356,"API"]],"Comments":[]}
{"id":60324,"text":"ID:30928476\nPost:\nText: The loc parameter always shifts the x variable. In other words, it generalizes the distribution to allow shifting x=0 to x=loc. So that when loc is nonzero, \nCode: maxwell.pdf(x) = sqrt(2\/pi)x**2 * exp(-x**2\/2), for x > 0\n\nText: becomes \nCode: maxwell.pdf(x, loc) = sqrt(2\/pi)(x-loc)**2 * exp(-(x-loc)**2\/2), for x > loc.\n\nText: The doc string for maxweell states: \nText: A special case of a chi distribution, with df = 3, loc = 0.0, and given scale = a, where a is the parameter used in the Mathworld description. \nText: So the scale corresponds to the parameter a in the equation \nText: (from MathWorld - A Wolfram Web Resource: wolfram.com) \nText: In general you need to read the distribution's doc string to know what parameters the distribution has. The beta distribution, for example, has a and b shape parameters in addition to loc and scale. \nText: However, I believe for all continuous distributions, distribution.pdf(x, loc, scale) is identically equivalent to distribution.pdf(y) \/ scale with y = (x - loc) \/ scale. \nAPI:\nscipy.stats.maxwell\n","label":[[372,380,"Mention"],[1057,1076,"API"]],"Comments":[]}
{"id":60325,"text":"ID:30964551\nPost:\nText: Alternatively to binary_erosion, you could use the generic_filter to achieve this, \nCode: from scipy.ndimage import generic_filter\ngeneric_filter(data, np.all, size=(5,5), mode='constant', cval=0).astype(np.bool)\n\nText: However you would obtain the same results as above, \nCode: array([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]])  # using 1, 0 instead of True, False for readability\n\nText: because it is the correct answer to your question. The 6 rows in your example matrix are symmetric, and with the given kernel, there would be 2 symmetric True element in the result. \nText: If you want to avoid this, you could use a non symmetric-kernel size: size=(6,5), although this produces one true element in row 3 not row 2. This could be fixed, manually padding the kernel array with zeros when using binary_erosion. \nAPI:\nscipy.ndimage.generic_filter\n","label":[[75,89,"Mention"],[956,984,"API"]],"Comments":[]}
{"id":60326,"text":"ID:31063741\nPost:\nText: I think you should drop the \"analog=True\" from your call to butter and use the default digital filter. When I did this with your data, it worked pretty well. \nText: From the docs: \nText: A scalar or length-2 sequence giving the critical frequencies. For a Butterworth filter, this is the point at which the gain drops to 1\/sqrt(2) that of the passband (the -3 dB point). For digital filters, Wn is normalized from 0 to 1, where 1 is the Nyquist frequency, pi radians\/sample. (Wn is thus in half-cycles \/ sample.) For analog filters, Wn is an angular frequency (e.g. rad\/s). \nText: Because you requested an analog filter, you shouldn't have normalized the start and stop frequencies to the nyquist rate. That's only appropriate for a digital filter. \nText: Is there a reason you want to use an analog filter here? I always use digital filters. \nText: Also, consider using filtfilt instead of scipy.signal.lfilter. Refs: \nText: How To apply a filter to a signal in python \nText: https:\/\/dsp.stackexchange.com\/questions\/19084\/applying-filter-in-scipy-signal-use-lfilter-or-filtfilt \nAPI:\nscipy.signal.butter\nscipy.signal.filtfilt\n","label":[[84,90,"Mention"],[897,905,"Mention"],[1111,1130,"API"],[1131,1152,"API"]],"Comments":[]}
{"id":60327,"text":"ID:31117560\nPost:\nText: Power law distribution as defined in numpy.random and tsats are not defined for negative a in the mathematical sense as explained in the answer to this question: they are not normalizable because of the singularity at zero. So, sadly, the math says 'no'. \nText: You can define a distribution with pdf proportional to x^{g-1} with g < 0 on an interval which does not contain zero, if that's what you are after. \nText: For pdf(x) = const * x**(g-1) for a <= x <= b, the transformation from a uniform variate (np.random.random) is: \nCode: In [3]: def rndm(a, b, g, size=1):\n    \"\"\"Power-law gen for pdf(x)\\propto x^{g-1} for a<=x<=b\"\"\"\n   ...:     r = np.random.random(size=size)\n   ...:     ag, bg = a**g, b**g\n   ...:     return (ag + (bg - ag)*r)**(1.\/g)\n\nText: Then you can do, for example, \nCode: In [4]: xx = rndm(1, 2, g=-2, size=10000)\n\nText: and so on. \nText: For completeness, here is pdf: \nCode: In [5]: def pdf(x, a, b, g):\n    ag, bg = a**g, b**g\n   ....:     return g * x**(g-1) \/ (bg - ag)\n\nText: This all assumes that a < b and g != 0. These formulas should agree with numpy.power and sp.stats.powerlaw for a=0, b=1 and g > 0. \nAPI:\nscipy.stats\nscipy.stats.powerlaw\n","label":[[78,83,"Mention"],[1122,1139,"Mention"],[1170,1181,"API"],[1182,1202,"API"]],"Comments":[]}
{"id":60328,"text":"ID:31129378\nPost:\nText: The three are very different but overlap in the parameter estimation for the very simple example with only one explanatory variable. \nText: By increasing generality: \nText: sp.stats.linregress only handles the case of a single explanatory variable with specialized code and calculates a few extra statistics. \nText: numpy.polynomial.polynomial.polyfit estimates the regression for a polynomial of a single variable, but doesn't return much in terms of extra statisics. \nText: statsmodels OLS is a generic linear model (OLS) estimation class. It doesn't prespecify what the explanatory variables are and can handle any multivariate array of explanatory variables, or formulas and pandas DataFrames. It not only returns the estimated parameters, but also a large set of results staistics and methods for statistical inference and prediction. \nText: For completeness of options for estimating linear models in Python (outside of Bayesian analysis), we should also consider scikit-learn LinearRegression and similar linear models, which are useful for selecting among a large number of explanatory variables but does not have the large number of results that statsmodels provides. \nAPI:\nscipy.stats.linregress\n","label":[[197,216,"Mention"],[1207,1229,"API"]],"Comments":[]}
{"id":60329,"text":"ID:31335255\nPost:\nText: This is called parametric interpolation. \nText: splprep provides spline approximations for such curves. This assumes you know the order in which the points are on the curve. \nText: If you don't know which point comes after which on the curve, the problem becomes more difficult. I think in this case, the problem is called manifold learning, and some of the algorithms in scikit-learn may be helpful in that. \nAPI:\nscipy.interpolate.splprep\n","label":[[72,79,"Mention"],[439,464,"API"]],"Comments":[]}
{"id":60330,"text":"ID:31381562\nPost:\nText: From scipy v0.14 onwards you can use vectBivarirteSpline with grid=False: \nCode: import numpy as np\nfrom scipy.interpolate import RectBivariateSpline\nfrom matplotlib import pyplot as plt\n\n\nx, y = np.ogrid[-1:1:10j,-1:1:10j]\nz = (x + y)*np.exp(-6.0 * (x * x + y * y))\n\nspl = RectBivariateSpline(x, y, z)\n\nxi = np.linspace(-1, 1, 50)\nyi = np.linspace(-1, 1, 50)\nzi = spl(xi, yi, grid=False)\n\nfig, ax = plt.subplots(1, 1)\nax.hold(True)\nax.imshow(z, cmap=plt.cm.coolwarm, origin='lower', extent=(-1, 1, -1, 1))\nax.scatter(xi, yi, s=60, c=zi, cmap=plt.cm.coolwarm)\n\nAPI:\nscipy.interpolate.RectBivariateSpline\n","label":[[61,80,"Mention"],[590,627,"API"]],"Comments":[]}
{"id":60331,"text":"ID:31466013\nPost:\nText: Actually, you were not far from the solution in your question. \nText: Using inp.splprep for parametric B-spline interpolation would be the simplest approach. It also natively supports closed curves, if you provide the per=1 parameter, \nCode: import numpy as np\nfrom scipy.interpolate import splprep, splev\nimport matplotlib.pyplot as plt\n\n# define pts from the question\n\ntck, u = splprep(pts.T, u=None, s=0.0, per=1) \nu_new = np.linspace(u.min(), u.max(), 1000)\nx_new, y_new = splev(u_new, tck, der=0)\n\nplt.plot(pts[:,0], pts[:,1], 'ro')\nplt.plot(x_new, y_new, 'b--')\nplt.show()\n\nText: Fundamentally, this approach not very different from the one in @Joe Kington's answer. Although, it will probably be a bit more robust, because the equivalent of the i vector is chosen, by default, based on the distances between points and not simply their index (see splprep documentation for the u parameter). \nAPI:\nscipy.interpolate.splprep\n","label":[[100,111,"Mention"],[928,953,"API"]],"Comments":[]}
{"id":60332,"text":"ID:31590468\nPost:\nText: @yevgeniy You are right, efficiently solving multiple independent linear systems A x = b with scipy a bit tricky (assuming an A array that changes for every iteration). \nText: For instance, here is a benchmark for solving 1000 systems of the form, A x = b, where A is a 10x10 matrix, and b a 10 element vector. Surprisingly, the approach to put all this into one block diagonal matrix and call solve once is indeed slower both with dense and sparse matrices. \nCode: import numpy as np\nfrom scipy.linalg import block_diag, solve\nfrom scipy.sparse import block_diag as sp_block_diag\nfrom scipy.sparse.linalg import spsolve\n\nN = 10\nM = 1000 # number of coordinates \nAi = np.random.randn(N, N) # we can compute the inverse here,\n# but let's assume that Ai are different matrices in the for loop loop\nbi = np.random.randn(N)\n\n%timeit [solve(Ai, bi) for el in range(M)]\n# 10 loops, best of 3: 32.1 ms per loop\n\nAfull = sp_block_diag([Ai]*M, format='csr')\nbfull = np.tile(bi, M)\n\n%timeit Afull = sp_block_diag([Ai]*M, format='csr')\n%timeit spsolve(Afull, bfull)\n\n# 1 loops, best of 3: 303 ms per loop\n# 100 loops, best of 3: 5.55 ms per loop\n\nAfull = block_diag(*[Ai]*M) \n\n%timeit Afull = block_diag(*[Ai]*M)\n%timeit solve(Afull, bfull)\n\n# 100 loops, best of 3: 14.1 ms per loop\n# 1 loops, best of 3: 23.6 s per loop\n\nText: The solution of the linear system, with sparse arrays is faster, but the time to create this block diagonal array is actually very slow. As to dense arrays, they are simply slower in this case (and take lots of RAM). \nText: Maybe I'm missing something about how to make this work efficiently with sparse arrays, but if you are keeping the for loops, there are two things that you could do for optimizations. \nText: From pure python, look at the source code of solve : remove unnecessary tests and factorize all repeated operations outside of your loops. For instance, assuming your arrays are not symmetrical positives, we could do \nCode: from scipy.linalg import get_lapack_funcs\n\ngesv, = get_lapack_funcs(('gesv',), (Ai, bi))\n\ndef solve_opt(A, b, gesv=gesv):\n    # not sure if copying A and B is necessary, but just in case (faster if arrays are not copied)\n    lu, piv, x, info = gesv(A.copy(), b.copy(), overwrite_a=False, overwrite_b=False)\n    if info == 0:\n        return x\n    if info > 0:\n        raise LinAlgError(\"singular matrix\")\n    raise ValueError('illegal value in %d-th argument of internal gesv|posv' % -info)\n\n%timeit [solve(Ai, bi) for el in range(M)]\n%timeit [solve_opt(Ai, bi) for el in range(M)]\n\n# 10 loops, best of 3: 30.1 ms per loop\n# 100 loops, best of 3: 3.77 ms per loop\n\nText: which results in a 6.5x speed up. \nText: If you need even better performance, you would have to port this for loop in Cython and interface the gesv BLAS functions directly in C, as discussed here, or better with the Cython API for BLAS\/LAPACK in Scipy 0.16. \nText: Edit: As @Eelco Hoogendoorn mentioned if your A matrix is fixed, there is a much simpler and more efficient approach. \nAPI:\nscipy.linalg.solve\nscipy.linalg.solve\n","label":[[418,423,"Mention"],[1801,1806,"Mention"],[3039,3057,"API"],[3058,3076,"API"]],"Comments":[]}
{"id":60333,"text":"ID:31597061\nPost:\nText: You can use mmread which does exactly what you want. \nCode: In [11]: mmread(\"sparse_from_file\")\nOut[11]: \n<4589x17366 sparse matrix of type '<class 'numpy.float64'>'\n    with 7 stored elements in COOrdinate format>\n\nText: Note the result is a COO sparse matrix. If you want a csc_matrix you can then use sparse.coo_matrix.tocsc. \nText: Now you mention you want to handle this very large and sparse matrix with numpy. That might turn out to be impractical since numpy operates on dense arrays only and if your matrix is indeed very large and sparse you probably can't afford to store it in dense format. \nText: So you could be better off sticking with the most efficient scipy.sparse format for your use case. \nAPI:\nscipy.io.mmread\n","label":[[36,42,"Mention"],[739,754,"API"]],"Comments":[]}
{"id":60334,"text":"ID:31712852\nPost:\nText: All you need is a function that approximates the CDF of A, and one that approximates the inverse CDF (or PPF) of B. You would then simply compute qcorrected = PPFB(CDFA(q)). \nText: For your example data we can simply use the .cdf and .ppf methods for sp.stats.gamma frozen distributions with the appropriate parameters: \nCode: from scipy import stats\n\ndistA = stats.gamma(0.7, scale=50)\ndistB = stats.gamma(0.5, scale=70)\n\ncorrected_quantiles = distB.ppf(distA.cdf(quantiles[1:]))\n\nText: Of course, for real data you are unlikely to know the parameters of the true underlying distributions. If you have a good idea of their functional form, you could try performing a maximum likelihood fit to your data in order to estimate them: \nCode: distA = stats.gamma(*stats.gamma.fit(A))\ndistB = stats.gamma(*stats.gamma.fit(B))\n\nText: Failing that, you could try to interpolate\/extrapolate from your empirical CDFs, for example using scipy.interpolate.InterpolatedUnivariateSpline: \nCode: from scipy.interpolate import InterpolatedUnivariateSpline\n\n# cubic spline interpolation\nitp_A_cdf = InterpolatedUnivariateSpline(quantiles[1:], A_cdf, k=3)\n# the PPF is the inverse of the CDF, so we simply reverse the order of the\n# x & y arguments to InterpolatedUnivariateSpline\nitp_B_ppf = InterpolatedUnivariateSpline(B_cdf, quantiles[1:], k=3)\n\nitp_corrected_quantiles = itp_B_ppf(itp_A_cdf(quantiles[1:]))\n\nfig, ax = plt.subplots(1, 1)\nax.hold(True)\nax.plot(quantiles[1:], A_cdf, '-r', lw=3, label='A')\nax.plot(quantiles[1:], B_cdf, '-k', lw=3, label='B')\nax.plot(corrected_quantiles, A_cdf, '--xr', lw=3, ms=10, mew=2, label='exact')\nax.plot(itp_corrected_quantiles, A_cdf, '--+b', lw=3, ms=10, mew=2,\n        label='interpolated')\nax.legend(loc=5)\n\nAPI:\nscipy.stats.gamma\n","label":[[275,289,"Mention"],[1768,1785,"API"]],"Comments":[]}
{"id":60335,"text":"ID:31724678\nPost:\nText: After a long time of putting up with excruciatingly slow performance of sp.interpolate.griddata I've decided to ditch griddata in favor of image transformation with OpenCV. Specifically, Perspective Transformation. \nText: So for the above example, the one in the question above, for which you can get the input files here, this is a piece of code which takes 1.1 ms as opposed to the 692 ms which takes the regridding part in the example above. \nCode: import cv2\nnew_data = data.T[::-1]\n\n# calculate the pixel coordinates of the\n# computational domain corners in the data array\nw,e,s,n = map_extent\ndx = float(e-w)\/new_data.shape[1]\ndy = float(n-s)\/new_data.shape[0]\nx = (lon.ravel()-w)\/dx\ny = (n-lat.ravel())\/dy\n\ncomputational_domain_corners = np.float32(zip(x,y))\n\ndata_array_corners = np.float32([[0,new_data.shape[0]],\n                                 [0,0],\n                                 [new_data.shape[1],new_data.shape[0]],\n                                 [new_data.shape[1],0]])\n\n# Compute the transformation matrix which places\n# the corners of the data array at the corners of\n# the computational domain in data array pixel coordinates\ntranformation_matrix = cv2.getPerspectiveTransform(data_array_corners,\n                                                   computational_domain_corners)\n\n# Make the transformation making the final array the same shape\n# as the data array, cubic interpolate the data placing NaN's\n# outside the new array geometry\nmapped_data = cv2.warpPerspective(new_data,tranformation_matrix,\n                                  (new_data.shape[1],new_data.shape[0]),\n                                  flags=2,\n                                  borderMode=0,\n                                  borderValue=np.nan)\n\nText: The only drawback I see to this solution is a slight offset in the data as illustrated by the non-overlapping contours in the attached image. regridded data contours (probably more accurate) in black and warpPerspective data contours in 'jet' colorscale. \nText: At the moment, I live just fine with the discrepancy at the advantage of performance and I hope this solution helps others as-well. \nText: Someone (not me...) should find a way to improve the performance of griddata :) Enjoy! \nAPI:\nscipy.interpolate.griddata\n","label":[[96,119,"Mention"],[2271,2297,"API"]],"Comments":[]}
{"id":60336,"text":"ID:31737890\nPost:\nText: Look up table \nText: If you have the complete table you don't need interpolation, you just need to look up the index of the nearest (x, y) value and use it on the table \nCode: In [1]: import numpy\n   ...: x = numpy.array([1.23, 2.63, 4.74, 6.43, 5.64])\n   ...: y = numpy.array([2.56, 4.79, 6.21])\n   ...: data = numpy.array([[0, 0, 1, 0, 1],\n   ...:                     [0, 1, 1, 1, 0],\n   ...:                     [1, 0, 0, 0, 0]])\n   ...: \n   ...: def lookupNearest(x0, y0):\n   ...:     xi = numpy.abs(x-x0).argmin()\n   ...:     yi = numpy.abs(y-y0).argmin()\n   ...:     return data[yi,xi]\n\nIn [2]: lookupNearest(5.1, 4.9)\nOut[2]: 1\n\nIn [3]: lookupNearest(3.54, 6.9)\nOut[3]: 0\n\nText: Nearest-neighbor interpolation \nText: NearestNDInterpolator will be really useful if your data is composed by scattered points \nText: For example, for data like: \nText: with red = 1, blue =0 \nCode: In [4]: points = numpy.array([[1.1, 2.5], \n   ...:                       [1.5, 5.2], \n   ...:                       [3.1, 3.0], \n   ...:                       [2.0, 6.0], \n   ...:                       [2.8, 4.7]])\n   ...: values = numpy.array([0, 1, 1, 0, 0])\n\nIn [5]: from scipy.interpolate import NearestNDInterpolator\n   ...: myInterpolator = NearestNDInterpolator(points, values)\n\nIn [6]: myInterpolator(1.7,4.5)\nOut[6]: 1\n\nIn [7]: myInterpolator(2.5,4.0)\nOut[7]: 0\n\nAPI:\nscipy.interpolate.NearestNDInterpolator\n","label":[[748,769,"Mention"],[1385,1424,"API"]],"Comments":[]}
{"id":60337,"text":"ID:31749226\nPost:\nText: It looks like you got the parameters of the CDF function correct. In general, if you have left, mode and right as used by numpy.random.triangular, you can convert those to the parameters of triang using \nCode: c = (mode - left) \/ (right - left)\nloc = left\nscale = right - left\n\nText: The arguments of the expect method can be confusing. You tried this: \nCode: scipy.stats.triang.expect(func=lambda x: x, c = 0.5, loc=0.5, scale=1)\n\nText: The correct call is \nCode: In [91]: triang.expect(lambda x: x, (0.5,), loc=0.5, scale=1)\nOut[91]: 1.0\n\nText: The second argument is a tuple that holds the shape parameters, which in this case is the tuple (c,). There isn't a separate keyword argument for the shape parameters. \nText: To draw samples from a triangular distribution with width 2 centered at 0, use the rvs method of scipy.triang, with c=0.5, loc=-1, and scale=2. For example, the following draws 10 samples: \nCode: In [96]: triang.rvs(c=0.5, loc=-1, scale=2, size=10)\nOut[96]: \narray([-0.61654942,  0.03949263,  0.44191603, -0.76464285, -0.5474533 ,\n        0.00343265,  0.222072  , -0.14161595,  0.46505966, -0.23557379])\n\nAPI:\nscipy.stats.triang\n","label":[[214,220,"Mention"],[1156,1174,"API"]],"Comments":[]}
{"id":60338,"text":"ID:31795167\nPost:\nText: If your list is a NumPy array, you can simply refer to its real attribute: \nCode: In [59]: a = np.array([1+0j, 2+0j, -1+0j])\nIn [60]: a\nOut[60]: array([ 1.+0.j,  2.+0.j, -1.+0.j])\n\nIn [61]: a.real\nOut[61]: array([ 1.,  2., -1.])\n\nText: If your list is a Python list, perhaps the following list comprehension would be the simplest way to get the real parts you want: \nCode: In [64]: l\nOut[64]: [(1+0j), (2+0j), (-1+0j)]\nIn [67]: [c.real for c in l]\nOut[67]: [1.0, 2.0, -1.0]\n\nText: You would need to do this conversion if you want to integrate a function returning a np.complex128 with quad: quad expects the function to return some kind of float. \nAPI:\nscipy.integrate.quad\n","label":[[615,619,"Mention"],[677,697,"API"]],"Comments":[]}
{"id":60339,"text":"ID:31840982\nPost:\nText: In 0.16.x I added options to build the cKDTree with median or sliding midpoint rules, as well as choosing whether to recompute the bounding hyperrectangle at each node in the kd-tree. The defaults are based on experiences about the performance of spsp.cKDTree and sklearn.neighbors.KDTree. In some contrived cases (data that are highly streched along a dimension) it can have negative impact, but usually it should be faster. Experiment with bulding the cKDTree with balanced_tree=False and\/or compact_nodes=False. Setting both to False gives you the same behavior as 0.15.x. Unfortunately it is difficult to set defaults that make everyone happy because the performance depends on the data. \nText: Also note that with balanced_tree=True we compute medians by quickselect when the kd-tree is constructed. If the data for some reason is pre-sorted, it will be very slow. In this case it will help to shuffle the rows of the input data. Or you can set balanced_tree=False to avoid the partial quicksorts. \nText: There is also a new option to multithread the nearest-neighbor query. Try to call query with n_jobs=-1 and see if it helps for you. \nText: Update June 2020: SciPy 1.5.0 will use a new algorithm (introselect based partial sort, from C++ STL) which solves the problems reported here. \nAPI:\nscipy.spatial.cKDTree\n","label":[[271,283,"Mention"],[1322,1343,"API"]],"Comments":[]}
{"id":60340,"text":"ID:32011629\nPost:\nText: The work-around solution for this problem is using the more generic si.ode function. This function has several integration schemes build-in, and you have more control of what happens during each iteration. See the example below: \nCode: import numpy as np\nfrom scipy.integrate import ode\n\ndef func(t, y, z):\n    return y+z\n\nt = np.linspace(0, 1.0, 100)\ndt = t[1]-t[0]\nz = np.random.rand(100)\noutput = np.empty_like(t)\nr = ode(func).set_integrator(\"dop853\")\nr.set_initial_value(0, 0).set_f_params(z[0])\n\nfor i in xrange(len(t)):\n    r.set_f_params(z[i])\n    r.integrate(r.t+dt)\n    output[i] = r.y\n\nText: During each iteration, the solver's value of z is updated accordingly. \nAPI:\nscipy.integrate.ode\n","label":[[92,98,"Mention"],[704,723,"API"]],"Comments":[]}
{"id":60341,"text":"ID:32059655\nPost:\nText: You could use the photutils.detection.find_peaks function, which is one of the photutils detection methods. \nText: If you look at the photutils.detection.find_peaks implementation, you'll see that it's using maximum_filter to compute a maximum image (by default in a 3x3 box size footprint) and finding the pixels where the original image equals the maximum image. \nText: The rest of the function is mostly for two things that might also be of interest to you: \nText: if you pass in a wcs object, you can get sky coordinates out, not just pixel coordinates. there's an option to get sub-pixel precision coordinates. \nAPI:\nscipy.ndimage.maximum_filter\n","label":[[232,246,"Mention"],[646,674,"API"]],"Comments":[]}
{"id":60342,"text":"ID:32302387\nPost:\nText: TL;DR: The integrand is equivalent to erfcx(-x), and the implementation of erfcx at erfqx takes care of the numerical issues: \nCode: In [10]: from scipy.integrate import quad\n\nIn [11]: from scipy.special import erfcx\n\nIn [12]: quad(lambda x: erfcx(-x), -14, -4)\nOut[12]: (0.6990732491815446, 1.4463494884581349e-13)\n\nIn [13]: quad(lambda x: erfcx(-x), -150, -50)\nOut[13]: (0.6197754761443759, 4.165648376274775e-14)\n\nText: You can avoid the lambda expression by changing the sign of the integration argument and limits: \nCode: In [14]: quad(erfcx, 4, 14)\nOut[14]: (0.6990732491815446, 1.4463494884581349e-13)\n\nText: The problem is the numerical evaluation of 1 + erf(x) for negative values of x. As x decreases, erf(x) approaches -1. When you then add 1, you get catastrophic loss of precision, and for sufficiently negative x (specifically x < -5.87), 1 + erf(x) is numerically 0. \nText: Note that the default behavior at Wolfram Alpha suffers from the same problem. I had to click on \"More digits\" twice to get a reasonable answer. \nText: The fix is to reformulate your function. You can express 1+erf(x) as 2*ndtr(x*sqrt(2)), where ndtr is the normal cumulative distribution function, available from spec.ndtr (see, for example, https:\/\/en.wikipedia.org\/wiki\/Error_function). Here's an alternative version of your function, and the result of integrating it with scipy.integrate.quad: \nCode: In [133]: def func2(x):\n   .....:     return np.exp(x**2) * 2 * ndtr(x * np.sqrt(2))\n   .....: \n\nIn [134]: my_func(-5)\nOut[134]: 0.1107029852258767\n\nIn [135]: func2(-5)\nOut[135]: 0.11070463773306743\n\nIn [136]: integrate.quad(func2, -14, -4)\nOut[136]: (0.6990732491815298, 1.4469372263470424e-13)\n\nText: The answer at Wolfram Alpha after clicking on \"More digits\" twice is 0.6990732491815446... \nText: This is what the plot of the function looks like when you use a numerically stable version: \nText: To avoid overflow or underflow for arguments with very large magnitudes, you can do part of the computation in log-space: \nCode: from scipy.special import log_ndtr\n\ndef func3(x):\n    t = x**2 + np.log(2) + log_ndtr(x * np.sqrt(2))\n    y = np.exp(t)\n    return y\n\nText: E.g. \nCode: In [20]: quad(func3, -150, -50)\nOut[20]: (0.6197754761435517, 4.6850379059597266e-14)\n\nText: (Looks like @ali_m beat me to it in the new question: Tricking numpy\/python into representing very large and very small numbers.) \nText: Finally, as Simon Byrne pointed out in an answer over at Tricking numpy\/python into representing very large and very small numbers, the function to be integrated can be expressed as erfcx(-x), where erfcx is the scaled complementary error function. It is available as scipy.special.erfcx. \nText: For example, \nCode: In [10]: from scipy.integrate import quad\n\nIn [11]: from scipy.special import erfcx\n\nIn [12]: quad(lambda x: erfcx(-x), -14, -4)\nOut[12]: (0.6990732491815446, 1.4463494884581349e-13)\n\nIn [13]: quad(lambda x: erfcx(-x), -150, -50)\nOut[13]: (0.6197754761443759, 4.165648376274775e-14)\n\nAPI:\nscipy.special.erfcx\nscipy.special.ndtr\n","label":[[108,113,"Mention"],[1227,1236,"Mention"],[3034,3053,"API"],[3054,3072,"API"]],"Comments":[]}
{"id":60343,"text":"ID:32417655\nPost:\nText: You need to set full_output to true, after which fopt, func_calls, grad_calls, and warnflag are included too; you can slice the returned sequence: \nCode: xopt, fopt = fmin_bfgs(f, 0, fprime = fprime, full_output=True)[:2]\n\nText: See the fmin_bfgs documentation: \nText: full_output : bool, optional If True,return fopt, func_calls, grad_calls, and warnflag in addition to xopt. \nText: Granted, the documentation for this function makes this far from obvious, I first had to look at the linked source code. By the looks of it, bopt and gopt are also included when full_output is set, but the documentation fails to properly explain this. \nAPI:\nscipy.optimize.fmin_bfgs\n","label":[[261,270,"Mention"],[666,690,"API"]],"Comments":[]}
{"id":60344,"text":"ID:32554880\nPost:\nText: This is algorithmically nontrivial. A future scipy version is likely to have this sort of routine. \nText: Today, you can cast your root finding problem into a least-square problem (minimize the sum of squares of lhs), and use the master branch of the scipy Git repo. \nText: EDIT : opt.least_squares is available in released scipy for a long long time now (was not so in 2015 when this question was asked). \nAPI:\nscipy.optimize.least_squares\n","label":[[305,322,"Mention"],[436,464,"API"]],"Comments":[]}
{"id":60345,"text":"ID:32704029\nPost:\nText: As pointed out by cel, the problem is simply I was (unintendly) returning a vector instead of a single value, the function incmse must be modified as follows \nCode: def incmse(*p):\n\n     v, sigma =p[0]\n     print v,sigma\n\n     msqdiff=[(( lambmarket[t]  - s_t(0.,xnew[t], v, 0.003, sigma) )**2)  for t in range(0,len(xnew))]\n\n     print sum(msqdiff)\n     return sum(msqdiff)\n\nText: Now a single value is passed to sp.optimize.brute as required for it to work. \nAPI:\nscipy.optimize.brute\n","label":[[438,455,"Mention"],[490,510,"API"]],"Comments":[]}
{"id":60346,"text":"ID:32729414\nPost:\nText: Those functions have been written a long time ago with scipy compatibility in mind. But there were several changes in scipy in the meantime. \nText: kstest has an args keyword for the distribution parameters. \nText: To get the distribution parameters we can try to estimate them by using the fit method of the sp.stats distributions. However, estimating all parameters prints some warnings and the estimated df parameter is large. If we fix df at specific values we get estimates without warnings that we can use in the call of kstest. \nCode: >>> ast.fit(x)\nC:\\programs\\WinPython-64bit-3.4.3.1\\python-3.4.3.amd64\\lib\\site-packages\\scipy\\integrate\\quadpack.py:352: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n  If increasing the limit yields no improvement it is advised to analyze \n  the integrand in order to determine the difficulties.  If the position of a \n  local difficulty can be determined (singularity, discontinuity) one will \n  probably gain from splitting up the interval and calling the integrator \n  on the subranges.  Perhaps a special-purpose integrator should be used.\n  warnings.warn(msg, IntegrationWarning)\nC:\\programs\\WinPython-64bit-3.4.3.1\\python-3.4.3.amd64\\lib\\site-packages\\scipy\\integrate\\quadpack.py:352: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n  warnings.warn(msg, IntegrationWarning)\n(31834.800527154337, -2.3475921468088172, 1.3720725621594987, 2.2766515091760722)\n\n>>> p = ast.fit(x, f0=100)\n>>> print(stats.kstest(x, ast.cdf, args=p)) \n(0.13897385693057401, 0.83458552699682509)\n\n>>> p = ast.fit(x, f0=5)\n>>> print(stats.kstest(x, ast.cdf, args=p)) \n(0.097960232618178544, 0.990756154198281)\n\nText: However, the distribution for the Kolmogorov-Smirnov test assumes that the distribution parameters are fixed and not estimated. If we estimate the parameters as above, then the p-value will not be correct since it is not based on the correct distribution. \nText: For some distributions we can use tables for the kstest with estimated mean and scale parameter, e.g. the Lilliefors test kstest_normal in statsmodels. If we have estimated shape parameters, then the distribution of the ks test statistics will depend on the parameters of the model, and we could get the pvalue from bootstrapping. \nText: (I don't remember anything about estimating the parameters of the SkewT distribution and whether maximum likelihood estimation has any specific problems.) \nAPI:\nscipy.stats\n","label":[[333,341,"Mention"],[2490,2501,"API"]],"Comments":[]}
{"id":60347,"text":"ID:32827878\nPost:\nText: Depending on Your final purpose, You may use cdo to regrid the whole file \nCode: cdo remapbil,r360x180 infile outfile\n\nText: or just plot every second or third value from original file like this: \nCode: plt.pcolormesh(lon[::2,::2],lat[::2,::2],var1[::2,::2])\n\nText: The error message You show just says that dimensions do not much, just print the shape of your variables before the error appears and try to get it working. \nText: Why Your code does not work? Your chosen method requires input coordinates as lon,lat pairs for data points, not mesh coordinates. If You have data points with shape 10000, your coordinates must be with the shape (10000,2), not (100,100). But as griddata is meant for unstructured data, it will not be efficient for Your purpose, I suggest using something like sp.interpolate.RegularGridInterpolator \nText: But anyway, if You need to use the interpolated data more than once, I suggest creating new netCDF files with cdo and process them, instead of interpolating data each time You run Your script. \nAPI:\nscipy.interpolate.RegularGridInterpolator\n","label":[[815,853,"Mention"],[1060,1101,"API"]],"Comments":[]}
{"id":60348,"text":"ID:33049876\nPost:\nText: The implementation of the Rice distribution in rice uses a slightly different parameterization than the parameterization described in the wikipedia article. \nText: To make your plots agree, change this line \nCode: Rsci=scst.rice(pr,scale=sigma)\n\nText: to \nCode: Rsci=scst.rice(pr\/sigma, scale=sigma)\n\nText: Here's a longer explanation: \nText: The PDF shown on wikipedia is \nText: The PDF in the docstring of sp.stats.rice is \nText: However, that formula does not show the scale parameter that all of the continuous distributions in scipy have. (It also doesn't show the location parameter loc, but I'll assume there is no interest in using a Rice distribution with a nonzero location.) To create the formula that includes the scale parameter, we use the standard scale family of a PDF: \nText: So the scipy PDF is actually \nText: If we make the identifications \nText: we obtain the PDF shown in the wikipedia article. \nText: In your code, your parameter pr is , so to convert to scipy's parameterization, you must use b = pr \/ sigma. \nAPI:\nscipy.stats.rice\nscipy.stats.rice\n","label":[[71,75,"Mention"],[432,445,"Mention"],[1064,1080,"API"],[1081,1097,"API"]],"Comments":[]}
{"id":60349,"text":"ID:33079243\nPost:\nText: It looks like you want to use the fit method of sp.stats.weibull_min (which is an alias for scipy.stats.frechet_r). Use the argument floc=0 to constrain the location to be 0. \nCode: In [9]: data\nOut[9]: \narray([ 0.02298851,  0.11494253,  0.2183908 ,  0.11494253,  0.14942529,\n        0.11494253,  0.06896552,  0.06896552,  0.03448276,  0.02298851,\n        0.02298851,  0.02298851,  0.02298851])\n\nIn [10]: from scipy.stats import weibull_min\n\nIn [11]: shape, loc, scale = weibull_min.fit(data, floc=0)\n\nIn [12]: shape\nOut[12]: 1.3419930069121602\n\nIn [13]: scale\nOut[13]: 0.084273047253525968\n\nAPI:\nscipy.stats.weibull_min\n","label":[[72,92,"Mention"],[621,644,"API"]],"Comments":[]}
{"id":60350,"text":"ID:33402456\nPost:\nText: As @Azad has already noted in a comment, you probably need scipy.optimize to do the bulk of the work for you. Specifically, either sp.optimize.fsolve or scipy.optimize.root. As the latter seems more general, I'll demonstrate that. Since it can use multiple methods, have a look at the help. \nText: Both of these functions are capable of finding roots for functions mapping from R^n to R^m, i.e. multivariate vector-valued functions. If you consider your stress function, that's exactly what you have: it maps from R^2 to R^2. For clarity, you could even define it as \nCode: def stress2(Rvec):\n    X,Y=Rvec\n    chi = 1\n    F = 1\n    a = 1\n    c = (1.0*a)\/(np.sqrt(np.power(X,2)+np.power(Y,2))*1.0)\n    A = 0.5*(1 - c**2. + (1 - 4*c**2 + 3*c**4.)*np.cos(2*np.arctan(Y\/X)))\n    B = 0.5*(1 - c**2. - (1 - 4*c**2 + 3*c**4.)*np.cos(2*np.arctan(Y\/X)))\n    C = 0.5*(1 + c**2. - (1 + 3*c**4.)*np.cos(2*np.arctan(Y\/X)))\n    D = 0.5*(1 + c**2. + (1 + 3*c**4.)*np.cos(2*np.arctan(Y\/X)))\n    E = 0.5*((1 + 2*c**2. - 3*c**4.)*np.sin(2*np.arctan(Y\/X)))\n    f = 1.0*F*c**2 + (A-1.0*chi*B) # Radial stress\n    g = -1.*F*c**2 + (C - 1.0*chi*D) # Tangential stress\n    return f,g\n\nText: Now, with this definition you can simply call \nCode: import scipy.optimize as opt\nsol=opt.root(stress2,[0.5,0.5])\n\nText: wich will try to find a zero starting from [0.5,0.5]. Note that the root of the vector-valued function is exactly where both of its components are zero, which is what you're after. \nText: The return OptimizeResult looks like this: \nCode: In [224]: sol\nOut[224]: \n  status: 1\n success: True\n     qtf: array([  2.94481987e-09,   4.76366933e-25])\n    nfev: 47\n       r: array([ -7.62669534e-06,   7.62669532e-06,   2.16965211e-21])\n     fun: array([  2.25125258e-10,  -2.25125258e-10])\n       x: array([ 167337.87789902,  167337.87786433])\n message: 'The solution converged.'\n    fjac: array([[-0.70710678,  0.70710678],\n       [ 0.70710678,  0.70710678]])\n\nText: It has a bunch of information. First of all, sol.status will tell you if it converged successfully. This is the most important output: your root and the possibility of finding it very sensitively depends on your starting point. If you try a starting point where X=0 or Y=0 in your example, you'll see that it has difficulties finding a root. \nText: If you do have a root, sol.x will tell you the coordinates, and sol.fun will tell you the value of your function (near 0 if sol.status==1). \nText: Now, as you also noticed, each call will tell you at most a single root. To find multiple roots, you can't avoid searching for them. You can do this by going over an X,Y grid of your choice, starting root\/fsolve from there, and checking whether the search succeeded. If it did: store the values for post-processing. \nText: Unfortunately, finding the zeros of a nonlinear multivariate function is far from easy, so you have to get your hands dirty sooner or later. \nText: Update \nText: You're in for some trouble. Consider: \nCode: v=np.linspace(-10,10,100)\nX,Y=np.meshgrid(v,v)\n\nfig = plt.figure()\nhc=plt.contourf(X,Y,stress2([X,Y])[0].clip(-1,1),levels=np.linspace(-1,1,20))\nplt.contour(X,Y,stress2([X,Y])[0].clip(-1,1),levels=[0],color=(1,0,0))\nplt.colorbar(hc)\n\nText: and the same for the other function. Here's how the x and y components of your function look like, respectively: \nText: They both have zeros along some hyperbolic-like curves. Which seem to be identical. This figure strongly suggests that there is a line of points where your function is zero: both components. This is as bad as it can get for numerical root-finding algorithms, as there are no clear-cut (isolated) zeroes. \nText: I suggest checking your function on paper for the case of X==Y, you might indeed see that your function disappears there, at least asymptotically. \nText: Update2 \nText: You added the original, polar form of your function. While I can't see where you went wrong (apart from using np.arctan instead of np.arctan2, but that doesn't seem to fix the problem), I tried plotting your polar function: \nCode: def stress_polar(Rvec):\n    R,theta=Rvec\n    chi = 0\n    F = 1\n    a = 1\n    c = (1.0*a)\/(R*1.0)                                   \n    A = 0.5*(1 - c**2. + (1 - 4*c**2 + 3*c**4.)*np.cos(2*theta))\n    B = 0.5*(1 - c**2. - (1 - 4*c**2 + 3*c**4.)*np.cos(2*theta))\n    C = 0.5*(1 + c**2. - (1 + 3*c**4.)*np.cos(2*theta))\n    D = 0.5*(1 + c**2. + (1 + 3*c**4.)*np.cos(2*theta))\n    E = 0.5*((1 + 2*c**2. - 3*c**4.)*np.sin(2*theta))\n    f = 1.0*F*c**2. + (A-1.0*chi*B)\n    g = -1.0*F*c**2. + (C-1.0*chi*D)\n    return f,g\n\nv1=np.linspace(0.01,10,100)\nv2=np.linspace(-np.pi,np.pi,100)\nR,theta=np.meshgrid(v1,v2)\n\nfig = plt.figure()\nax=plt.subplot(111, polar=True)\nhc=plt.contourf(theta,R,stress_polar([R,theta])[0].clip(-1,1),levels=np.linspace(-1,1,20))\nplt.contour(theta,R,stress_polar([R,theta])[0].clip(-1,1),levels=[0],color=(1,0,0))\nplt.colorbar(hc)\n\nText: and the same for the tangential component. Note that the polar plot needs to get theta first, then R. The result: \nText: this shows a quite different picture, with a finite support for the zeros of the radial component. Now, I've never used polar plots before in matplotlib, so it's equally possible that I messed something up during plotting. But it might be worth looking at the A,B,C,D parameters that are computed by your polar and Cartesian functions, to make sure they compute the same thing. \nAPI:\nscipy.optimize.fsolve\n","label":[[155,173,"Mention"],[5432,5453,"API"]],"Comments":[]}
{"id":60351,"text":"ID:33559862\nPost:\nText: A key difference between iterative solvers and direct solvers is that direct solvers can more efficiently solve for multiple right-hand values by using a factorization (usually either Cholesky or LU), while iterative solvers can't. This means that for direct solvers there is a computational advantage to solving for multiple columns simultaneously. \nText: For iterative solvers, on the other hand, there's no computational gain to be had in simultaneously solving multiple columns, and this is probably why matrix solutions are not supported natively in the API of cg, bicg, etc. \nText: Because of this, a direct solution like sp.sparse.linalg.spsolve will probably be optimal for your case. If for some reason you still desire an iterative solution, I'd just create a simple convenience function like this: \nCode: from scipy.sparse.linalg import bicg\n\ndef bicg_solve(M, B):\n    X, info = zip(*(bicg(M, b) for b in B.T))\n    return np.transpose(X), info\n\nText: Then you can create some data and call it as follows: \nCode: import numpy as np\nfrom scipy.sparse import csc_matrix\n\n# create some matrices\nM = csc_matrix(np.random.rand(5, 5))\nB = np.random.rand(5, 4)\n\nX, info = bicg_solve(M, B)\nprint(X.shape)\n# (5, 4)\n\nText: Any iterative solver API which accepts a matrix on the right-hand-side will essentially just be a wrapper for something like this. \nAPI:\nscipy.sparse.linalg.spsolve\n","label":[[652,676,"Mention"],[1384,1411,"API"]],"Comments":[]}
{"id":60352,"text":"ID:33579599\nPost:\nText: The UnivariateSpline code you link to is a lower-level routine  it's probably better to use interp1d for your case. \nText: As far as your data goes, you should be able to read the files using np.loadtxt, extract the columns into numpy arrays, and then use interp1d like this: \nCode: x1, y1 = np.loadtxt('file1.txt').transpose()\nx2, y2 = np.loadtxt('file2.txt').transpose()\n\n# interpolate (x2, y2) onto grid x1\nfrom scipy.interpolate import interp1d\nfunc = interp1d(x2, y2, kind='cubic')\ny2_interp = func(x1)\n\nAPI:\nscipy.interpolate.interp1d\n","label":[[117,125,"Mention"],[539,565,"API"]],"Comments":[]}
{"id":60353,"text":"ID:33664132\nPost:\nText: Solved! I just needed to use fftconvolve which takes care of zero padding itself. No normalization was required. So the working code for me is: \nCode:    from scipy.signal import fftconvolve\n   def similarity(template, test):\n       corr = fftconvolve(template, test, mode='same')           \n\n       return max(abs(corr))\n\nAPI:\nscipy.signal.fftconvolve\n","label":[[53,64,"Mention"],[352,376,"API"]],"Comments":[]}
{"id":60354,"text":"ID:33696128\nPost:\nText: The first return value of read is the samplerate and it is taken from the header of the audio file. If you want to change the sample rate, you have to do some samplerate rate conversion. \nText: Here some basic code to change the samplerate by interpolation. Note that this is only harmless, if the original sample rate is lower than the new one. The other way around, you could introduce aliasing if the audio file contains frequencies higher than the Nyquist frequency. If this is the case you should apply a filter before interpolating. \nCode: import numpy as np\nfrom scipy.io import wavfile\nfrom scipy import interpolate\n\nNEW_SAMPLERATE = 48000\n\nold_samplerate, old_audio = wavfile.read(\"test.wav\")\n\nif old_samplerate != NEW_SAMPLERATE:\n    duration = old_audio.shape[0] \/ old_samplerate\n\n    time_old  = np.linspace(0, duration, old_audio.shape[0])\n    time_new  = np.linspace(0, duration, int(old_audio.shape[0] * NEW_SAMPLERATE \/ old_samplerate))\n\n    interpolator = interpolate.interp1d(time_old, old_audio.T)\n    new_audio = interpolator(time_new).T\n\n    wavfile.write(\"out.wav\", NEW_SAMPLERATE, np.round(new_audio).astype(old_audio.dtype))\n\nText: EDIT: You may have a look at the Python package resampy, which implements efficient sample rate conversion. \nAPI:\nscipy.io.wavfile.read\n","label":[[50,54,"Mention"],[1294,1315,"API"]],"Comments":[]}
{"id":60355,"text":"ID:33708904\nPost:\nText: You can find which pixels satisfy your cut-off using a simple boolean condition, then use label and sp.ndimage.center_of_mass to find the connected regions and compute their centers of mass: \nCode: import numpy as np\nfrom scipy import ndimage\nfrom matplotlib import pyplot as plt\n\n# generate some lowpass-filtered noise as a test image\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg \/= img.max()\n\n# use a boolean condition to find where pixel values are > 0.75\nblobs = img > 0.75\n\n# label connected regions that satisfy this condition\nlabels, nlabels = ndimage.label(blobs)\n\n# find their centres of mass. in this case I'm weighting by the pixel values in\n# `img`, but you could also pass the boolean values in `blobs` to compute the\n# unweighted centroids.\nr, c = np.vstack(ndimage.center_of_mass(img, labels, np.arange(nlabels) + 1)).T\n\n# find their distances from the top-left corner\nd = np.sqrt(r*r + c*c)\n\n# plot\nfig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(10, 5))\nax[0].imshow(img)\nax[1].hold(True)\nax[1].imshow(np.ma.masked_array(labels, ~blobs), cmap=plt.cm.rainbow)\nfor ri, ci, di in zip(r, c, d):\n    ax[1].annotate('', xy=(0, 0), xytext=(ci, ri),\n                   arrowprops={'arrowstyle':'<-', 'shrinkA':0})\n    ax[1].annotate('d=%.1f' % di, xy=(ci, ri),  xytext=(0, -5),\n                   textcoords='offset points', ha='center', va='top',\n                   fontsize='x-large')\nfor aa in ax.flat:\n    aa.set_axis_off()\nfig.tight_layout()\nplt.show()\n\nAPI:\nscipy.ndimage.label\nscipy.ndimage.center_of_mass\n","label":[[114,119,"Mention"],[124,149,"Mention"],[1621,1640,"API"],[1641,1669,"API"]],"Comments":[]}
{"id":60356,"text":"ID:33785045\nPost:\nText: I believe I have answered my own question: \nText: All I needed was to generate a new sp.interpolate.interp2d method using the x and y vectors and the scalar array of choice, then feed the desired coordinates back into the method: \nCode: >>> from scipy import interpolate\n>>> f = interpolate.interp2d(x, y, gx, kind='cubic')\n>>> results = f(cx, cy)\n[[-2.51109575 -2.51109575 -2.51109575 ..., -2.51109575 -2.51109575\n  -2.51109575]\n [-2.50638115 -2.50638115 -2.50638115 ..., -2.50638115 -2.50638115\n  -2.50638115]\n [-2.50638115 -2.50638115 -2.50638115 ..., -2.50638115 -2.50638115\n  -2.50638115]\n ..., \n [ 2.50638115  2.50638115  2.50638115 ...,  2.50638115  2.50638115\n   2.50638115]\n [ 2.50638115  2.50638115  2.50638115 ...,  2.50638115  2.50638115\n   2.50638115]\n [ 2.51109575  2.51109575  2.51109575 ...,  2.51109575  2.51109575\n   2.51109575]]\n\nText: As a way of checking, I can give x and y for a known value: \nCode: >>> print f(0.5, 0.0)\n[ -1.38777878e-17]\n\nText: ... which is basically zero, as expected for the gradient at this point on the surface. \nText: Simple, really! \nAPI:\nscipy.interpolate.interp2d\n","label":[[109,132,"Mention"],[1111,1137,"API"]],"Comments":[]}
{"id":60357,"text":"ID:33955922\nPost:\nText: Some things you could try: \nText: (1) Use scikits.audiolab as in this question \nText: (2) Convert the wav format from NIST format to the standard RIFF format with a tool like sndfile-convert from 'libsndfile' (You'll need to change the original file endings to .nist). \nText: I got (2) to work on my own system and could read the files with wavr \nAPI:\nscipy.io.wavfile.read\n","label":[[365,369,"Mention"],[376,397,"API"]],"Comments":[]}
{"id":60358,"text":"ID:33962986\nPost:\nText: Your closed path can be considered as a parametric curve, x=f(u), y=g(u) where u is distance along the curve, bounded on the interval [0, 1). You can use splprep with per=True to treat your x and y points as periodic, then evaluate the fitted splines using scipy.interpolate.splev: \nCode: import numpy as np\nfrom scipy import interpolate\nfrom matplotlib import pyplot as plt\n\nx = np.array([23, 24, 24, 25, 25])\ny = np.array([13, 12, 13, 12, 13])\n\n# append the starting x,y coordinates\nx = np.r_[x, x[0]]\ny = np.r_[y, y[0]]\n\n# fit splines to x=f(u) and y=g(u), treating both as periodic. also note that s=0\n# is needed in order to force the spline fit to pass through all the input points.\ntck, u = interpolate.splprep([x, y], s=0, per=True)\n\n# evaluate the spline fits for 1000 evenly spaced distance values\nxi, yi = interpolate.splev(np.linspace(0, 1, 1000), tck)\n\n# plot the result\nfig, ax = plt.subplots(1, 1)\nax.plot(x, y, 'or')\nax.plot(xi, yi, '-b')\n\nAPI:\nscipy.interpolate.splprep\n","label":[[178,185,"Mention"],[985,1010,"API"]],"Comments":[]}
{"id":60359,"text":"ID:33970685\nPost:\nText: I was going to suggest matplotlib.pyplot.streamplot which supports the keyword argument start_points as of version 1.5.0, however it's not practical and also very inaccurate. \nText: Your code examples are a bit confusing to me: if you have vx, vy vector field coordinates, then you should have two meshes: x and y. Using these you can indeed use griddata to obtain a smooth vector field for integration, however that seemed to eat up too much memory when I tried to do that. Here's a similar solution based on scipy.interpolate.interp2d: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.interpolate as interp\nimport scipy.integrate as integrate\n\n#dummy input from the streamplot demo\ny, x = np.mgrid[-3:3:100j, -3:3:100j]\nvx = -1 - x**2 + y\nvy = 1 + x - y**2\n\n#dfun = lambda x,y: [interp.griddata((x,y),vx,np.array([[x,y]])), interp.griddata((x,y),vy,np.array([[x,y]]))]\ndfunx = interp.interp2d(x[:],y[:],vx[:])\ndfuny = interp.interp2d(x[:],y[:],vy[:])\ndfun = lambda xy,t: [dfunx(xy[0],xy[1])[0], dfuny(xy[0],xy[1])[0]]\n\np0 = (0.5,0.5)\ndt = 0.01\nt0 = 0\nt1 = 1\nt = np.arange(t0,t1+dt,dt)\n\nstreamline=integrate.odeint(dfun,p0,t)\n\n#plot it\nplt.figure()\nplt.plot(streamline[:,0],streamline[:,1])\nplt.axis('equal')\nmymask = (streamline[:,0].min()*0.9<=x) & (x<=streamline[:,0].max()*1.1) & (streamline[:,1].min()*0.9<=y) & (y<=streamline[:,1].max()*1.1)\nplt.quiver(x[mymask],y[mymask],vx[mymask],vy[mymask])\nplt.show()\n\nText: Note that I made the integration mesh more dense for additional precision, but it didn't change much in this case. \nText: Result: \nText: Update \nText: After some notes in comments I revisited my original griddata-based approach. The reason for this was that while interp2d computes an interpolant for the entire data grid, griddata only computes the interpolating value at the points given to it, so in case of a few points the latter should be much faster. \nText: I fixed the bugs in my earlier griddata attempt and came up with \nCode: xyarr = np.array(zip(x.flatten(),y.flatten()))\ndfun = lambda p,t: [interp.griddata(xyarr,vx.flatten(),np.array([p]))[0], interp.griddata(xyarr,vy.flatten(),np.array([p]))[0]]\n\nText: which is compatible with odeint. It computes the interpolated values for each p point given to it by odeint. This solution doesn't consume excessive memory, however it takes much much longer to run with the above parameters. This is probably due to a lot of evaluations of dfun in odeint, much more than what would be evident from the 100 time points given to it as input. \nText: However, the resulting streamline is much smoother than the one obtained with interp2d, even though both methods used the default linear interpolation method: \nAPI:\nscipy.interpolate.griddata\n","label":[[370,378,"Mention"],[2731,2757,"API"]],"Comments":[]}
{"id":60360,"text":"ID:34256730\nPost:\nText: Is there any reason why a straightforward use of sp.integrate.quad won't work? I mean: \nCode: import scipy as sp\nimport scipy.integrate\n\n#define some dummy p1 and p2\ndef p1(y):\n    return 3*y+2\ndef p2(y):\n    return -4*y-4\n\n#define p_{xi1+xi2}\ndef pplus(x):\n    return sp.integrate.quad(lambda u,x=x: p1(u)*p2(x-u), 0, x)[0]\n#define p_{xi1\/xi2}\ndef pdivide(x):\n    return sp.integrate.quad(lambda u,x=x: u*p1(u)*p2(u\/x), 0, sp.minimum(x,1))[0]\/x**2\n\n#use it\nx = 0.2\noutplus = pplus(x)\noutdivide = pdivide(x)\n\nText: This will result in \nCode: print(outplus, outdivide)\n-2.016 -8.06666666667\n\nText: You might want to define a proper function instead of the latter lambdas, in order to catch the full output of quad to check if everything went OK with the integration. \nText: Let's check with sympy: \nCode: import sympy as sym\n\nU,X = sym.symbols('U,X')\npplus_sym = sym.lambdify(X, sym.integrate((3*U+2)*(-4*(X-U)-4), (U,0,X)))\ndct = {'Min': sp.minimum}; #it's best if we tell lambdify what to use for Min\npdivide_sym = sym.lambdify(X, sym.integrate(U*(3*U+2)*(-4*(U\/X)-4), (U,0,sym.Min(X,1)))\/(X**2), dct)\n\nText: Then the result is \nCode: print(pplus_sym(x), pdivide_sym(x))\n-2.016 -8.06666666667\n\nAPI:\nscipy.integrate.quad\n","label":[[73,90,"Mention"],[1224,1244,"API"]],"Comments":[]}
{"id":60361,"text":"ID:34257083\nPost:\nText: In my experience, there are two fastest ways to find neighbor lists in 3D. One is to use a most naive double-for-loop code written in C++ or Cython (in my case, both). It runs in N^2, but is very fast for small systems. The other way is to use a linear time algorithm. Scipy ckdtree is a good choice, but has limitations. Neighbor list finders from molecular dynamics software are most powerful, but are very hard to wrap, and likely have slow initialization time. \nText: Below I compare four methods: \nText: Naive cython code Wrapper around OpenMM (is very hard to install, see below) Scipy.spatial.ckdtree pdist \nText: Test setup: n points scattered in a rectangular box at volume density 0.2. System size ranging from 10 to a 1000000 (a million) particles. Contact radius is taken from 0.5, 1, 2, 4, 7, 10. Note that because density is 0.2, at contact radius 0.5 we'll have on average about 0.1 contacts per particle, at 1 = 0.8, at 2 = 6.4, and at 10 - about 800! Contact finding was repeated several times for small systems, done once for systems >30k particles. If time per call exceeded 5 seconds, the run was aborted. \nText: Setup: dual xeon 2687Wv3, 128GB RAM, Ubuntu 14.04, python 2.7.11, scipy 0.16.0, numpy 1.10.1. None of the code was using parallel optimizations (except for OpenMM, though parallel part went so quick that it was not even noticeable on a CPU graph, most of the time was spend piping data to-from OpenMM). \nText: Results: Note that plots below are logscale, and spread over 6 orders of magnitude. Even small visual difference may be actually 10-fold. For systems less than 1000 particles, Cython code was always faster. However, after 1000 particles results are dependent on the contact radius. pdist implementation was always slower than cython, and takes much more memory, because it explicitly creates a distance matrix, which is slow because of sqrt. \nText: At small contact radius (<1 contact per particle), ckdtree is a good choice for all system sizes. At medium contact radius, (5-50 contacts per particle) naive cython implementation is the best up to 10000 particles, then OpenMM starts to win by about several orders of magnitude, but ckdtree performs just 3-10 times worse At high contact radius (>200 contacts per particle) naive methods work up to 100k or 1M particles, then OpenMM may win. \nText: Installing OpenMM is very tricky; you can read more in http:\/\/bitbucket.org\/mirnylab\/openmm-polymer file \"contactmaps.py\" or in the readme. However, the results below show that it is only advantageous for 5-50 contacts per particle, for N>100k particles. \nText: Cython code below: \nCode: import numpy as np\ncimport numpy as np\ncimport cython\n\ncdef extern from \"<vector>\" namespace \"std\":\n    cdef cppclass vector[T]:\n        cppclass iterator:\n            T operator*()\n            iterator operator++()\n            bint operator==(iterator)\n            bint operator!=(iterator)\n        vector()\n        void push_back(T&)\n        T& operator[](int)\n        T& at(int)\n        iterator begin()\n        iterator end()\n\nnp.import_array() # initialize C API to call PyArray_SimpleNewFromData\ncdef public api tonumpyarray(int* data, long long size) with gil:\n    if not (data and size >= 0): raise ValueError\n    cdef np.npy_intp dims = size\n    #NOTE: it doesn't take ownership of `data`. You must free `data` yourself\n    return np.PyArray_SimpleNewFromData(1, &dims, np.NPY_INT, <void*>data)\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef contactsCython(inArray, cutoff):\n    inArray = np.asarray(inArray, dtype = np.float64, order = \"C\")\n    cdef int N = len(inArray)\n    cdef np.ndarray[np.double_t, ndim = 2] data = inArray\n    cdef int j,i\n    cdef double curdist\n    cdef double cutoff2 = cutoff * cutoff  # IMPORTANT to avoid slow sqrt calculation\n    cdef vector[int] contacts1\n    cdef vector[int] contacts2\n    for i in range(N):\n        for j in range(i+1, N):\n            curdist = (data[i,0] - data[j,0]) **2 +(data[i,1] - data[j,1]) **2 + (data[i,2] - data[j,2]) **2\n            if curdist < cutoff2:\n                contacts1.push_back(i)\n                contacts2.push_back(j)\n    cdef int M = len(contacts1)\n\n    cdef np.ndarray[np.int32_t, ndim = 2] contacts = np.zeros((M,2), dtype = np.int32)\n    for i in range(M):\n        contacts[i,0] = contacts1[i]\n        contacts[i,1] = contacts2[i]\n    return contacts\n\nText: Compilation (or makefile) for Cython code: \nCode:     cython --cplus fastContacts.pyx\n    g++  -g -march=native -Ofast -fpic -c   fastContacts.cpp -o fastContacts.o `python-config --includes`\n    g++  -g -march=native -Ofast -shared  -o fastContacts.so  fastContacts.o `python-config --libs`\n\nText: Testing code: \nCode: from __future__ import print_function, division\n\nimport signal\nimport time\nfrom contextlib import contextmanager\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.spatial import ckdtree\nfrom scipy.spatial.distance import pdist\n\nfrom contactmaps import giveContactsOpenMM  # remove this unless you have OpenMM and openmm-polymer libraries installed\nfrom fastContacts import contactsCython\n\n\nclass TimeoutException(Exception): pass\n\n\n@contextmanager\ndef time_limit(seconds):\n    def signal_handler(signum, frame):\n        raise TimeoutException(\"Timed out!\")\n\n    signal.signal(signal.SIGALRM, signal_handler)\n    signal.alarm(seconds)\n    try:\n        yield\n    finally:\n        signal.alarm(0)\n\n\nmatplotlib.rcParams.update({'font.size': 8})\n\n\ndef close_pairs_ckdtree(X, max_d):\n    tree = ckdtree.cKDTree(X)\n    pairs = tree.query_pairs(max_d)\n    return np.array(list(pairs))\n\n\ndef condensed_to_pair_indices(n, k):\n    x = n - (4. * n ** 2 - 4 * n - 8 * k + 1) ** .5 \/ 2 - .5\n    i = x.astype(int)\n    j = k + i * (i + 3 - 2 * n) \/ 2 + 1\n    return np.array([i, j]).T\n\n\ndef close_pairs_pdist(X, max_d):\n    d = pdist(X)\n    k = (d < max_d).nonzero()[0]\n    return condensed_to_pair_indices(X.shape[0], k)\n\n\na = np.random.random((100, 3)) * 3  # test set\nmethods = {\"cython\": contactsCython, \"ckdtree\": close_pairs_ckdtree, \"OpenMM\": giveContactsOpenMM,\n           \"pdist\": close_pairs_pdist}\n\n# checking that each method gives the same value\nallUniqueInds = []\nfor ind, method in methods.items():\n    contacts = method(a, 1)\n    uniqueInds = contacts[:, 0] + 100 * contacts[:, 1]  # unique index of each contacts\n    allUniqueInds.append(np.sort(uniqueInds))  # adding sorted unique conatcts\nfor j in allUniqueInds:\n    assert np.allclose(j, allUniqueInds[0])\n\n# now actually doing testing\nrepeats = [30,30,30, 30, 30, 20,  20,   10,   5,   3,     2 ,       1,     1,      1]\nsizes =    [10,30,100, 200, 300,  500, 1000, 2000, 3000, 10000, 30000, 100000, 300000, 1000000]\nsystems = [[np.random.random((n, 3)) * ((n \/ 0.2) ** 0.333333) for k in range(repeat)] for n, repeat in\n           zip(sizes, repeats)]\n\nfor j, radius in enumerate([0.5, 1, 2, 4, 7, 10]):\n    plt.subplot(2, 3, j + 1)\n    plt.title(\"Radius = {0}; {1:.2f} cont per particle\".format(radius, 0.2 * (4 \/ 3 * np.pi * radius ** 3)))\n\n    times = {i: [] for i in methods}\n\n    for name, method in methods.items():\n        for n, system, repeat in zip(sizes, systems, repeats):\n            if name == \"pdist\" and n > 30000:\n                break  # memory issues\n            st = time.time()\n            try:\n                with time_limit(5 * repeat):\n                    for ind in range(repeat):\n                        k = len(method(system[ind], radius))\n            except:\n                print(\"Run aborted\")\n                break\n            end = time.time()\n            mytime = (end - st) \/ repeat\n            times[name].append((n, mytime))\n            print(\"{0} radius={1} n={2} time={3} repeat={4} contPerParticle={5}\".format(name, radius, n, mytime,repeat, 2 * k \/ n))\n\n    for name in sorted(times.keys()):\n        plt.plot(*zip(*times[name]), label=name)\n    plt.xscale(\"log\")\n    plt.yscale(\"log\")\n    plt.xlabel(\"System size\")\n    plt.ylabel(\"Time (seconds)\")\n    plt.legend(loc=0)\n\nplt.show()\n\nAPI:\nscipy.spatial.distance.pdist\n","label":[[632,637,"Mention"],[8050,8078,"API"]],"Comments":[]}
{"id":60362,"text":"ID:34345234\nPost:\nText: You could approximate the curves as piecewise polynomials: \nCode: p1 = interpolate.PiecewisePolynomial(x1, y1[:, np.newaxis])\np2 = interpolate.PiecewisePolynomial(x2, y2[:, np.newaxis])\n\nText: p1 and p2 are functions of x. You can then use fsolve to find x values where p1(x) equals p2(x). \nCode: import pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy import interpolate\nimport matplotlib.pyplot as plt\n\ndef find_intersections(x1, y1, x2, y2):\n    x1 = np.asarray(x1)\n    y1 = np.asarray(y1)\n    x2 = np.asarray(x2)\n    y2 = np.asarray(y2)\n    p1 = interpolate.PiecewisePolynomial(x1, y1[:, np.newaxis])\n    p2 = interpolate.PiecewisePolynomial(x2, y2[:, np.newaxis])\n\n    def pdiff(x):\n        return p1(x) - p2(x)\n\n    xs = np.r_[x1, x2]\n    xs.sort()\n    x_min = xs.min()\n    x_max = xs.max()\n    x_mid = xs[:-1] + np.diff(xs) \/ 2\n    roots = set()\n    for x_guess in x_mid:\n        root, infodict, ier, mesg = optimize.fsolve(\n            pdiff, x_guess, full_output=True)\n        # ier==1 indicates a root has been found\n        if ier == 1 and x_min < root < x_max:\n            roots.add(root[0])\n    x_roots = np.array(list(roots))\n    y_roots = p1(x_roots)\n    return x_roots, y_roots\n\n\ndf = pd.DataFrame({\n    'SomeNumber': [0.85, 0.98, 1.06, 1.1, 1.13, 1.2, 1.22, 1.23, 1.31, 1.43],\n    'Events': [24, 39, 20, 28, 20, 24, 26, 29, 30, 24],\n    'Amount': [35.78, 35.78, 35.78, 35.78, 35.78, 35.78, 35.78, 35.78, 35.78, 35.78]},\n                  columns=['Amount', 'Events', 'SomeNumber'])\n\ndf = df.sort('SomeNumber')\n\nx = df['SomeNumber']\ny = df['Amount']\/df['SomeNumber']\n\ndf_below = df[df['Events'] < y]\ndf_above = df[df['Events'] >= y]\n\nx_coords = [df_below['SomeNumber'].min(), df_above['SomeNumber'].min()]\ny_coords = [df_below.ix[df_below['SomeNumber'].idxmin(), 'Events'],\n            df_above.ix[df_above['SomeNumber'].idxmin(), 'Events']]\n\nx_roots, y_roots = find_intersections(x, y, x_coords, y_coords)\n\nplt.plot(x, y, label='Potential Events')\nplt.scatter(x, df['Events'], label='Actual Events')\nplt.plot(x_coords, y_coords)\nplt.scatter(x_roots, y_roots, s=50, c='red')\nplt.xlabel('Some Number')\nplt.ylabel('Events')\nplt.legend(loc='upper right')\nplt.show()\n\nText: The intersection was found near (0.96, 37.19): \nCode: In [218]: x_roots\nOut[218]: array([0.9642754164139411])\n\nIn [219]: y_roots\nOut[219]: array([ 37.18562497])\n\nAPI:\nscipy.optimize.fsolve\n","label":[[264,270,"Mention"],[2398,2419,"API"]],"Comments":[]}
{"id":60363,"text":"ID:34434830\nPost:\nText: A two-part approach: \nText: wrap your A in a scipy.sparse LinearOperator (easy) patch the lsmr solver to use it (took some debugging). \nText: The idea of LinearOperator is easy and powerful: it's a virtual linear operator, or actually two: \nCode: Aop = Linop( A )  # see below\nA.dot(x) -> Aop.matvec(x)\nA.T.dot(y) -> Aop.rmatvec(y)\nx = solver( Aop, b ... )  # uses matvec and rmatvec, not A.dot A.T.dot\n\nText: Here matvec and rmatvec can do anything (within reason). For example, they could linearize horrible nonlinear equations near args x and y of any type whatever. Unfortunately aslinearoperator doesn't work asis for sparse x. The doc suggests two ways of implementing LinearOperator, but \nText: Whenever something can be done in two ways, someone will be confused. \nText: Anyway, Linop below works with sparse x -- with a patched lsmr.py, under gist.github.com\/denis-bz . Other sparse iterative solvers ? Don't know. \nText: If what you really want to do is: minimize |A x - b| and also keep |x| small, a.k.a. regularization, in the L1 or L2 norm \nText: then you should definitely look at scikit-learn . It targets different corners of Speed - Accuracy - Problems - People (SAPP) space than scipy.sparse.isolve . I've found scikit-learn solid, pleasant, pretty well-documented, but haven't compared the two on real problems. \nText: How big, how sparse are your problems ? Could you point to test cases on the web ? \nCode: \"\"\" Linop( A ): .matvec .rmatvec( sparse vecs )\nhttp:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.linalg.LinearOperator.html\nhttp:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.linalg.lsmr.html\n\"\"\"\n\nfrom __future__ import division\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse.linalg import LinearOperator  # $scipy\/sparse\/linalg\/interface.py\n\n__version__ = \"2015-12-24 dec  denis + safe_sparse_dot\"\n\n\n#...............................................................................\nclass Linop( LinearOperator ):  # subclass ?\n    \"\"\" Aop = Linop( scipy sparse matrix A )\n        ->  Aop.matvec(x) = A dot x, x ndarray or sparse\n            Aop.rmatvec(x) = A.T dot x\n        for scipy.sparse.linalg solvers like lsmr\n    \"\"\"\n\n    def __init__( self, A ):\n        self.A = A\n\n    def matvec( self, x ):\n        return safe_sparse_dot( self.A, x )\n\n    def rmatvec( self, y ):\n        return safe_sparse_dot( self.A.T, y )\n\n        # LinearOperator subclass should implement at least one of _matvec and _matmat.\n    def _matvec( self, b ):\n        raise NotImplementedError( \"_matvec\" )\n\n        # not _matvec only:\n        # $scipy\/sparse\/linalg\/interface.py\n        # def matvec(self, x):\n        #     x = np.asanyarray(x)  <-- kills sparse x, should raise an error\n\n    def _rmatvec( self, b ):\n        raise NotImplementedError( \"_rmatvec\" )\n\n    @property\n    def shape( self ):\n        return self.A.shape\n\n\ndef safe_sparse_dot( a, b ):\n    \"\"\" -> a * b or np.dot(a, b) \"\"\"\n        # from sklearn\n    if sparse.issparse(a) or sparse.issparse(b):\n        try:\n            return a * b\n        except ValueError:  # dimension mismatch: print shapes\n            print \"error: %s %s  *  %s %s\" % (\n                    type(a).__name__, a.shape,\n                    type(b).__name__, b.shape )\n            raise\n    else:\n        return np.dot(a, b)\n\n#...........................................................................\nif __name__ == \"__main__\":\n    import sys\n    from lsmr import lsmr  # patched $scipy\/sparse\/linalg\/lsmr.py\n\n    np.set_printoptions( threshold=20, edgeitems=10, linewidth=100, suppress=True,\n        formatter = dict( float = lambda x: \"%.2g\" % x ))\n\n        # test sparse.rand A m n, x n 1, b m 1\n    m = 10\n    n = 100\n    density = .1\n    bdense = 0\n    seed = 0\n    damp = 1\n\n        # to change these params in sh or ipython, run this.py  a=1  b=None  c=[3] ...\n    for arg in sys.argv[1:]:\n        exec( arg )\n\n    np.random.seed(seed)\n\n    print \"\\n\", 80 * \"-\"\n    paramstr = \"%s  m %d  n %d  density %g  bdense %d  seed %d  damp %g \" % (\n            __file__, m, n, density, bdense, seed, damp )\n    print paramstr\n\n    A = sparse.rand( m, n, density, format=\"csr\", random_state=seed )\n    x = sparse.rand( n, 1, density, format=\"csr\", random_state=seed )\n    b = sparse.rand( m, 1, density, format=\"csr\", random_state=seed )\n    if bdense:\n        b = b.toarray().squeeze()  # matrix (m,1) -> ndarray (m,)\n\n    #...........................................................................\n    Aop = Linop( A )\n        # aslinearoperator( A ): not for sparse x\n\n        # check Aop matvec rmatvec --\n    Ax = Aop.matvec( x )\n    bA = Aop.rmatvec( b )\n    for nm in \"A Aop x b Ax bA \".split():\n        x = eval(nm)\n        print \"%s: %s %s \" % (nm, x.shape, type(x))\n    print \"\"\n\n    print \"lsmr( Aop, b )\"\n\n    #...........................................................................\n    xetc = lsmr( Aop, b, damp=damp, show=1 )\n    #...........................................................................\n\n    x, istop, niter, normr, normar, norma, conda, normx = xetc\n    x = getattr( x, \"A\", x ) .squeeze()\n    print \"x:\", x.shape, x\n\n    #     print \"lsmr( A, b ) -- Valueerror in $scipy\/sparse\/linalg\/interface.py\"\n    #     xetc = lsmr( A, b, damp=damp, show=1 )  # Valueerror\n\n    safe_sparse_dot( A, b.T )  # ValueError: dimension mismatch\n\nAPI:\nscipy.sparse.linalg.LinearOperator\n","label":[[82,96,"Mention"],[5396,5430,"API"]],"Comments":[]}
{"id":60364,"text":"ID:34452715\nPost:\nText: I haven't used the frechet_r function from sp.stats (when just quickly testing it I got the same plot out as you) but you can get the required behaviour from genextreme in scipy.stats. It is worth noting that for genextreme the Frechet and Weibull shape parameter have the 'opposite' sign to usual. That is, in your case you would need to use a shape parameter of -0.198: \nCode: from scipy.stats import genextreme as gev\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig, ax = plt.subplots(1, 1)\n\nx=np.arange(0.01,120,0.01)\n# The order for this is array, shape, loc, scale\nF=gev.pdf(x,-0.198,loc=17.44,scale=8.153)\n\nplt.plot(x,F,'g',lw=2)\nplt.show()\n\nAPI:\nscipy.stats\n","label":[[67,75,"Mention"],[685,696,"API"]],"Comments":[]}
{"id":60365,"text":"ID:34537508\nPost:\nText: k-D trees are commonly used to partition multidimensional data in order to perform fast nearest-neighbour searches. \nText: For example, NearestNDInterpolator is essentially just a wrapper around sp.spatial.cKDTree (see the source code here). cKDTree.query will return the nearest-neighbour indices as well as the corresponding distances for a given set of input coordinates. \nAPI:\nscipy.interpolate.NearestNDInterpolator\nscipy.spatial.cKDTree\n","label":[[160,181,"Mention"],[219,237,"Mention"],[405,444,"API"],[445,466,"API"]],"Comments":[]}
{"id":60366,"text":"ID:34610985\nPost:\nText: Do you want to find an optimal T, by e.g. Golden_section_search on the function of one variable: \nCode: ftemperature( T ) = basinhopping( ... T=T ... ) .func  ?\n\nText: If so, build up history lists of func T and res that feed your Tsearch function: \nCode: # initialize history lists for a few T  (grid search) --\nfhist, Thist, reshist = [], [], []\n\nfor T in [...]:\n    res = basinhopping( cost, guess, T=T ... )\n    print \"T %.3g  func %.3g\" % (T, res.func)\n    fhist.append( res.func )\n    Thist.append( T )\n    reshist.append( res )\n\n# search for new T --\nwhile True:\n    T = Tsearch( Thist, fhist )  # golden search or ...\n    if T is None:  break\n    res = basinhopping( cost, guess, T=T ... )\n    print \"T %.3g  func %.3g\" % (T, res.func)\n    fhist.append( res.func )\n    Thist.append( T )\n    reshist.append( res )\n\nText: If not, please clarify. \nText: (You could do the same thing inside callbacks, as @Jacob Stevenson says.) \nText: (There are fancier methods for minimizing functions of one variable, see e.g. opt.minimize_scalar .) \nAPI:\nscipy.optimize.minimize_scalar\n","label":[[1042,1061,"Mention"],[1071,1101,"API"]],"Comments":[]}
{"id":60367,"text":"ID:34656728\nPost:\nText: I was originally going to show you how much of a difference it makes for 2d interpolation if your input data are oriented along the coordinate axes rather than in some general direction, but it turns out that the result would be even messier than I had anticipated. I tried using a random dataset over an interpolated rectangular mesh, and comparing that to a case where the same x and y coordinates were rotated by 45 degrees for interpolation. The result was abysmal. \nText: I then tried doing a comparison with a smoother dataset: turns out interp2d has quite a few issues. So my bottom line will be \"use scipy.interpolate.griddata\". \nText: For instructive purposes, here's my (quite messy) code: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.cm as cm\n\nn = 10                            # rough number of points\ndom = np.linspace(-2,2,n+1)       # 1d input grid\nx1,y1 = np.meshgrid(dom,dom)      # 2d input grid\nz = np.random.rand(*x1.shape)     # ill-conditioned sample\n#z = np.cos(x1)*np.sin(y1)        # smooth sample\n\n# first interpolator with interp2d:\nfun1 = interp.interp2d(x1,y1,z,kind='linear')\n\n# construct twice finer plotting and interpolating mesh\nplotdom = np.linspace(-1,1,2*n+1)             # for interpolation and plotting\nplotx1,ploty1 = np.meshgrid(plotdom,plotdom)\nplotz1 = fun1(plotdom,plotdom)                # interpolated points\n\n\n# construct 45-degree rotated input and interpolating meshes\nrotmat = np.array([[1,-1],[1,1]])\/np.sqrt(2)           # 45-degree rotation\nx2,y2 = rotmat.dot(np.vstack([x1.ravel(),y1.ravel()])) # rotate input mesh\nplotx2,ploty2 = rotmat.dot(np.vstack([plotx1.ravel(),ploty1.ravel()])) # rotate plotting\/interp mesh\n\n# interpolate on rotated mesh with interp2d\n# (reverse rotate by using plotx1, ploty1 later!)\nfun2 = interp.interp2d(x2,y2,z.ravel(),kind='linear')\n\n# I had to generate the rotated points element-by-element\n# since fun2() accepts only rectangular meshes as input\nplotz2 = np.array([fun2(xx,yy) for (xx,yy) in zip(plotx2.ravel(),ploty2.ravel())])\n\n# try interpolating with griddata\nplotz3 = interp.griddata(np.array([x1.ravel(),y1.ravel()]).T,z.ravel(),np.array([plotx1.ravel(),ploty1.ravel()]).T,method='linear')\nplotz4 = interp.griddata(np.array([x2,y2]).T,z.ravel(),np.array([plotx2,ploty2]).T,method='linear')\n\n\n# function to plot a surface\ndef myplot(X,Y,Z):\n    fig = plt.figure()\n    ax = Axes3D(fig)\n    ax.plot_surface(X, Y, Z,rstride=1, cstride=1,\n                    linewidth=0, antialiased=False,cmap=cm.coolwarm)\n    plt.show()\n\n\n# plot interp2d versions\nmyplot(plotx1,ploty1,plotz1)                    # Cartesian meshes\nmyplot(plotx1,ploty1,plotz2.reshape(2*n+1,-1))  # rotated meshes\n\n# plot griddata versions\nmyplot(plotx1,ploty1,plotz3.reshape(2*n+1,-1))  # Cartesian meshes\nmyplot(plotx1,ploty1,plotz4.reshape(2*n+1,-1))  # rotated meshes\n\nText: So here's a gallery of the results. Using random input z data, and interp2d, Cartesian (left) vs rotated interpolation (right): \nText: Note the horrible scale on the right side, noting that the input points are between 0 and 1. Even its mother wouldn't recognize the data set. Note that there are runtime warnings during the evaluation of the rotated data set, so we're being warned that it's all crap. \nText: Now let's do the same with griddata: \nText: We should note that these figures are much closer to each other, and they seem to make way more sense than the output of interp2d. For instance, note the overshoot in the scale of the very first figure. \nText: These artifacts always arise between input data points. Since it's still interpolation, the input points have to be reproduced by the interpolating function, but it's pretty weird that a linear interpolating function overshoots between data points. It's clear that griddata doesn't suffer from this issue. \nText: Consider an even more clear case: the other set of z values, which are smooth and deterministic. The surfaces with interp2d: \nText: HELP! Call the interpolation police! Already the Cartesian input case has inexplicable (well, at least by me) spurious features in it, and the rotated input case poses the threat of summoning zalgo. \nText: So let's do the same with griddata: \nText: The day is saved, thanks to The Powerpuff Girls scipy.interpolate.griddata. Homework: check the same with cubic interpolation. \nText: By the way, a very short answer to your original question is in help(interp.interp2d): \nCode:  |  Notes\n |  -----\n |  The minimum number of data points required along the interpolation\n |  axis is ``(k+1)**2``, with k=1 for linear, k=3 for cubic and k=5 for\n |  quintic interpolation.\n\nText: For linear interpolation you need at least 4 points along the interpolation axis, i.e. at least 4 unique x and y values have to be present to get a meaningful result. Check these: \nCode: nvals = 3  # -> RuntimeWarning\nx = np.linspace(0,1,10)\ny = np.random.randint(low=0,high=nvals,size=x.shape)\nz = x\ninterp.interp2d(x,y,z)\n\nnvals = 4  # -> no problem here\nx = np.linspace(0,1,10)\ny = np.random.randint(low=0,high=nvals,size=x.shape)\nz = x\ninterp.interp2d(x,y,z)\n\nText: And of course this all ties in to you question like this: it makes a huge difference if your geometrically 1d data set is along one of the Cartesian axes, or if it's in a general way such that the coordinate values assume various different values. It's probably meaningless (or at least very ill-defined) to try 2d interpolation from a geometrically 1d data set, but at least the algorithm shouldn't break if your data are along a general direction of the x,y plane. \nAPI:\nscipy.interpolate.interp2d\n","label":[[568,576,"Mention"],[5779,5805,"API"]],"Comments":[]}
{"id":60368,"text":"ID:34821934\nPost:\nText: Your input data seems to be quite ill-defined. Here's a surface plot of your input points: \nText: This is not an easy problem to interpolate. Incidentally, I've recently ran into problems where interp2d couldn't even interpolate a smooth data set. So I would suggest checking out sp.interpolate.griddata instead: \nCode: import numpy as np\nimport scipy.interpolate as interp\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#define your data as you did in your question: a, b and betaTab\n\nip = interp.interp2d(a,b,betaTab)           # original interpolator\n\naplotv = np.linspace(a.min(),a.max(),100)   # to interpolate at\nbplotv = np.linspace(b.min(),b.max(),100)   # to interpolate at\naplot,bplot = np.meshgrid(aplotv,bplotv)    # mesh to interpolate at\n\n# actual values from interp2d:\nbetainterp2d = ip(aplotv,bplotv)\n\n# actual values from griddata:\nbetagriddata = interp.griddata(np.array([a.ravel(),b.ravel()]).T,betaTab.ravel(),np.array([aplot.ravel(),bplot.ravel()]).T)\n# ^ this probably could be written in a less messy way,\n# I'll keep thinking about it\n\n#plot results\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(aplot,bplot,betainterp2d,cmap='viridis',cstride=1,rstride=1)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(aplot,bplot,betagriddata,cmap='viridis',cstride=1,rstride=1)\n\nText: Results: (left: interp2d, right: griddata) \nText: Conclusion: use scipy.interpolate.griddata. \nAPI:\nscipy.interpolate.griddata\n","label":[[304,327,"Mention"],[1510,1536,"API"]],"Comments":[]}
{"id":60369,"text":"ID:34829355\nPost:\nText: You can use sp.ndimage.label to label contiguous regions in your value array, then sum the number of elements for each label: \nCode: import numpy as np\nfrom scipy import ndimage\n\nvalue = np.array([[0, 0, 1, 1, 0, 0],\n                  [0, 0, 0, 1, 0, 0],\n                  [1, 1, 0, 0, 1, 0],\n                  [0, 0, 0, 1, 0, 0]])\n\n# label contiguous regions of non-zero elements\nlabels, nfeatures = ndimage.label(value)\n\n# sizes of each region\nsizes = (labels == (np.arange(nfeatures) + 1)[:, None, None]).sum((1, 2))\n\nbiggest = sizes.max()   # number of non-zero elements in largest contiguous region\n\nprint(biggest)\n# 3\n\nAPI:\nscipy.ndimage.label\n","label":[[36,52,"Mention"],[654,673,"API"]],"Comments":[]}
{"id":60370,"text":"ID:34998576\nPost:\nText: Further to @Paul R's answer, spectrogram is a spectrogram function in scipy's signal processing module. \nText: The example at the above link is as follows: \nCode: from scipy import signal\nimport matplotlib.pyplot as plt\n\n# Generate a test signal, a 2 Vrms sine wave whose frequency linearly\n# changes with time from 1kHz to 2kHz, corrupted by 0.001 V**2\/Hz of\n# white noise sampled at 10 kHz.\n\nfs = 10e3\nN = 1e5\namp = 2 * np.sqrt(2)\nnoise_power = 0.001 * fs \/ 2\ntime = np.arange(N) \/ fs\nfreq = np.linspace(1e3, 2e3, N)\nx = amp * np.sin(2*np.pi*freq*time)\nx += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)\n\n\n#Compute and plot the spectrogram.\n\nf, t, Sxx = signal.spectrogram(x, fs)\nplt.pcolormesh(t, f, Sxx)\nplt.ylabel('Frequency [Hz]')\nplt.xlabel('Time [sec]')\nplt.show()\n\nAPI:\nscipy.signal.spectrogram\n","label":[[53,64,"Mention"],[819,843,"API"]],"Comments":[]}
{"id":60371,"text":"ID:35007804\nPost:\nText: So after obsessing a lot about my question, and much research, i finally have my answer. Everything is available in scipy , and i'm putting my code here so hopefully someone else can find this useful. \nText: The function takes in an array of N-d points, a curve degree, a periodic state (opened or closed) and will return n samples along that curve. There are ways to make sure the curve samples are equidistant but for the time being i'll focus on this question, as it is all about speed. \nText: Worthy of note: I can't seem to be able to go beyond a curve of 20th degree. Granted, that's overkill already, but i figured it's worth mentioning. \nText: Also worthy of note: on my machine the code below can calculate 100,000 samples in 0.017s \nCode: import numpy as np\nimport scipy.interpolate as si\n\n\ndef bspline(cv, n=100, degree=3, periodic=False):\n    \"\"\" Calculate n samples on a bspline\n\n        cv :      Array ov control vertices\n        n  :      Number of samples to return\n        degree:   Curve degree\n        periodic: True - Curve is closed\n                  False - Curve is open\n    \"\"\"\n\n    # If periodic, extend the point array by count+degree+1\n    cv = np.asarray(cv)\n    count = len(cv)\n\n    if periodic:\n        factor, fraction = divmod(count+degree+1, count)\n        cv = np.concatenate((cv,) * factor + (cv[:fraction],))\n        count = len(cv)\n        degree = np.clip(degree,1,degree)\n\n    # If opened, prevent degree from exceeding count-1\n    else:\n        degree = np.clip(degree,1,count-1)\n\n\n    # Calculate knot vector\n    kv = None\n    if periodic:\n        kv = np.arange(0-degree,count+degree+degree-1)\n    else:\n        kv = np.clip(np.arange(count+degree+1)-degree,0,count-degree)\n\n    # Calculate query range\n    u = np.linspace(periodic,(count-degree),n)\n\n\n    # Calculate result\n    return np.array(si.splev(u, (kv,cv.T,degree))).T\n\nText: To test it: \nCode: import matplotlib.pyplot as plt\ncolors = ('b', 'g', 'r', 'c', 'm', 'y', 'k')\n\ncv = np.array([[ 50.,  25.],\n   [ 59.,  12.],\n   [ 50.,  10.],\n   [ 57.,   2.],\n   [ 40.,   4.],\n   [ 40.,   14.]])\n\nplt.plot(cv[:,0],cv[:,1], 'o-', label='Control Points')\n\nfor d in range(1,21):\n    p = bspline(cv,n=100,degree=d,periodic=True)\n    x,y = p.T\n    plt.plot(x,y,'k-',label='Degree %s'%d,color=colors[d%len(colors)])\n\nplt.minorticks_on()\nplt.legend()\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xlim(35, 70)\nplt.ylim(0, 30)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\nText: Results for both opened or periodic curves: \nText: ADDENDUM \nText: As of scipy-0.19.0 there is a new sp.interpolate.BSpline function that can be used. \nCode: import numpy as np\nimport scipy.interpolate as si\ndef scipy_bspline(cv, n=100, degree=3, periodic=False):\n    \"\"\" Calculate n samples on a bspline\n\n        cv :      Array ov control vertices\n        n  :      Number of samples to return\n        degree:   Curve degree\n        periodic: True - Curve is closed\n    \"\"\"\n    cv = np.asarray(cv)\n    count = cv.shape[0]\n\n    # Closed curve\n    if periodic:\n        kv = np.arange(-degree,count+degree+1)\n        factor, fraction = divmod(count+degree+1, count)\n        cv = np.roll(np.concatenate((cv,) * factor + (cv[:fraction],)),-1,axis=0)\n        degree = np.clip(degree,1,degree)\n\n    # Opened curve\n    else:\n        degree = np.clip(degree,1,count-1)\n        kv = np.clip(np.arange(count+degree+1)-degree,0,count-degree)\n\n    # Return samples\n    max_param = count - (degree * (1-periodic))\n    spl = si.BSpline(kv, cv, degree)\n    return spl(np.linspace(0,max_param,n))\n\nText: Testing for equivalency: \nCode: p1 = bspline(cv,n=10**6,degree=3,periodic=True) # 1 million samples: 0.0882 sec\np2 = scipy_bspline(cv,n=10**6,degree=3,periodic=True) # 1 million samples: 0.0789 sec\nprint np.allclose(p1,p2) # returns True\n\nAPI:\nscipy.interpolate.BSpline\n","label":[[2595,2617,"Mention"],[3827,3852,"API"]],"Comments":[]}
{"id":60372,"text":"ID:35070448\nPost:\nText: Least squares (scipy.linalg.lstsq) is guaranteed to converge. In fact, there is a closed form analytical solution (given by (A^T A)^-1 A^Tb (where ^T is matrix transpose and ^-1 is matrix inversion) \nText: The standard optimization problem, however, is not generally solvable - we are not guaranteed to find a minimizing value. However, for the given equation, find some a, b, c such that z = a*x + b*y + c, we have a linear optimization problem (the constraints and objective are linear in the variables that we're trying to optimize for). Linear optimization problems are generally solvable, so minimize should converge to the optimal value. \nText: Note: this is linear in our constraints even if we do z = a*x + b*y + d*x^2 + e*y^2 + f -- we don't have to restrict ourselves to a linear space of (x,y), since we will have these points (x, y, x^2, y^2) already. To our algorithm, these just look like points in the matrix A. So we can actually get a higher order polynomial using least squares! \nText: A brief aside: In reality, all of the solvers which don't use an exact analytical solution generally stop within some acceptable range of the actual answer, so it is rarely the case that we get the exact solution, but it tends to be so close that we accept it as exact in practice. Additionally, even least-squares solvers rarely use the analytical solution and instead resort to something quicker like Newton's Method. \nText: If you were to change the optimization problem, this would not be true. There are certain classes of problems for which we can generally find an optimal value (the largest class of these are called convex optimization problems -- although there are many non-convex problems which we can find an optimal value for under certain conditions). \nText: If you're interested in learning more, take a look at Convex Optimization by Boyd and Vandenberghe. The first chapter doesn't require much mathematical background and it overviews the general optimization problem and how it relates to solvable optimization problems like linear and convex programming. \nAPI:\nscipy.optimize.minimize\n","label":[[621,629,"Mention"],[2110,2133,"API"]],"Comments":[]}
{"id":60373,"text":"ID:35118683\nPost:\nText: The formula for the plane can be written as \nCode: z_plane = a*x + b*y + d\n\nText: The vertical z-distance from the point to the plane is given by \nCode: |z_plane - z| = |a*x + b*y + d - z|\n\nText: la.lstsq minimizes the square of the sum of these distances. \nCode: def zerror(x, y, z, a, b, d):\n    return (((a*x + b*y + d) - z)**2).sum()\n\nText: Indeed, the parameters returned by la.lstsq yield a smaller zerror than the hand-picked values: \nCode: In [113]: zerror(x, y, z, C[0], C[1], C[2])\nOut[113]: 245.03516402045813\n\nIn [114]: zerror(x, y, z, 0.28, -0.14, 0.)\nOut[114]: 323.81785779708787\n\nText: The formula \nText: gives the perpendicular distance between the point (x_0, y_0, z_0) and the plane, ax + by + cz + d = 0. \nText: You could minimize the perpendicular distance to the plane using sp.optimize.minimize (see minimize_perp_distance below). \nCode: import math\nimport numpy as np\nimport scipy.optimize as optimize\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d.axes3d as axes3d\nnp.random.seed(2016)\n\nmean = np.array([0.0,0.0,0.0])\ncov = np.array([[1.0,-0.5,0.8], [-0.5,1.1,0.0], [0.8,0.0,1.0]])\nxyz = np.random.multivariate_normal(mean, cov, 50)\nx, y, z = xyz[:, 0], xyz[:, 1], xyz[:, 2]\n\ndef minimize_z_error(x, y, z):\n    # Best-fit linear plane, for the Eq: z = a*x + b*y + c.\n    # See: https:\/\/gist.github.com\/amroamroamro\/1db8d69b4b65e8bc66a6\n    A = np.c_[x, y, np.ones(x.shape)]\n    C, resid, rank, singular_values = np.linalg.lstsq(A, z)\n\n    # Coefficients in the form: a*x + b*y + c*z + d = 0.\n    return C[0], C[1], -1., C[2]\n\ndef minimize_perp_distance(x, y, z):\n    def model(params, xyz):\n        a, b, c, d = params\n        x, y, z = xyz\n        length_squared = a**2 + b**2 + c**2\n        return ((a * x + b * y + c * z + d) ** 2 \/ length_squared).sum() \n\n    def unit_length(params):\n        a, b, c, d = params\n        return a**2 + b**2 + c**2 - 1\n\n    # constrain the vector perpendicular to the plane be of unit length\n    cons = ({'type':'eq', 'fun': unit_length})\n    sol = optimize.minimize(model, initial_guess, args=[x, y, z], constraints=cons)\n    return tuple(sol.x)\n\ninitial_guess = 0.28, -0.14, 0.95, 0.\nvert_params = minimize_z_error(x, y, z)\nperp_params = minimize_perp_distance(x, y, z)\n\ndef z_error(x, y, z, a, b, d):\n    return math.sqrt((((a*x + b*y + d) - z)**2).sum())\n\ndef perp_error(x, y, z, a, b, c, d):\n    length_squared = a**2 + b**2 + c**2\n    return ((a * x + b * y + c * z + d) ** 2 \/ length_squared).sum() \n\ndef report(kind, params):\n    a, b, c, d = params\n    paramstr = ','.join(['{:.2f}'.format(p) for p in params])\n    print('{:7}: params: ({}), z_error: {:>5.2f}, perp_error: {:>5.2f}'.format(\n        kind, paramstr, z_error(x, y, z, a, b, d), perp_error(x, y, z, a, b, c, d)))\n\nreport('vert', vert_params)\nreport('perp', perp_params)\nreport('guess', initial_guess)\n\nX, Y = np.meshgrid(np.arange(-3.0, 3.0, 0.5), np.arange(-3.0, 3.0, 0.5))\nfig = plt.figure()\nax = fig.gca(projection='3d')\n\ndef Z(X, Y, params):\n    a, b, c, d = params\n    return -(a*X + b*Y + d)\/c\n\nax.plot_surface(X, Y, Z(X, Y, initial_guess), rstride=1, cstride=1, alpha=0.3, color='magenta')\nax.plot_surface(X, Y, Z(X, Y, vert_params), rstride=1, cstride=1, alpha=0.3, color='yellow')\nax.plot_surface(X, Y, Z(X, Y, perp_params), rstride=1, cstride=1, alpha=0.3, color='green')\nax.scatter(x, y, z, c='r', s=50)\nplt.xlabel('X')\nplt.ylabel('Y')\nax.set_zlabel('Z')\nax.axis('equal')\nax.axis('tight')\nplt.show()\n\nText: The code above computes the parameters which minimize vertical distance from the plane and perpendicular distance from the plane. We can then compute the total error: \nCode: vert   : params: (0.94,0.52,-1.00,0.10), z_error:  2.63, perp_error:  3.21\nperp   : params: (-0.68,-0.39,0.63,-0.06), z_error:  9.50, perp_error:  2.96\nguess  : params: (0.28,-0.14,0.95,0.00), z_error:  5.22, perp_error: 52.31\n\nText: Notice that the vert_params minimize z_error, but perp_params minimize perp_error. \nText: The magenta plane corresponds to the initial_guess, the yellow plane corresponds to the vert_params and the green plane corresponds to the perp_params. \nAPI:\nscipy.linalg.lstsq\nscipy.linalg.lstsq\nscipy.optimize.minimize\n","label":[[220,228,"Mention"],[404,412,"Mention"],[820,840,"Mention"],[4143,4161,"API"],[4162,4180,"API"],[4181,4204,"API"]],"Comments":[]}
{"id":60374,"text":"ID:35177238\nPost:\nText: Assuming linear interpolation is a good model you could define a continuous data function and integrate using quad like that: \nCode: import scipy as sp\nimport scipy.integrate\n\nxp = sp.linspace(0, 1000, 1000)\nyp = sp.randn(1000)\n\ndatafunc = lambda x: sp.interp(x, xp, yp)\n\nsp.integrate.quad(datafunc, 3, 1000)\n\nText: Depending on the domain size the integration take 2 to 4 ms on my machine. That would mean something like 4 hours for 1000 * 200 integrations which I think is OK, if you only need to do it once. But the time will heavily depend on your data. \nAPI:\nscipy.integrate.quad\n","label":[[134,138,"Mention"],[588,608,"API"]],"Comments":[]}
{"id":60375,"text":"ID:35179787\nPost:\nText: The problem is that the objective function that curve_fit tries to minimize is not continuous. x0 controls the location of the discontinuities in the inverter function. When a discontinuity crosses one of the grid points in x, there is a jump in the objective function. Between these points, the objective function is constant. curve_fit (actually, leastsq, the function used by curve_fit) is not designed to handle such a function. \nText: The following function sse is (in effect) the function that curve_fit tries to minimize, with x being the same x defined in your example, and y = sin(x): \nCode: def sse(x0, x, y):\n    f = inverter(x, x0)\n    diff = y - f\n    s = (diff**2).sum()\n    return s\n\nText: If you plot this function on a fine grid with code such as \nCode: xx = np.linspace(0, 1, 10000)\nyy = [sse(x0, x, y) for x0 in xx]\nplot(xx, yy)\n\nText: and zoom in, you'll see \nText: To use scipy to find your optimal value, you can use fmin with a smooth objective function. For example, here's the continuous objective function, using only the interval [0, pi\/2] (quad is scipy.integrate.quad): \nCode: def func(x0):\n    s0, e0 = quad(lambda x: np.sin(x)**2, 0, x0)\n    s1, e0 = quad(lambda x: (1 - np.sin(x))**2, x0, 0.5*np.pi)\n    return s0 + s1\n\nText: fmin can be used to find the minimum of that function, as in this snippet from an ipython session: \nCode: In [202]: fmin(func, 0.3, xtol=1e-8)\nOptimization terminated successfully.\n         Current function value: 0.100545\n         Iterations: 28\n         Function evaluations: 56\nOut[202]: array([ 0.52359878])\n\nIn [203]: np.arcsin(0.5)\nOut[203]: 0.52359877559829882\n\nAPI:\nscipy.optimize.fmin\n","label":[[1282,1286,"Mention"],[1656,1675,"API"]],"Comments":[]}
{"id":60376,"text":"ID:35301638\nPost:\nText: Each non-keyword argument passed to st.kruskal is treated as a separate group of y-values. By passing x as one of the arguments, kruskal attempts to treat your label strings as though they were a second group of y-values. The strings will get cast to NaNs (which ought to raise a RuntimeWarning). \nText: Instead, you need to group your y values by label, then pass them as separate input arrays to kruskal. For example: \nCode: # convert `y` to a numpy array for more convenient indexing\ny = np.array(y)\n\n# find unique group labels and their corresponding indices\nlabel, idx = np.unique(x, return_inverse=True)\n\n# make a list of arrays containing the y-values corresponding to each unique label\ngroups = [y[idx == i] for i, l in enumerate(label)]\n\n# use `*` to unpack the list as a sequence of arguments to `stats.kruskal`\nH, p = stats.kruskal(*groups)\n\nprint(H, p)\n# 2.94545454545 0.22929927\n\nAPI:\nscipy.stats.kruskal\n","label":[[60,70,"Mention"],[922,941,"API"]],"Comments":[]}
{"id":60377,"text":"ID:35302153\nPost:\nText: From the pages, sp.signal.hilbert computes the analytic signal, using the Hilbert transform. Namely, the analytical signal, \nCode: x_a = x + i*y \n\nText: where y is the hilbert transform. On the other hand, syfp.hilbert is just the Hilbert transform. If you want the Hilbert transform, not the analytical signal, use scipy.fftpack.hilbert. \nAPI:\nscipy.signal.hilbert\nscipy.fftpack.hilbert\n","label":[[40,57,"Mention"],[230,242,"Mention"],[369,389,"API"],[390,411,"API"]],"Comments":[]}
{"id":60378,"text":"ID:35376120\nPost:\nText: cf expects the model function to be vectorized: that is, it must be able to receive an array (ndarray, to be precise), and return an array of values. You can see the problem right away by adding a debug printout \nCode: def u(time,maxtemp,k):                    \n    print time                % for debugging\n    def integ(t): return k*exp(k*t)*temp(t,maxtemp)\n    return exp(-k*time)*( numerical_integral(integ, 0, time)[0] + u0 )\n\nText: The output of print will be the entire array ltimes you are passing to curve_fit. This is something numerical_integral is not designed to handle. You need to give it values one by one. \nText: Like this: \nCode: def u(time,maxtemp,k):                    \n    def integ(t): return k*exp(k*t)*temp(t,maxtemp)\n    return [exp(-k*time_i)*( numerical_integral(integ, 0, time_i)[0] + u0) for time_i in time]\n\nText: This will take care of the only length-1 arrays can be converted\" error. You will then have another one, because your lists ltimes and lT are of different length, which doesn't make sense since lT is supposed to be the target outputs for inputs ltimes. You should revise the definitions of these arrays to figure out what size you want. \nAPI:\nscipy.optimize.curve_fit\n","label":[[24,26,"Mention"],[1213,1237,"API"]],"Comments":[]}
{"id":60379,"text":"ID:35582695\nPost:\nText: Since the function you're trying to fit is a polynomial, you can use numpy.polyfit \nCode: >>> numpy.polyfit(x, y, 2) # The 2 signifies a polynomial of degree 2\narray([  -1.04978546,  115.16698544,  236.16191491])\n\nText: This means that the best fit was y ~ -1.05 x2 + 115.157x + 236.16. \nText: For a general function, the more you know about it (e.g., is it convex, differentiable, twice differentiable, etc.), the better you can do with scipy.optimize.minimize. E.g., if you hardly know anything about it, you can use it specifying to use the Nelder-Mead method. Other methods there (refer to the documentation) can make use of the Jacobian and the Hessian, if they are defined, and you can calculate them. \nText: Personally, I find that using it with Nelder-Mead (requiring almost no parameters) gives me adequate results for my needs. \nText: Example \nText: Suppose you're trying to fit y = kx with k as the parameter to optimize. You'd write a function \nCode: x = ...\ny = ...\n\ndef ss(k):\n    # use numpy.linalg.norm to find the sum-of-squares error between y and kx\n\nText: Then you'd use minimize on the function ss. \nAPI:\nscipy.optimize.minimize\n","label":[[1115,1123,"Mention"],[1150,1173,"API"]],"Comments":[]}
{"id":60380,"text":"ID:35683768\nPost:\nText: I think a problem with your code is that it overwrites the constraint cons. As a result, the only constraint that the minimize function sees is np.sum(np.abs(ff[-1, -1, :])**2) = 1 where -1 means the last element L-1. I suggest initializing an empty list, then appending to it: \nCode:     cons = []\n    for i_x in range(L):\n        for i_y in range(L):\n            cons.append({'type':'eq', 'fun': ...)\n\nText: The second problem is that the constraint must be parametrized by the same vector as the function you are trying to minimize, i.e. it must be a function of a vector with shape (L * L * N * 2, ). Yours instead is a function of ff, which has shape (L,L,N,) and is a constant (\"frozen\" at creation time with the initial value x0) that does not depend on the current iteration of the minimization algorithm. \nAPI:\nscipy.optimize.minimize\n","label":[[142,150,"Mention"],[844,867,"API"]],"Comments":[]}
{"id":60381,"text":"ID:35797216\nPost:\nText: Many of the functions you're using don't know how to handle numpy arrays. This includes the sp.integrate.quad function. \nText: quad docs: http:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.integrate.quad.html#scipy.integrate.quad \nAPI:\nscipy.integrate.quad\n","label":[[116,133,"Mention"],[267,287,"API"]],"Comments":[]}
{"id":60382,"text":"ID:35954596\nPost:\nText: Even though the heading says Integrate with numpy, I suppose you mean scipy.. Your problem is that you have complex numbers in your function. For example, the part (x-1)**(1\/3) becomes complex for x in [0, 1), but quad does not handle complex numbers. See e.g. Use si.quad to integrate complex numbers for more information. \nAPI:\nscipy.integrate.quad\nscipy.integrate.quad\n","label":[[238,242,"Mention"],[289,296,"Mention"],[354,374,"API"],[375,395,"API"]],"Comments":[]}
{"id":60383,"text":"ID:35959125\nPost:\nText: The problem on a wide scale is that you're looking at the documentation but you're not trying to understand it. If you're using a specific function interp2d in the module scipy.interpolate, then look at the function's documentation, as @pv also suggested in a comment. The fact that you're throwing the arguments all around the place clearly demonstrates that you're trying to use a function based on guesswork. It won't work: a function was implemented with a given syntax, and it will only work like that. \nText: So look at the function's signature: \nText: class scipy.interpolate.interp2d(x, y, z, kind='linear', copy=True, bounds_error=False, fill_value=nan) \nText: The meaning of the parameters is explained afterwards. You can clearly see that there are 3 mandatory parameters: x, y, z. No other array-valued inputs are allowed This is because interp2d only constructs an interpolating function, which you should then use to compute the interpolated values on a mesh (unlike MATLAB, where interp2d gives you the interpolated values directly). So you can call \nCode: myfun = interp2(uu,vv,proj,'linear')\n\nText: to get an interpolating function. You can then substitute at the given values, but note that the function myfun will expect 1d input, and it will construct a mesh internally. So assuming that your output mesh is constructed as \nCode: puu,puv = np.meshgrid(puu_vec,puv_vec)\n\nText: (which is probably not the case, but I'll get back to this later), you need \nCode: vol[:,:,iz] = Ratio*myfun(puu_vec,puv_vec)\n\nText: to obtain the output you need. \nText: However, there are some important points you should note. \nText: In MATLAB you have interp2d(uu,vv,proj',...), but make sure that for scipy the elements of uu, vv, and proj are in the same order. To be on the safe side: in case of an asymmetric mesh size, the shape of uu, vv and proj should all be the same. In MATLAB you're using 'linear' interpolation, while in python you're using 'cubic'. I'm not sure this is really what you want, if you're porting your code to a new language. It seems to me that your output mesh is not defined by a rectangular grid as if from meshgrid, which suggests that interp2d might not be suitable for your case. Anyway, I've had some odd experiences with interp2d, and I wouldn't trust it. So if it's not suitable for your expected output, I'd strongly suggest using inp.griddata instead. This function gives you interpolated points directly: I suggest that you try to figure out its use based on the manual:) My linked answer can also help. You can set the kind of interpolation in the same way, but your output can be any set of scattered points if you like. \nAPI:\nscipy.interpolate.griddata\n","label":[[2391,2403,"Mention"],[2691,2717,"API"]],"Comments":[]}
{"id":60384,"text":"ID:35960129\nPost:\nText: You need to reverse the order of the arguments to your derivative function - it should be f(y, t), not f(t, y). This is the opposite order to that used by the sp.integrate.ode class. \nText: Also, the concatenation S = np.append(X,T,axis=1) will fail because X is a tuple containing your integrals and a dict. Use S = np.append(X[0],T,axis=1) instead. \nAPI:\nscipy.integrate.ode\n","label":[[183,199,"Mention"],[381,400,"API"]],"Comments":[]}
{"id":60385,"text":"ID:36052000\nPost:\nText: The fit method of tdist returns (df, loc, scale), so this line \nCode: pdf_fitted = t.pdf(x,loc=param[0],scale=param[1],df=param[2])\n\nText: should be \nCode: pdf_fitted = t.pdf(x, loc=param[1], scale=param[2], df=param[0])\n\nText: The example that you linked to uses the normal distribution, which doesn't have an additional shape parameter, so in that case, param[0] is the location and param[1] is the scale. \nAPI:\nscipy.stats.t\n","label":[[42,47,"Mention"],[438,451,"API"]],"Comments":[]}
{"id":60386,"text":"ID:36094728\nPost:\nText: EDIT: @ev-br in the comments for this answer provided an important correction to my answer. In fact interp1D spline is not FITPACK based. Check the comments for the link provided by @ev-br. \nText: The Scipy functions for curve fitting are based on FITPACK. Try seeing the documentation on the functions you are using and you'll be able to see a \"References\" chapter where something like this will appear: \nCode: Notes\n-----\nSee splev for evaluation of the spline and its derivatives. Uses the\nFORTRAN routine curfit from FITPACK.\nIf provided, knots `t` must satisfy the Schoenberg-Whitney conditions,\ni.e., there must be a subset of data points ``x[j]`` such that\n``t[j] < x[j] < t[j+k+1]``, for ``j=0, 1,...,n-k-2``.\nReferences\n----------\nBased on algorithms described in [1]_, [2]_, [3]_, and [4]_:\n.. [1] P. Dierckx, \"An algorithm for smoothing, differentiation and\n   integration of experimental data using spline functions\",\n   J.Comp.Appl.Maths 1 (1975) 165-184.\n.. [2] P. Dierckx, \"A fast algorithm for smoothing data on a rectangular\n   grid while using spline functions\", SIAM J.Numer.Anal. 19 (1982)\n   1286-1304.\n.. [3] P. Dierckx, \"An improved algorithm for curve fitting with spline\n   functions\", report tw54, Dept. Computer Science,K.U. Leuven, 1981.\n.. [4] P. Dierckx, \"Curve and surface fitting with splines\", Monographs on\n   Numerical Analysis, Oxford University Press, 1993.\n\nText: These references in particular where taken from the source of fitpack.py on the function \"splrep\". If you need to do a very thorough comparison between your algorithm and the spline from interp1D just go to the docs: \nText: interp1d \nText: And you'll see a link called [source] right after the definition of the name of the function (so: scipy.interpolate.interp1D [source]). Remember that there's a lot of routine handlers on these functions so be patient while navigating the source. \nAPI:\nscipy.interpolate.interp1d\n","label":[[1650,1658,"Mention"],[1918,1944,"API"]],"Comments":[]}
{"id":60387,"text":"ID:36213176\nPost:\nText: I will assume that you are trying to interpolate values of z. \nText: Now, what happens when you do call interpolation function? It creates the entire landscape of the inputs (x and y) and the outputs (z). In the code above, you didn't really ask for its value at any point. To use this function, you need to specify the inputs and it will give you the interpolated output. \nText: You used the function LinearNDInterpolator which is constructed by triangulating the input data and on each triangle performing linear barycentric interpolation. Depending on your inputs, there are likely to be regions where this breaks down and you get Nan. For instance, try this in your code \nCode: print interp(-4386790, 3720137)\n\nText: This is within the limits of the min-max of your x and y. We could set the Nan to zero via the fill_value argument if that is acceptable to you. \nText: Read up on the docs. Often people might find the following function acceptable as well, scipy.interpolate.interp2d. It uses spline interpolation instead. In the code below, I've implemented both functions (the former with nan values set to 0) and plotted them on a heatmap. \nText: As for the heatmap, it is as you've suspected. You have to create a grid of values. Below is my the output graphs for LinearNDInterpolator with nan set to zero and interp2d as well as the codes. \nText: Using LinearNDInterpolator(cartcoord, z, fill_value=0) \nText: Using interp2d(x, y, z) \nText: P.S. I am using Python3. If you run into issues in Python2, remove the list from cartcoord = list(zip(x, y)). \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.interpolate\n\nx = np.array([-4386795.73911443, -1239996.25110694, -3974316.43669208,\n               1560260.49911342,  4977361.53694849, -1996458.01768192,\n               5888021.46423068,  2969439.36068243,   562498.56468588,\n               4940040.00457585])\n\ny = np.array([ -572081.11495993, -5663387.07621326,  3841976.34982795,\n               3761230.61316845,  -942281.80271223,  5414546.28275767,\n               1320445.40098735, -4234503.89305636,  4621185.12249923,\n               1172328.8107458 ])\n\nz = np.array([ 4579159.6898615 ,  2649940.2481702 ,  3171358.81564312,\n               4892740.54647532,  3862475.79651847,  2707177.605241  ,\n               2059175.83411223,  3720138.47529587,  4345385.04025412,\n               3847493.83999694])\n\n# Create coordinate pairs\ncartcoord = list(zip(x, y))\n\n\nX = np.linspace(min(x), max(x))\nY = np.linspace(min(y), max(y))\nX, Y = np.meshgrid(X, Y)\n\n# Approach 1\ninterp = scipy.interpolate.LinearNDInterpolator(cartcoord, z, fill_value=0)\nZ0 = interp(X, Y)\nplt.figure()\nplt.pcolormesh(X, Y, Z0)\nplt.colorbar() # Color Bar\nplt.show()\n\n# Approach 2\nfunc = scipy.interpolate.interp2d(x, y, z)\nZ = func(X[0, :], Y[:, 0])\nplt.figure()\nplt.pcolormesh(X, Y, Z)\nplt.colorbar() # Color Bar\nplt.show()\n\nAPI:\nscipy.interpolate.LinearNDInterpolator\n","label":[[426,446,"Mention"],[2905,2943,"API"]],"Comments":[]}
{"id":60388,"text":"ID:36384223\nPost:\nText: You can use functools.partial to turn your function to partial function with only one argument. In order to make it work with sp.optimize.minimize you will need to keep the variable argument at the last position: \nCode: def fun(X, y, alpha):\n    #some stuff\n    return J, gradient\n\nText: then: \nCode: from functools import partial\n\noptfunc = partial(func, X, y)\noptimized_alpha = sp.optimize.minimize(optfunc, alpha, method=\"Newton-CG\", jac=True)\n\nAPI:\nscipy.optimize.minimize\n","label":[[150,170,"Mention"],[477,500,"API"]],"Comments":[]}
{"id":60389,"text":"ID:36428574\nPost:\nText: You can use lcastsq with custom weights: \nCode: import scipy.optimize as optimize\nimport numpy as np\n# redefine lists as array\nx=np.array(r)\ny=np.array(logf)\nerrup=np.array(upper)\nerrlow=np.array(lower)\n# error function\ndef fit_func(x,a,b,c):\n    return np.log10(a*x**b + c)\ndef my_error(V):\n    a,b,c=V\n    yfit=fit_func(x,a,b,c)\n    weight=np.ones_like(yfit)\n    weight[yfit>y]=errup[yfit>y] # if the fit point is above the measure, use upper weight\n    weight[yfit<=y]=errlow[yfit<=y] # else use lower weight\n    return (yfit-y)**2\/weight**2\nanswer=optimize.leastsq(my_error,x0=[0.0001,-1,0.0006])\na,b,c=answer[0]\nprint(a,b,c)\n\nText: It works, but is very sensitive to initial values, since there is a log which can go in wrong domain (negative numbers) and then it fails. Here I find a=9.14464745425e-06 b=-1.75179880756 c=0.00066720486385which is pretty close to data. \nAPI:\nscipy.optimize.leastsq\n","label":[[36,43,"Mention"],[904,926,"API"]],"Comments":[]}
{"id":60390,"text":"ID:36490349\nPost:\nText: I have the relevant source for the welch method and I have isolated the relevant code down to the _spectral_helper and _fft_helper functions, but I can't identify where the averaging of the spectra takes place, or where is the best place to implement my spectra rejection operation. \nText: Perhaps the reason you cannot find where the averaging is done in _spectral_helper and _fft_helper is because this is not where it's done, as indicated with the function documentation included in both functions: \nText: The windows are not averaged over; the result from each window is returned. \nText: Instead, the averaging is performed on the result from _spectral_helper in the function csd (called from welch) with: \nCode: # Average over windows.\nif len(Pxy.shape) >= 2 and Pxy.size > 0:\n    if Pxy.shape[-1] > 1:\n        Pxy = Pxy.mean(axis=-1)\n    else:\n        Pxy = np.reshape(Pxy, Pxy.shape[:-1])\n\nText: Note that instead of modifying welch, you could use spectrogram, which does not do the averaging and thus leaves you the option to reject spectra as you please before you perform the averaging. \nAPI:\nscipy.signal.welch\n","label":[[59,64,"Mention"],[1127,1145,"API"]],"Comments":[]}
{"id":60391,"text":"ID:36521079\nPost:\nText: When a 2D array is passed as the first argument to scipy.cluster.hierarchy.linkage, it is treated as a sequence of observations, and scipy.spatial.pdist is used to convert it to a squence of pairwise distances between observations. \nText: There is a github issue regarding this behavior since it means that passing a \"distance matrix\" such as DF_dissm.values (silently) produces an incorrect result. \nText: So the upshot of this is that none of these \nCode: Z_square = linkage((DF_dissm.values),method=\"average\")\nZ_triu = linkage(np.triu(DF_dissm.values),method=\"average\")\nZ_tril = linkage(np.tril(DF_dissm.values),method=\"average\")\n\nText: produce the desired result. Instead use \nText: np.triu_indices: h, w = arr.shape Z = linkage(arr[np.triu_indices(h, 1)], method=\"average\") or spatial.distance.squareform: from spatial import distance as ssd Z = linkage(ssd.squareform(arr), method=\"average\") or apply spatial.distance.pdist to the original points: Z = hierarchy.linkage(ssd.pdist(points), method=\"average\") or pass the 2D array points: Z = hierarchy.linkage(points, method=\"average\") \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.cluster import hierarchy as hier\nfrom scipy.spatial import distance as ssd\nnp.random.seed(2016)\n\npoints = np.random.random((10, 2))\narr = ssd.cdist(points, points)\n\nfig, ax = plt.subplots(nrows=4)\n\nax[0].set_title(\"condensed upper triangular\")\nZ = hier.linkage(arr[np.triu_indices(arr.shape[0], 1)], method=\"average\")\nhier.dendrogram(Z, ax=ax[0])\n\nax[1].set_title(\"squareform\")\nZ = hier.linkage(ssd.squareform(arr), method=\"average\")\nhier.dendrogram(Z, ax=ax[1])\n\nax[2].set_title(\"pdist\")\nZ = hier.linkage(ssd.pdist(points), method=\"average\")\nhier.dendrogram(Z, ax=ax[2])\n\nax[3].set_title(\"sequence of observations\")\nZ = hier.linkage(points, method=\"average\")\nhier.dendrogram(Z, ax=ax[3])\n\nplt.show()\n\nAPI:\nscipy.spatial\n","label":[[840,847,"Mention"],[1890,1903,"API"]],"Comments":[]}
{"id":60392,"text":"ID:36669909\nPost:\nText: seim only acquired the keyword argument nan_policy in release 0.17.0. You are most likely using release 0.16.0 as many people still do, in which case you only have access to the other two keyword arguments axis and ddof. \nText: From the release notes for 0.17.0: \nText: Many functions in stats have gained a nan_policy keyword, which allows specifying how to treat input with NaNs in them: propagate the NaNs, raise an error, or omit the NaNs. \nText: Unfortunately you'll have to treat NaN-ridden inputs on your own. \nAPI:\nscipy.stats.sem\nscipy.stats\n","label":[[24,28,"Mention"],[312,317,"Mention"],[547,562,"API"],[563,574,"API"]],"Comments":[]}
{"id":60393,"text":"ID:36680300\nPost:\nText: From the source code, ttest_1samp returns nothing more than a namedtuple Ttest_1sampResult with the statistic and p-value. Hence, you do not need to use .ravel - you can simply use \nCode: scipy.stats.ttest_1samp([0,1,2], 0)[0]\n\nText: to access the statistic. \nText: Note: From a further look at the source code, it is clear that this namedtuple only began being returned in release 0.14.0. In release 0.13.0 and earlier, it appears that a zero dim array is returned (source code), which for all intents and purposes can act just like a plain number as mentioned by BrenBarn. \nAPI:\nscipy.stats.ttest_1samp\n","label":[[46,57,"Mention"],[605,628,"API"]],"Comments":[]}
{"id":60394,"text":"ID:36795076\nPost:\nText: Use the rvs method to generate a sample using the estimated parameters. For example, suppose x holds my initial data. \nCode: In [56]: x\nOut[56]: \narray([ 0.366,  0.235,  0.286,  0.84 ,  0.073,  0.108,  0.156,  0.029,\n        0.11 ,  0.122,  0.227,  0.148,  0.095,  0.233,  0.317,  0.027])\n\nText: Use sp.stats.expon to fit the expononential distribution to this data. I assume we are interested in the usual case where the location parameter is 0, so I use floc=0 in the fit call. \nCode: In [57]: from scipy.stats import expon\n\nIn [58]: loc, scale = expon.fit(x, floc=0)\n\nIn [59]: scale\nOut[59]: 0.21076203455218898\n\nText: Now use those parameters to generate a random sample. \nCode: In [60]: sample = expon.rvs(loc=0, scale=scale, size=8)\n\nIn [61]: sample\nOut[61]: \narray([ 0.21576877,  0.23415911,  0.6547364 ,  0.44424148,  0.07870868,\n        0.10415167,  0.12905163,  0.23428833])\n\nAPI:\nscipy.stats.expon\n","label":[[324,338,"Mention"],[915,932,"API"]],"Comments":[]}
{"id":60395,"text":"ID:36842136\nPost:\nText: array_resized_image has a shape of (320, 240, 3) - three dimensional because red, green and blue components are stored in this way. You can use scipy.misc.imread and smp.imsave for easier handling file loading and storing, so your example boils down to this: \nCode: import scipy.misc\n\nimage_path = \"img0.jpg\"\n\narray_image = scipy.misc.imread(image_path)\narray_resized_image = scipy.misc.imresize(array_image, (320, 240), interp='nearest', mode=None)\nscipy.misc.imsave(\"i1.jpg\", array_resized_image)\n\nAPI:\nscipy.misc.imsave\n","label":[[190,200,"Mention"],[529,546,"API"]],"Comments":[]}
{"id":60396,"text":"ID:36968458\nPost:\nText: If A is csr_matrix, you can use .toarray() (there's also .todense() that produces a numpy matrix, which is also works for the DataFrame constructor): \nCode: df = pd.DataFrame(A.toarray())\n\nText: You can then use this with pd.concat(). \nCode: A = csr_matrix([[1, 0, 2], [0, 3, 0]])\n    \n  (0, 0)    1\n  (0, 2)    2\n  (1, 1)    3\n\n<class 'scipy.sparse.csr.csr_matrix'>\n\npd.DataFrame(A.todense())\n\n   0  1  2\n0  1  0  2\n1  0  3  0\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2 entries, 0 to 1\nData columns (total 3 columns):\n0    2 non-null int64\n1    2 non-null int64\n2    2 non-null int64\n\nText: In version 0.20, pandas introduced sparse data structures, including the SparseDataFrame. \nText: In pandas 1.0, SparseDataFrame was removed: \nText: In older versions of pandas, the SparseSeries and SparseDataFrame classes were the preferred way to work with sparse data. With the advent of extension arrays, these subclasses are no longer needed. Their purpose is better served by using a regular Series or DataFrame with sparse values instead. \nText: The migration guide shows how to use these new data structures. \nText: For instance, to create a DataFrame from a sparse matrix: \nCode: from scipy.sparse import csr_matrix\n\nA = csr_matrix([[1, 0, 2], [0, 3, 0]])\n\ndf = pd.DataFrame.sparse.from_spmatrix(A, columns=['A', 'B', 'C'])\n\ndf\n\n   A  B  C\n0  1  0  2\n1  0  3  0\n\ndf.dtypes\nA    Sparse[float64, 0]\nB    Sparse[float64, 0]\nC    Sparse[float64, 0]\ndtype: object\n\nText: Alternatively, you can pass sparse matrices to sklearn to avoid running out of memory when converting back to pandas. Just convert your other data to sparse format by passing a numpy array to the csr_matrix constructor and use chstack to combine (see docs). \nAPI:\nscipy.sparse.csr_matrix\nscipy.sparse.hstack\n","label":[[1696,1706,"Mention"],[1727,1734,"Mention"],[1764,1787,"API"],[1788,1807,"API"]],"Comments":[]}
{"id":60397,"text":"ID:36970643\nPost:\nText: You can print out the gradient as described here and hand-code it into Scipy. You can also do the optimization in Theano - see this question. \nText: However, probably the most straight-forward approach is to create a function get_gradients() that uses theano.grad() to return the gradients of the filters with respect to an input, then call sp.optimize.minimize with jac=get_gradients. According to the documentation: \nText: jac : bool or callable, optional Jacobian (gradient) of objective function. [...] jac can also be a callable returning the gradient of the objective. In this case, it must accept the same arguments as fun. \nAPI:\nscipy.optimize.minimize\n","label":[[365,385,"Mention"],[661,684,"API"]],"Comments":[]}
{"id":60398,"text":"ID:37001441\nPost:\nText: Omitting the imports... I don't have your data, so I have to fake some... \nCode: In [40]: x = np.random.random(100)\nIn [41]: y = np.random.random(100)    \nIn [42]: z = np.random.random(100)    \nIn [43]: p = np.random.random(3)    \nIn [44]: p\nOut[44]: array([ 0.60515083,  0.39263392,  0.36129813])\n\nText: namely three arrays of coordinates and a point for which I will search the neighbors. \nText: Next, let's see how can I construct an array with as many rows as the different data points and three colunms... \nCode: In [45]: np.vstack((x,y,z)).T.shape\nOut[45]: (100, 3)\n\nText: well, it is correct. \nText: We build the k-d tree using KDTree from spsp \nCode: In [46]: tree = KDTree(np.vstack((x,y,z)).T)\n\nText: and then we use one of the methods of the tree, the aptly-named .query_ball_point(), to find the indices of the points near to p \nCode: In [47]: indices = tree.query_ball_point(p, 0.33)\n\nText: where I've used, arbitrarily, a radius equal to 1\/3. \nText: Eventually we want to see these neighbors, so I'll use the .data attribute of the tree and the indices that I've just computed like this \nCode: In [48]: tree.data[indices]\nOut[48]: \narray([[ 0.4117843 ,  0.21440852,  0.3352732 ],\n       [ 0.48921727,  0.13855976,  0.43331816],\n       [ 0.71598133,  0.32270361,  0.20292187],\n       [ 0.71761991,  0.27309708,  0.12670474],\n       [ 0.6282775 ,  0.13752325,  0.4143872 ],\n       [ 0.55995847,  0.31302848,  0.2780926 ],\n       [ 0.75896359,  0.16043536,  0.33530071],\n       [ 0.81138529,  0.64635994,  0.33819097],\n       [ 0.43537193,  0.5353203 ,  0.52095431],\n       [ 0.66996807,  0.48346547,  0.52761835],\n       [ 0.69426851,  0.24725511,  0.57650329],\n       [ 0.5350322 ,  0.23155768,  0.62545958],\n       [ 0.51228139,  0.38078056,  0.61246054]])\n\nText: and that's all... \nAPI:\nscipy.spatial\n","label":[[671,675,"Mention"],[1826,1839,"API"]],"Comments":[]}
{"id":60399,"text":"ID:37121993\nPost:\nText: ndi.rotate accepts a reshape= parameter: \nText: reshape : bool, optional If reshape is true, the output shape is adapted so that the input array is contained completely in the output. Default is True. \nText: So to \"clip\" the edges you can simply call scipy.ndimage.rotate(img, ..., reshape=False). \nCode: from scipy.ndimage import rotate\nfrom scipy.misc import face\nfrom matplotlib import pyplot as plt\n\nimg = face()\nrot = rotate(img, 30, reshape=False)\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(img)\nax[1].imshow(rot)\n\nText: Things are more complicated for scipy.ndimage.zoom. \nText: A naive method would be to zoom the entire input array, then use slice indexing and\/or zero-padding to make the output the same size as your input. However, in cases where you're increasing the size of the image it's wasteful to interpolate pixels that are only going to get clipped off at the edges anyway. \nText: Instead you could index only the part of the input that will fall within the bounds of the output array before you apply zoom: \nCode: import numpy as np\nfrom scipy.ndimage import zoom\n\n\ndef clipped_zoom(img, zoom_factor, **kwargs):\n\n    h, w = img.shape[:2]\n\n    # For multichannel images we don't want to apply the zoom factor to the RGB\n    # dimension, so instead we create a tuple of zoom factors, one per array\n    # dimension, with 1's for any trailing dimensions after the width and height.\n    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n\n    # Zooming out\n    if zoom_factor < 1:\n\n        # Bounding box of the zoomed-out image within the output array\n        zh = int(np.round(h * zoom_factor))\n        zw = int(np.round(w * zoom_factor))\n        top = (h - zh) \/\/ 2\n        left = (w - zw) \/\/ 2\n\n        # Zero-padding\n        out = np.zeros_like(img)\n        out[top:top+zh, left:left+zw] = zoom(img, zoom_tuple, **kwargs)\n\n    # Zooming in\n    elif zoom_factor > 1:\n\n        # Bounding box of the zoomed-in region within the input array\n        zh = int(np.round(h \/ zoom_factor))\n        zw = int(np.round(w \/ zoom_factor))\n        top = (h - zh) \/\/ 2\n        left = (w - zw) \/\/ 2\n\n        out = zoom(img[top:top+zh, left:left+zw], zoom_tuple, **kwargs)\n\n        # `out` might still be slightly larger than `img` due to rounding, so\n        # trim off any extra pixels at the edges\n        trim_top = ((out.shape[0] - h) \/\/ 2)\n        trim_left = ((out.shape[1] - w) \/\/ 2)\n        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n\n    # If zoom_factor == 1, just return the input array\n    else:\n        out = img\n    return out\n\nText: For example: \nCode: zm1 = clipped_zoom(img, 0.5)\nzm2 = clipped_zoom(img, 1.5)\n\nfig, ax = plt.subplots(1, 3)\nax[0].imshow(img)\nax[1].imshow(zm1)\nax[2].imshow(zm2)\n\nAPI:\nscipy.ndimage.rotate\n","label":[[24,34,"Mention"],[2760,2780,"API"]],"Comments":[]}
{"id":60400,"text":"ID:37190089\nPost:\nText: I haven't been able to work out how to get around the need for x to be increasing. So I took a slightly \"cheating\" option instead. Since my y is always increasing I can just use the standard sp.interpolate spline routines to get the desired output. \nText: The bit of code that needs adding to my example in the question to make it work is: \nCode: x = np.concatenate((line1_x, line2_x))\ny = np.concatenate((line1_y, line2_y))\norder = np.argsort(y)\n\nspline_fit = interp.UnivariateSpline(y[order], x[order])\ny_points = np.linspace(515, 1000, 20)\n\nplt.plot(spline_fit(y_points), y_points, 'k--', label='Fit')\n\nText: Which gives the desired result: \nAPI:\nscipy.interpolate\n","label":[[215,229,"Mention"],[674,691,"API"]],"Comments":[]}
{"id":60401,"text":"ID:37223558\nPost:\nText: By now you may have found the Wikipedia page for the Centripetal Catmull-Rom spline, but in case you haven't, it includes this sample code: \nCode: import numpy\nimport matplotlib.pyplot as plt\n\ndef CatmullRomSpline(P0, P1, P2, P3, nPoints=100):\n  \"\"\"\n  P0, P1, P2, and P3 should be (x,y) point pairs that define the\n  Catmull-Rom spline.\n  nPoints is the number of points to include in this curve segment.\n  \"\"\"\n  # Convert the points to numpy so that we can do array multiplication\n  P0, P1, P2, P3 = map(numpy.array, [P0, P1, P2, P3])\n\n  # Calculate t0 to t4\n  alpha = 0.5\n  def tj(ti, Pi, Pj):\n    xi, yi = Pi\n    xj, yj = Pj\n    return ( ( (xj-xi)**2 + (yj-yi)**2 )**0.5 )**alpha + ti\n\n  t0 = 0\n  t1 = tj(t0, P0, P1)\n  t2 = tj(t1, P1, P2)\n  t3 = tj(t2, P2, P3)\n\n  # Only calculate points between P1 and P2\n  t = numpy.linspace(t1,t2,nPoints)\n\n  # Reshape so that we can multiply by the points P0 to P3\n  # and get a point for each value of t.\n  t = t.reshape(len(t),1)\n\n  A1 = (t1-t)\/(t1-t0)*P0 + (t-t0)\/(t1-t0)*P1\n  A2 = (t2-t)\/(t2-t1)*P1 + (t-t1)\/(t2-t1)*P2\n  A3 = (t3-t)\/(t3-t2)*P2 + (t-t2)\/(t3-t2)*P3\n\n  B1 = (t2-t)\/(t2-t0)*A1 + (t-t0)\/(t2-t0)*A2\n  B2 = (t3-t)\/(t3-t1)*A2 + (t-t1)\/(t3-t1)*A3\n\n  C  = (t2-t)\/(t2-t1)*B1 + (t-t1)\/(t2-t1)*B2\n  return C\n\ndef CatmullRomChain(P):\n  \"\"\"\n  Calculate Catmull Rom for a chain of points and return the combined curve.\n  \"\"\"\n  sz = len(P)\n\n  # The curve C will contain an array of (x,y) points.\n  C = []\n  for i in range(sz-3):\n    c = CatmullRomSpline(P[i], P[i+1], P[i+2], P[i+3])\n    C.extend(c)\n\n  return C\n\nText: which nicely computes the interpolation for n >= 4 points like so: \nCode: points = [[0,1.5],[2,2],[3,1],[4,0.5],[5,1],[6,2],[7,3]]\nc = CatmullRomChain(points)\npx, py = zip(*points)\nx, y = zip(*c)\n\nplt.plot(x, y)\nplt.plot(px, py, 'or')\n\nText: resulting in this matplotlib image: \nText: Update: \nText: Alternatively, there is a inp function for BarycentricInterpolator that appears to do what you're looking for. It is rather straightforward to use and works for cases in which you have only 3 data points. \nCode: from scipy.interpolate import BarycentricInterpolator\n\n# create some data points\npoints1 = [[0, 2], [1, 4], [2, -2], [3, 6], [4, 2]]\npoints2 = [[1, 1], [2, 5], [3, -1]]\n\n# put data into x, y tuples\nx1, y1 =zip(*points1)\nx2, y2 = zip(*points2)\n\n# create the interpolator\nbci1 = BarycentricInterpolator(x1, y1)\nbci2 = BarycentricInterpolator(x2, y2)\n\n# define dense x-axis for interpolating over\nx1_new = np.linspace(min(x1), max(x1), 1000)\nx2_new = np.linspace(min(x2), max(x2), 1000)\n\n# plot it all\nplt.plot(x1, y1, 'o')\nplt.plot(x2, y2, 'o')\nplt.plot(x1_new, bci1(x1_new))\nplt.plot(x2_new, bci2(x2_new))\nplt.xlim(-1, 5)\n\nText: Update 2 \nText: Another option within scipy is akima interpolation via Akima1DInterpolator. It is as easy to implement as Barycentric, but has the advantage that it avoids large oscillations at the edge of a data set. Here's a few test cases that exhibit all the criteria you've asked for so far. \nCode: from scipy.interpolate import Akima1DInterpolator\n\nx1, y1 = np.arange(13), np.random.randint(-10, 10, 13)\nx2, y2 = [0,2,3,6,12], [100,50,30,18,14]\nx3, y3 = [4, 6, 8], [60, 80, 40]\n\nakima1 = Akima1DInterpolator(x1, y1)\nakima2 = Akima1DInterpolator(x2, y2)\nakima3 = Akima1DInterpolator(x3, y3)\n\nx1_new = np.linspace(min(x1), max(x1), 1000)\nx2_new = np.linspace(min(x2), max(x2), 1000)\nx3_new = np.linspace(min(x3), max(x3), 1000)\n\nplt.plot(x1, y1, 'bo')\nplt.plot(x2, y2, 'go')\nplt.plot(x3, y3, 'ro')\nplt.plot(x1_new, akima1(x1_new), 'b', label='random points')\nplt.plot(x2_new, akima2(x2_new), 'g', label='exponential')\nplt.plot(x3_new, akima3(x3_new), 'r', label='3 points')\nplt.xlim(-1, 15)\nplt.ylim(-10, 110)\nplt.legend(loc='best')\n\nAPI:\nscipy.interpolate\n","label":[[1913,1916,"Mention"],[3770,3787,"API"]],"Comments":[]}
{"id":60402,"text":"ID:37387766\nPost:\nText: Your calling to brute function is wrong. You have to call with fo function and rranges defined as follow: \nCode: def fo(xy):\n    x, y = xy\n    z = np.sin(x) + 0.05*x**2 + np.sin(y) + 0.05*y**2\n    if output == True:\n        print(x, y, z)\n    return z\n\nrranges = (slice(-10, 10.1, 5), slice(-10, 10.1, 5))\n\nspo.brute(fo, rranges, finish=None)\n\nAPI:\nscipy.optimize.brute\n","label":[[40,45,"Mention"],[373,393,"API"]],"Comments":[]}
{"id":60403,"text":"ID:37500643\nPost:\nText: The Poisson distribution (implemented in scipy as scipy.stats.poisson) is a discrete distribution. The discrete distributions in scipy do not have a fit method. \nText: I'm not very familiar with the seaborn.distplot function, but it appears to assume that the data comes from a continuous distribution. If that is the case, then even if poisson had a fit method, it would not be an appropriate distribution to pass to distplot. \nText: The question title is \"How to fit a poisson distribution with seaborn?\", so for the sake of completeness, here's one way to get a plot of the data and its fit. seaborn is only used for the bar plot, using @mwaskom's suggestion to use seaborn.countplot. The fitting is actually trivial, because the maximum likelihood estimation for the Poisson distribution is simply the mean of the data. \nText: First, the imports: \nCode: In [136]: import numpy as np\n\nIn [137]: from scipy.stats import poisson\n\nIn [138]: import matplotlib.pyplot as plt\n\nIn [139]: import seaborn\n\nText: Generate some data to work with: \nCode: In [140]: x = poisson.rvs(0.4, size=100)\n\nText: These are the values in the x: \nCode: In [141]: k = np.arange(x.max()+1)\n\nIn [142]: k\nOut[142]: array([0, 1, 2, 3])\n\nText: Use seaborn.countplot to plot the data: \nCode: In [143]: seaborn.countplot(x, order=k, color='g', alpha=0.5)\nOut[143]: <matplotlib.axes._subplots.AxesSubplot at 0x114700490>\n\nText: The maximum likelihood estimation of the Poisson parameter is simply the mean of the data: \nCode: In [144]: mlest = x.mean()\n\nText: Use poisson.pmf() to get the expected probability, and multiply by the size of the data set to get the expected counts, and then plot using matplotlib. The bars are the counts of the actual data, and the dots are the expected counts of the fitted distribution: \nCode: In [145]: plt.plot(k, poisson.pmf(k, mlest)*len(x), 'go', markersize=9)\nOut[145]: [<matplotlib.lines.Line2D at 0x114da74d0>]\n\nAPI:\nscipy.stats.poisson\n","label":[[361,368,"Mention"],[1953,1972,"API"]],"Comments":[]}
{"id":60404,"text":"ID:37559471\nPost:\nText: Visualizing all tsats distributions \nText: Based on the list of st distributions, plotted below are the histograms and PDFs of each continuous random variable. The code used to generate each distribution is at the bottom. Note: The shape constants were taken from the examples on the st distribution documentation pages. \nText: alpha(a=3.57, loc=0.00, scale=1.00) \nText: anglit(loc=0.00, scale=1.00) \nText: arcsine(loc=0.00, scale=1.00) \nText: beta(a=2.31, loc=0.00, scale=1.00, b=0.63) \nText: betaprime(a=5.00, loc=0.00, scale=1.00, b=6.00) \nText: bradford(loc=0.00, c=0.30, scale=1.00) \nText: burr(loc=0.00, c=10.50, scale=1.00, d=4.30) \nText: cauchy(loc=0.00, scale=1.00) \nText: chi(df=78.00, loc=0.00, scale=1.00) \nText: chi2(df=55.00, loc=0.00, scale=1.00) \nText: cosine(loc=0.00, scale=1.00) \nText: dgamma(a=1.10, loc=0.00, scale=1.00) \nText: dweibull(loc=0.00, c=2.07, scale=1.00) \nText: erlang(a=2.00, loc=0.00, scale=1.00) \nText: expon(loc=0.00, scale=1.00) \nText: exponnorm(loc=0.00, K=1.50, scale=1.00) \nText: exponpow(loc=0.00, scale=1.00, b=2.70) \nText: exponweib(a=2.89, loc=0.00, c=1.95, scale=1.00) \nText: f(loc=0.00, dfn=29.00, scale=1.00, dfd=18.00) \nText: fatiguelife(loc=0.00, c=29.00, scale=1.00) \nText: fisk(loc=0.00, c=3.09, scale=1.00) \nText: foldcauchy(loc=0.00, c=4.72, scale=1.00) \nText: foldnorm(loc=0.00, c=1.95, scale=1.00) \nText: frechet_l(loc=0.00, c=3.63, scale=1.00) \nText: frechet_r(loc=0.00, c=1.89, scale=1.00) \nText: gamma(a=1.99, loc=0.00, scale=1.00) \nText: gausshyper(a=13.80, loc=0.00, c=2.51, scale=1.00, b=3.12, z=5.18) \nText: genexpon(a=9.13, loc=0.00, c=3.28, scale=1.00, b=16.20) \nText: genextreme(loc=0.00, c=-0.10, scale=1.00) \nText: gengamma(a=4.42, loc=0.00, c=-3.12, scale=1.00) \nText: genhalflogistic(loc=0.00, c=0.77, scale=1.00) \nText: genlogistic(loc=0.00, c=0.41, scale=1.00) \nText: gennorm(loc=0.00, beta=1.30, scale=1.00) \nText: genpareto(loc=0.00, c=0.10, scale=1.00) \nText: gilbrat(loc=0.00, scale=1.00) \nText: gompertz(loc=0.00, c=0.95, scale=1.00) \nText: gumbel_l(loc=0.00, scale=1.00) \nText: gumbel_r(loc=0.00, scale=1.00) \nText: halfcauchy(loc=0.00, scale=1.00) \nText: halfgennorm(loc=0.00, beta=0.68, scale=1.00) \nText: halflogistic(loc=0.00, scale=1.00) \nText: halfnorm(loc=0.00, scale=1.00) \nText: hypsecant(loc=0.00, scale=1.00) \nText: invgamma(a=4.07, loc=0.00, scale=1.00) \nText: invgauss(mu=0.14, loc=0.00, scale=1.00) \nText: invweibull(loc=0.00, c=10.60, scale=1.00) \nText: johnsonsb(a=4.32, loc=0.00, scale=1.00, b=3.18) \nText: johnsonsu(a=2.55, loc=0.00, scale=1.00, b=2.25) \nText: ksone(loc=0.00, scale=1.00, n=1000.00) \nText: kstwobign(loc=0.00, scale=1.00) \nText: laplace(loc=0.00, scale=1.00) \nText: levy(loc=0.00, scale=1.00) \nText: levy_l(loc=0.00, scale=1.00) \nText: loggamma(loc=0.00, c=0.41, scale=1.00) \nText: logistic(loc=0.00, scale=1.00) \nText: loglaplace(loc=0.00, c=3.25, scale=1.00) \nText: lognorm(loc=0.00, s=0.95, scale=1.00) \nText: lomax(loc=0.00, c=1.88, scale=1.00) \nText: maxwell(loc=0.00, scale=1.00) \nText: mielke(loc=0.00, s=3.60, scale=1.00, k=10.40) \nText: nakagami(loc=0.00, scale=1.00, nu=4.97) \nText: ncf(loc=0.00, dfn=27.00, nc=0.42, dfd=27.00, scale=1.00) \nText: nct(df=14.00, loc=0.00, scale=1.00, nc=0.24) \nText: ncx2(df=21.00, loc=0.00, scale=1.00, nc=1.06) \nText: norm(loc=0.00, scale=1.00) \nText: pareto(loc=0.00, scale=1.00, b=2.62) \nText: pearson3(loc=0.00, skew=0.10, scale=1.00) \nText: powerlaw(a=1.66, loc=0.00, scale=1.00) \nText: powerlognorm(loc=0.00, s=0.45, scale=1.00, c=2.14) \nText: powernorm(loc=0.00, c=4.45, scale=1.00) \nText: rayleigh(loc=0.00, scale=1.00) \nText: rdist(loc=0.00, c=0.90, scale=1.00) \nText: recipinvgauss(mu=0.63, loc=0.00, scale=1.00) \nText: reciprocal(a=0.01, loc=0.00, scale=1.00, b=1.01) \nText: rice(loc=0.00, scale=1.00, b=0.78) \nText: semicircular(loc=0.00, scale=1.00) \nText: t(df=2.74, loc=0.00, scale=1.00) \nText: triang(loc=0.00, c=0.16, scale=1.00) \nText: truncexpon(loc=0.00, scale=1.00, b=4.69) \nText: truncnorm(a=0.10, loc=0.00, scale=1.00, b=2.00) \nText: tukeylambda(loc=0.00, scale=1.00, lam=3.13) \nText: uniform(loc=0.00, scale=1.00) \nText: vonmises(loc=0.00, scale=1.00, kappa=3.99) \nText: vonmises_line(loc=0.00, scale=1.00, kappa=3.99) \nText: wald(loc=0.00, scale=1.00) \nText: weibull_max(loc=0.00, c=2.87, scale=1.00) \nText: weibull_min(loc=0.00, c=1.79, scale=1.00) \nText: wrapcauchy(loc=0.00, c=0.03, scale=1.00) \nText: Generation Code \nText: Here is the Jupyter Notebook used to generate the plots. \nCode: %matplotlib inline\n\nimport io\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.rcParams['figure.figsize'] = (16.0, 14.0)\nmatplotlib.style.use('ggplot')\n\nCode: # Distributions to check, shape constants were taken from the examples on the scipy.stats distribution documentation pages.\nDISTRIBUTIONS = [\n    stats.alpha(a=3.57, loc=0.0, scale=1.0), stats.anglit(loc=0.0, scale=1.0), \n    stats.arcsine(loc=0.0, scale=1.0), stats.beta(a=2.31, b=0.627, loc=0.0, scale=1.0), \n    stats.betaprime(a=5, b=6, loc=0.0, scale=1.0), stats.bradford(c=0.299, loc=0.0, scale=1.0),\n    stats.burr(c=10.5, d=4.3, loc=0.0, scale=1.0), stats.cauchy(loc=0.0, scale=1.0), \n    stats.chi(df=78, loc=0.0, scale=1.0), stats.chi2(df=55, loc=0.0, scale=1.0),\n    stats.cosine(loc=0.0, scale=1.0), stats.dgamma(a=1.1, loc=0.0, scale=1.0), \n    stats.dweibull(c=2.07, loc=0.0, scale=1.0), stats.erlang(a=2, loc=0.0, scale=1.0), \n    stats.expon(loc=0.0, scale=1.0), stats.exponnorm(K=1.5, loc=0.0, scale=1.0),\n    stats.exponweib(a=2.89, c=1.95, loc=0.0, scale=1.0), stats.exponpow(b=2.7, loc=0.0, scale=1.0),\n    stats.f(dfn=29, dfd=18, loc=0.0, scale=1.0), stats.fatiguelife(c=29, loc=0.0, scale=1.0), \n    stats.fisk(c=3.09, loc=0.0, scale=1.0), stats.foldcauchy(c=4.72, loc=0.0, scale=1.0),\n    stats.foldnorm(c=1.95, loc=0.0, scale=1.0), stats.frechet_r(c=1.89, loc=0.0, scale=1.0),\n    stats.frechet_l(c=3.63, loc=0.0, scale=1.0), stats.genlogistic(c=0.412, loc=0.0, scale=1.0),\n    stats.genpareto(c=0.1, loc=0.0, scale=1.0), stats.gennorm(beta=1.3, loc=0.0, scale=1.0), \n    stats.genexpon(a=9.13, b=16.2, c=3.28, loc=0.0, scale=1.0), stats.genextreme(c=-0.1, loc=0.0, scale=1.0),\n    stats.gausshyper(a=13.8, b=3.12, c=2.51, z=5.18, loc=0.0, scale=1.0), stats.gamma(a=1.99, loc=0.0, scale=1.0),\n    stats.gengamma(a=4.42, c=-3.12, loc=0.0, scale=1.0), stats.genhalflogistic(c=0.773, loc=0.0, scale=1.0),\n    stats.gilbrat(loc=0.0, scale=1.0), stats.gompertz(c=0.947, loc=0.0, scale=1.0),\n    stats.gumbel_r(loc=0.0, scale=1.0), stats.gumbel_l(loc=0.0, scale=1.0),\n    stats.halfcauchy(loc=0.0, scale=1.0), stats.halflogistic(loc=0.0, scale=1.0),\n    stats.halfnorm(loc=0.0, scale=1.0), stats.halfgennorm(beta=0.675, loc=0.0, scale=1.0),\n    stats.hypsecant(loc=0.0, scale=1.0), stats.invgamma(a=4.07, loc=0.0, scale=1.0),\n    stats.invgauss(mu=0.145, loc=0.0, scale=1.0), stats.invweibull(c=10.6, loc=0.0, scale=1.0),\n    stats.johnsonsb(a=4.32, b=3.18, loc=0.0, scale=1.0), stats.johnsonsu(a=2.55, b=2.25, loc=0.0, scale=1.0),\n    stats.ksone(n=1e+03, loc=0.0, scale=1.0), stats.kstwobign(loc=0.0, scale=1.0),\n    stats.laplace(loc=0.0, scale=1.0), stats.levy(loc=0.0, scale=1.0),\n    stats.levy_l(loc=0.0, scale=1.0), stats.levy_stable(alpha=0.357, beta=-0.675, loc=0.0, scale=1.0),\n    stats.logistic(loc=0.0, scale=1.0), stats.loggamma(c=0.414, loc=0.0, scale=1.0),\n    stats.loglaplace(c=3.25, loc=0.0, scale=1.0), stats.lognorm(s=0.954, loc=0.0, scale=1.0),\n    stats.lomax(c=1.88, loc=0.0, scale=1.0), stats.maxwell(loc=0.0, scale=1.0),\n    stats.mielke(k=10.4, s=3.6, loc=0.0, scale=1.0), stats.nakagami(nu=4.97, loc=0.0, scale=1.0),\n    stats.ncx2(df=21, nc=1.06, loc=0.0, scale=1.0), stats.ncf(dfn=27, dfd=27, nc=0.416, loc=0.0, scale=1.0),\n    stats.nct(df=14, nc=0.24, loc=0.0, scale=1.0), stats.norm(loc=0.0, scale=1.0),\n    stats.pareto(b=2.62, loc=0.0, scale=1.0), stats.pearson3(skew=0.1, loc=0.0, scale=1.0),\n    stats.powerlaw(a=1.66, loc=0.0, scale=1.0), stats.powerlognorm(c=2.14, s=0.446, loc=0.0, scale=1.0),\n    stats.powernorm(c=4.45, loc=0.0, scale=1.0), stats.rdist(c=0.9, loc=0.0, scale=1.0),\n    stats.reciprocal(a=0.00623, b=1.01, loc=0.0, scale=1.0), stats.rayleigh(loc=0.0, scale=1.0),\n    stats.rice(b=0.775, loc=0.0, scale=1.0), stats.recipinvgauss(mu=0.63, loc=0.0, scale=1.0),\n    stats.semicircular(loc=0.0, scale=1.0), stats.t(df=2.74, loc=0.0, scale=1.0),\n    stats.triang(c=0.158, loc=0.0, scale=1.0), stats.truncexpon(b=4.69, loc=0.0, scale=1.0),\n    stats.truncnorm(a=0.1, b=2, loc=0.0, scale=1.0), stats.tukeylambda(lam=3.13, loc=0.0, scale=1.0),\n    stats.uniform(loc=0.0, scale=1.0), stats.vonmises(kappa=3.99, loc=0.0, scale=1.0),\n    stats.vonmises_line(kappa=3.99, loc=0.0, scale=1.0), stats.wald(loc=0.0, scale=1.0),\n    stats.weibull_min(c=1.79, loc=0.0, scale=1.0), stats.weibull_max(c=2.87, loc=0.0, scale=1.0),\n    stats.wrapcauchy(c=0.0311, loc=0.0, scale=1.0)\n]\n\nCode: bins = 32\nsize = 16384\nplotData = []\nfor distribution in DISTRIBUTIONS:\n    try:  \n        # Create random data\n        rv = pd.Series(distribution.rvs(size=size))\n        # Get sane start and end points of distribution\n        start = distribution.ppf(0.01)\n        end = distribution.ppf(0.99)\n\n        # Build PDF and turn into pandas Series\n        x = np.linspace(start, end, size)\n        y = distribution.pdf(x)\n        pdf = pd.Series(y, x)\n\n        # Get histogram of random data\n        b = np.linspace(start, end, bins+1)\n        y, x = np.histogram(rv, bins=b, normed=True)\n        x = [(a+x[i+1])\/2.0 for i,a in enumerate(x[0:-1])]\n        hist = pd.Series(y, x)\n\n        # Create distribution name and parameter string\n        title = '{}({})'.format(distribution.dist.name, ', '.join(['{}={:0.2f}'.format(k,v) for k,v in distribution.kwds.items()]))\n\n        # Store data for later\n        plotData.append({\n            'pdf': pdf,\n            'hist': hist,\n            'title': title\n        })\n\n    except Exception:\n        print 'could not create data', distribution.dist.name\n\nCode: plotMax = len(plotData)\n\nfor i, data in enumerate(plotData):\n    w = abs(abs(data['hist'].index[0]) - abs(data['hist'].index[1]))\n\n    # Display\n    plt.figure(figsize=(10, 6))\n    ax = data['pdf'].plot(kind='line', label='Model PDF', legend=True, lw=2)\n    ax.bar(data['hist'].index, data['hist'].values, label='Random Sample', width=w, align='center', alpha=0.5)\n    ax.set_title(data['title'])\n\n    # Grab figure\n    fig = matplotlib.pyplot.gcf()\n    # Output 'file'\n    fig.savefig('~\/Desktop\/dist\/'+data['title']+'.png', format='png', bbox_inches='tight')\n    matplotlib.pyplot.close()\n\nAPI:\nscipy.stats\nscipy.stats\nscipy.stats\n","label":[[40,45,"Mention"],[88,90,"Mention"],[308,310,"Mention"],[10681,10692,"API"],[10693,10704,"API"],[10705,10716,"API"]],"Comments":[]}
{"id":60405,"text":"ID:37763111\nPost:\nText: Take a look at sp.linalg.circulant \nCode: In [255]: r\nOut[255]: array([1, 2, 3, 4, 5])\n\nIn [256]: circulant(r).T\nOut[256]: \narray([[1, 2, 3, 4, 5],\n       [5, 1, 2, 3, 4],\n       [4, 5, 1, 2, 3],\n       [3, 4, 5, 1, 2],\n       [2, 3, 4, 5, 1]])\n\nText: or la.toeplitz \nCode: In [257]: toeplitz(np.roll(r[::-1], 1), r)\nOut[257]: \narray([[1, 2, 3, 4, 5],\n       [5, 1, 2, 3, 4],\n       [4, 5, 1, 2, 3],\n       [3, 4, 5, 1, 2],\n       [2, 3, 4, 5, 1]])\n\nAPI:\nscipy.linalg.circulant\nscipy.linalg.toeplitz\n","label":[[39,58,"Mention"],[279,290,"Mention"],[479,501,"API"],[502,523,"API"]],"Comments":[]}
{"id":60406,"text":"ID:37770230\nPost:\nText: \"Smoothing\" is not a well-defined operation; what it means is open to interpretation. There are many operations that result in \"smoother\" data, and most of them have at least one parameter that controls the amount of smoothing. Without knowing more about what you are going to do with the smoothed data, it is hard to give a definitive answer to this question. \nText: Here's an answer anyway. :) \nText: Rbf is an interpolator for n-dimensional data that includes a smoothing parameter. When this parameter is 0 (which is the default), a true interpolator is created, i.e. it returns the given z values at the given (x, y) values, and at other points it returns interpolated z values. Rbf includes a smooth parameter about which the docstring says \"Values greater than zero increase the smoothness of the approximation.\" \nText: So, an answer to your question that uses Rbf is: \nCode: f = Rbf(x, y, z, smooth=<a positive number>)\nz_smoothed = f(x, y)\n\nText: (Unfortunately, the Rbf docstring doesn't explain how smooth is used. You'd have to dig into the code to find out exactly what it does. In the meantime, you can try some values and see if the results meet your needs.) \nText: The following script is an example of using Rbf with a nonzero smooth argument. The input points and the smoothed surface are plotted using matplotlib. \nCode: import numpy as np\nfrom scipy.interpolate import Rbf\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n# Generate some random (x, y, z) values.\nnpoints = 36\nnp.random.seed(12345)\nx, y = np.random.rand(2, npoints)\nz = np.cos(3*x)*np.sin(2*y) + 0.4*np.random.randn(npoints)\n\n\n# \"Interpolator\" with smoothing\nf = Rbf(x, y, z, smooth=0.05)\n\nprint(\"Original z values:\")\nprint(z)\nprint(\"Smoothed z values:\")\nprint(f(x, y))\n\nu = np.linspace(0, 1, 25)\nxx, yy = np.meshgrid(u, u)\nzz = f(xx, yy)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\n\nsurf1 = ax.plot_surface(xx, yy, zz, rstride=1, cstride=1, color='g',\n                       linewidth=0, antialiased=False, alpha=0.5)\n\nplt.show()\n\nText: The script generates the following plot. The blue dots are the original data, and the green surface is the graph of the function created by Rbf. \nText: The script prints: \nCode: Original z values:\n[-0.34127933 -0.30729404  0.21155127  0.82107652  0.17163933 -0.44447561\n -0.62316986 -0.07631452 -0.2452825   0.08006371 -0.16038592 -1.15094797\n  0.97879369 -0.59069121  0.28481385 -0.61505364 -1.28958296 -0.40040525\n -0.62065409  0.10887611  0.11082111 -0.57756184 -0.08303365  0.1736536\n -0.11741524 -0.25279036 -0.87523777 -0.62589892  0.14774674  1.02822874\n  1.40065013  0.0570847  -1.24442082  1.29216089  0.04075983  0.35829967]\nSmoothed z values:\n[-0.4760952  -0.32638375  0.33082556  0.81805681  0.04136433 -0.04617472\n -0.6941891  -0.17280308 -0.21626414 -0.25286811 -0.19661876 -1.04547018\n  1.19599927 -0.55479106  0.3257578  -0.35879233 -0.9914419  -0.74646378\n -0.60559207 -0.11546096 -0.10684431 -0.35038102  0.05290993  0.10818459\n -0.07302746 -0.33240211 -0.82955756 -0.32360917  0.11565045  0.98144511\n  1.22421926 -0.08092414 -0.97381114  1.16754806  0.01186976  0.11594726]\n\nAPI:\nscipy.interpolate.Rbf\n","label":[[427,430,"Mention"],[3212,3233,"API"]],"Comments":[]}
{"id":60407,"text":"ID:37872172\nPost:\nText: Disclaimer: I'm mostly writing this post with syntactical considerations and general behaviour in mind. I'm not familiar with the memory and CPU aspect of the methods described, and I aim this answer at those who have reasonably small sets of data, such that the quality of the interpolation can be the main aspect to consider. I am aware that when working with very large data sets, the better-performing methods (namely griddata and RBFInterpolator without a neighbors keyword argument) might not be feasible. \nText: Note that this answer uses the new RBFInterpolator class introduced in SciPy 1.7.0. For the legacy Rbf class see the previous version of this answer. \nText: I'm going to compare three kinds of multi-dimensional interpolation methods (interp2d\/splines, griddata and RBFInterpolator). I will subject them to two kinds of interpolation tasks and two kinds of underlying functions (points from which are to be interpolated). The specific examples will demonstrate two-dimensional interpolation, but the viable methods are applicable in arbitrary dimensions. Each method provides various kinds of interpolation; in all cases I will use cubic interpolation (or something close1). It's important to note that whenever you use interpolation you introduce bias compared to your raw data, and the specific methods used affect the artifacts that you will end up with. Always be aware of this, and interpolate responsibly. \nText: The two interpolation tasks will be \nText: upsampling (input data is on a rectangular grid, output data is on a denser grid) interpolation of scattered data onto a regular grid \nText: The two functions (over the domain [x, y] in [-1, 1]x[-1, 1]) will be \nText: a smooth and friendly function: cos(pi*x)*sin(pi*y); range in [-1, 1] an evil (and in particular, non-continuous) function: x*y \/ (x^2 + y^2) with a value of 0.5 near the origin; range in [-0.5, 0.5] \nText: Here's how they look: \nText: I will first demonstrate how the three methods behave under these four tests, then I'll detail the syntax of all three. If you know what you should expect from a method, you might not want to waste your time learning its syntax (looking at you, interp2d). \nText: Test data \nText: For the sake of explicitness, here is the code with which I generated the input data. While in this specific case I'm obviously aware of the function underlying the data, I will only use this to generate input for the interpolation methods. I use numpy for convenience (and mostly for generating the data), but scipy alone would suffice too. \nCode: import numpy as np\nimport scipy.interpolate as interp\n\n# auxiliary function for mesh generation\ndef gimme_mesh(n):\n    minval = -1\n    maxval =  1\n    # produce an asymmetric shape in order to catch issues with transpositions\n    return np.meshgrid(np.linspace(minval, maxval, n),\n                       np.linspace(minval, maxval, n + 1))\n\n# set up underlying test functions, vectorized\ndef fun_smooth(x, y):\n    return np.cos(np.pi*x) * np.sin(np.pi*y)\n\ndef fun_evil(x, y):\n    # watch out for singular origin; function has no unique limit there\n    return np.where(x**2 + y**2 > 1e-10, x*y\/(x**2+y**2), 0.5)\n\n# sparse input mesh, 6x7 in shape\nN_sparse = 6\nx_sparse, y_sparse = gimme_mesh(N_sparse)\nz_sparse_smooth = fun_smooth(x_sparse, y_sparse)\nz_sparse_evil = fun_evil(x_sparse, y_sparse)\n\n# scattered input points, 10^2 altogether (shape (100,))\nN_scattered = 10\nrng = np.random.default_rng()\nx_scattered, y_scattered = rng.random((2, N_scattered**2))*2 - 1\nz_scattered_smooth = fun_smooth(x_scattered, y_scattered)\nz_scattered_evil = fun_evil(x_scattered, y_scattered)\n\n# dense output mesh, 20x21 in shape\nN_dense = 20\nx_dense, y_dense = gimme_mesh(N_dense)\n\nText: Smooth function and upsampling \nText: Let's start with the easiest task. Here's how an upsampling from a mesh of shape [6, 7] to one of [20, 21] works out for the smooth test function: \nText: Even though this is a simple task, there are already subtle differences between the outputs. At a first glance all three outputs are reasonable. There are two features to note, based on our prior knowledge of the underlying function: the middle case of griddata distorts the data most. Note the y == -1 boundary of the plot (nearest the x label): the function should be strictly zero (since y == -1 is a nodal line for the smooth function), yet this is not the case for griddata. Also note the x == -1 boundary of the plots (behind, to the left): the underlying function has a local maximum (implying zero gradient near the boundary) at [-1, -0.5], yet the griddata output shows clearly non-zero gradient in this region. The effect is subtle, but it's a bias none the less. \nText: Evil function and upsampling \nText: A bit harder task is to perform upsampling on our evil function: \nText: Clear differences are starting to show among the three methods. Looking at the surface plots, there are clear spurious extrema appearing in the output from interp2d (note the two humps on the right side of the plotted surface). While griddata and RBFInterpolator seem to produce similar results at first glance, producing local minima near [0.4, -0.4] that is absent from the underlying function. \nText: However, there is one crucial aspect in which RBFInterpolator is far superior: it respects the symmetry of the underlying function (which is of course also made possible by the symmetry of the sample mesh). The output from griddata breaks the symmetry of the sample points, which is already weakly visible in the smooth case. \nText: Smooth function and scattered data \nText: Most often one wants to perform interpolation on scattered data. For this reason I expect these tests to be more important. As shown above, the sample points were chosen pseudo-uniformly in the domain of interest. In realistic scenarios you might have additional noise with each measurement, and you should consider whether it makes sense to interpolate your raw data to begin with. \nText: Output for the smooth function: \nText: Now there's already a bit of a horror show going on. I clipped the output from interp2d to between [-1, 1] exclusively for plotting, in order to preserve at least a minimal amount of information. It's clear that while some of the underlying shape is present, there are huge noisy regions where the method completely breaks down. The second case of griddata reproduces the shape fairly nicely, but note the white regions at the border of the contour plot. This is due to the fact that griddata only works inside the convex hull of the input data points (in other words, it doesn't perform any extrapolation). I kept the default NaN value for output points lying outside the convex hull.2 Considering these features, RBFInterpolator seems to perform best. \nText: Evil function and scattered data \nText: And the moment we've all been waiting for: \nText: It's no huge surprise that interp2d gives up. In fact, during the call to interp2d you should expect some friendly RuntimeWarnings complaining about the impossibility of the spline to be constructed. As for the other two methods, RBFInterpolator seems to produce the best output, even near the borders of the domain where the result is extrapolated. \nText: So let me say a few words about the three methods, in decreasing order of preference (so that the worst is the least likely to be read by anybody). \nText: RBFInterpolator \nText: The RBF in the name of the RBFInterpolator class stands for \"radial basis functions\". To be honest I've never considered this approach until I started researching for this post, but I'm pretty sure I'll be using these in the future. \nText: Just like the spline-based methods (see later), usage comes in two steps: first one creates a callable RBFInterpolator class instance based on the input data, and then calls this object for a given output mesh to obtain the interpolated result. Example from the smooth upsampling test: \nCode: import scipy.interpolate as interp\n\nsparse_points = np.stack([x_sparse.ravel(), y_sparse.ravel()], -1)  # shape (N, 2) in 2d\ndense_points = np.stack([x_dense.ravel(), y_dense.ravel()], -1)  # shape (N, 2) in 2d\n\nzfun_smooth_rbf = interp.RBFInterpolator(sparse_points, z_sparse_smooth.ravel(),\n                                         smoothing=0, kernel='cubic')  # explicit default smoothing=0 for interpolation\nz_dense_smooth_rbf = zfun_smooth_rbf(dense_points).reshape(x_dense.shape)  # not really a function, but a callable class instance\n\nzfun_evil_rbf = interp.RBFInterpolator(sparse_points, z_sparse_evil.ravel(),\n                                       smoothing=0, kernel='cubic')  # explicit default smoothing=0 for interpolation\nz_dense_evil_rbf = zfun_evil_rbf(dense_points).reshape(x_dense.shape)  # not really a function, but a callable class instance\n\nText: Note that we had to do some array building gymnastics to make the API of RBFInterpolator happy. Since we have to pass the 2d points as arrays of shape (N, 2), we have to flatten the input grid and stack the two flattened arrays. The constructed interpolator also expects query points in this format, and the result will be a 1d array of shape (N,) which we have to reshape back to match our 2d grid for plotting. Since RBFInterpolator makes no assumptions about the number of dimensions of the input points, it supports arbitrary dimensions for interpolation. \nText: So, RBFInterpolator \nText: produces well-behaved output even for crazy input data supports interpolation in higher dimensions extrapolates outside the convex hull of the input points (of course extrapolation is always a gamble, and you should generally not rely on it at all) creates an interpolator as a first step, so evaluating it in various output points is less additional effort can have output point arrays of arbitrary shape (as opposed to being constrained to rectangular meshes, see later) more likely to preserving the symmetry of the input data supports multiple kinds of radial functions for keyword kernel: multiquadric, inverse_multiquadric, inverse_quadratic, gaussian, linear, cubic, quintic, thin_plate_spline (the default). As of SciPy 1.7.0 the class doesn't allow passing a custom callable due to technical reasons, but this is likely to be added in a future version. can give inexact interpolations by increasing the smoothing parameter \nText: One drawback of RBF interpolation is that interpolating N data points involves inverting an N x N matrix. This quadratic complexity very quickly blows up memory need for a large number of data points. However, the new RBFInterpolator class also supports a neighbors keyword parameter that restricts computation of each radial basis function to k nearest neighbours, thereby reducing memory need. \nText: gd \nText: My former favourite, griddata, is a general workhorse for interpolation in arbitrary dimensions. It doesn't perform extrapolation beyond setting a single preset value for points outside the convex hull of the nodal points, but since extrapolation is a very fickle and dangerous thing, this is not necessarily a con. Usage example: \nCode: sparse_points = np.stack([x_sparse.ravel(), y_sparse.ravel()], -1)  # shape (N, 2) in 2d\nz_dense_smooth_griddata = interp.griddata(sparse_points, z_sparse_smooth.ravel(),\n                                          (x_dense, y_dense), method='cubic')  # default method is linear\n\nText: Note that the same array transformations were necessary for the input arrays as for RBFInterpolator. The input points have to be specified in an array of shape [N, D] in D dimensions, or alternatively as a tuple of 1d arrays: \nCode: z_dense_smooth_griddata = interp.griddata((x_sparse.ravel(), y_sparse.ravel()),\n                                          z_sparse_smooth.ravel(), (x_dense, y_dense), method='cubic')\n\nText: The output point arrays can be specified as a tuple of arrays of arbitrary dimensions (as in both above snippets), which gives us some more flexibility. \nText: In a nutshell, griddata \nText: produces well-behaved output even for crazy input data supports interpolation in higher dimensions does not perform extrapolation, a single value can be set for the output outside the convex hull of the input points (see fill_value) computes the interpolated values in a single call, so probing multiple sets of output points starts from scratch can have output points of arbitrary shape supports nearest-neighbour and linear interpolation in arbitrary dimensions, cubic in 1d and 2d. Nearest-neighbour and linear interpolation use NearestNDInterpolator and LinearNDInterpolator under the hood, respectively. 1d cubic interpolation uses a spline, 2d cubic interpolation uses CloughTocher2DInterpolator to construct a continuously differentiable piecewise-cubic interpolator. might violate the symmetry of the input data \nText: scipy.interpolate.interp2d\/scipy.interpolate.bisplrep \nText: The only reason I'm discussing interp2d and its relatives is that it has a deceptive name, and people are likely to try using it. Spoiler alert: don't use it. interp2d was deprecated in SciPy version 1.10, and will be removed in SciPy 1.12. See this mailing list discussion for details. It's also more special than the previous subjects in that it's specifically used for two-dimensional interpolation, but I suspect this is by far the most common case for multivariate interpolation. \nText: As far as syntax goes, interp2d is similar to RBFInterpolator in that it first needs constructing an interpolation instance, which can be called to provide the actual interpolated values. There's a catch, however: the output points have to be located on a rectangular mesh, so inputs going into the call to the interpolator have to be 1d vectors which span the output grid, as if from numpy.meshgrid: \nCode: # reminder: x_sparse and y_sparse are of shape [6, 7] from numpy.meshgrid\nzfun_smooth_interp2d = interp.interp2d(x_sparse, y_sparse, z_sparse_smooth, kind='cubic')   # default kind is 'linear'\n# reminder: x_dense and y_dense are of shape (20, 21) from numpy.meshgrid\nxvec = x_dense[0,:] # 1d array of unique x values, 20 elements\nyvec = y_dense[:,0] # 1d array of unique y values, 21 elements\nz_dense_smooth_interp2d = zfun_smooth_interp2d(xvec, yvec)   # output is (20, 21)-shaped array\n\nText: One of the most common mistakes when using interp2d is putting your full 2d meshes into the interpolation call, which leads to explosive memory consumption, and hopefully to a hasty MemoryError. \nText: Now, the greatest problem with interp2d is that it often doesn't work. In order to understand this, we have to look under the hood. It turns out that interp2d is a wrapper for the lower-level functions bisplrep + bisplev, which are in turn wrappers for FITPACK routines (written in Fortran). The equivalent call to the previous example would be \nCode: kind = 'cubic'\nif kind == 'linear':\n    kx = ky = 1\nelif kind == 'cubic':\n    kx = ky = 3\nelif kind == 'quintic':\n    kx = ky = 5\n# bisplrep constructs a spline representation, bisplev evaluates the spline at given points\nbisp_smooth = interp.bisplrep(x_sparse.ravel(), y_sparse.ravel(),\n                              z_sparse_smooth.ravel(), kx=kx, ky=ky, s=0)\nz_dense_smooth_bisplrep = interp.bisplev(xvec, yvec, bisp_smooth).T  # note the transpose\n\nText: Now, here's the thing about interp2d: (in scipy version 1.7.0) there is a nice comment in interpolate\/interpolate.py for interp2d: \nCode: if not rectangular_grid:\n    # TODO: surfit is really not meant for interpolation!\n    self.tck = fitpack.bisplrep(x, y, z, kx=kx, ky=ky, s=0.0)\n\nText: and indeed in interpolate\/fitpack.py, in bisplrep there's some setup and ultimately \nCode: tx, ty, c, o = _fitpack._surfit(x, y, z, w, xb, xe, yb, ye, kx, ky,\n                                task, s, eps, tx, ty, nxest, nyest,\n                                wrk, lwrk1, lwrk2)                 \n\nText: And that's it. The routines underlying interp2d are not really meant to perform interpolation. They might suffice for sufficiently well-behaved data, but under realistic circumstances you will probably want to use something else. \nText: Just to conclude, interpolate.interp2d \nText: can lead to artifacts even with well-tempered data is specifically for bivariate problems (although there's the limited interpn for input points defined on a grid) performs extrapolation creates an interpolator as a first step, so evaluating it in various output points is less additional effort can only produce output over a rectangular grid, for scattered output you would have to call the interpolator in a loop supports linear, cubic and quintic interpolation might violate the symmetry of the input data \nText: 1I'm fairly certain that the cubic and linear kind of basis functions of RBFInterpolator do not exactly correspond to the other interpolators of the same name. 2These NaNs are also the reason for why the surface plot seems so odd: matplotlib historically has difficulties with plotting complex 3d objects with proper depth information. The NaN values in the data confuse the renderer, so parts of the surface that should be in the back are plotted to be in the front. This is an issue with visualization, and not interpolation. \nAPI:\nscipy.interpolate.RBFInterpolator\nscipy.interpolate.RBFInterpolator\nscipy.interpolate.griddata\nscipy.interpolate.griddata\n","label":[[7412,7427,"Mention"],[9411,9426,"Mention"],[10776,10778,"Mention"],[12006,12014,"Mention"],[17244,17277,"API"],[17278,17311,"API"],[17312,17338,"API"],[17339,17365,"API"]],"Comments":[]}
{"id":60408,"text":"ID:37980933\nPost:\nText: If you really only want the select values, this will give it to you. Note, I make use of the groupby function so that I only have to create a interp1d call once per date \nText: The data munging: \nCode: A = pd.DataFrame({\"date\":[\"06\/24\/2014\",\"06\/25\/2014\",\"06\/26\/2014\"], \"value\":[2, 4, 6]})\nB = pd.DataFrame({\"date\":[\"06\/25\/2014\",\"06\/26\/2014\",\"06\/24\/2014\"], \n                  \"1\":[0.1, 0.5, 0.9],\"3\":[0.2, 0.6, 1.0],\"5\":[0.3, 0.7, 1.1],\"7\":[0.4, 0.8, 1.2]})\nB = B.set_index('date').T\nB.index = B.index.astype(int)\n\nText: Then the actual work \nCode: from scipy.interpolate import interp1d\nimport pandas as pd\n\ndef interped(series,targets):\n    x,y = zip(*series.items())\n    f = interp1d(x,y)\n    return [(i,f(i)) for i in targets]\n\ndef getResults(dfA, dfB):\n    grouped = dfA.groupby('date')\n    res = []\n    for key in grouped.groups:\n        targets = grouped.get_group(key)['value'].values\n        values = interped(dfB[key], targets)\n        res.extend([(key, target, value) for target,value in values])\n\n    return pd.DataFrame(res, columns=[\"date\", \"target\", \"interp\"])\n\ngetResults(A, B)\n\nText: Outputs: \nCode:     date    target  interp\n0   06\/24\/2014  2   0.95\n1   06\/26\/2014  6   0.75\n2   06\/25\/2014  4   0.25\n\nText: AND if you insist on calling A.apply .... \nCode: import pandas as pd\nfrom scipy.interpolate import interp1d\n\nA = pd.DataFrame({\"date\":[\"06\/24\/2014\",\"06\/25\/2014\",\"06\/26\/2014\"], \"value\":[2, 4, 6]})\nB = pd.DataFrame({\"date\":[\"06\/25\/2014\",\"06\/26\/2014\",\"06\/24\/2014\"], \n                  \"1\":[0.1, 0.5, 0.9],\"3\":[0.2, 0.6, 1.0],\"5\":[0.3, 0.7, 1.1],\"7\":[0.4, 0.8, 1.2]})\nB = B.set_index('date').T\nB.index = B.index.astype(int)\n\n\ndef getRowApplyFunc():\n    funcs = {}\n    def interped(row):\n        date = row['date']\n        target = row['value']\n        if date in funcs:\n            interpFunc = funcs[date]\n        else:\n            x,y = zip(*B[date].items())\n            interpFunc = interp1d(x,y)\n            funcs[date] = interpFunc\n        return interpFunc(target)\n    return interped\n\nA['interpd'] = A.apply(getRowApplyFunc(), axis=1)\nA\n\nText: Also outputs: \nCode:     date    value   interpd\n0   06\/24\/2014  2   0.95\n1   06\/25\/2014  4   0.25\n2   06\/26\/2014  6   0.75\n\nAPI:\nscipy.interpolate.interp1d\n","label":[[166,174,"Mention"],[2226,2252,"API"]],"Comments":[]}
{"id":60409,"text":"ID:38134745\nPost:\nText: dweibull implements the double Weibull distribution. Its support is the real line. Your function weib corresponds to the PDF of scipy's weibull_min distribution. \nText: Compare your function weib to weibull_min.pdf: \nCode: In [128]: from scipy.stats import weibull_min\n\nIn [129]: x = np.arange(0, 20, 1.0)\n\nIn [130]: K = 2.0\n\nIn [131]: A = 10.0\n\nText: Your implementation: \nCode: In [132]: weib(x, A, K)\nOut[132]: \narray([ 0.        ,  0.019801  ,  0.03843158,  0.05483587,  0.0681715 ,\n        0.07788008,  0.08372116,  0.0857677 ,  0.08436679,  0.08007445,\n        0.07357589,  0.0656034 ,  0.05686266,  0.04797508,  0.03944036,\n        0.03161977,  0.02473752,  0.01889591,  0.014099  ,  0.0102797 ])\n\nText: scipy.stats.weibull_min.pdf: \nCode: In [133]: weibull_min.pdf(x, K, scale=A)\nOut[133]: \narray([ 0.        ,  0.019801  ,  0.03843158,  0.05483587,  0.0681715 ,\n        0.07788008,  0.08372116,  0.0857677 ,  0.08436679,  0.08007445,\n        0.07357589,  0.0656034 ,  0.05686266,  0.04797508,  0.03944036,\n        0.03161977,  0.02473752,  0.01889591,  0.014099  ,  0.0102797 ])\n\nText: By the way, there is a mistake in this line of your code: \nCode: dist=dweibull(K,1,A)\n\nText: The order of the parameters is shape, location, scale, so you are setting the location parameter to 1. That's why the values in your second plot are shifted by one. That line should have been \nCode: dist = dweibull(K, 0, A)\n\nAPI:\nscipy.stats.dweibull\n","label":[[24,32,"Mention"],[1442,1462,"API"]],"Comments":[]}
{"id":60410,"text":"ID:38149245\nPost:\nText: This is called deconvolution: deconvolve will do this for you. Example where you know the original input signal x: \nCode: import numpy as np\nimport scipy.signal as signal\n\n# We want to try and recover x from y, where y is made like this:\nx = np.array([0.5, 2.2, -1.8, -0.1])\nh = np.array([1.0, 0.5])\ny = signal.convolve(x, h)\n\n# This is called deconvolution:\nxRecovered, xRemainder = signal.deconvolve(y, h)\n# >>> xRecovered\n# array([ 0.5,  2.2, -1.8, -0.1])\n# >>> xRemainder\n# array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n#          0.00000000e+00,  -1.38777878e-17])\n\n# Check the answer\nassert(np.allclose(xRecovered, x))\nassert(np.allclose(xRemainder, 0))\n\nText: It sounds like you dont know the original signal, so your xRemainder will not be 0 to machine precisionitll represent noise in the recorded signal y. \nAPI:\nscipy.signal.deconvolve\n","label":[[54,64,"Mention"],[865,888,"API"]],"Comments":[]}
{"id":60411,"text":"ID:38152773\nPost:\nText: fmin will pass the value it is trying to minimize as the first argument to the function. If you rewrite your function as \nCode: def e(r,c,u,s): #calculates average of the MAPEs\n    return np.mean(mape(c,u,s,r))\n\nText: You get the correct results \nCode: for d in range(1,4):\n    div_data = data[data.DIV==d]\n    c = return_array(div_data.C)\n    u = return_array(div_data.U)\n    s = return_array(div_data.S)\n    r0 = [[1.0]]\n    t = fmin(e,r0,args=(c,u,s))\n    print 'r:',t\n\nText: Optimization terminated successfully. Current function value: 0.000011 Iterations: 16 Function evaluations: 32 r: [ 0.33330078] Optimization terminated successfully. Current function value: 0.000000 Iterations: 15 Function evaluations: 30 r: [ 0.5] Optimization terminated successfully. Current function value: 0.000000 Iterations: 10 Function evaluations: 20 r: [ 1.] \nAPI:\nscipy.optimize.fmin\n","label":[[24,28,"Mention"],[878,897,"API"]],"Comments":[]}
{"id":60412,"text":"ID:38356533\nPost:\nText: You are looking for svds which will Compute the largest k singular values\/vectors for a sparse matrix (though thats a bit misleading since itll do that for dense matrixes too!). \nText: It uses the Arnoldi iteration as implemented in the popular Fortran77 package, ARPACK, which in turn has wrappers in most math systems (Matlab, R, etc.). \nAPI:\nscipy.sparse.linalg.svds\n","label":[[44,48,"Mention"],[373,397,"API"]],"Comments":[]}
{"id":60413,"text":"ID:38624040\nPost:\nText: As your distribution is not in st you can either add it to the package or try doing things \"by hand\". \nText: For the former have a look at the source code of the sp.stats package - it might not be all that much work to add a new distribution! \nText: For the latter option you can use a maximum Likelihood approach. To do so define first a method giving you the pdf of the distribution. Based on the pdf construct a function calculating the log likelihood of the data given specific parameters of the distribution. Finally fit your model to the data by maximizing this log likelihood function using scipy.optimize. \nAPI:\nscipy.stats\nscipy.stats\n","label":[[55,57,"Mention"],[186,194,"Mention"],[644,655,"API"],[656,667,"API"]],"Comments":[]}
{"id":60414,"text":"ID:38687139\nPost:\nText: Your X values are reversed, scipy.interpolate.spline requires the independent variable to be monotonically increasing, and this method is deprecated - use interp1d instead (see below). \nCode: >>> from scipy.interpolate import spline\n>>> import numpy as np\n>>> X = [736176.0, 736175.0, 736174.0]  # <-- your original X is decreasing\n>>> Y = [711.74, 730.0, 698.0]\n>>> Xsmooth = np.linspace(736174.0, 736176.0, 10)\n>>> spline(X, Y, Xsmooth)\narray([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\nText: reverse X and Y first and it works \nCode: >>> spline(\n...     list(reversed(X)),  # <-- reverse order of X so also\n...     list(reversed(Y)),  # <-- reverse order of Y to match\n...     Xsmooth\n... )\narray([  698.        ,   262.18297973,   159.33767533,   293.62017489,\n         569.18656683,   890.19293934,  1160.79538066,  1285.149979  ,\n        1167.41282274,   711.74      ])\n\nText: Note that many spline interpolation methods require X to be monotonically increasing: \nText: UnivariateSpline \nText: x : (N,) array_like - 1-D array of independent input data. Must be increasing. \nText: InterpolatedUnivariateSpline \nText: x : (N,) array_like - Input dimension of data points  must be increasing \nText: The default order of scipy.interpolate.spline is cubic. Because there are only 3 data points there are large differences between a cubic spline (order=3) and a quadratic spline (order=2). The plot below shows the difference between different order splines; note: 100 points were used to smooth the fitted curve more. \nText: The documentation for scipy.interpolate.splineis vague and suggests it may not be supported. For example, it is not listed on the sp.interpolate main page or on the interploation tutorial. The source for spline shows that it actually calls spleval and splmake which are listed under Additional Tools as: \nText: Functions existing for backward compatibility (should not be used in new code). \nText: I would follow cricket_007's suggestion and use interp1d. It is the currently suggested method, it is very well documented with detailed examples in both the tutorial and API, and it allows the independent variable to be unsorted (any order) by default (see assume_sorted argument in API). \nCode: >>> from scipy.interpolate import interp1d\n>>> f = interp1d(X, Y, kind='quadratic')\n>>> f(Xsmooth)\narray([ 711.74      ,  720.14123457,  726.06049383,  729.49777778,\n        730.45308642,  728.92641975,  724.91777778,  718.4271605 ,\n        709.4545679 ,  698.        ])\n\nText: Also it will raise an error if the data is rank deficient. \nCode: >>> f = interp1d(X, Y, kind='cubic')\n\nText: ValueError: x and y arrays must have at least 4 entries \nAPI:\nscipy.interpolate\n","label":[[1690,1704,"Mention"],[2705,2722,"API"]],"Comments":[]}
{"id":60415,"text":"ID:38819851\nPost:\nText: spec.spherical_jn was added in scipy version 0.18.0, which was released on July 25, 2016. My guess is you are using an older version of scipy. To check, run \nCode: import scipy\nprint(scipy.__version__)\n\nAPI:\nscipy.special.spherical_jn\n","label":[[24,41,"Mention"],[232,258,"API"]],"Comments":[]}
{"id":60416,"text":"ID:39226973\nPost:\nText: As far as I can tell, there is no such thing as pdf_multivariate_gauss (as pointed out already). There is a python implementation of this in scipy, however: st.multivariate_normal \nText: One would use it like this: \nCode: from scipy.stats import multivariate_normal\nmvn = multivariate_normal(mu,cov) #create a multivariate Gaussian object with specified mean and covariance matrix\np = mvn.pdf(x) #evaluate the probability density at x\n\nAPI:\nscipy.stats.multivariate_normal\n","label":[[181,203,"Mention"],[465,496,"API"]],"Comments":[]}
{"id":60417,"text":"ID:39232797\nPost:\nText: The cosine distance is not defined if one of the input vectors is all 0. cosine returns nan in that case: \nCode: In [70]: a\nOut[70]: array([0, 1, 1, 1, 0, 0, 0, 1, 0, 0])\n\nIn [71]: b\nOut[71]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nIn [72]: cosine(a, b)\n\/Users\/warren\/miniconda3\/lib\/python3.5\/site-packages\/scipy\/spatial\/distance.py:329: RuntimeWarning: invalid value encountered in true_divide\n  dist = 1.0 - np.dot(u, v) \/ (norm(u) * norm(v))\nOut[72]: nan\n\nText: This may be happening in your code. Before calling cosine, check that neither input is all 0. \nText: P.S. I haven't tried to decipher what you are doing with A and ratings, but I suspect you'll eventually want to use cdist with the argument method='cosine'. \nAPI:\nscipy.spatial.distance.cosine\nscipy.spatial.distance.cdist\n","label":[[97,103,"Mention"],[705,710,"Mention"],[752,781,"API"],[782,810,"API"]],"Comments":[]}
{"id":60418,"text":"ID:39290639\nPost:\nText: The function sp.optimize.minimize takes a tuple of extra arguments, not a set. Change: \nCode: min_result=spo.minimize(error_func, l, args={data,}, method=\"SLSQP\", options={\"disp\":True})\n\nText: to: \nCode: min_result=spo.minimize(error_func, l, args=(data,), method=\"SLSQP\", options={\"disp\":True})\n\nAPI:\nscipy.optimize.minimize\n","label":[[37,57,"Mention"],[326,349,"API"]],"Comments":[]}
{"id":60419,"text":"ID:39759756\nPost:\nText: I found the problem really interesting, so I decided to give it a try. I don't know about pythonic or natural, but I think I've found a more accurate way of fitting an edge to a data set like yours while using information from every point. \nText: First off, let's generate a random data that looks like the one you've shown. This part can be easily skipped, I'm posting it simply so that the code will be complete and reproducible. I've used two bivariate normal distributions to simulate those overdensities and sprinkled them with a layer of uniformly distributed random points. Then they were added to a line equation similar to yours, and everything below the line was cut off, with the end result looking like this: \nText: Here's the code snippet to make it: \nCode: import numpy as np\n\nx_res = 1000\nx_data = np.linspace(0, 2000, x_res)\n\n# true parameters and a function that takes them\ntrue_pars = [80, 70, -5]\nmodel = lambda x, a, b, c: (a \/ np.sqrt(x + b) + c)\ny_truth = model(x_data, *true_pars)\n\nmu_prim, mu_sec = [1750, 0], [450, 1.5]\ncov_prim = [[300**2, 0     ],\n            [     0, 0.2**2]]\n# covariance matrix of the second dist is trickier\ncov_sec = [[200**2, -1     ],\n           [    -1,  1.0**2]]\nprim = np.random.multivariate_normal(mu_prim, cov_prim, x_res*10).T\nsec = np.random.multivariate_normal(mu_sec, cov_sec, x_res*1).T\nuni = np.vstack([x_data, np.random.rand(x_res) * 7])\n\n# censoring points that will end up below the curve\nprim = prim[np.vstack([[prim[1] > 0], [prim[1] > 0]])].reshape(2, -1)\nsec = sec[np.vstack([[sec[1] > 0], [sec[1] > 0]])].reshape(2, -1)\n\n# rescaling to data\nfor dset in [uni, sec, prim]:\n    dset[1] += model(dset[0], *true_pars)\n\n# this code block generates the figure above:\nimport matplotlib.pylab as plt\nplt.figure()\nplt.plot(prim[0], prim[1], '.', alpha=0.1, label = '2D Gaussian #1')\nplt.plot(sec[0], sec[1], '.', alpha=0.5, label = '2D Gaussian #2')\nplt.plot(uni[0], uni[1], '.', alpha=0.5, label = 'Uniform')\nplt.plot(x_data, y_truth, 'k:', lw = 3, zorder = 1.0, label = 'True edge')\nplt.xlim(0, 2000)\nplt.ylim(-8, 6)\nplt.legend(loc = 'lower left')\nplt.show()\n\n# mashing it all together\ndset = np.concatenate([prim, sec, uni], axis = 1)\n\nText: Now that we have the data and the model, we can brainstorm how to fit an edge of the point distribution. Commonly used regression methods like the nonlinear least-squares cf take the data values y and optimise the free parameters of a model so that the residual between y and model(x) is minimal. Nonlinear least-squares is an iterative process that tries to wiggle the curve parameters at every step to improve the fit at every step. Now clearly, this is one thing we don't want to do, as we want our minimisation routine to take us as far away from the best-fit curve as possible (but not too far away). \nText: So instead, lets consider the following function. Instead of simply returning the residual, it will also \"flip\" the points above the curve at every step of the iteration and factor them in as well. This way there are effectively always more points below the curve than above it, causing the curve to be shifted down with every iteration! Once the lowest points are reached, the minimum of the function was found, and so was the edge of the scatter. Of course, this method assumes you don't have outliers below the curve - but then your figure doesn't seem to suffer from them much. \nText: Here are the functions implementing this idea: \nCode: def get_flipped(y_data, y_model):\n    flipped = y_model - y_data\n    flipped[flipped > 0] = 0\n    return flipped\n\ndef flipped_resid(pars, x, y):\n    \"\"\"\n    For every iteration, everything above the currently proposed\n    curve is going to be mirrored down, so that the next iterations\n    is going to progressively shift downwards.\n    \"\"\"\n    y_model = model(x, *pars)\n    flipped = get_flipped(y, y_model)\n    resid = np.square(y + flipped - y_model)\n    #print pars, resid.sum() # uncomment to check the iteration parameters\n    return np.nan_to_num(resid)\n\nText: Let's see how this looks for the data above: \nCode: # plotting the mock data\nplt.plot(dset[0], dset[1], '.', alpha=0.2, label = 'Test data')\n\n# mask bad data (we accidentaly generated some NaN values)\ngmask = np.isfinite(dset[1])\ndset = dset[np.vstack([gmask, gmask])].reshape((2, -1))\n\nfrom scipy.optimize import leastsq\nguesses =[100, 100, 0]\nfit_pars, flag = leastsq(func = flipped_resid, x0 = guesses,\n                         args = (dset[0], dset[1]))\n# plot the fit:\ny_fit = model(x_data, *fit_pars)\ny_guess = model(x_data, *guesses)\nplt.plot(x_data, y_fit, 'r-', zorder = 0.9, label = 'Edge')\nplt.plot(x_data, y_guess, 'g-', zorder = 0.9, label = 'Guess')\nplt.legend(loc = 'lower left')\nplt.show()\n\nText: The most important part above is the call to leastsq function. Make sure you are careful with the initial guesses - if the guess doesn't fall onto the scatter, the model may not converge properly. After putting an appropriate guess in... \nText: Voil! The edge is perfectly matched to the real one. \nAPI:\nscipy.optimize.curve_fit\n","label":[[2400,2402,"Mention"],[5071,5095,"API"]],"Comments":[]}
{"id":60420,"text":"ID:39835236\nPost:\nText: You don't. The import directives in scipy don't allow that. By design it is modular. You have to use the from scipy import optimize kind of statements. \nText: With from scipy import optimize you can then use optimize.somefunction(). \nText: This has been raised a number of times before. A good duplicate is \nText: Why does from scipy import spatial work, while spsp doesn't work after import scipy? \nAPI:\nscipy.spatial\n","label":[[385,389,"Mention"],[429,442,"API"]],"Comments":[]}
{"id":60421,"text":"ID:39962346\nPost:\nText: This is just a guess, because you haven't included enough information in the question for anyone to really know what the problem is. Whenever you ask a question about code that generates an error, always include the complete error message in the question. Ideally, you should include a minimal, complete and verifiable example that we can run to reproduce the problem. Currently, you define function, but later you use the undefined function chirplet. That makes it a little bit harder for anyone to understand your problem. \nText: Having said that... \nText: quad returns two values: the estimate of the integral, and an estimate of the absolute error of the integral. It looks like you haven't taken this into account in function. Try something like this: \nCode: def function(a):\n    intgrl, abserr = quad(lambda t: np.cos(a[0])*np.sin(a[1])*t, 0, 3)\n    return intgrl\n\nAPI:\nscipy.integrate.quad\n","label":[[583,587,"Mention"],[900,920,"API"]],"Comments":[]}
{"id":60422,"text":"ID:40002922\nPost:\nText: I am guessing this has to do with the infinite bounds. quad is a wrapper around quadpack routines. \nText: https:\/\/people.sc.fsu.edu\/~jburkardt\/f_src\/quadpack\/quadpack.html \nText: In the end, these routines chose suitable intervals and try to get the value of the integral through function evaluations and then numerical integrations. This works fine for finite integrals, assuming you know roughly how fine you can make the steps of the function evaluation. \nText: For infinite integrals it depends how well the algorithms choose respective subintervals and how accurately they are computed. \nText: My advice: do NOT use numerical integration software AT ALL if you are interested in accurate values for infinite integrals. \nText: If your problem can be solved analytically, try that or confine yourself to certain bounds. \nAPI:\nscipy.integrate.quad\n","label":[[79,83,"Mention"],[853,873,"API"]],"Comments":[]}
{"id":60423,"text":"ID:40047570\nPost:\nText: First, you are using the wrong function. Your function func_nl_lsq calculates the residual, it is not the model function. To use scipy.otimize.curve_fit, you have to define model function, as answers by @DerWeh and @saullo_castro suggest. You still can use custom residual function as you like with least_squares instead of scipy.optimize.curve_fit. \nCode: t_data = np.array([0.5, 1.0, 1.5, 2.0])\ny_data = np.array([6.8, 3., 1.5, 0.75])\n\ndef func_nl_lsq(x, t=t_data, y=y_data):\n    return x[0]*np.exp(x[1]*t) -  y\n    # removed one level of []'s\n\nscipy.optimize.least_squares(func_nl_lsq, [0, 0])\n\nText: Also, please note, that the remark by @MadPhysicist is correct: the two problems you are considering (the initial problem and the problem where model function is under logarithm) are not equivalent to each other. Note that if you apply logarithm to your model function, you apply it also to the residuals, and residual sum of squares now means something different. This lead to different optimization problem and different results. \nAPI:\nscipy.optimize.least_squares\n","label":[[323,336,"Mention"],[1066,1094,"API"]],"Comments":[]}
{"id":60424,"text":"ID:40152367\nPost:\nText: I spoke with a few colleague and solve partly my problem. My conclusion is that my matrix is simply very ill conditioned... \nText: In my project, I can simplify my matrix by imposing boundary condition as follow: \nCode: DM[0,:] = 0\nDM[:,0] = 0\nDM[N-1,:] = 0\nDM[:,N-1] = 0\n\nText: which produces a matrix similar to that for N=4: \nCode: [[ 0     0               0               0]\n [ 0     -0.33333333     -1.             0]\n [ 0      1.             0.33333333      0]\n [ 0      0              0               0]]\n\nText: By using such condition, I obtain eigenvalues for sps.linalg.eigs which are equal to the one in scipy.linalg.eig. I also tried using Matlab, and it return the same values. \nText: To continue my work, I actually needed to use the generalized eigenvalue problem in the standard form \nText:  B x= DM x \nText: It seems that it does not work in my case because of my matrix B (which represents a Laplacian operator matrix). If you have a similar problem, I advise you to visit that question: https:\/\/scicomp.stackexchange.com\/questions\/10940\/solving-a-generalised-eigenvalue-problem \nText: (I think that) the matrix B needs to be positive definite to use scipy.sparse. A solution would be to change B, to use sp.linalg.eig or to use Matlab. I will confirm that later. \nText: EDIT: \nText: I wrote a solution to the stack exchange question I post above which explains how I solve my problem. I appears that eigs has indeed a bug if matrix B is not positive definite, and will return bad eigenvalues. \nAPI:\nscipy.sparse.linalg.eigs\nscipy.linalg.eig\nscipy.sparse.linalg.eigs\n","label":[[593,608,"Mention"],[1248,1261,"Mention"],[1444,1448,"Mention"],[1543,1567,"API"],[1568,1584,"API"],[1585,1609,"API"]],"Comments":[]}
{"id":60425,"text":"ID:40580320\nPost:\nText: As I noted in a comment, the method you were trying to use isn't available in newer versions of scipy, but it didn't do what you expected it to do anyway. \nText: My suggestion is to use numpy.interp1 or interp1d to construct a linear interpolator of your functions, then use fsolve as you did to find all possible intersections. Since fsolve (much like MATLAB's fzero) can only find a single intersection at a time, you indeed need to loop over sections in your data to look for intersections. \nCode: import scipy.interpolate as interpolate\nimport scipy.optimize as optimize\nimport numpy as np\n\nx1 = np.array([1.4,2.1,3,5.9,8,9,23])\ny1 = np.array([2.3,3.1,1,3.9,8,9,11])\nx2 = np.array([1,2,3,4,6,8,9])\ny2 = np.array([4,12,7,1,6.3,8.5,12])    \n\n# linear interpolators\nopts = {'fill_value': 'extrapolate'}\nf1 = interpolate.interp1d(x1,y1,**opts)\nf2 = interpolate.interp1d(x2,y2,**opts)\n\n# possible range for an intersection\nxmin = np.min((x1,x2))\nxmax = np.max((x1,x2))\n\n# number of intersections\nxuniq = np.unique((x1,x2))\nxvals = xuniq[(xmin<=xuniq) & (xuniq<=xmax)]\n# note that it's bad practice to compare floats exactly\n# but worst case here is a bit of redundance, no harm\n\n# for each combined interval there can be at most 1 intersection,\n# so looping over xvals should hopefully be enough\n# one can always err on the safe side and loop over a `np.linspace`\n\nintersects = []\nfor xval in xvals:\n    x0, = optimize.fsolve(lambda x: f1(x)-f2(x), xval)\n    if (xmin<=x0<=xmax\n        and np.isclose(f1(x0),f2(x0))\n        and not any(np.isclose(x0,intersects))):\n        intersects.append(x0)\n\nprint(intersects)\nprint(f1(intersects))\nprint(f2(intersects))\n\nText: Apart from a few runtime warnings from the more problematic sections of the algorithm, the above finds the two intersections of your functions: \nText: Key steps are checking that the results from fsolve are new (not close to your previous roots), and that you did actually find a root at the given x0. \nText: Alternatively, you could use the intervals defined in xvals, check the piecewise linear functions on each interval, and check analytically whether two lines with these parameters (I mean x1=xvals[i],x2=xvals[i+1], y11=f1[x1], y12=f1[x2] etc) have an intersection on a given segment. You will likely be able to vectorize this approach, you won't have to worry about stochasticity in your results, and you only have to watch out for possible singularities in the data (where np.diff(xvals) is small, and you were to divide by this). \nText: 1numpy.interp doesn't define an interpolator function, rather it directly computes the interpolated values on a grid. In order to do the same using this function, you would have to define a function that calls numpy.interp for the given x value. This would likely be less efficient than the above, due to the high number of function evaluations during the zero search. \nAPI:\nscipy.interpolate.interp1d\n","label":[[227,235,"Mention"],[2910,2936,"API"]],"Comments":[]}
{"id":60426,"text":"ID:40598136\nPost:\nText: You could formulate the curve fitting problem as a constrained optimization problem and use minimize to solve it. Considering a data set for which the optimal a should be positive, it follows that the requirement on the range of the fitted function is equivalent to the constraints a*np.log(3200)+b>=0 and a*np.log(42000)+b<=100. \nText: One can proceed as follows (I used a simple data set). \nCode: from scipy.optimize import minimize\n\n\nx = np.array([3200, 14500, 42000])\nyn = np.array([0, 78, 100])\n\ndef LS_obj(p):\n    a, b = p\n    return ((log_func(x, a, b) - yn)**2).sum()\n\ncons = ({'type': 'ineq', 'fun': lambda p: p[0] * np.log(3200) + p[1]},\n        {'type': 'ineq', 'fun': lambda p: 100 -p[0] * np.log(42000) - p[1]})\n\np0 = [10,-100] #initial estimate \nsol = minimize(LS_obj,p0 ,constraints=cons)\nprint(sol.x)  #optimal parameters\n\nText: [ 36.1955 -285.316 ] \nText: The following figure compares the curve_fit and minimize solutions. As expected, the minimize solution is within the required range. \nAPI:\nscipy.optimize.minimize\n","label":[[116,124,"Mention"],[1036,1059,"API"]],"Comments":[]}
{"id":60427,"text":"ID:40833299\nPost:\nText: Like Warren said, si.odeint is the 'SciPy' way to solve this. \nText: But before you take your problem to SciPy (or whatever solver you end up using) you'll want to convert your 2nd order ODE to a first order ODE using something like: http:\/\/tutorial.math.lamar.edu\/Classes\/DE\/SystemsDE.aspx \nText: To get things into SciPy you need to get your equation looking like: \nText: y' = f(y) \nText: But right now your equation is written like: \nText: y'' = f(y, y') \nText: The solution is to add more variables to your system, but the link will explain it more thoroughly. \nAPI:\nscipy.integrate.odeint\n","label":[[42,51,"Mention"],[595,617,"API"]],"Comments":[]}
{"id":60428,"text":"ID:40836526\nPost:\nText: No. \nText: Here is what RegularGridInterpolator does under the hood: \nCode: class CartesianGrid(object):\n    \"\"\"\n    Linear Multivariate Cartesian Grid interpolation in arbitrary dimensions\n    This is a regular grid with equal spacing.\n    \"\"\"\n    def __init__(self, limits, values):\n        self.values = values\n        self.limits = limits\n\n    def __call__(self, *coords):\n        # transform coords into pixel values\n        coords = numpy.asarray(coords)\n        coords = [(c - lo) * (n - 1) \/ (hi - lo) for (lo, hi), c, n in zip(self.limits, coords, self.values.shape)]\n\n        return scipy.ndimage.map_coordinates(self.values, coords, \n            cval=numpy.nan, order=1)\n\nText: https:\/\/github.com\/JohannesBuchner\/regulargrid\/blob\/master\/regulargrid\/cartesiangrid.py \nText: It uses map_coordinates to do the linear interpolation. coords contains the location in pixel coordinates. You should be able to use these weights, and the lower and upper values at each dimension to figure out how steep the interpolation rises. \nText: However, the gradient also depends on the values of the corner points. \nText: You can find the math here: https:\/\/en.wikipedia.org\/wiki\/Trilinear_interpolation \nAPI:\nscipy.interpolate.RegularGridInterpolator\nscipy.ndimage.map_coordinates\n","label":[[48,71,"Mention"],[816,831,"Mention"],[1227,1268,"API"],[1269,1298,"API"]],"Comments":[]}
{"id":60429,"text":"ID:40920867\nPost:\nText: In my humble opinion, the simplest way of doing this, is to define new function, for example: \nCode: def difference(x):\n   return f1(x)-f2(x)\n\nText: and then using sp.optimize.brentq \nAPI:\nscipy.optimize.brentq\n","label":[[188,206,"Mention"],[213,234,"API"]],"Comments":[]}
{"id":60430,"text":"ID:40944356\nPost:\nText: nanmean was a deprecated function that was removed from stats in version 0.18.0. You will have to either use an older version of SciPy or use the equivalent function from NumPy. \nCode: from numpy import nanmean\n\nAPI:\nscipy.stats\n","label":[[80,85,"Mention"],[241,252,"API"]],"Comments":[]}
{"id":60431,"text":"ID:41121152\nPost:\nText: You must pass a Jacobian function to use the dogleg method, as it is a gradient-based optimization method. If you look at the jac argument of sp.optimize.minimize it says \nText: jac : bool or callable, optional Jacobian (gradient) of objective function. Only for CG, BFGS, Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If jac is a Boolean and is True, fun is assumed to return the gradient along with the objective function. If False, the gradient will be estimated numerically. jac can also be a callable returning the gradient of the objective. In this case, it must accept the same arguments as fun. \nText: Then if you look down at the Notes at the bottom of the page: \nText: Method dogleg uses the dog-leg trust-region algorithm for unconstrained minimization. This algorithm requires the gradient and Hessian; furthermore the Hessian is required to be positive definite. \nText: Some gradient-based methods do not require an explicit Jacobian (and\/or Hessian), as they will use a differencing method to approximate them. However dogleg does need you to explicitly pass such a function. \nAPI:\nscipy.optimize.minimize\n","label":[[166,186,"Mention"],[1126,1149,"API"]],"Comments":[]}
{"id":60432,"text":"ID:41148016\nPost:\nText: The SLSQP code in scipy can do this. You can use minimize with method='SLSQP, or you can use the function fmin_slsqp directly. In the following, I use fmin_slsqp. \nText: The scipy solvers generally pass a one-dimensional array to the objective function, so to be consistent, I'll change W and X1 to be 1-d arrays, and I'll write the objective function (now called w_rss1) to expect a 1-d argument w. \nText: The condition that all the elements in w must be between 0 and 1 is specified using the bounds argument, and the condition that the sum must be 1 is specified using the f_eqcons argument. The constraint function returns np.sum(w) - 1, so it is 0 when the sum of the elements is 1. \nText: Here's the code: \nCode: import numpy as np\nfrom scipy.optimize import fmin_slsqp\n\n\ndef w_rss1(w, x0, x1):\n    predictions = np.dot(x0, w)\n    errors = x1 - predictions\n    rss = (errors**2).sum()\n    return rss\n\n\ndef sum1constraint(w, x0, x1):\n    return np.sum(w) - 1\n\n\nX0 = np.array([[3,4,5,3],\n               [1,2,2,4],\n               [6,5,3,7],\n               [1,0,5,2]])  \n\nX1 = np.array([4, 2, 4, 2]) \n\nW = np.array([.0, .5, .5, .0])\n\nresult = fmin_slsqp(w_rss1, W, f_eqcons=sum1constraint, bounds=[(0.0, 1.0)]*len(W),\n                    args=(X0, X1), disp=False, full_output=True)\nWopt, fW, its, imode, smode = result\n\nif imode != 0:\n    print(\"Optimization failed: \" + smode)\nelse:\n    print(Wopt)\n\nText: When I run this, the output is \nCode: [ 0.05172414  0.55172414  0.39655172  0.        ]\n\nAPI:\nscipy.optimize.minimize\n","label":[[73,81,"Mention"],[1528,1551,"API"]],"Comments":[]}
{"id":60433,"text":"ID:41302948\nPost:\nText: I found a solution with griddata but I'm not sure that's the best one. \nText: I interpolate data with the nearest method parameter which returns the value at the data point closest to the point of interpolation. \nCode: points = np.meshgrid(np.linspace(0, 1200, data.shape[1]),\n                     np.linspace(0, 1200, data.shape[0]))\npoints = zip(points[0].flatten(), points[1].flatten())\nxi = np.meshgrid(np.arange(1200), np.arange(1200))\nxi = zip(xi[0].flatten(), xi[1].flatten())\n\ntck = griddata(np.array(points), data.flatten(), np.array(xi), method='nearest')\ndata = tck.reshape((1200, 1200))\n\nAPI:\nscipy.interpolate.griddata\n","label":[[48,56,"Mention"],[629,655,"API"]],"Comments":[]}
{"id":60434,"text":"ID:41373955\nPost:\nText: butter is generating an unstable filter: \nCode: In [17]: z, p, k = signal.tf2zpk(b, a)\n\nIn [18]: np.max(np.abs(p))\nOut[18]: 1.0005162676670694\n\nText: For a stable filter, that maximum must be less than 1. Unfortunately, the code doesn't warn you about this. \nText: I suspect the problem is b1, not b2. In the normalized units, you are trying to create a lower cutoff of 2.1e-4, which is pretty small. If, for example, the lower cutoff is 200.0\/nyq, the filter is stable: \nCode: In [13]: b, a = signal.butter(5, [200.0 \/ nyq, 20000.0 \/ nyq], btype='band')\n\nIn [14]: z, p, k = signal.tf2zpk(b, a)\n\nIn [15]: np.max(np.abs(p))\nOut[15]: 0.99601892668982284\n\nText: Instead of using the (b, a) format for the filter, you can use the more robust sos (second order sections) format, which was added to scipy version 0.16. To use it, change these two lines \nCode: b, a = signal.butter(5, [20.0 \/ nyq, 20000.0 \/ nyq], btype='band')\nx = signal.lfilter(b, a, x)\n\nText: to \nCode: sos = signal.butter(5, [20.0 \/ nyq, 20000.0 \/ nyq], btype='band', output='sos')\nx = signal.sosfilt(sos, x)\n\nText: That SOS filter does not suffer from the instability problem. \nAPI:\nscipy.signal.butter\n","label":[[24,30,"Mention"],[1172,1191,"API"]],"Comments":[]}
{"id":60435,"text":"ID:41418241\nPost:\nText: The first argument of linkage should not be the square distance matrix. It must be the condensed distance matrix. In your case, that would be np.array([2.0, 3.8459253727671276e-16, 2]). You can convert from the square distance matrix to the condensed form using spsp.distance.squareform \nText: If you pass a two dimensional array to linkage with shape (m, n), it treats it as an array of m points in n-dimensional space and it computes the distances of those points itself. That's why you didn't get an error when you passed in the square distance matrix--but you got an incorrect plot. (This is an undocumented \"feature\" of linkage.) \nText: Also note that because the distance 3.8e-16 is so small, the horizontal line associated with the link between points 0 and 2 might not be visible in the plot--it is on the x axis. \nText: Here's a modified version of your script. For this example, I've changed that tiny distance to 0.1, so the associated cluster is not obscured by the x axis. \nCode: import numpy as np\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\n\nimport matplotlib.pyplot as plt\n\n\nmat = np.array([[0.0, 2.0, 0.1], [2.0, 0.0, 2.0], [0.1, 2.0, 0.0]])\ndists = squareform(mat)\nlinkage_matrix = linkage(dists, \"single\")\ndendrogram(linkage_matrix, labels=[\"0\", \"1\", \"2\"])\nplt.title(\"test\")\nplt.show()\n\nText: Here is the plot created by the script: \nAPI:\nscipy.spatial.distance.squareform\n","label":[[286,310,"Mention"],[1441,1474,"API"]],"Comments":[]}
{"id":60436,"text":"ID:41664700\nPost:\nText: I guess you are looking for rv_discrete here. From the docs: \nText: rv_discrete is a base class to construct specific distribution classes and instances for discrete random variables. It can also be used to construct an arbitrary distribution defined by a list of support points and corresponding probabilities. \nText: Example from docs: \nCode: from scipy import stats\nxk = np.arange(7)\npk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)\ncustm = stats.rv_discrete(name='custm', values=(xk, pk))\n\nText: And your example: \nCode: In [1]: import numpy as np\n\nIn [2]: from scipy import stats\n\nIn [3]: custm = stats.rv_discrete(name='custm', values=((1, 2, 3), (1.\/3, 1.\/3, 1.\/3)))\n\nIn [4]: custm.cdf(2.5)\nOut[4]: 0.66666666666666663\n\nAPI:\nscipy.stats.rv_discrete\n","label":[[52,63,"Mention"],[748,771,"API"]],"Comments":[]}
{"id":60437,"text":"ID:41731906\nPost:\nText: You can simply use convolve and define the weights (filter) yourself. There is also a specialized and optimized rectangular filter function: uniform_filter \nText: For example to apply a width 3 mean-filter (multiply by ndim*size if you want a sum-filter): \nCode: >>> from scipy.ndimage import uniform_filter\n>>> uniform_filter([1,4,2,56,2,3,6,1,3,1,3], size=3)\narray([ 2,  2, 20, 19, 20,  3,  3,  3,  1,  2,  2])\n\nText: This can also be applied to multidimensional arrays: \nCode: >>> uniform_filter(np.random.randint(0, 20, (10, 10)), size=3)  # 3x3 filter\narray([[ 6,  7, 10,  9,  9,  7,  5,  7,  9, 12],\n       [ 6,  7,  9,  9,  7,  6,  5,  5,  7,  9],\n       [ 5,  8,  8,  9,  7,  6,  4,  4,  6,  8],\n       [ 9, 10,  9, 10,  8,  6,  4,  4,  8, 11],\n       [10, 12,  9, 10,  9, 10,  8,  8,  9, 10],\n       [12, 12,  9, 10, 10, 10,  9,  9,  9,  9],\n       [12, 11,  9,  8,  7,  8,  7,  8,  6,  5],\n       [11, 10,  9,  9,  9,  9,  6,  8,  8,  9],\n       [12,  9,  7,  7,  9,  8,  6,  6,  6,  8],\n       [12,  9,  8,  9, 12, 10,  7,  5,  6,  9]])\n>>> uniform_filter(np.random.randint(0, 20, (10, 10)), size=(5, 3))  # 5x3 filter\narray([[ 7,  7,  8, 10, 11, 10, 11, 11, 12, 12],\n       [ 7,  7,  7,  9, 10, 11, 11, 10,  9,  9],\n       [ 7,  6,  6,  8,  8,  9,  9,  9,  8,  8],\n       [ 6,  6,  6,  8,  8,  9,  8,  7,  7,  8],\n       [ 8,  8,  7,  9,  8, 11,  8,  8,  6,  6],\n       [ 7,  8,  7,  9,  8, 11,  7,  7,  5,  7],\n       [ 9,  8,  8,  9,  9,  9,  7,  7,  7,  8],\n       [ 8,  8,  8,  9, 10, 10,  8,  7,  6,  6],\n       [ 9,  7,  6,  8,  9, 10,  8,  7,  6,  6],\n       [ 9,  6,  6,  7, 10,  8,  8,  6,  6,  6]])\n\nAPI:\nscipy.ndimage.convolve\n","label":[[43,51,"Mention"],[1651,1673,"API"]],"Comments":[]}
{"id":60438,"text":"ID:41944372\nPost:\nText: sp.special.lpmv is vectorized. \nAPI:\nscipy.special.lpmv\n","label":[[24,39,"Mention"],[61,79,"API"]],"Comments":[]}
{"id":60439,"text":"ID:42136782\nPost:\nText: Here is the solution of your problem : \nText: first, if you use cubic interpolation, you need at least 4 values for a and 4 values for b (scipy.interpolate.interp1d with kind=\"cubic\" is not working otherwise) second, you can not interpolate values with sp.interpolate.interp1d that are not in the range you define (the range of b times) \nText: I changed a bit your initial code to show you : \nCode: time_a_full = ['30-01-2017 12:02:15.880922','30-01-2017 12:02:16.880922','30-01-2017 12:02:17.880922','30-01-2017 12:02:18.880922','30-01-2017 12:02:19.880922','30-01-2017 12:02:22.880922']\ntime_b_full = ['30-01-2017 12:02:15.123444','30-01-2017 12:02:16.880919','30-01-2017 12:02:18.880920', '30-01-2017 12:02:19.880922','30-01-2017 12:02:20.880922']\n\n# Here I transform the time in seconds as suggested\ntime_a = np.array([time.mktime(datetime.strptime(s, \"%d-%m-%Y %H:%M:%S.%f\").timetuple()) for s in time_a_full])\ntime_b = np.array([time.mktime(datetime.strptime(s, \"%d-%m-%Y %H:%M:%S.%f\").timetuple()) for s in time_b_full])\n\nvalues_a = np.array([100,100,110,99,96,95])\nvalues_b = np.array([10,12,13,16,20])\n\n# result of the linear interp with the numpy function\nnp.interp(time_a, time_b, values_b)\n\n# result of the cubic interpolation\nf = interpolate.interp1d(time_b,values_b, kind=\"cubic\")\ntime_a[time_a<time_b.min()]=time_b.min() # use this to stay on range define by the times of b\ntime_a[time_a>time_b.max()]=time_b.max() # use this to stay on range define by the times of b\nf(time_a)\n\nAPI:\nscipy.interpolate.interp1d\n","label":[[277,300,"Mention"],[1523,1549,"API"]],"Comments":[]}
{"id":60440,"text":"ID:42142784\nPost:\nText: As you can read here, the sp.stats.ttest_ind has two outputs \nText: The calculated t-statistic. The two-tailed p-value. \nText: Very intuitively, you can read the t-statistic as a normalized difference of averages in both populations, considering their variances and sizes: \nText: The larger are the samples, the more serious the difference of averages is because we have more evidence for that. The larger are the variances, the less serious the difference of averages is because the absolute difference can be given by randomness only. \nText: The higher is the value of the t-statistic, the more serious is the difference. \nText: The p-value makes this intuition more explicit: it is the probability that the difference of averages can be considered as zero. If the p-value is bellow a threshold, e.g. 0.05, we say that the difference in not zero. \nAPI:\nscipy.stats.ttest_ind\n","label":[[50,68,"Mention"],[879,900,"API"]],"Comments":[]}
{"id":60441,"text":"ID:42143851\nPost:\nText: I think the root approach holds water, but steering clear of the trivial solution might be the real challenge for this system of equations. \nText: In any event, this function uses root to solve the system of equations. \nCode: def solver(x0, alpha0, K, A, a):\n'''\nx0     - nx1 numpy array. Initial guess on x.\nalpha0 - nx1 numpy array. Initial guess on alpha.\nK      - nxn numpy.array.\nA      - Length N List of nxn numpy.arrays.\na      - Length N list of nx1 numpy.arrays.\n'''\n\n# Establish the function that produces the rhs of the system of equations.\nn = K.shape[0]\nN = len(A)\ndef lhs(x_alpha):\n    '''\n    x_alpha is a concatenation of x and alpha.\n    '''\n\n    x = np.ravel(x_alpha[:n])\n    alpha = np.ravel(x_alpha[n:])\n    lhs_top = np.ravel(K.dot(x))\n    for k in xrange(N):\n        lhs_top += alpha[k]*(np.ravel(np.dot(A[k], x)) + np.ravel(a[k]))\n\n    lhs_bottom = [0.5*x.dot(np.ravel(A[k].dot(x))) + np.ravel(a[k]).dot(x)\n                  for k in xrange(N)]\n\n    lhs = np.array(lhs_top.tolist() + lhs_bottom)\n\n    return lhs\n\n# Solve the system of equations.\nx0.shape = (n, 1)\nalpha0.shape = (N, 1)\nx_alpha_0 = np.vstack((x0, alpha0))\nsol = root(lhs, x_alpha_0)\nx_alpha_root = sol['x']\n\n# Compute norm of residual.\nres = sol['fun']\nres_norm = np.linalg.norm(res)\n\n# Break out the x and alpha components.\nx_root = x_alpha_root[:n]\nalpha_root = x_alpha_root[n:]\n\n\nreturn x_root, alpha_root, res_norm\n\nText: Running on the toy example, however, only produces the trivial solution. \nCode: # Toy example.\nn = 4\nN = 2\nK = np.matrix([[0.5, 0, 0, 0], [0, 1, 0, 0],[0,0,1,0], [0,0,0,0.5]])\nA_1 = np.matrix([[0.98,0,0.46,0.80],[0,0,0.56,0],[0.93,0.82,0,0.27],      \n                [0,0,0,0.23]])\nA_2 = np.matrix([[0.23, 0,0,0],[0.03,0.01,0,0],[0,0.32,0,0],\n      [0.62,0,0,0.45]])\na_1 = np.matrix(scipy.rand(4,1))\na_2 = np.matrix(scipy.rand(4,1))\nA = [A_1, A_2]\na = [a_1, a_2]\nx0 = scipy.rand(n, 1)\nalpha0 = scipy.rand(N, 1)\n\nprint 'x0 =', x0\nprint 'alpha0 =', alpha0\n\nx_root, alpha_root, res_norm = solver(x0, alpha0, K, A, a)\n\nprint 'x_root =', x_root\nprint 'alpha_root =', alpha_root\nprint 'res_norm =', res_norm\n\nText: Output is \nCode: x0 = [[ 0.00764503]\n [ 0.08058471]\n [ 0.88300129]\n [ 0.85299622]]\nalpha0 = [[ 0.67872815]\n [ 0.69693346]]\nx_root = [  9.88131292e-324  -4.94065646e-324   0.00000000e+000        \n          0.00000000e+000]\nalpha_root = [ -4.94065646e-324   0.00000000e+000]\nres_norm = 0.0\n\nAPI:\nscipy.optimize.root\n","label":[[36,40,"Mention"],[2443,2462,"API"]],"Comments":[]}
{"id":60442,"text":"ID:42184536\nPost:\nText: The numpy (used by you) and sp.stats (used by ks test) versions of uniform work differently: \nCode: >>> np.random.uniform(2,3,5000).max()\n2.9999333044165271\n>>> stats.uniform(2,3).rvs(5000).max()\n4.9995316751114043\n\nText: In numpy the second parameter is interpreted as the upper bound, in stats it is the scale paramter, i.e. the width. \nAPI:\nscipy.stats\nscipy.stats\n","label":[[52,60,"Mention"],[314,319,"Mention"],[368,379,"API"],[380,391,"API"]],"Comments":[]}
{"id":60443,"text":"ID:42198582\nPost:\nText: The function rdiff computes the derivative, but it assumes that the input is periodic. The period argument gives the period (i.e. the total length of the x interval) of the input sequence. \nText: In your case, this is len(x)*dx where dx = x[1] - x[0]. \nText: Here's some code that plots the simple (centered) finite difference (in blue) and the result of diff using the period argument (in red). The variables x and y are the same as those used in your code: \nCode: In [115]: plt.plot(0.5*(x[1:]+x[:-1]), np.diff(y)\/np.diff(x), 'b')\nOut[115]: [<matplotlib.lines.Line2D at 0x1188d01d0>]\n\nIn [116]: plt.plot(x, sp.diff(y, period=len(x)*(x[1]-x[0])), 'r')\nOut[116]: [<matplotlib.lines.Line2D at 0x1188fc9d0>]\n\nIn [117]: plt.xlabel('x')\nOut[117]: <matplotlib.text.Text at 0x1157425d0>\n\nText: Note that if your input is not actually periodic, the derivative computed by diff will be inaccurate near the ends of the interval. \nText: Here's another example, using a shorter sequence that contains just one full period of a sine function in the interval [0, 1]: \nCode: In [149]: x = np.linspace(0, 1, 20, endpoint=False)\n\nIn [150]: y = np.sin(2*np.pi*x)\n\nIn [151]: plt.plot(0.5*(x[1:]+x[:-1]), np.diff(y)\/np.diff(x), 'b')\nOut[151]: [<matplotlib.lines.Line2D at 0x119872d90>]\n\nIn [152]: plt.plot(x, sp.diff(y, period=len(x)*(x[1]-x[0])), 'r')\nOut[152]: [<matplotlib.lines.Line2D at 0x119c49090>]\n\nIn [153]: plt.xlabel('x')\nOut[153]: <matplotlib.text.Text at 0x1197823d0>\n\nAPI:\nscipy.fftpack.diff\n","label":[[37,42,"Mention"],[1492,1510,"API"]],"Comments":[]}
{"id":60444,"text":"ID:42505919\nPost:\nText: you can also use griddata : points = np.array( (X.flatten(), Y.flatten()) ).T values = Z.flatten() from inp import griddata Z0 = griddata( points, values, (X0,Y0) ) X0 and Y0 can be arrays or even a grid. you can also choose the interpolation with method= perhaps you can find a way to get ride of the flatten(), but it should work. \nText: (https:\/\/docs.scipy.org\/doc\/scipy\/reference\/tutorial\/interpolate.html) \nAPI:\nscipy.interpolate\n","label":[[128,131,"Mention"],[441,458,"API"]],"Comments":[]}
{"id":60445,"text":"ID:42532116\nPost:\nText: np.linalg.solve only works for array-like objects. For example it would work on a np.ndarray or np.matrix (Example from the numpy documentation): \nCode: import numpy as np\n\na = np.array([[3,1], [1,2]])\nb = np.array([9,8])\nx = np.linalg.solve(a, b)\n\nText: or \nCode: import numpy as np\n\na = np.matrix([[3,1], [1,2]])\nb = np.array([9,8])\nx = np.linalg.solve(a, b)\n\nText: or on A.todense() where A=scipy.sparse.csr_matrix(np.matrix([[3,1], [1,2]])) as this returns a np.matrix object. \nText: To work with a sparse matrix, you have to use spsolve (as already pointed out by rakesh) \nCode: import numpy as np\nimport scipy.sparse\nimport scipy.sparse.linalg\n\na = scipy.sparse.csr_matrix(np.matrix([[3,1], [1,2]]))\nb = np.array([9,8])\nx = scipy.sparse.linalg.spsolve(a, b)\n\nText: Note that x is still a np.ndarray and not a sparse matrix. A sparse matrix will only be returned if you solve Ax=b, with b being a matrix and not a vector. \nAPI:\nscipy.sparse.linalg.spsolve\n","label":[[558,565,"Mention"],[957,984,"API"]],"Comments":[]}
{"id":60446,"text":"ID:42590090\nPost:\nText: Mathematically speaking Binomial is more precise in this case than Poisson. For example, using Poisson you'll get a positive probability of more than 18 of your 18 candidates making the conversion. Poisson owes its popularity to being easier to compute. \nText: The result also depends on your prior knowledge. For example if both your outcomes look very high compared to typical conversion rates then all else being equal the difference you see is more significant. \nText: Assuming no prior knowledge, i.e. assuming that every conversion rate between 0 and 1 is equally likely if you know nothing else, the probability of a given conversion rate r once you take into account your observation of 6 out of 18 possible conversions is given by the Beta distribution, in this case Beta(r; 6+1, 18-6+1) \nText: Technically speaking this is not a probability but a likelihood. The difference is the following: a probablity describes how often you will observe different outcomes if you compare \"parallel universes\" that are identical, even though reputable statisticians probably wouldn't use that terminology. A likelihood is the other way round: given a fixed outcome comparing different universes how often will you observe a particular kind of universe. (To be even more technical, this description is only fully correct if as we did a \"flat prior\" is assumed.) In your case there are two kinds of universe, one where A is better than B and one where B is better than A. \nText: The probability of B being better than A is then \nText: integral_0^1 dr Beta_cdf(r; 6+1, 18-6+1) x Beta_pdf(r; 8+1, 18-8+1) \nText: You can use beta and quad to calculate that and you'll get a 0.746 probability of B being better than A: \nCode: quad(lambda r: beta(7, 13).cdf(r) * beta(9,11).pdf(r), 0, 1)\n# (0.7461608994979401, 1.3388378385104094e-08)\n\nText: To conclude, by this measure the evidence for B being better than A is not very strong. \nText: UPDATE: \nText: The two step case can be solved conceptually similarly, but is a bit more challenging to compute. \nText: We have two steps 135 \/ 144 -> 18 -> 8 \/ 6. Given these numbers how are the conversion rates for A and B and step 1 and step 2 distributed? Ultimately we are interested in the product of step 1 and step 2 for A and for B. Since I couldn't get scipy to solve the integrals in reasonable time I fell back to a Monte Carlo scheme. Just draw the conversion rates with appropriate probabilites N=10^7 times and count how often B is better than A: \nCode: (beta(9,11).rvs(N)*beta(19,118).rvs(N) > beta(7,13).rvs(N)*beta(19,127).rvs(N)).mean()\n\nText: The result is very similar to the single step one: 0.742 in favour of B. Again, not very strong evidence. \nAPI:\nscipy.stats.beta\nscipy.integrate.quad\n","label":[[1641,1645,"Mention"],[1650,1654,"Mention"],[2726,2742,"API"],[2743,2763,"API"]],"Comments":[]}
{"id":60447,"text":"ID:42597244\nPost:\nText: The least squares FIR filter design function in scipy is firls (not scipy.signal.firwin). \nText: firls requires an odd number of taps, so you'll have to ensure that filter_order is odd. \nText: If firwin is actually the function that you meant to use, then take another look at the docstring. In particular: \nText: firwin does not take an argument for the ideal response. It is only given the band edges in the cutoff argument. The description of the cutoff argument specifically says this argument must not contain 0 and the Nyquist frequency. You appear to be creating a bandpass filter. There is an example of this in the docstring: Band-pass: >>> f1, f2 = 0.1, 0.2 >>> signal.firwin(numtaps, [f1, f2], pass_zero=False) array([ 0.06301614, 0.88770441, 0.06301614]) The first argument of firwin must be an integer, not a float. \nText: Here's how you implement your filter using firwin: \nCode: lower = lower_filter_bound\/nyquist\nupper = upper_filter_bound\/nyquist\nfilterweights = sig.firwin(int(filter_order), [lower, upper], pass_zero=False)\n\nText: If you need more flexibility in the design of your FIR filter, take a look at scipy.signal.firwin2. \nAPI:\nscipy.signal.firls\n","label":[[81,86,"Mention"],[1180,1198,"API"]],"Comments":[]}
{"id":60448,"text":"ID:42807804\nPost:\nText: The documentation of norm says for its fit function \nText: fit(data, loc=0, scale=1) Parameter estimates for generic data. \nText: To me this is highly ununderstandable and I'm pretty sure that one cannot expect this function to return a fit in the usual sense. \nText: However, to fit a gaussian is rather straight forward: \nCode: from __future__ import division\nimport numpy as np\n\nx=np.linspace(-50,50,100)\nsig=10\nmu=0\ny=1\/np.sqrt(2*sig*sig*np.pi)*np.exp(-(x-mu)*(x-mu)\/(2*sig*sig))  #\n\ndef gaussian_fit(xdata,ydata):\n    mu = np.sum(xdata*ydata)\/np.sum(ydata)\n    sigma = np.sqrt(np.abs(np.sum((xdata-mu)**2*ydata)\/np.sum(ydata)))\n    return mu, sigma\n\nprint gaussian_fit(x,y)\n\nText: This prints (-7.474196315587989e-16, 9.9999422983567516) which is sufficiently close to the expected values of (0, 10). \nAPI:\nscipy.stats.norm\n","label":[[45,49,"Mention"],[836,852,"API"]],"Comments":[]}
{"id":60449,"text":"ID:43014672\nPost:\nText: Yes, sps.save_npz \/ load_npz are new in version 0.19.0 http:\/\/scipy.github.io\/devdocs\/release.0.19.0.html \nAPI:\nscipy.sparse.save_npz\n","label":[[29,41,"Mention"],[136,157,"API"]],"Comments":[]}
{"id":60450,"text":"ID:43135480\nPost:\nText: While on the surface it seems like a good idea to restrict the search space by setting the cost function to a high value, this approach has a few drawbacks. \nText: You need to be sure that this value dominates the cost function. The optimizer does not know that the range is forbidden. It just sees a high cost but that does not prevent it to go there. You are creating a plateau in the cost function. When the optimizer runs into the forbidden area it may not know how to get out because the function looks the same in every direction. \nText: In general, it is better to use a bounded or constrained optimizer. That way the algorithm knows where it may not go. Usually I use opt.minimize and let it choose automatically which optimization algorithm to use, based on the information (cost function, derivatives, bounds, constraints) it gets. \nText: However, if you really need to use a specific optimization algorithm that does not take bounds or constraints you can help it get back into the feasible region by making the cost function slope towards positive values. For example: \nCode: if (coeff[0] or coeff[1] or coeff[2] or coeff[3]) < 0: \n    return (np.minimum(coef, 0)**2 + 1) * 100000000.0\n\nAPI:\nscipy.optimize.minimize\n","label":[[700,712,"Mention"],[1228,1251,"API"]],"Comments":[]}
{"id":60451,"text":"ID:43156913\nPost:\nText: To make this answer somehow useful for other people, find here first a general explanation. Below there is a more concrete solution to the question. \nText: The general explanation, np.meshgrid vs. np.mgrid in the use with scipy.interpolate.griddata. \nText: I here provide an example which compares the use of np.meshgrid with np.mgrid when it comes to interpolation with scipy.interpolate.griddata. Gnerally speaking, the returns of np.meshgrid are the transposed returns of np.mgrid for the same grid. \nCode: import numpy as np; np.random.seed(0)\nimport scipy.interpolate\nimport matplotlib.pyplot as plt\n\n# np. meshgrid\nxgrid = np.arange(21)[::2]\nygrid = np.linspace(0,5,6)\nXgrid, Ygrid = np.meshgrid(xgrid, ygrid)\n\n# np. mgrid\nXgrid2, Ygrid2 = np.mgrid[0:20:11j,0:5:6j]\n\n# points for interpolation\npoints = np.random.rand(200, 2)\npoints[:,0] *= 20 \npoints[:,1] *= 5\n\n# values\nf = lambda x,y: np.sin(x)+ y\nvalues = f(points[:,0], points[:,1])\n\n# initerpolation using grid defined with np.meshgrid\nresampled = scipy.interpolate.griddata(points, values, (Xgrid2, Ygrid2), method='cubic')\n\n# interpolation using grid defined with np.mgrid\nresampled2 = scipy.interpolate.griddata(points, values, (Xgrid.T, Ygrid.T), method='cubic')\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(3,1)\nkws = dict( extent=[-1,21,-0.5,5.5], vmin=-1, vmax=6, origin=\"lower\")\nax1.set_title(\"function evaluated on grid\")\nax1.imshow(f(Xgrid, Ygrid), **kws)\n\nax2.set_title(\"interpolation using grid defined with np.meshgrid\")\nax2.imshow(resampled.T, **kws)\n\nax3.set_title(\"interpolation using grid defined with np.mgrid\")\nax3.imshow(resampled2.T, **kws)\n\nfor ax in (ax1, ax2, ax3):\n    ax.set_yticks(range(6))\n    ax.set_xticks(range(21)[::2])\n\nplt.tight_layout()\nplt.show()\n\nText: Now to the question and its solution. \nText: Step 1. Create a MCVE \nText: (can be omitted, since more experienced users create those themselves when asking a question) \nCode: import numpy as np; np.random.seed(0)\nimport scipy.interpolate\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\na = np.random.rand(532, 7)\ndfList = [pd.DataFrame(a[:,i], columns=[\"Y\"]) for i in range(7)]\n\n# Meshgrid\nxgrid = dfList[0].index.tolist()\nygrid = np.linspace(266, 1, 532)\nXgrid, Ygrid = np.meshgrid(xgrid, ygrid)\n\n# Points\nxo = dfList[0].index.tolist()\nyo = [266, 300, 350, 400, 450, 500, 532]    # one for each DataFrame\npoints = [ [x, y] for y in yo for x in xo]\npoints = np.array(points)\n\n# Values\nvalues = []\nfor df in dfList:\n    values.extend(df['Y'].real)\n\nvalues = np.array(values)\n\n# Griddata\nresampled = scipy.interpolate.griddata(points, values, (Xgrid, Ygrid), method='cubic')\n\nplt.imshow(resampled.T, extent=[365,1099,266,532], origin='lower')\nplt.show()\n\nText: creates \nText: Step 2. The Problem. \nText: We see a blank plot with only a small line of dots in the left handside of the image, while we would expect the complete graph to be filled with an image of shape (266, 532). \nText: Step 3. The solution. \nText: Using griddata we need to supply the grids to the xi argument as a tuple (Xgrid.T, Ygrid.T), where the grids are generated via numpy.meshgrid: Xgrid, Ygrid = np.meshgrid(xgrid, ygrid). Note that meshgrid is different from numpy.mgrid. \nText: There are some other inconsistencies with the points of the meshgrid compared to the sample points, so here I assume that you want to have the values between 266 and 532 being interpolated. \nCode: import numpy as np; np.random.seed(0)\nimport scipy.interpolate\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\na = np.random.rand(532, 7)\ndfList = [pd.DataFrame(a[:,i], columns=[\"Y\"]) for i in range(7)]\n\n# Meshgrid\nxgrid = dfList[0].index.values\nygrid = np.arange(266,532)\nXgrid, Ygrid = np.meshgrid(xgrid, ygrid)\n\n# Points\nxo = dfList[0].index.tolist()\nyo = [266, 300, 350, 400, 450, 500, 532]    # one for each DataFrame\npoints = [ [x, y] for y in yo for x in xo]\npoints = np.array(points)\nprint points.shape\n\n# Values\nvalues = []\nfor df in dfList:\n    values.extend(df['Y'].real)\nvalues = np.array(values)\n\n# Griddata\nresampled = scipy.interpolate.griddata(points, values, (Xgrid.T, Ygrid.T), method='cubic')\nprint resampled.T.shape\nplt.imshow(resampled.T, extent=[365,1099,266,532], origin='lower') #, \n\nplt.show()\n\nAPI:\nscipy.interpolate.griddata\n","label":[[2993,3001,"Mention"],[4256,4282,"API"]],"Comments":[]}
{"id":60452,"text":"ID:43267450\nPost:\nText: Try scipy.sparse.csr_matrix: \nCode: from scipy.sparse import *\nfrom scipy import *\na=csr_matrix( (2750086,1000), dtype=int8 )\n\nText: Then a is \nCode: <2750086x1000 sparse matrix of type '<class 'numpy.int8'>'\n    with 0 stored elements in Compressed Sparse Row format>\n\nText: For example, if you do: \nCode: from scipy.sparse import *\nfrom scipy import *\na=csr_matrix( (5,4), dtype=int8 ).todense()\nprint(a)\n\nText: You get: \nCode: [[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\n\nText: Another options is to use sps.lil_matrix \nCode: a = scipy.sparse.lil_matrix((2750086,1000), dtype=int8 )\n\nText: This seems to be more efficient for setting elements (like a[1,1]=2). \nAPI:\nscipy.sparse.lil_matrix\n","label":[[543,557,"Mention"],[705,728,"API"]],"Comments":[]}
{"id":60453,"text":"ID:43287930\nPost:\nText: In terms of statistical tests, scipy has a two sample Kolmogorov-Smirnov test for the continuous variables. The binned histogram data can be used with a chisquare test. st also has a k-sample Anderson-Darling test. \nText: For plotting: \nText: The equivalent of a probability plot for two histograms would be to plot the cumulative frequencies for the two samples, i.e. with cumulative probabilities on each axis corresponding to the bin boundaries. \nText: statsmodels has a qq-plot for two sample comparison, however it currently assumes that the sample sizes are the same. If the sample sizes are different, then the quantiles need to be computed for the same probabilities. https:\/\/github.com\/statsmodels\/statsmodels\/issues\/2896 https:\/\/github.com\/statsmodels\/statsmodels\/pull\/3169 (I don't remember what the status of this is.) \nAPI:\nscipy.stats\n","label":[[193,195,"Mention"],[861,872,"API"]],"Comments":[]}
{"id":60454,"text":"ID:43391660\nPost:\nText: You can use saove_npz method \nText: Alternatively consider using Pandas.SparseDataFrame, but be aware that this method is very slow (thanks to @hpaulj for testing and pointing it out) \nText: Demo: \nText: generating sparse matrix and SparseDataFrame \nCode: In [55]: import pandas as pd\n\nIn [56]: from scipy.sparse import *\n\nIn [57]: m = csr_matrix((20, 10), dtype=np.int8)\n\nIn [58]: m\nOut[58]:\n<20x10 sparse matrix of type '<class 'numpy.int8'>'\n        with 0 stored elements in Compressed Sparse Row format>\n\nIn [59]: sdf = pd.SparseDataFrame([pd.SparseSeries(m[i].toarray().ravel(), fill_value=0)\n    ...:                           for i in np.arange(m.shape[0])])\n    ...:\n\nIn [61]: type(sdf)\nOut[61]: pandas.sparse.frame.SparseDataFrame\n\nIn [62]: sdf.info()\n<class 'pandas.sparse.frame.SparseDataFrame'>\nRangeIndex: 20 entries, 0 to 19\nData columns (total 10 columns):\n0    20 non-null int8\n1    20 non-null int8\n2    20 non-null int8\n3    20 non-null int8\n4    20 non-null int8\n5    20 non-null int8\n6    20 non-null int8\n7    20 non-null int8\n8    20 non-null int8\n9    20 non-null int8\ndtypes: int8(10)\nmemory usage: 280.0 bytes\n\nText: saving SparseDataFrame to HDF file \nCode: In [64]: sdf.to_hdf('d:\/temp\/sparse_df.h5', 'sparse_df')\n\nText: reading from HDF file \nCode: In [65]: store = pd.HDFStore('d:\/temp\/sparse_df.h5')\n\nIn [66]: store\nOut[66]:\n<class 'pandas.io.pytables.HDFStore'>\nFile path: d:\/temp\/sparse_df.h5\n\/sparse_df            sparse_frame\n\nIn [67]: x = store['sparse_df']\n\nIn [68]: type(x)\nOut[68]: pandas.sparse.frame.SparseDataFrame\n\nIn [69]: x.info()\n<class 'pandas.sparse.frame.SparseDataFrame'>\nInt64Index: 20 entries, 0 to 19\nData columns (total 10 columns):\n0    20 non-null int8\n1    20 non-null int8\n2    20 non-null int8\n3    20 non-null int8\n4    20 non-null int8\n5    20 non-null int8\n6    20 non-null int8\n7    20 non-null int8\n8    20 non-null int8\n9    20 non-null int8\ndtypes: int8(10)\nmemory usage: 360.0 bytes\n\nAPI:\nscipy.sparse.save_npz\n","label":[[36,45,"Mention"],[1980,2001,"API"]],"Comments":[]}
{"id":60455,"text":"ID:43435108\nPost:\nText: odeint \nText: odeint has an option full_output that allows you to obtain a dictionary with information on the integration, including tcur which is: \nText: vector with the value of t reached for each time step. (will always be at least as large as the input times). \nText: (Note the second sentence: The actual steps are always as fine as your desired output. If you want use the minimum number of necessary step, you must ask for a coarse sampling.) \nText: Now, this does not give you the values, but we can obtain those by integrating a second time using these very steps: \nCode: from scipy.integrate import odeint\nimport numpy as np\n\ndef f(y,t):\n    return np.array([y[1]-y[0],y[0]-y[1]])\n\nstart,end = 0,10 # time range we want to integrate\ny0 = [1,0]       # initial conditions\n\n# Function to add the initial time and the target time if needed:\ndef ensure_start_and_end(times):\n    times = np.insert(times,0,start)\n    if times[-1] < end:\n        times = np.append(times,end)\n    return times\n\n# First run to establish the steps\nfirst_times = np.linspace(start,end,100)\nfirst_run   = odeint(f,y0,first_times,full_output=True)\nfirst_steps = np.unique(first_run[1][\"tcur\"])\n\n# Second run to obtain the results at the steps\nsecond_times = ensure_start_and_end(first_steps)\nsecond_run   = odeint(f,y0,second_times,full_output=True,h0=second_times[0])\nsecond_steps = np.unique(second_run[1][\"tcur\"])\n\n# ensuring that the second run actually uses (almost) the same steps.\nnp.testing.assert_allclose(first_steps,second_steps,rtol=1e-5)\n\n# Your desired output\nactual_steps = np.vstack((second_times, second_run[0].T)).T\n\nText: si.ode \nText: Having some experience with this module, I am not aware of any way to obtain the step size without digging deeply into the internals. \nAPI:\nscipy.integrate.odeint\nscipy.integrate.ode\n","label":[[24,30,"Mention"],[1646,1652,"Mention"],[1800,1822,"API"],[1823,1842,"API"]],"Comments":[]}
{"id":60456,"text":"ID:43558407\nPost:\nText: As described in sps.csr_matrix docs you can create a sparse matrix in the following way: \nText: csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)]) where data, row_ind and col_ind satisfy the relationship a[row_ind[k], col_ind[k]] = data[k]. \nText: I tried this with some random-generated data of the same size and matrix creation took about 18 seconds. \nCode: from scipy.sparse import csr_matrix\nimport numpy as np\ntotalRows = 23094592\nUniqueUsers = 11701890\nUniqueItems = 1000000\nusers = np.random.randint(UniqueUsers, size=totalRows)\nitems = np.random.randint(UniqueItems, size=totalRows)\nratings = np.random.randint(5, size=totalRows)\nM = csr_matrix((ratings, (users, items)), shape=(UniqueUsers, UniqueItems))\n    \n\nText: This will create a sparse matrix where M[users[k], items[k]] = ratings[k]. \nText: You should make sure that users and items are 0-based indices of every unique user and item, perhaps by using LabelEncoder from scikit-learn. \nAPI:\nscipy.sparse.csr_matrix\n","label":[[40,54,"Mention"],[984,1007,"API"]],"Comments":[]}
{"id":60457,"text":"ID:43618073\nPost:\nText: Hi @Raghuram, \nText: Welcome to StackOverflow! Thanks you for asking your question, and I hope that you find an answer. Here are some links to asking questions from the StackOverflow help center: \nText: Your question might be off-topic because it has to do software on a particular plaform, so it might be better suited to askUbuntu or SuperUser Your question doesn't have any code in it, and it also doesn't have minimum, complete and verifiable example. See the comments that ask about what version of Python and scipy you're using, how you installed them (Anaconda, system, etc?) and what platform you are on (Ubuntu? 16.04LTS?) and what code you used (how did you import scipy?). Although you mentioned you did some Googling, your question lacks a sense of rigor - see \"How do I ask a good question?. EG. Provide links to the sites you searched from which you obtained your attempted solution (to install the Pillow\/PIL fork) - this link could be useful to someone else with a similar issue, even though it didn't help you. \nText: Suggested solution \nText: From your answer it appears that you are not using the system Python in \/usr\/lib\/python2.7 and that your packages have been installed using the --prefix installation scheme into \/home\/raghuram and \/home\/raghuram\/local\/. \nText: Unfortunately, pip will not use wheels if it gets --install-option so you will have to install BLAS first. \nCode: $ sudo apt install gfortran libblas-dev liblapack-dev libatlas-dev\n\nText: Then try to use the --install-option with pip to pass the --prefix option to install. \nCode: $ pip install --install-option=\"--prefix=\/home\/raghuram\/\" numpy scipy pillow\n\nText: Another perhaps easier option is to see where your python interpreter thinks site-packages should go. To do this, import site and call site.getsitepackages(). If \/home\/raghuram is in that list, then chances are you can just call pip from Python as a module using the -m option. \nCode: $ python -m pip install numpy scipy pillow\n\nText: Finally, if all else fails, you can fall back on distutils, but this is tricky because you can't mix the scipy\/numpy BLAS dependencies. They can only be either ATLAS, OpenBLAS, MKL, or etc., not a mix. To see what you are using, first import scipy numpy and then call numpy.show_configs() and scipy.show_configs(). It get's even trickier from here because you need to edit the setup.cfg to tell numpy\/scipy where your BLAS is, so let's assume that you can remove both of these and start from scratch. First install the dependencies from your distro's repo; I think by default they will always build with ATLAS. \nCode: $ sudo apt install gfortran libblas-dev liblapack-dev libatlas-dev\n\nText: Then download the numpy and scipy zip files from PyPI and extract. For each you need to enter the extracted folder and run: \nCode: $ python setup.py install --prefix=~\n\nText: Now try to use sp.misc.imsave like their help docstring example \nCode: >>> import numpy as np\n>>> from scipy.misc import imsave\n>>> help(imsave)  # view docstring\n>>> # then hit q key to return to interpreter\n>>> x = np.zeros((255, 255))\n>>> x = np.zeros((255, 255), dtype=np.uint8)\n>>> x[:] = np.arange(255)\n>>> imsave('gradient.png', x)    \n>>> rgb = np.zeros((255, 255, 3), dtype=np.uint8)\n>>> rgb[..., 0] = np.arange(255)\n>>> rgb[..., 1] = 55\n>>> rgb[..., 2] = 1 - np.arange(255)\n>>> imsave('rgb_gradient.png', rgb)\n\nText: NB: you can always search for Ubuntu packages online or using apt search. \nText: PS IMO you should probably remove any packages you've installed to system python using sudo pip and IMO never do that again. Check in \/usr\/local\/lib\/python2.7\/dist-packages. \nText: PPS IMHO you should never install Python packages on Linux using sudo, instead either install from the software repository of your distro using apt or yum, install using the pip --user option or create a Python virtual environment with virtualenv. See my AskUbuntu answer. \nAPI:\nscipy.misc.imsave\n","label":[[2894,2908,"Mention"],[3947,3964,"API"]],"Comments":[]}
{"id":60458,"text":"ID:43621603\nPost:\nText: mathieu_cem takes x in degrees. This output looks about right for the Python code going up to 31.4 degrees and the Mathematica code going up to 31.4 radians. \nAPI:\nscipy.special.mathieu_cem\n","label":[[24,35,"Mention"],[188,213,"API"]],"Comments":[]}
{"id":60459,"text":"ID:43629100\nPost:\nText: Creating a spline of degree k trough k points is not necessarily a good idea. Anything can happen there. \nText: Since scipy.interpolate.spline is anyways depreciated, you may use inp.splev and sp.interpolate.splrep and restrict the spline to degree k=2. (Trying to use k=3 will cause an error, which is expected for 3 points) \nCode: from scipy.interpolate import splev, splrep\n\nequinoxX_new = np.linspace(equinoxX.min(),equinoxX.max(),30)\nequinoxY_smooth = splev(equinoxX_new, splrep(equinoxX, equinoxY, k=2))\n\nsummerX_new = np.linspace(summerX.min(), summerX.max(),30)\nsummerY_smooth = splev(summerX_new, splrep(summerX, summerY, k=2))\n\nwinterX_new = np.linspace(winterX.min(), winterX.max(),30)\nwinterY_smooth = splev(winterX_new, splrep(winterX, winterY, k=2))\n\nText: Using those for the dataset from the comments, \nCode: equinoxAzi = np.array([90, 180, 270]) \nequinoxAlt = np.array([0, 80, 0]) \nsummerAzi = np.array([180-121.5, 180, 180+121.5]) \nsummerAlt = np.array([0, 60, 0]) \nwinterAzi = np.array([180-58.46, 180, 180+58.46]) \nwinterAlt = np.array([0, 40, 0])\n\nText: the result is \nAPI:\nscipy.interpolate.splev\nscipy.interpolate.splrep\n","label":[[203,212,"Mention"],[217,238,"Mention"],[1119,1142,"API"],[1143,1167,"API"]],"Comments":[]}
{"id":60460,"text":"ID:43745704\nPost:\nText: Given two sequences (a[0], .., a[A-1]) and (b[0], .., b[B-1]) of lengths A and B, respectively, the convolution is calculated as \nText: c[n] = sum_m a[m] * b[n-m] \nText: If mode==\"full\" then the convolution is calculated for n ranging from 0 to A+B-2, so the return array has A+B-1 elements. \nText: If mode==\"same\" then sp.signal.correlate computes the convolution for n ranging from (B-1)\/2 to A-1+(B-1)\/2, where integer division is assumed. The return array has A elements. numpy.correlate behaves the same way only if A>=B; if A is less than B it switches the two arrays (and the returned array has B elements). \nText: If mode==\"valid\" then the convolution is calculated for n ranging from min(A,B)-1 to max(A,B)-1, and therefore has max(A,B)-min(A,B)+1 elements. \nAPI:\nscipy.signal.correlate\n","label":[[344,363,"Mention"],[797,819,"API"]],"Comments":[]}
{"id":60461,"text":"ID:44000780\nPost:\nText: Delaunay is an alias for scipy.spatial.qhull.Delaunay. If the import is broken for some reason, importing the latter directly may work in some cases: \nCode: from scipy.spatial.qhull import Delaunay\n\nAPI:\nscipy.spatial.Delaunay\n","label":[[24,32,"Mention"],[228,250,"API"]],"Comments":[]}
{"id":60462,"text":"ID:44122142\nPost:\nText: Given that I suspect that pandas apply is just looping over all rows, and the sp.stats distributions have quite a bit of overhead in each call, I would use a vectorized version: \nCode: >>> from scipy import stats\n>>> df['p'] = stats.beta.cdf(df['x'], df['alpha'], df['beta'], loc=df['A'], scale=df['B']-df['A'])\n>>> df\n   A             B     alpha        beta          x         p\n0  0  148000000000  1.501710  628.110247  640495496  0.858060\n1  0  148000000000  1.501704  620.110000  640495440  0.853758\n\nAPI:\nscipy.stats\n","label":[[102,110,"Mention"],[535,546,"API"]],"Comments":[]}
{"id":60463,"text":"ID:44227540\nPost:\nText: Consider kurtosisetst and scipy.stats.skewtest. \nText: To your second question, use .axvline to mark your line there. Depending on how granular the bins are, try finding the first point left of zero that meets the following condition: \nCode: df\nOut[20]: \n      Normal  Empirical\nBin                    \n-1.0       0        2.0\n-0.9       1        2.5\n-0.8       2        3.0\n-0.7       3        3.5\n-0.6       4        4.0\n-0.5       5        4.5\n-0.4       6        5.0\n-0.3       7        6.0\n-0.2       8        8.0\n-0.1       9       10.0\n 0.0      10       12.0\n\ndf.index[(df.Normal.shift() < df.Empirical.shift()) \n          & (df.Normal == df.Empirical)].values\nOut[38]: array([-0.6])\n\nText: And lastly, you could consider plotting the actual histogram in addition to fitted distribution, and using an inset, as is done here. \nAPI:\nscipy.stats.kurtosistest\n","label":[[33,45,"Mention"],[863,887,"API"]],"Comments":[]}
{"id":60464,"text":"ID:44229671\nPost:\nText: MATLAB v7.3 uses HDF5 storage; sp.io.loadmat cannot handle that \nText: MATLAB: Differences between .mat versions \nText: Instead you have to use numpy plus h5py \nText: How to read a v7.3 mat file via h5py? \nText: how to read Mat v7.3 files in python  \nText: and a scattering of more recent questions. \nText: Try that, and come back with a new question it you still have problems sorting out the results. \nAPI:\nscipy.io.loadmat\n","label":[[55,68,"Mention"],[434,450,"API"]],"Comments":[]}
{"id":60465,"text":"ID:44252276\nPost:\nText: You are attempting to create a Butterworth filter (which is an IIR filter) with order 1001, represented using the transfer function coefficients (b, a). The transfer function is a rational function, which is the ratio of two polynomials, and the evaluation of high order polynomials is very susceptible to numerical error. What you are trying to do is doomed to fail. \nText: Some suggestions: \nText: Rethink the need for a filter with order 1001. Why are you trying to create a filter with such a high order? You'll get better numerical behavior if you use the SOS (second order sections) format for the filter, instead of the transfer function (b, a). \nText: Try using a much lower order, and try using sp.signal.sosfilt to filter the signal: \nCode: sos = butter(filter_order, [normalized_low, normalized_high], btype='bandpass', output='sos')\ny = sosfilt(sos, y)\n\nAPI:\nscipy.signal.sosfilt\n","label":[[728,745,"Mention"],[895,915,"API"]],"Comments":[]}
{"id":60466,"text":"ID:44252503\nPost:\nText: opt.leastsq does not support bounds, and was used by curve_fit until scipy version 0.17. OTOH, sp.optimize.least_squares (which is used by curve_fit in more recent versions of scipy) can support bounds, but not when using the lm (Levenberg-Marquardt) method, because that is a simple wrapper around scipy.optimize.leastsq. It is somewhat confusing. \nText: Allow me to recommend trying lmfit (http:\/\/lmfit.github.io\/lmfit-py\/) which does support bounds on all parameters, and makes it easy to fix\/vary parameters without having to alter the model function. Lmfit has a convenient approach to curve fitting with its Model class, and a good number of tools for exploring the confidence intervals of parameters. \nAPI:\nscipy.optimize.leastsq\nscipy.optimize.least_squares\n","label":[[24,35,"Mention"],[119,144,"Mention"],[738,760,"API"],[761,789,"API"]],"Comments":[]}
{"id":60467,"text":"ID:44271701\nPost:\nText: spectrogram works by splitting the signal into (partially overlapping) segments of time, and then computing the power spectrum from the Fast Fourier Transform (FFT) of each segment. The length of these segments can be controlled using the nperseg argument, which lets you adjust the trade-off between resolution in the frequency and time domains that arises due to the uncertainty principle. Making nperseg larger gives you more resolution in the frequency domain at the cost of less resolution in the time domain. \nText: In addition to varying the number of samples that go into each segment, it's also sometimes desirable to apply zero-padding to each segment before taking its FFT. This is what the nfft argument is for: \nText: nfft : int, optional Length of the FFT used, if a zero padded FFT is desired. If None, the FFT length is nperseg. Defaults to None. \nText: By default, nfft == nperseg, meaning that no zero-padding will be used. \nText: Why would you want to apply zero-padding? \nText: One reason is that this makes the FFT result longer, meaning that you end up with more frequency bins and a spectrogram that looks \"smoother\" over the frequency dimension. However, note that this doesn't actually give you any more resolution in the frequency domain - it's basically an efficient way of doing sinc interpolation on the FFT result (see here for a more detailed explanation). From a performance perspective it might make sense to pad the segments so that their length is a power of 2, since radix-2 FFTs can be significantly faster than more general methods. \nAPI:\nscipy.signal.spectrogram\n","label":[[24,35,"Mention"],[1601,1625,"API"]],"Comments":[]}
{"id":60468,"text":"ID:44311083\nPost:\nText: To see one reason why this does not work, you can do from the (i)python commandline \nCode: from scipy import spatial\nimport inspect\ninspect.getsourcelines(spatial)\n\nText: Under the comment text, you can see several imports. If you also look at the first of those, \nCode: inspect.getsourcelines(spatial.kdtree)\n\nText: there is another import line \nCode: import scipy.sparse\n\nText: This is just the first example I found, there are surely many more dependencies and spatial won't work as is if you don't have access to them. Note also that numpy is imported, and if your server does not have scipy, it probably does not have numpy either. \nText: My suggestion is to either copy\/rewrite the scipy code you need (if this is allowed), write your code without scipy or ask for the server admin to either allow larger files or install scipy. \nAPI:\nscipy.spatial\n","label":[[488,495,"Mention"],[865,878,"API"]],"Comments":[]}
{"id":60469,"text":"ID:44336864\nPost:\nText: Update: \nText: After much testing and restructuring, it seems that the best way to take care of this is not to nest the functions or the definitions, but rather to make use of the args parameter in the quad function to pass external variables through to inner integrations. \nText: Many thanks to those who commented! \nAPI:\nscipy.integrate.quad\n","label":[[226,230,"Mention"],[347,367,"API"]],"Comments":[]}
{"id":60470,"text":"ID:44421932\nPost:\nText: For a more accurate fit, you could look into inp module. The functions there do a good job with interpolating and fitting. \nText: Other fitting techniques which could do a good job are: a) CSTs b) BSplines c) Polynomial interpolation \nText: Scipy also has an implementation for BSplines. The other two, you may have to implement yourself. \nAPI:\nscipy.interpolate\n","label":[[69,72,"Mention"],[369,386,"API"]],"Comments":[]}
{"id":60471,"text":"ID:44436947\nPost:\nText: Here's what I get in R: \nCode: > binom.test(2, 8, 11\/2364, alternative = \"greater\")\n\n    Exact binomial test\n\ndata:  2 and 8\nnumber of successes = 2, number of trials = 8, p-value = 0.0005951\nalternative hypothesis: true probability of success is greater than 0.00465313\n95 percent confidence interval:\n 0.04638926 1.00000000\nsample estimates:\nprobability of success \n                  0.25 \n\n>\n\nText: Note that the p-value is 0.0005951. \nText: Compare that to the result of binom_test (which returns just the p-value): \nCode: In [25]: from scipy.stats import binom_test\n\nIn [26]: binom_test(2, 8, 11\/2364, alternative='greater')\nOut[26]: 0.00059505960517880572\n\nText: So that agrees with R. \nText: To use the survival function of scipy.stats.binom, you have to adjust the first argument (as noted in a comment by Marius): \nCode: In [27]: from scipy.stats import binom\n\nIn [28]: binom.sf(1, 8, 11\/2364)\nOut[28]: 0.00059505960517880572\n\nText: (I am using Python 3, so 11\/2364 equals 0.004653130287648054. If you are using Python 2, be sure to write that fraction as 11.0\/2364 or float(11)\/2364.) \nAPI:\nscipy.stats.binom_test\n","label":[[499,509,"Mention"],[1125,1147,"API"]],"Comments":[]}
{"id":60472,"text":"ID:44542234\nPost:\nText: While the width and the slope are sufficient to define a triangular signal, you would need a third parameter for a trapezoidal signal: the amplitude. \nText: Using those three parameters, you can easily adjust the sp.signal.sawtooth function to give you a trapeziodal shape by truncating and offsetting the triangular shaped function. \nCode: from scipy import signal\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef trapzoid_signal(t, width=2., slope=1., amp=1., offs=0):\n    a = slope*width*signal.sawtooth(2*np.pi*t\/width, width=0.5)\/4.\n    a[a>amp\/2.] = amp\/2.\n    a[a<-amp\/2.] = -amp\/2.\n    return a + amp\/2. + offs\n\nt = np.linspace(0, 6, 501)\nplt.plot(t,trapzoid_signal(t, width=2, slope=2, amp=1.), label=\"width=2, slope=2, amp=1\")\nplt.plot(t,trapzoid_signal(t, width=4, slope=1, amp=0.6), label=\"width=4, slope=1, amp=0.6\")\n\nplt.legend( loc=(0.25,1.015))\nplt.show()\n\nText: Note that you may also like to define a phase, depeding on the use case. \nText: In order to define a single pulse, you might want to modify the function a bit and supply an array which ranges over [0,width]. \nCode: from scipy import signal\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef trapzoid_signal(t, width=2., slope=1., amp=1., offs=0):\n    a = slope*width*signal.sawtooth(2*np.pi*t\/width, width=0.5)\/4.\n    a += slope*width\/4.\n    a[a>amp] = amp\n    return a + offs\n\nfor w,s,a in zip([2,5], [2,1], [1,0.6]):\n    t = np.linspace(0, w, 501)\n    l = \"width={}, slope={}, amp={}\".format(w,s,a)\n    plt.plot(t,trapzoid_signal(t, width=w, slope=s, amp=a), label=l)\n\nplt.legend( loc=\"upper right\")\nplt.show()\n\nAPI:\nscipy.signal.sawtooth\n","label":[[237,255,"Mention"],[1635,1656,"API"]],"Comments":[]}
{"id":60473,"text":"ID:44736997\nPost:\nText: The variance in the fitted parameters represents the uncertainty in the best-fit value based on the quality of the fit of the model to the data. That is, it describes by how much the value could change away from the best-fit value and still have a fit that is almost as good as the best-fit value. \nText: With standard definition of chi-square, chi_square = ( ( (data - model)\/epsilon )**2 ).sum() \nText: and reduced_chi_square = chi_square \/ (ndata - nvarys) (where data is the array of the data values, model the array of the calculated model, epsilon is uncertainty in the data, ndata is the number of data points, and nvarys the number of variables), a good fit should have reduced_chi_square around 1 or chi_square around ndata-nvary. (Note: not 0 -- the fit will not be perfect as there is noise in the data). \nText: The variance in the best-fit value for a variable gives the amount by which you can change the value (and re-optimize all other values) and increase chi-square by 1. That gives the so-called '1-sigma' value of the uncertainty. \nText: As you say, these values are expressed in the diagonal terms of the covariance matrix returned by curve_fit (the off-diagonal terms give the correlations between variables: if a value for one variable is changed away from its optimal value, how would the others respond to make the fit better). This covariance matrix is built using the trial values and derivatives near the solution as the fit is being done -- it calculates the \"curvature\" of the parameter space (ie, how much chi-square changes when a variables value changes). \nText: You can calculate these uncertainties by hand. The lmfit library (https:\/\/lmfit.github.io\/lmfit-py\/) has routines to more explicitly explore the confidence intervals of variables from least-squares minimization or curve-fitting. These are described in more detail at https:\/\/lmfit.github.io\/lmfit-py\/confidence.html. It's probably easiest to use lmfit for the curve-fitting rather than trying to re-implement the confidence interval code for curve_fit. \nAPI:\nscipy.optimize.curve_fit\n","label":[[1179,1188,"Mention"],[2078,2102,"API"]],"Comments":[]}
{"id":60474,"text":"ID:45016479\nPost:\nText: I would use pandas dataframe.where method. \nCode: group1_air = df.where(df.Group== 1).dropna()['air']\ngroup2_air = df.where(df.Group== 2).dropna()['air']\n\nText: This bit of code returns into group1_air all the values of the air column where the group column is 1 and all the values of air where group is 2 in group2_air. The drop.na() is required because the .where method will return NAN for every row in which the specified conditions is not met. So all rows where group is 2 will return with NAN values when you use df.where(df.Group== 1). \nText: Whether you need to use ttest_rel or ttest_ind depends on your groups. If you samples are from independent groups you should use ttest_ind if your samples are from related groups you should use ttest_rel. \nText: So if your samples are independent from oneanother your final piece of required code is. \nCode: scipy.stats.ttest_ind(group1_air,group2_air)\n\nText: else you need to use \nCode: scipy.stats.ttest_rel(group1_air,group2_air)\n\nText: When you want to also test co2 you simply need to change air for co2 in the given example. \nText: Edit: \nText: This is a rough sketch of the code you should run to execute ttests over every column in your dataframe except for the group column. You may need to tamper a bit with the column_list to get it completely compliant with your needs (you may not want to loop over every column for example). \nCode: # get a list of all columns in the dataframe without the Group column\ncolumn_list = [x for x in df.columns if x != 'Group']\n# create an empty dictionary\nt_test_results = {}\n# loop over column_list and execute code explained above\nfor column in column_list:\n    group1 = df.where(df.Group== 1).dropna()[column]\n    group2 = df.where(df.Group== 2).dropna()[column]\n    # add the output to the dictionary \n    t_test_results[column] = scipy.stats.ttest_ind(group1,group2)\nresults_df = pd.DataFrame.from_dict(t_test_results,orient='Index')\nresults_df.columns = ['statistic','pvalue']\n\nText: At the end of this code you have a dataframe with the output of the ttest over every column you will have looped over. \nAPI:\nscipy.stats.ttest_rel\nscipy.stats.ttest_ind\n","label":[[598,607,"Mention"],[611,620,"Mention"],[2132,2153,"API"],[2154,2175,"API"]],"Comments":[]}
{"id":60475,"text":"ID:45073864\nPost:\nText: To vary the left hand side you could add an extra argument to your func and then pass this via the args keyword of opt.root (keeping rths answer in mind) : \nCode: import numpy as np\nfrom scipy import optimize\n\ndef func(x, lhs):\n    theta = np.arccos(1 - 2*x)\n    return 2*(theta - np.sin(theta)) -  lhs\n\ninitial_guess = 0.001\nlhs = 1\nsol = optimize.root(func, initial_guess, args=(lhs,))\n\nAPI:\nscipy.optimize.root\n","label":[[139,147,"Mention"],[418,437,"API"]],"Comments":[]}
{"id":60476,"text":"ID:45122395\nPost:\nText: Since distance.cdist accepts an arbitrary metric, provided as a callable, the problem is just in writing a function for your metric. \nText: If it was wrapped distance between points p and q, that would be \nCode: def wrapped_euclidean_points(p, q):\n    diff = np.abs(p - q)\n    return np.linalg.norm(np.minimum(diff, m - diff))\n\nText: where m is the size of the lattice (unfortunately cdist does not support passing extra parameters to the distance function, so m has to be taken from the larger scope.) \nText: But in your case, you want minimal distances between squares of sidelength 1. What this means is that after computing the wrapped-difference vector np.minimum(diff, m - diff) we can reduce each of its components by 1, because, say, the smallest x-difference between points of two squares is 1 less than the x-difference between the centers of those squares. Of course this subtraction should not make the difference negative. So the function becomes \nCode: def wrapped_euclidean_squares(p, q):\n    diff = np.abs(p - q)\n    return np.linalg.norm(np.clip(np.minimum(diff, m - diff) - 1, 0, None))\n\nText: where clip takes care of the case when two centers had the same x-coordinate or the same y-coordinate. \nText: The rest is just two lines (not counting from sp.spatial import distance): \nCode: coords = np.vstack(np.nonzero(lattice)).T\ndist = distance.cdist(coords, coords, metric=wrapped_euclidean_squares)\n\nText: Output for your example: \nCode: [[ 0.          2.          2.          2.          2.82842712  2.82842712       2.          2.82842712  2.82842712]\n [ 2.          0.          2.          2.82842712  2.          2.82842712       2.82842712  2.          2.82842712]\n [ 2.          2.          0.          2.82842712  2.82842712  2.       2.82842712  2.82842712  2.        ]\n [ 2.          2.82842712  2.82842712  0.          2.          2.          2.       2.82842712  2.82842712]\n [ 2.82842712  2.          2.82842712  2.          0.          2.       2.82842712  2.          2.82842712]\n [ 2.82842712  2.82842712  2.          2.          2.          0.       2.82842712  2.82842712  2.        ]\n [ 2.          2.82842712  2.82842712  2.          2.82842712  2.82842712       0.          2.          2.        ]\n [ 2.82842712  2.          2.82842712  2.82842712  2.          2.82842712       2.          0.          2.        ]\n [ 2.82842712  2.82842712  2.          2.82842712  2.82842712  2.          2.       2.          0.        ]]\n\nAPI:\nscipy.spatial\n","label":[[1292,1302,"Mention"],[2492,2505,"API"]],"Comments":[]}
{"id":60477,"text":"ID:45123174\nPost:\nText: A quick investigation reveals the problem: this function here \nCode: def adaptive_threshold(image):  \n    print(type(image))  \n    print(image)  \n    block_size = 41  \n    binary_adaptive = threshold_local(image, block_size, offset=10)  \n    binary_adaptive = np.asarray(binary_adaptive, dtype=int)  \n    return np.invert(binary_adaptive) * 1. \n\nText: is supposed to create a mask of the image by adaptive thresholding - but this goes (very) wrong. \nText: The main reason seems to be a misunderstanding of how threshold_local works: this code expects it to return a binarized segmented version of the input image, when in reality it returns a threshold image, see explanation here. \nText: This is not the only problem, however. For images like the one in your example, offset=10 reduces the threshold produced by threshold_local way too far, so the entire image would be above the threshold. \nText: Here's a working version of the function: \nCode: def adaptive_threshold(image):\n\n    # Create threshold image\n    # Offset is not desirable for these images\n    block_size = 41 \n    threshold_img = threshold_local(image, block_size)\n\n    # Binarize the image with the threshold image\n    binary_adaptive = image < threshold_img\n\n    # Convert the mask (which has dtype bool) to dtype int\n    # This is required for the code in `segmentize` (below) to work\n    binary_adaptive = binary_adaptive.astype(int)   \n\n    # Return the binarized image\n    return binary_adaptive\n\nText: If the code is run with this function (with python; this problem has nothing to do with Processing, as far as I can tell), it returns Segments detected: 108 and it produces a nice segmentation: \nCode: plt.imshow(segments[0],interpolation='none')\nplt.show()\n\nText: Side note: based on how you phrased your question, am I correct to assume that you did not write this code yourself and that you perhaps have limited expertise in this field? \nText: If so, you may be interested in learning a bit more about python-based image processing and segmentation. I recently ran a short course on this topic that includes a completely self-explanatory hands-on tutorial of a pipeline similar to the one you are using here. The materials are openly accessible, so feel free to have a look. \nText: Edit: \nText: As per your comment, here is a solution that should allow the program to run with full paths as input. \nText: First, remove all this: \nCode: folder = os.path.join(os.getcwd(), 'segments')  \nos.path.isfile(folder) and os.remove(folder)  \nos.path.isdir(folder) or os.mkdir(folder)  \n\nText: so that only this remains: \nCode: for f in sys.argv[1:]:  \n    run(f)\n\nText: Next, replace this: \nCode:     for i in range(len(seg)):  \n        imsave('segments\/' + f + '_' + str(i) + '.png', seg[i][1])  \n\nText: by this: \nCode:     # Get the directory name (if a full path is given)\n    folder = os.path.dirname(f)\n\n    # Get the file name\n    filenm = os.path.basename(f)\n\n    # If it doesn't already exist, create a new dir \"segments\" \n    # to save the PNGs\n    segments_folder = os.path.join(folder,\"segments\")\n    os.path.isdir(segments_folder) or os.mkdir(segments_folder)\n\n    # Save the segments to the \"segments\" directory\n    for i in range(len(seg)):\n        imsave(os.path.join(segments_folder, filenm + '_' + str(i) + '.png'), seg[i][1]) \n\nText: This solution can handle both files-only input (e.g 'test.png') and path input (e.g. 'C:\\Users\\Me\\etc\\test.png'). \nText: Edit 2: \nText: For transparency, imsave allows an alpha layer if arrays are saved as RGBA (MxNx4), see here. \nText: Replace this \nCode:         imsave(os.path.join(segments_folder, filenm + '_' + str(i) + '.png'), seg[i][1]) \n\nText: by this \nCode:         # Create an MxNx4 array (RGBA)\n        seg_rgba = np.zeros((seg[i][1].shape[0],seg[i][1].shape[1],4),dtype=np.bool)\n\n        # Fill R, G and B with copies of the image\n        for c in range(3):\n            seg_rgba[:,:,c] = seg[i][1]\n\n        # For A (alpha), use the invert of the image (so background is 0=transparent)\n        seg_rgba[:,:,3] = ~seg[i][1]\n\n        # Save image\n        imsave(os.path.join(segments_folder, filenm + '_' + str(i) + '.png'), seg_rgba) \n\nText: Edit 3: \nText: For saving into a different target folder with individual subfolders for each segmented image: \nText: Instead of this line \nCode:     folder = os.path.dirname(f)\n\nText: you can specify the target folder, for example \nCode:     folder = r'C:\\Users\\Dude\\Desktop'\n\nText: (Note the r'...' formatting, which produces a raw string literal.) \nText: Next, replace this \nCode:     segments_folder = os.path.join(folder,\"segments\")\n\nText: by this \nCode:     segments_folder = os.path.join(folder,filenm[:-4]+\"_segments\")\n\nText: and to be extra-clean replace this \nCode:         imsave(os.path.join(segments_folder, filenm + '_' + str(i) + '.png'), seg_rgba) \n\nText: by this \nCode:         imsave(os.path.join(segments_folder, filenm[:-4] + '_' + str(i) + '.png'), seg_rgba) \n\nAPI:\nscipy.misc.imsave\n","label":[[3498,3504,"Mention"],[4984,5001,"API"]],"Comments":[]}
{"id":60478,"text":"ID:45135850\nPost:\nText: Maybe the documentation from leastsq could help since it documents both Dfun (Jacobian) and col_deriv. From Dfun we get: \nText: Dfun : callable, optional A function or method to compute the Jacobian of func with derivatives across the rows. If this is None, the Jacobian will be estimated. \nText: From col_deriv we get: \nText: col_deriv : bool, optional non-zero to specify that the Jacobian function computes derivatives down the columns (faster, because there is no transpose operation). \nText: My reading of this is as follows: \nText: By default, scipy expects that a function that computes the Jacobian matrix return a matrix that follows \"normal\" definition (see, e.g., https:\/\/en.wikipedia.org\/wiki\/Jacobian_matrix_and_determinant). However, scipy itself calls other functions, possibly written in Fortran see, e.g., minpack, which expect that derivatives (with regard to coordinates) are placed in columns. \nText: It follows that if the function that computes the Jacobian matrix could return a matrix with derivatives placed along columns instead of rows, then scipy will not need to transpose the Jacobian matrix before passing it to the minpack function thus saving computational time. \nAPI:\nscipy.optimize.leastsq\n","label":[[53,60,"Mention"],[1226,1248,"API"]],"Comments":[]}
{"id":60479,"text":"ID:45320782\nPost:\nText: First, a solution: \nText: Turns out least_squares supports exploiting the structure of the jacobian by setting the jac_sparsity argument. \nText: The least_squares function works slightly different than minimize so the cost function needs to be rewritten to return residuals instead: \nCode: def residuals(x, a, b):\n    return np.sum(a * x.reshape(a.shape), axis=1) - b\n\nText: The jacobian has block-diagonal sparsity structure, given by \nCode: jacs = sp.sparse.block_diag([np.ones((1, 40), dtype=bool)]*500)\n\nText: And calling the optimization routine: \nCode: res = sp.optimize.least_squares(residuals, np.zeros(500*40),\n                                jac_sparsity=jacs, args=(a, b))\nx = res.x.reshape(500, 40)\n\nText: But is it really faster? \nCode: %timeit opt1_loopy_min(a, b)        # 1 loop, best of 3: 2.43 s per loop\n%timeit opt2_loopy_min_start(a, b)  # 1 loop, best of 3: 2.55 s per loop\n%timeit opt3_loopy_lsq(a, b)        # 1 loop, best of 3: 13.7 s per loop\n%timeit opt4_dense_lsq(a, b)        # ValueError: array is too big; ...\n%timeit opt5_jacs_lsq(a, b)         # 1 loop, best of 3: 1.04 s per loop\n\nText: Conclusions: \nText: There is no obvious difference between the original solution (opt1) and re-use of the starting point (opt2) without sorting. looping over least_squares (opt3) is considerable slower than looping over minimize (opt1, opt2). The problem is too big to naiively run with least_squares because the jacobian matrix does not fit in memory. Exploiting the sparsity structure of the jacobian in least_squares (opt5) seems to be the fastest approach. \nText: This is the timing test environment: \nCode: import numpy as np\nimport scipy as sp\n\ndef cost(x, a, b):\n    return np.sum((np.sum(a * x.reshape(a.shape), axis=1) - b)**2)\n\ndef residuals(x, a, b):\n    return np.sum(a * x.reshape(a.shape), axis=1) - b\n\na = np.random.randn(500, 40)\nb = np.arange(500)\n\ndef opt1_loopy_min(a, b):\n    x = []\n    x0 = np.zeros(a.shape[1])\n    for i in range(a.shape[0]):\n        res = sp.optimize.minimize(cost, x0, args=(a[None, i], b[None, i]))\n        x.append(res.x)\n    return np.stack(x)\n\ndef opt2_loopy_min_start(a, b):\n    x = []\n    x0 = np.zeros(a.shape[1])\n    for i in range(a.shape[0]):\n        res = sp.optimize.minimize(cost, x0, args=(a[None, i], b[None, i]))\n        x.append(res.x)\n        x0 = res.x\n    return np.stack(x)\n\ndef opt3_loopy_lsq(a, b):\n    x = []\n    x0 = np.zeros(a.shape[1])\n    for i in range(a.shape[0]):\n        res = sp.optimize.least_squares(residuals, x0, args=(a[None, i], b[None, i]))\n        x.append(res.x)\n    return x\n\ndef opt4_dense_lsq(a, b):\n    res = sp.optimize.least_squares(residuals, np.zeros(a.size), args=(a, b))\n    return res.x.reshape(a.shape)\n\ndef opt5_jacs_lsq(a, b):\n    jacs = sp.sparse.block_diag([np.ones((1, a.shape[1]), dtype=bool)]*a.shape[0])\n    res = sp.optimize.least_squares(residuals, np.zeros(a.size), jac_sparsity=jacs, args=(a, b))\n    return res.x.reshape(a.shape)\n\nAPI:\nscipy.optimize.least_squares\n","label":[[60,73,"Mention"],[2989,3017,"API"]],"Comments":[]}
{"id":60480,"text":"ID:45399872\nPost:\nText: Both skew and kurtosis call the function of scipy.stats.moment, which computes the following for the k-th central moment of a data sample: \nText: <script src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.0\/MathJax.js?config=TeX-AMS_HTML-full\"><\/script> <script type=\"text\/x-mathjax-config\"> MathJax.Hub.Config({\"HTML-CSS\": { preferredFont: \"TeX\", availableFonts:[\"STIX\",\"TeX\"], linebreaks: { automatic:true }, EqnChunk:(MathJax.Hub.Browser.isMobile ? 10 : 50) }, tex2jax: { inlineMath: [ [\"$\", \"$\"], [\"\\\\\\\\(\",\"\\\\\\\\)\"] ], displayMath: [ [\"$$\",\"$$\"], [\"\\\\[\", \"\\\\]\"] ], processEscapes: true, ignoreClass: \"tex2jax_ignore|dno\" }, TeX: { noUndefined: { attributes: { mathcolor: \"red\", mathbackground: \"#FFEEEE\", mathsize: \"90%\" } }, Macros: { href: \"{}\" } }, messageStyle: \"none\" }); <\/script> $$m_k = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^k$$ \nText: Accordingly, st.skew with default settings (e.g. bias=True) computes: \nText: <script src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.0\/MathJax.js?config=TeX-AMS_HTML-full\"><\/script> <script type=\"text\/x-mathjax-config\"> MathJax.Hub.Config({\"HTML-CSS\": { preferredFont: \"TeX\", availableFonts:[\"STIX\",\"TeX\"], linebreaks: { automatic:true }, EqnChunk:(MathJax.Hub.Browser.isMobile ? 10 : 50) }, tex2jax: { inlineMath: [ [\"$\", \"$\"], [\"\\\\\\\\(\",\"\\\\\\\\)\"] ], displayMath: [ [\"$$\",\"$$\"], [\"\\\\[\", \"\\\\]\"] ], processEscapes: true, ignoreClass: \"tex2jax_ignore|dno\" }, TeX: { noUndefined: { attributes: { mathcolor: \"red\", mathbackground: \"#FFEEEE\", mathsize: \"90%\" } }, Macros: { href: \"{}\" } }, messageStyle: \"none\" }); <\/script> $$ S = \\frac{m_3}{(m_2)^{1.5}} $$ \nText: st.kurtosis with default settings: \nText: <script src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.0\/MathJax.js?config=TeX-AMS_HTML-full\"><\/script> <script type=\"text\/x-mathjax-config\"> MathJax.Hub.Config({\"HTML-CSS\": { preferredFont: \"TeX\", availableFonts:[\"STIX\",\"TeX\"], linebreaks: { automatic:true }, EqnChunk:(MathJax.Hub.Browser.isMobile ? 10 : 50) }, tex2jax: { inlineMath: [ [\"$\", \"$\"], [\"\\\\\\\\(\",\"\\\\\\\\)\"] ], displayMath: [ [\"$$\",\"$$\"], [\"\\\\[\", \"\\\\]\"] ], processEscapes: true, ignoreClass: \"tex2jax_ignore|dno\" }, TeX: { noUndefined: { attributes: { mathcolor: \"red\", mathbackground: \"#FFEEEE\", mathsize: \"90%\" } }, Macros: { href: \"{}\" } }, messageStyle: \"none\" }); <\/script> $$ K = \\frac{m_4}{(m_2)^{2}} $$ \nAPI:\nscipy.stats.skew\nscipy.stats.kurtosis\nscipy.stats.skew\nscipy.stats.kurtosis\n","label":[[29,33,"Mention"],[38,46,"Mention"],[894,901,"Mention"],[1648,1659,"Mention"],[2377,2393,"API"],[2394,2414,"API"],[2415,2431,"API"],[2432,2452,"API"]],"Comments":[]}
{"id":60481,"text":"ID:45415453\nPost:\nText: From your example I reconstruct that you want to solve for the second and third component of some vector x as well as the parameter alpha. With the args keyword of scipy.optmize.root that would look something like \nCode: def func(x_solve, x0, x3):\n    #x_solve.size should be 3 \n    x = np.empty(4)\n    x[0], x[3] = x0, x3\n    x[1:3] = x_solve[:2]\n    alpha = x_solve[2]\n    ...\n\nscipy.optimize.root(func, [-1,2,-1], args=(.5, .3))\n\nText: As Azat and kazemakase pointed out, I'm also not sure if you actually want to use root, but the usage of minimize is pretty much the same. \nText: Edit: It should be possible to have a flexible set of fixed variables by using a dictionary as an additional argument which specifies those: \nCode: def func(x_solve, fixed):\n    x = x_solve[:-1] # last value is alpha\n    for idx in fixed.keys(): # overwrite fixed entries\n        x[idx] = fixed[idx]\n    alpha = x_solve[-1]\n\n# fixed variables, key is the index\nfixed_vars = {0:.5, 3:.3}\n\n# find roots\nscipy.optimize.root(func, \n                    [.5, -1, 2, .3, -1], \n                    args=(fixed_vars,))\n\nText: That way, when the optimizer in root numerically evaluates the Jacobian it obtains zero for the fixed variables and should therefore leave those invariant. However, that might lead to complications in the convergence of the algorithm. \nAPI:\nscipy.optimize.minimize\n","label":[[568,576,"Mention"],[1367,1390,"API"]],"Comments":[]}
{"id":60482,"text":"ID:45416493\nPost:\nText: The reason for that error is that although you don't want these bounds to depend on the variables, nquad still passes the variables to the functions you provide to it. So the bound functions have to take the right number of variables: \nCode: def wbound():\n    return [1.23*10**10,3.1*10**16]\n\ndef zbound(w_foo):\n    return [10**(-10),pi-10**(-10)]\n\ndef ybound(z, w_foo):\n    return [0,bmax(z)-10**(-10)]\n\ndef xbound(z,y,w): \n    return [thetal(z,y,w),pi-thetal(z,y,w)]\n\nText: Now the functions zbound and ybound accept the extra variables but simply ignore them. \nText: I'm not sure about the last bound, xbound(...): Do you want the variables y and z to be flipped? The supposedly correct ordering according to the definition of si.nquad would be \nCode: def xbound(y,z,w):\n    ... \n\nText: Edit: As kazemakase pointed out, the function f should return a float instead of a list so the brackets [...] in the return statement should be removed. \nAPI:\nscipy.integrate.nquad\n","label":[[754,762,"Mention"],[973,994,"API"]],"Comments":[]}
{"id":60483,"text":"ID:45535523\nPost:\nText: I'd say the \"correct\" way of going about this is to use the SVD, look at your singular value spectrum, and figure out how many singular values you want to keep, i.e., figure out how close you want A^T x to be to b. Something along these lines: \nCode: def svd_solve(a, b):\n    [U, s, Vt] = la.svd(a, full_matrices=False)\n    r = max(np.where(s >= 1e-12)[0])\n    temp = np.dot(U[:, :r].T, b) \/ s[:r]\n    return np.dot(Vt[:r, :].T, temp)\n\nText: However, for a matrix of size (100000, 500), this is just going to be way too slow. I would recommend implementing least squares by yourself, and adding a small amount of regularization to avoid the issue of the matrix becoming singular. \nCode: def naive_solve(a, b, lamda):\n    return la.solve(np.dot(a.T, a) + lamda * np.identity(a.shape[1]),\n                    np.dot(a.T, b))\n\ndef pos_solve(a, b, lamda):\n    return la.solve(np.dot(a.T, a) + lamda * np.identity(a.shape[1]),\n                    np.dot(a.T, b), assume_a='pos')\n\nText: Here's a timing analysis on my workstation*: \nCode: >>> %timeit la.lstsq(a, b)\n1.84 s  39.2 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n>>> %timeit naive_solve(a, b, 1e-25)\n140 ms  4.15 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n>>> %timeit pos_solve(a, b, 1e-25)\n135 ms  768 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n\nText: *I somehow don't seem to have sps.linalg.lsmr on my machine, so I couldn't compare against that. \nText: It doesn't seem to do much here, but I've seen elsewhere that adding the assume_a='pos' flag can actually give you a lot of benefit. You can certainly do this here, since A^T A is guaranteed to be positive semi-definite, and the lamda makes it positive definite. You might have to play with lamda a little to bring your error sufficiently low. \nText: And in terms of error: \nCode: >>> xhat_lstsq = la.lstsq(a, b)[0]\n>>> la.norm(np.dot(a, xhat_lstsq) - b)\n1.4628232073579952e-13\n>>> xhat_naive = naive_solve(a, b, 1e-25)\n>>> la.norm(np.dot(a, xhat_naive) - b)\n7.474566255470176e-13\n>>> xhat_pos = pos_solve(a, b, 1e-25)\n>>> la.norm(np.dot(a, xhat_pos) - b)\n7.476075564322223e-13\n\nText: PS: I generated an a and a b of my own like this: \nCode: s = np.logspace(1, -20, 500)\nu = np.random.randn(100000, 500)\nu \/= la.norm(u, axis=0)[np.newaxis, :]\na = np.dot(u, np.diag(s))\nx = np.random.randn(500)\nb = np.dot(a, x)\n\nText: My a isn't completely singular, but near-singular. \nText: Response to comment \nText: I guess what you are trying to do is to find a feasible point under some linear equality constraints. The trouble here is that you don't know which constraints are important. Each of the 100,000 rows of A gives you a new constraint, out of which at most 500, but possibly far fewer (because of underdetermined-ness), actually matter. The SVD gives you a way of figuring out which dimensions are important. I don't know of another way to do this: you might find something in convex optimization or linear programming literature. If you know a priori that the rank of A is r, then you can try to find only the first r singular values and corresponding vectors, which might save time if r << n. \nText: Regarding your other question, the minimum norm solution isn't the \"best\" or even the \"correct\" solution. Since your system is underdetermined, you need to throw in some additional constraints or assumptions which will help you find a unique solution. The minimum norm constraint is one such. The minimum norm solution is often considered to be \"good\", because if x is some physical signal which you are trying to design, then an x with lower norm often corresponds to a physical signal with lower energy, which then translates to cost savings, etc. Alternatively, if x is a parameter of some system you are trying to estimate, then choosing the minimum norm solution means you are assuming that the system is efficient in some way, and uses only the minimum energy needed to produce the outcome b. Hope that all makes sense. \nAPI:\nscipy.sparse.linalg.lsmr\n","label":[[1400,1415,"Mention"],[4008,4032,"API"]],"Comments":[]}
{"id":60484,"text":"ID:45800567\nPost:\nText: statsmodels is not trying to duplicate staets (*). So there is no equivalent to scipy.stats.distributions. \nText: The emphasis in statsmodels is in regression models for different distributions. So there is some overlap in that statsmodels uses normal distribution, discrete distributions like Binomial, Poisson and NegativeBinomial or distribution families for generalized linear models. \nText: There are also a few cases where statmodels supports distribution functionality that is not available in scipy. \nText: (*) There is overlap and duplication between statsmodels and vstats in some of the basic statistics and hypothesis tests like t-tests with different options and extensions in some cases. \nAPI:\nscipy.stats\nscipy.stats\n","label":[[63,69,"Mention"],[600,606,"Mention"],[732,743,"API"],[744,755,"API"]],"Comments":[]}
{"id":60485,"text":"ID:45832616\nPost:\nText: Wrapping the function in a sp.LowLevelCallable makes nquad happy: \nCode: si.nquad(sp.LowLevelCallable(nb_func.ctypes), [[0,1],[0,1]], full_output=True)\n# (-2.3958561404687756e-19, 7.002641250699693e-15, {'neval': 1323})\n\nAPI:\nscipy.LowLevelCallable\n","label":[[51,70,"Mention"],[250,272,"API"]],"Comments":[]}
{"id":60486,"text":"ID:45902569\nPost:\nText: The chance that a sample out of a Poisson distribution with mean equal to 2 is greater than 140 is so small that you would not get one out of just 10000 samples. \nText: Indeed, the Poisson distribution has only one parameter  and a probability mass function defined so that \nCode: P(x=k) = ^k * exp(-) \/ k!\n\nText: The mean value is also equal to . If  = 2 then \nCode: P(x=140) = 7.7e-199\n\nText: so if there are 10000 samples the chance that there would be at least one sample at 140 out of 10000 would be less than 7.7e-195. This is a number so small that you cannot expect this to occur in a lifetime. \nText: It is a bit harder to compute the probability that a sample out of Poisson distribution with =2 lies above 140. You can use scipy.stats.poisson.cdf to see that \nCode: P(x>=22) = 1 - scipy.stats.poisson.cdf(21,2) = 5.5e-16\n\nText: Therefore even the chance that you would have one sample out of 10000 above 21 is less than 5.5e-12. Computing P(x>=140) in the same way would return 0 because of floating point rounding in intermediate results. \nText: Conclusion \nText: If you want distribution mean equal 2.0, and a heavy tail reaching up to 140 on 10000 samples you need a distribution different from Poisson. You could consider Pareto distribution, st.pareto with parameter b = 2. \nText: Here is a comparison of 10000 random samples from \nCode: scipy.stats.poisson.rvs(2,size=10000)\n\nText: and \nCode: numpy.rint(scipy.stats.pareto.rvs(2,size=10000))\n\nText: It is clearly visible that Pareto distribution with the same mean has a much heavier tail. \nText: For reference the code for the plot is below \nCode: import matplotlib.pyplot as plt\nimport scipy.stats\nimport numpy as np\npareto_x = np.rint(scipy.stats.pareto.rvs(2,size=10000))\npoisson_x = scipy.stats.poisson.rvs(2,size=10000)\nplt.figure(figsize=(8,4))\nplt.subplot(121)\nplt.title(\"Poisson distribution, a = 2\")\nplt.xlabel(\"sample number\")\nplt.ylabel(\"sample value\")\nplt.axis([0,10000,0,180])\nplt.plot(range(0,10000),poisson_x,\"o\")\nplt.subplot(122)\nplt.axis([0,10000,0,180])\nplt.title(\"Pareto distribution, b = 2\")\nplt.xlabel(\"sample number\")\nplt.plot(range(0,10000),pareto_x,\"o\")\nplt.subplots_adjust(hspace=0.4,bottom=0.2)\nplt.savefig(\"poisson_pareto.png\")\n\nAPI:\nscipy.stats.pareto\n","label":[[1288,1297,"Mention"],[2259,2277,"API"]],"Comments":[]}
{"id":60487,"text":"ID:46018731\nPost:\nText: Code: \nCode: import numpy as np\nfrom scipy.optimize import minimize\n\ndef fun(x):\n  return np.sum(x[2:])\n\nx0 = np.zeros(4)  # lambda1, lambda 2, w1, w2\ncons = ({'type': 'ineq', 'fun': lambda x:  x[2] - 2 + 10 * x[0] + 3 * x[1]},\n        {'type': 'ineq', 'fun': lambda x:  x[3] - 2 + 5 * x[0] + 5 * x[1]},\n        {'type': 'ineq', 'fun': lambda x:  x[2]},\n        {'type': 'ineq', 'fun': lambda x:  x[3]},\n        {'type': 'eq', 'fun': lambda x:  x[0] + x[1] - 1})\nres = minimize(fun, x0, constraints=cons)\nprint(res)\nprint(np.round(res.x, 2))\n\nText: Output: \nCode: fun: -3.3306690738754696e-16\njac: array([ 0.,  0.,  1.,  1.])\nmessage: 'Optimization terminated successfully.'\nnfev: 7\nnit: 1\nnjev: 1\nstatus: 0\nsuccess: True\n  x: array([  5.00000000e-01,   5.00000000e-01,  -3.33066907e-16,\n    0.00000000e+00])\n[ 0.5  0.5 -0.   0. ]\n\nText: This is basically only using information from the official docs. \nText: Edit I used the general optimization functions here, but you probably should use linprog as this is an LP! \nText: I did not check it, but linprog-usage looks somewhat like: \nCode: from scipy.optimize import linprog\nc = [0, 0, 1, 1]\nA = [[-10, -3, -1, 0], [-5, -5, 0, -1]]\nb = [-2, -2]\nA_eq = [[1, 1, 0, 0]]\nb_eq = [1]\nx0_bnds = (-np.inf, -np.inf, 0, 0)\nx1_bnds = (np.inf, np.inf, np.inf, np.inf)\nres = linprog(c, A, b, A_eq, b_eq, bounds=list(zip(x0_bnds, x1_bnds)))\nprint(res)\n\nText: Output: \nCode:  fun: -0.0\n message: 'Optimization terminated successfully.'\n nit: 4\n slack: array([ 0.,  3.])\n status: 0\n success: True\n   x: array([-0.14285714,  1.14285714,  0.        ,  0.        ])\n\nAPI:\nscipy.optimize.linprog\n","label":[[1015,1022,"Mention"],[1627,1649,"API"]],"Comments":[]}
{"id":60488,"text":"ID:46039946\nPost:\nText: The LowLevelCallable is a class of functions that must be accepted by the underlying Python module. This work has been done for a few modules, including the quadrature routine sp.integrate.quad \nText: If you wish to use the same wrapping method, you must either go through the SciPy routines that make use of it, such as ndi.generic_filter1d or scipy.integrate.quad. The code sits in compiled extensions, however. \nText: The alternative, if your problem is reasonably well defined for the callback, is to implement this yourself. I have done this in one of my codes, so I post the link for simplicity: \nText: In a .pxd file, I define the interface cyfunc_d_d: https:\/\/github.com\/pdebuyl\/skl1\/blob\/master\/skl1\/core.pxd I can re-use this interface in the \"base\" cython module https:\/\/github.com\/pdebuyl\/skl1\/blob\/master\/skl1\/euler.pyx and also in a \"user-defined\" module. \nText: The final code makes plain \"cython-cython\" calls while allowing the passing of objects at the Cython level \nText: I adapted the code to your problem: \nText: test_interface.pxd cdef class cyfunc: cpdef double f(self, double x) cdef class pyfunc(cyfunc): cdef object py_f cpdef double f(self, double x) test_interface.pyx cdef class cyfunc: cpdef double f(self, double x): return 0 def __cinit__(self): pass cdef class pyfunc(cyfunc): cpdef double f(self, double x): return self.py_f(x) def __init__(self, f): self.py_f = f setup.py from setuptools import setup, Extension from Cython.Build import cythonize setup( ext_modules=cythonize((Extension('test_interface', [\"test_interface.pyx\"]), Extension('test_module', [\"test_module.pyx\"])) ) ) test_module.pyx from test_interface cimport cyfunc, pyfunc cpdef min_arg(f, int N): cdef double x = 100000. cdef int best_i = -1 cdef int i cdef double current_value cdef cyfunc py_f if isinstance(f, cyfunc): py_f = f print('cyfunc') elif callable(f): py_f = pyfunc(f) print('no cyfunc') else: raise ValueError(\"f should be a callable or a cyfunc\") for i in range(N): current_value = py_f.f(i) if current_value < x: x = current_value best_i = i return best_i def py_f(x): return (x-5)**2 cdef class cy_f(cyfunc): cpdef double f(self, double x): return (x-5)**2 \nText: To use: \nCode: python3 setup.py build_ext --inplace\npython3 -c 'import test_module ; print(test_module.min_arg(test_module.cy_f(), 10))'\npython3 -c 'import test_module ; print(test_module.min_arg(test_module.py_f, 10))'\n\nAPI:\nscipy.integrate.quad\nscipy.ndimage.generic_filter1d\n","label":[[200,217,"Mention"],[345,365,"Mention"],[2435,2455,"API"],[2456,2486,"API"]],"Comments":[]}
{"id":60489,"text":"ID:46042457\nPost:\nText: You could use hstac to stack those two horizontally (column wise). We just need to convert the list to a column vector (speaking in terms of sparse matrices) or a 2D array with a single column - \nCode: scipy.sparse.hstack(( tweets, csr_matrix(lex).T ))\n\nscipy.sparse.hstack(( tweets, np.asarray(lex)[:,None] ))\n\nText: Sample run - \nCode: In [189]: from scipy.sparse import csr_matrix\n\nIn [194]: import scipy as sp\n\nIn [190]: a = np.random.randint(0,4,(5,10))\n\nIn [192]: a\nOut[192]: \narray([[2, 1, 1, 1, 0, 3, 1, 3, 2, 1],\n       [0, 2, 1, 2, 3, 0, 1, 1, 2, 3],\n       [0, 1, 1, 1, 2, 3, 0, 1, 0, 1],\n       [0, 0, 3, 0, 3, 0, 1, 0, 3, 1],\n       [1, 0, 2, 3, 3, 3, 2, 2, 0, 1]])\n\nIn [193]: b = [9,8,7,6,5]  # equivalent to lex\n\nIn [191]: A = csr_matrix(a)  # equivalent to tweets\n\nIn [195]: sp.sparse.hstack(( A, csr_matrix(b).T ))\nOut[195]: \n<5x11 sparse matrix of type '<type 'numpy.int64'>'\n    with 42 stored elements in COOrdinate format>\n\nIn [197]: _.toarray() # verify values by converting to dense array\nOut[197]: \narray([[2, 1, 1, 1, 0, 3, 1, 3, 2, 1, 9],\n       [0, 2, 1, 2, 3, 0, 1, 1, 2, 3, 8],\n       [0, 1, 1, 1, 2, 3, 0, 1, 0, 1, 7],\n       [0, 0, 3, 0, 3, 0, 1, 0, 3, 1, 6],\n       [1, 0, 2, 3, 3, 3, 2, 2, 0, 1, 5]])\n\nAPI:\nscipy.sparse.hstack\n","label":[[38,43,"Mention"],[1264,1283,"API"]],"Comments":[]}
{"id":60490,"text":"ID:46050624\nPost:\nText: rotate (manual) performs matrix rotation using spline interpolation and by default uses the spline order of 3, which may cause an output value to become larger than the input data. \nText: You could reduce the order of the used spline interpolation to 1, which changes the behavior to linear interpolation. To do this, modify the rotate line as follows: \nCode: B=rotate(A, angle=10, order=1, reshape=False, axes=(0,1))\n\nText: For more information about spline interpolation, see this introductory paper. \nAPI:\nscipy.ndimage.rotate\n","label":[[24,30,"Mention"],[533,553,"API"]],"Comments":[]}
{"id":60491,"text":"ID:46267570\nPost:\nText: I assume that what you want to do is a chi-squared test for independence of the proportions between the different classes (see https:\/\/en.wikipedia.org\/wiki\/Pearson%27s_chi-squared_test#Testing_for_statistical_independence). You can do this in Python using the function st.chi2_contingency (see https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.chi2_contingency.html for documentation). \nAPI:\nscipy.stats.chi2_contingency\n","label":[[294,313,"Mention"],[432,460,"API"]],"Comments":[]}
{"id":60492,"text":"ID:46366383\nPost:\nText: The implementation of stats_mode has a Python loop for handling the axis argument with multidimensional arrays. The following simple implementation, for one-dimensional arrays only, is faster: \nCode: def mode1(x):\n    values, counts = np.unique(x, return_counts=True)\n    m = counts.argmax()\n    return values[m], counts[m]\n\nText: Here's an example. First, make an array of integers with length 1000000. \nCode: In [40]: x = np.random.randint(0, 1000, size=(2, 1000000)).sum(axis=0)\n\nIn [41]: x.shape\nOut[41]: (1000000,)\n\nText: Check that mode and mode1 give the same result. \nCode: In [42]: from scipy.stats import mode\n\nIn [43]: mode(x)\nOut[43]: ModeResult(mode=array([1009]), count=array([1066]))\n\nIn [44]: mode1(x)\nOut[44]: (1009, 1066)\n\nText: Now check the performance. \nCode: In [45]: %timeit mode(x)\n2.91 s  18 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n\nIn [46]: %timeit mode1(x)\n39.6 ms  83.8 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n\nText: 2.91 seconds for mode(x) and only 39.6 milliseconds for mode1(x). \nAPI:\nscipy.stats.mode\nscipy.stats.mode\n","label":[[46,56,"Mention"],[562,566,"Mention"],[1073,1089,"API"],[1090,1106,"API"]],"Comments":[]}
{"id":60493,"text":"ID:46732189\nPost:\nText: So, after some work and an idea from the scipy mailing list, I think that in my case (with a constant A and slowly varying k), the best way to do this is to use the following implementation. \nCode: class SearchSorted:\n    def __init__(self, tensor, use_k_optimization=True):\n\n        '''\n        use_k_optimization requires storing 4x the size of the tensor.\n        If use_k_optimization is True, the class will assume that successive calls will be made with similar k.\n        When this happens, we can cut the running time significantly by storing additional variables. If it won't be\n        called with successive k, set the flag to False, as otherwise would just consume more memory for no\n        good reason\n        '''\n\n        self.indices_sort = np.argsort(tensor)\n        self.sorted_tensor = tensor[self.indices_sort]\n        self.inv_indices_sort = np.argsort(self.indices_sort)\n        self.use_k_optimization = use_k_optimization\n\n        self.previous_indices_results = None\n        self.prev_idx_A_k_pair = None\n\n    def query(self, k):\n        midpoints = k[:-1] + np.diff(k) \/ 2\n        idx_count = np.searchsorted(self.sorted_tensor, midpoints)\n        idx_A_k_pair = []\n        count = 0\n\n        old_obj = 0\n        for obj in idx_count:\n            if obj != old_obj:\n                idx_A_k_pair.append((obj, count))\n                old_obj = obj\n            count += 1\n\n        if not self.use_k_optimization or self.previous_indices_results is None:\n            #creates the index matrix in the sorted case\n            final_indices = self._create_indices_matrix(idx_A_k_pair, self.sorted_tensor.shape, len(k))\n            #and now unsort it to match the original tensor position\n            indicesClosest = final_indices[self.inv_indices_sort]\n            if self.use_k_optimization:\n                self.prev_idx_A_k_pair = idx_A_k_pair\n                self.previous_indices_results = indicesClosest\n            return indicesClosest\n\n        old_indices_unsorted = self._create_indices_matrix(self.prev_idx_A_k_pair, self.sorted_tensor.shape, len(k))\n        new_indices_unsorted = self._create_indices_matrix(idx_A_k_pair, self.sorted_tensor.shape, len(k))\n        mask = new_indices_unsorted != old_indices_unsorted\n\n        self.prev_idx_A_k_pair = idx_A_k_pair\n        self.previous_indices_results[self.indices_sort[mask]] = new_indices_unsorted[mask]\n        indicesClosest = self.previous_indices_results\n\n        return indicesClosest\n\n    @staticmethod\n    def _create_indices_matrix(idx_A_k_pair, matrix_shape, len_quant_points):\n        old_idx = 0\n        final_indices = np.zeros(matrix_shape, dtype=int)\n        for idx_A, idx_k in idx_A_k_pair:\n            final_indices[old_idx:idx_A] = idx_k\n            old_idx = idx_A\n        final_indices[old_idx:] = len_quant_points - 1\n        return final_indices\n\nText: The idea is to sort the array A beforehand, then use searchsorted of A on the midpoints of k. This gives the same information as before, in that it tells us exactly which points of A are closer to which points of k. The method _create_indices_matrix will create the full indices array from these informations, and then we will unsort it to recover the original order of A. To take advantage of slowly varying k, we save the last indices and we determine which indices we have to change; we then change only those. For slowly varying k, this produces superior performance (at a quite bigger memory cost, however). \nText: For random matrix A of 5 million elements and k of about 30 elements, and repeating the experiments 60 times, we get \nCode: Function search_sorted1; 15.72285795211792s\nFunction search_sorted2; 13.030786037445068s\nFunction query; 2.3306031227111816s <- the one with use_k_optimization = True\nFunction query; 4.81286096572876s   <- with use_k_optimization = False\n\nText: quwry is too slow, and I don't time it (above 1 minute, though). This is the code used to do the timing; contains also the implementation of search_sorted1 and 2. \nCode: import numpy as np\nimport scipy\nimport scipy.spatial\nimport time\n\n\nA = np.random.rand(10000*500) #5 million elements\nk = np.random.rand(32)\nk.sort()\n\n#first attempt, detailed in the answer, too\ndef search_sorted1(A, k):\n    indicesClosest = np.searchsorted(k, A)\n    flagToReduce = indicesClosest == k.shape[0]\n    modifiedIndicesToAvoidOutOfBoundsException = indicesClosest.copy()\n    modifiedIndicesToAvoidOutOfBoundsException[flagToReduce] -= 1\n\n    flagToReduce = np.logical_or(flagToReduce,\n                        np.abs(A-k[indicesClosest-1]) <\n                        np.abs(A - k[modifiedIndicesToAvoidOutOfBoundsException]))\n    flagToReduce = np.logical_and(indicesClosest > 0, flagToReduce)\n    indicesClosest[flagToReduce] -= 1\n\n    return indicesClosest\n\n#taken from @Divakar answer linked in the comments under the question\ndef search_sorted2(A, k):\n    indicesClosest = np.searchsorted(k, A, side=\"left\").clip(max=k.size - 1)\n    mask = (indicesClosest > 0) & \\\n           ((indicesClosest == len(k)) | (np.fabs(A - k[indicesClosest - 1]) < np.fabs(A - k[indicesClosest])))\n    indicesClosest = indicesClosest - mask\n\n    return indicesClosest\ndef kdquery1(A, k):\n    d = scipy.spatial.cKDTree(k, compact_nodes=False, balanced_tree=False)\n    _, indices = d.query(A)\n    return indices\n\n#After an indea on scipy mailing list\nclass SearchSorted:\n    def __init__(self, tensor, use_k_optimization=True):\n\n        '''\n        Using this requires storing 4x the size of the tensor.\n        If use_k_optimization is True, the class will assume that successive calls will be made with similar k.\n        When this happens, we can cut the running time significantly by storing additional variables. If it won't be\n        called with successive k, set the flag to False, as otherwise would just consume more memory for no\n        good reason\n        '''\n\n        self.indices_sort = np.argsort(tensor)\n        self.sorted_tensor = tensor[self.indices_sort]\n        self.inv_indices_sort = np.argsort(self.indices_sort)\n        self.use_k_optimization = use_k_optimization\n\n        self.previous_indices_results = None\n        self.prev_idx_A_k_pair = None\n\n    def query(self, k):\n        midpoints = k[:-1] + np.diff(k) \/ 2\n        idx_count = np.searchsorted(self.sorted_tensor, midpoints)\n        idx_A_k_pair = []\n        count = 0\n\n        old_obj = 0\n        for obj in idx_count:\n            if obj != old_obj:\n                idx_A_k_pair.append((obj, count))\n                old_obj = obj\n            count += 1\n\n        if not self.use_k_optimization or self.previous_indices_results is None:\n            #creates the index matrix in the sorted case\n            final_indices = self._create_indices_matrix(idx_A_k_pair, self.sorted_tensor.shape, len(k))\n            #and now unsort it to match the original tensor position\n            indicesClosest = final_indices[self.inv_indices_sort]\n            if self.use_k_optimization:\n                self.prev_idx_A_k_pair = idx_A_k_pair\n                self.previous_indices_results = indicesClosest\n            return indicesClosest\n\n        old_indices_unsorted = self._create_indices_matrix(self.prev_idx_A_k_pair, self.sorted_tensor.shape, len(k))\n        new_indices_unsorted = self._create_indices_matrix(idx_A_k_pair, self.sorted_tensor.shape, len(k))\n        mask = new_indices_unsorted != old_indices_unsorted\n\n        self.prev_idx_A_k_pair = idx_A_k_pair\n        self.previous_indices_results[self.indices_sort[mask]] = new_indices_unsorted[mask]\n        indicesClosest = self.previous_indices_results\n\n        return indicesClosest\n\n    @staticmethod\n    def _create_indices_matrix(idx_A_k_pair, matrix_shape, len_quant_points):\n        old_idx = 0\n        final_indices = np.zeros(matrix_shape, dtype=int)\n        for idx_A, idx_k in idx_A_k_pair:\n            final_indices[old_idx:idx_A] = idx_k\n            old_idx = idx_A\n        final_indices[old_idx:] = len_quant_points - 1\n        return final_indices\n\nmySearchSorted = SearchSorted(A, use_k_optimization=True)\nmySearchSorted2 = SearchSorted(A, use_k_optimization=False)\nallFunctions = [search_sorted1, search_sorted2,\n                mySearchSorted.query,\n                mySearchSorted2.query]\n\nprint(np.array_equal(mySearchSorted.query(k), kdquery1(A, k)[1]))\nprint(np.array_equal(mySearchSorted.query(k), search_sorted2(A, k)[1]))\nprint(np.array_equal(mySearchSorted2.query(k), search_sorted2(A, k)[1]))\n\nif __name__== '__main__':\n    num_to_average = 3\n    for func in allFunctions:\n        if func.__name__ == 'search_sorted3':\n            indices_sort = np.argsort(A)\n            sA = A[indices_sort].copy()\n            inv_indices_sort = np.argsort(indices_sort)\n        else:\n            sA = A.copy()\n        if func.__name__ != 'query':\n            func_to_use = lambda x: func(sA, x)\n        else:\n            func_to_use = func\n        k_to_use = k\n        start_time = time.time()\n        for idx_average in range(num_to_average):\n            for idx_repeat in range(10):\n                k_to_use += (2*np.random.rand(*k.shape)-1)\/100 #uniform between (-1\/100, 1\/100)\n                k_to_use.sort()\n                indices = func_to_use(k_to_use)\n                if func.__name__ == 'search_sorted3':\n                    indices = indices[inv_indices_sort]\n                val = k[indices]\n\n        end_time = time.time()\n        total_time = end_time-start_time\n\n        print('Function {}; {}s'.format(func.__name__, total_time))\n\nText: I'm sure that it still possible to do better (I use a loot of space for SerchSorted class, so we could probably save something). If you have any ideas for an improvement, please let me know! \nAPI:\nscipy.spatial.KDTree.query\n","label":[[3872,3877,"Mention"],[9728,9754,"API"]],"Comments":[]}
{"id":60494,"text":"ID:46838479\nPost:\nText: Your gradient is broken! (somehow: math vs. np.shapes?) \nText: I'm not gonna analyze your gradient (looks like logistic regression; read this then), but here a demo which supports my claim: \nText: sp.optimize.check_grad -> failing! \nCode: from scipy.optimize import check_grad\nfrom sklearn.datasets import make_classification\nX_examples, Y_labels = make_classification()\n\n...\n\nprint(check_grad(cost, gradient, np.random.random(size=20), X_examples, Y_labels))\n# 61.666912359 (example value; indeterministic because of PRNG-usage above)\n# result: The square root of the sum of squares (i.e. the 2-norm) of the difference between\n# grad(x0, *args) and the finite difference approximation of grad using func\n# at the points x0.\n\nText: scipy.optimize.fmin_tnc(...fprime=None, approx_grad=True...) -> works! \nText: Using numerical-differentiation automatically: \nCode: result = opt.fmin_tnc(func=cost, x0=x0, fprime=None, approx_grad=True, args=\n(X_examples, Y_labels))\n\nText: Output: \nCode: NIT   NF   F                       GTG\n  0    1  6.931471805599453E+01   3.69939729E+03\ntnc: fscale = 0.0164412\n  1   14  5.488792098836968E+01   1.91876351E+03\n  2   26  4.598767699927674E+01   5.90835511E+02\n  3   39  4.255784649333560E+01   3.42105829E+02\n  4   44  3.153577598035866E+01   3.09160832E+02\n  5   50  2.224511577357391E+01   4.36685983E+01\n  6   54  2.157944424362721E+01   3.39632081E+01\n  7   59  2.136340974081865E+01   2.97596794E+01\n  8   73  1.997400905570375E+01   1.08022452E+01\n  9   75  1.984787529493228E+01   1.05379689E+01\n 10   79  1.979578396181381E+01   1.16542972E+01\n 11   88  1.939906531954665E+01   9.19521103E+00\ntnc: fscale = 0.329776\n 12   91  1.867469105176042E+01   4.61533306E+00\n 13   94  1.834698220306902E+01   1.37837652E+00\n 14   98  1.818150860822102E+01   1.39090344E+00\n 15  102  1.817553527302105E+01   1.22472879E+00\n 16  118  1.810790027768505E+01   6.81994565E-01\n 17  134  1.807103645037105E+01   1.11197844E+00\n 18  148  1.805232746344993E+01   1.31428979E+00\n 19  154  1.804213064407819E+01   7.10363935E-01\n 20  168  1.803844265918305E+01   5.98840568E-01\n 21  170  1.803426849431969E+01   7.44231905E-01\n 22  175  1.803266497733540E+01   8.68201237E-01\n 23  189  1.799550032713761E+01   1.13982866E+00\n 24  191  1.799048283812780E+01   7.41273919E-01\n 24  200  1.799048283812780E+01   7.41273919E-01\ntnc: Maximum number of function evaluations reached\n\nText: Additional remark: np.matrix is not recommended (use np.array)! \nAPI:\nscipy.optimize.check_grad\n","label":[[221,243,"Mention"],[2500,2525,"API"]],"Comments":[]}
{"id":60495,"text":"ID:46839809\nPost:\nText: You may want to look at Utilizing minimize with multiple variables of different shapes. What is important to understand is that if you want to use minimize with arrays, you should pass in the flattened version and then reshape. For that reason I always include the desired shape as one of the arguments to the minimize function. In your case I would do something like so: \nCode: def cross_section_obj(x, *args):\n    xyz, pt0, pt1, pt2, shape = args\n    x = x.reshape(shape)\n    s = beam_shear(xyz, pt0, pt1, pt2, x)\n    l_s = normrow(s)\n    val = sum(l_s)\n    return val\n\nText: Then your minimize call would change like so: \nCode: res = minimize(cross_section_obj, EIx.flatten(), method='SLSQP',\n               bounds=bounds, args=(xyz, pt0, pt1, pt2, EIx.shape))\n\nAPI:\nscipy.optimize.minimize\n","label":[[58,66,"Mention"],[794,817,"API"]],"Comments":[]}
{"id":60496,"text":"ID:46854730\nPost:\nText: It is my understanding that the a and b are internal parameters, are not used in scipy.stats.uniform, because their normal functionality is basically duplicative of the loc and scale parameters. \nText: As mentioned in the uniform documentation \"This distribution is constant between loc and loc + scale.\" \nText: So I don't think this is a bug, because the values of a and b should be treated as an implementation detail rather than a user-facing feature. \nAPI:\nscipy.stats.uniform\n","label":[[246,253,"Mention"],[485,504,"API"]],"Comments":[]}
{"id":60497,"text":"ID:46895784\nPost:\nText: One way to obtain k flat clusters is to use sch.fcluster with criterion='maxclust': \nCode: from scipy.cluster.hierarchy import fcluster\nclust = fcluster(Z, k, criterion='maxclust')\n\nAPI:\nscipy.cluster.hierarchy.fcluster\n","label":[[68,80,"Mention"],[211,243,"API"]],"Comments":[]}
{"id":60498,"text":"ID:46927121\nPost:\nText: The distributions in stats have a mean method that (unsurprisingly) computes the mean. \nText: You take the fitted parameters returned by scipy.stats.gamma.fit and pass them to scipy.stats.gamma.mean: \nCode: data = stats.gamma.rvs(5, 2, size=1000);  # generate example data\n\nparams = scipy.stats.gamma.fit(data)\nprint(scipy.stats.gamma.mean(*params))\n# 6.99807037952\n\nAPI:\nscipy.stats\n","label":[[45,50,"Mention"],[396,407,"API"]],"Comments":[]}
{"id":60499,"text":"ID:46953322\nPost:\nText: You can generate use from the stats module the f distribution and ask random values from it given the parameters you already found using the f.rvs method which accepts the four parameters plus the size (number of draws you want). \nCode: from scipy.stats import f\nimport matplotlib.pyplot as plt\n\nvalues = f.rvs(1441.41, 19.1, -0.24, 26.5, 100000)\n\nText: values is a 100000 length array with draws from the given distribution. You can see it as follows \nCode: plt.hist(values, bins=25)\nplt.show()\n\nAPI:\nscipy.stats\n","label":[[54,59,"Mention"],[526,537,"API"]],"Comments":[]}
{"id":60500,"text":"ID:47081379\nPost:\nText: Neither sklearn.BallTree nor cKDTree can be easily modified to handle periodic boundary conditions. Periodic boundaries require a fundamentally different set of assumptions than those used in these implementations. \nText: You should consider using an alternative implementation, such as periodic_kdtree, though note that this is a (somewhat stale) Python implementation and won't be nearly as fast as the Cython\/C++ implementations you mention. \nAPI:\nscipy.spatial.cKDTree\n","label":[[53,60,"Mention"],[475,496,"API"]],"Comments":[]}
{"id":60501,"text":"ID:47143789\nPost:\nText: If the distribution is one of those that is available in stats then you can evaluate its integral between the two bounds using the cdf for that distribution. Otherwise, you can define the pdf for rv_continuous and then use its cdf to get this integral. \nText: Now, you have, in effect, the pdf for the bounded version of the pdf you want because you have calculated the normalising constant for it, in that integral. You can proceed to use rv_continuous with the form that you have for the pdf plus the normalising constant and with the bounds. \nText: Here's what your code might be like. The variable scale is set according to the scipy documents. norm is the integral of the exponential pdf over [0,1]. Only about .49 of the probability mass is accounted for. Therefore, to make the exponential, when truncated to the [0,1] interval give a mass of one we must divide its pdf by this factor. \nText: Truncated_expon is defined as a subclass of rv_continuous as in the documentation. By supplying its pdf we make it possible (at least for such a simple integral!) for scipy to calculate this distribution's cdf and thereby to calculate random samples. \nText: I have calculated the cdf at one as a check. \nCode: >>> from scipy import stats\n>>> lamda = 2\/3\n>>> scale = 1\/lamda\n>>> norm = stats.expon.cdf(1, scale=scale)\n>>> norm\n0.48658288096740798\n>>> from math import exp\n>>> class Truncated_expon(stats.rv_continuous):\n...     def _pdf(self, x, lamda):\n...         return lamda*exp(-lamda*x)\/0.48658288096740798\n... \n>>> e = Truncated_expon(a=0, b=1, shapes='lamda')\n>>> e.cdf(1, lamda=lamda)\n1.0\n>>> e.rvs(size=20, lamda=lamda)\narray([ 0.20064067,  0.67646465,  0.89118679,  0.86093035,  0.14334989,\n        0.10505598,  0.53488779,  0.11606106,  0.41296616,  0.33650899,\n        0.95126415,  0.57481087,  0.04495104,  0.00308469,  0.23585195,\n        0.00653972,  0.59400395,  0.34919065,  0.91762547,  0.40098409])\n\nAPI:\nscipy.stats\n","label":[[81,86,"Mention"],[1948,1959,"API"]],"Comments":[]}
{"id":60502,"text":"ID:47427029\nPost:\nText: We can use scipy.optimize library of python. \nText: I used opt.fmin_tnc in the following way. \nCode: def neg_bspline( x ):\nglobal tck\nf = -interpolate.bisplev( x[0], x[1], tck, dx=0, dy=0)\ng = [-interpolate.bisplev( x[0], x[1], tck, dx=1, dy=0 ), -interpolate.bisplev( x[0], x[1], tck, dx=0, dy=1)]\nreturn f, g\n\nfor i in sensor_array:\n    x0 = i.get_coordinate()\n    print x0\n    bounds = [(0,200) , (0,200)]\n    x0 = fmin_tnc(neg_bspline, x0=x0, bounds=bounds)\n    print x0\n    solutions.append( x0[0] )\nresult_plot( solutions )\n\nAPI:\nscipy.optimize.fmin_tnc\n","label":[[83,95,"Mention"],[560,583,"API"]],"Comments":[]}
{"id":60503,"text":"ID:47548310\nPost:\nText: This is a solution using laestsq which gives me more freedom. On error evaluation you have to take some care, though. On the other hand it is not as strict as curve_fit concerning the number of parameters. In this solution I fit basically three lists, the amplitudes, frequencies, and phases. At seemed convenient to pass it sorted like this to the function. At the end you can fix any subset of frequencies. I had the impression, though, that convergence is very sensitive to starting parameters. \nCode: import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.optimize as so\n\n\ndef multisine(x, ampList, freqList, phaseList):\n    assert len( ampList ) == len( freqList )\n    assert len( ampList ) == len( phaseList )\n    out=0\n    for a, f, p in zip( ampList, freqList, phaseList ):\n        out += a * np.sin( x * f + p )\n    return out\n\n\n### FixedFrequencies is a list of values and positions in the list to pass to multisine....remember counting from zero\ndef multisine_fixed_fs(x, params, n, FixedFrequencies=None):\n    if FixedFrequencies is None:\n        assert len( params ) == 3 *  n\n        ampList = params[ : n]\n        freqList = params[ n : 2* n] \n        phaseList = params[ 2 * n : ]\n    else:\n        assert len( params ) + len( FixedFrequencies ) == 3 *  n\n        ampList = params[ : n]\n        freqList = list(params[ n : -n ])\n        phaseList = params[ -n : ]\n        sortedList = sorted( list(FixedFrequencies), key=lambda x: x[-1] )\n        for fixed in sortedList:\n            freqList.insert(fixed[-1], fixed[0] )\n\n    return multisine(x, ampList, freqList, phaseList)\n\n\ndef residuals(params, data, n, FixedFrequencies=None):\n    xList, yList = zip( *data )\n    thyList = [ multisine_fixed_fs( x, params, n , FixedFrequencies=FixedFrequencies ) for x in xList ]\n    d = [ y1- y2 for y1, y2 in zip( yList, thyList ) ]\n    return d\n\n\n\nxList = np.linspace( 0, 100, 100 )\nyList = np.fromiter( ( multisine(x, [ 1, .3 ], [ .4, .42 ],[ 0, .1] ) for x in xList ), np.float )\n\ndata = zip( xList, yList )\n\nfit, err = so.leastsq( residuals,  x0=[ 1.2, .32 ] + [ .42, .43 ] + [ 0.1, 0.12 ], args=( data, 2 ) )\nprint fit\n\nfit, err = so.leastsq( residuals,  x0=[ 1.2, .32 ] + [ .42 ] + [ 0.1, 0.12 ], args=( data, 2 , [ [ .45, 1 ] ]) )\nprint fit\ny2List = np.fromiter( ( multisine(x, [ fit[0], fit[1] ], [ fit[2], .45 ],[ fit[-2], fit[-1] ] ) for x in xList ), np.float )\n\nfit, err = so.leastsq( residuals,  x0=[ 1.2, .32 ]  + [ 0.1, 0.12 ], args=( data, 2 , [ [ .39, 0 ],[ .45, 1 ] ]) )\nprint fit\ny3List = np.fromiter( ( multisine(x, [ fit[0], fit[1] ], [ .39, .45 ],[ fit[-2], fit[-1] ] ) for x in xList ), np.float )\n\nfig = plt.figure(1)\nax = fig.add_subplot( 1, 1, 1 )\nax.plot(xList,yList)\nax.plot(xList,y2List)\nax.plot(xList,y3List)\n\nplt.show()\n\nText: Providing: \nCode: >> [ 1.00000006e+00   2.99999889e-01   3.99999999e-01   4.20000009e-01 1.47117910e-07   6.38318486e+00 ]\n>> [ 1.12714624  0.12278804  0.40198029  0.08039605 -1.08564396 ]\n>> [ 1.05124097 -0.32600116  0.6633511   1.18400026 ]\n\nAPI:\nscipy.optimize.leastsq\n","label":[[49,56,"Mention"],[3048,3070,"API"]],"Comments":[]}
{"id":60504,"text":"ID:47578145\nPost:\nText: Working with binned data is essentially the same as working with weighted data. One uses the midpoint of each bin as a data point, and the count of that bin as its weight. If moment supported weights, we could do this computation directly. As is, use the method numpy.average which supports weights. \nCode: midpoints = 0.5 * (H[1][1:] + H[1][:-1])\nev = np.average(midpoints, weights = H[0])\nprint(ev)\nfor k in range(2, 5):\n  print(np.average((midpoints - ev)**k, weights = H[0]))\n\nText: Output (obviously random): \nCode: 1.08242834443\n4.21602099286\n0.713129264647\n51.6257736139\n\nText: I didn't print the centered 1st moment (which is 0 by construction), printing the expected value instead. Theoretically*, these are 1, 4, 0, 48 but for any given sample, there is going to be some deviation from the parameters of the distribution. \nText: (*) Not exactly. In the formula for variance I didn't include the correction factor n\/(n-1) (where n is the total size of data set, i.e., the sum of weights). This factor adjusts the sample variance so it becomes an unbiased estimator of the population variance. You can include it if you like. Similar adjustments are probably needed for higher-order moments (if the goal is to have unbiased estimators), but I'd have to look this up, and in any case this is not a statistics site. \nAPI:\nscipy.stats.moment\n","label":[[199,205,"Mention"],[1352,1370,"API"]],"Comments":[]}
{"id":60505,"text":"ID:47626085\nPost:\nText: You should just need sns.distplot with kde=True. This shows the kernel density estimator as a frequency curve. Your manual bar plots will obscure the curve due to the difference in scale however, so you should plot them on a secondary y axis or rescale them if they are needed. \nText: Alternatively sns.kdeplot plots only the KDE curve without the histogram bars. \nText: Update \nText: Try this: \nCode: fig, ax = plt.subplots()\nfor a in [x, y]:\n    sns.distplot(\n        a, bins=range(1, 25, 1), ax=ax,\n        kde=True, hist=False, fit=None)\n\nText: It is not really clear what sort of curve fit you want, but read the docs for distplot. kde=True adds KDE curves, hist=True adds bars, fit=stats.gamma fits a gamma distribution (shown as a black line). \nText: As for removing negative values, the curve will extend into the negative region because it is fitted to the data, which may have come from a distribution where negative values are allowed. Choose a different distribution from st (e.g. one that deals with strictly positive values) if you are looking for a different fit. \nAPI:\nscipy.stats\n","label":[[1008,1010,"Mention"],[1109,1120,"API"]],"Comments":[]}
{"id":60506,"text":"ID:47665568\nPost:\nText: Not sure I understand 100% what you want but how about: \nCode: freqs = np.random.uniform(100, 400, (20,))\ntimes = np.cumsum(np.random.uniform(0.01, 0.02, (20,)))\n# or np.arange(20) \/ sample_freq if evenly sampled\ngrid, dt = np.linspace(0, times[-1], 10000, retstep=True)\nsignal = np.sin(2*np.pi*dt*np.cumsum(np.interp(grid, times, freqs))) \n\nText: This uses linear interpolation between the sampled time points. Other methods are available in interpolate \nAPI:\nscipy.interpolate\n","label":[[467,478,"Mention"],[485,502,"API"]],"Comments":[]}
{"id":60507,"text":"ID:47708736\nPost:\nText: As pointed out by @Crasy Ivan, the chisquare test needs the same number of observations or counts in observed and expected counts. \nText: In this case, it seems to be that the data comes from two different samples. This is essentially a 2 by K contingency table and the hypothesis that each row has the same distribution will be corrected by the row totals in computing the expected frequency counts. \nText: So using chisquare test directly is the wrong test for the two sample case. atats has the chisquare test for independence in contingency tables. \nText: A general remark about very large counts which seems to be the case here: \nText: As the sample size grows the null hypothesis will be rejected and the pvalue goes to zero for any small but nonzero deviation from the null hypothesis. With counts, i.e. total number of observations, of more that 50,000 the proper hypthesis test will most likely reject even small differences that are statistically significant but irrelevant in applications. \nText: An alternative would be to use equivalence tests to test the hypothesis that the two distributions do not differ by more than some small margin. The difficulty is that it is difficult to specify what the equivalence thresholds should be in terms of some goodness-of-fit statistics. \nAPI:\nscipy.stats\n","label":[[508,513,"Mention"],[1320,1331,"API"]],"Comments":[]}
{"id":60508,"text":"ID:47711052\nPost:\nText: The short answer is: floc (and fscale for that matter) are used to specify that the location parameter (and scale parameter respectively) are to be kept fixed at the specified value. loc and scale merely give starting values for the fit. \nText: sp.stats.weibull_min inherits the fit method from scipy.stat.rv_continuous. The documentation of sp.stats.rv_continuous.fit specifies the fact, that floc and fscale keep said parameters fixed. loc, scale and other keyword arguments that are recognized by derived distributions are simply used as starting values. \nText: So if you want to fit keeping the location fixed, you should use floc=0 if you only want to provide a starting parameter, you should use loc=0. \nAPI:\nscipy.stats.rv_continuous.fit\n","label":[[366,392,"Mention"],[739,768,"API"]],"Comments":[]}
{"id":60509,"text":"ID:48063385\nPost:\nText: You will almost certainly need better starting values for the sigma and mu parameters. \nText: The lognorm().ppf() function diverges at x=1, giving huge values which will completely dominate any measure of misfit such as chi-square. In addition, small changes in parameter values will have essentially no effect on the total misfit, and all fitting algorithms will quickly give up. The huge value at x=1 will also make any fit insensitive to the other data. Perhaps you actually meant some other method of lognorm such pdf or cdf? \nText: If not, you may want to fit \"in log space\" -- fit the log of your data to the log of the model. That will reduce the importance of the x=1 datum. \nText: Also, though it is not the cause of the problem, your fit did not actually use the leastsq method as your comment says. To use the leastsq (Levenberg-Marquardt method) use: \nCode: # do fit, here with leastsq model\nresult = minimize(invlognorm, params, args=(x, y))\n\nText: To use sp.optimize.least_squares (which actually use trust region reflective use \nCode: # do fit, here with least_squares model\nresult = minimize(invlognorm,params, method='least_squares', args=(x, y))\n\nText: (note spelling. Your example used least-squares which is not recognized, and so causes the Nelder-Mead method to be used). \nAPI:\nscipy.optimize.least_squares\n","label":[[993,1018,"Mention"],[1324,1352,"API"]],"Comments":[]}
{"id":60510,"text":"ID:48147822\nPost:\nText: norm does indeed calculate the distribution parameters using maximum likelihood estimation. Specifically, it minimizes the log-likelihood. \nText: norm is a subclass of rv_continuous, which implements the fit method. Hence, you'll find the documentation of fit in the docs of rv_continuous. \nAPI:\nscipy.stats.norm\n","label":[[24,28,"Mention"],[320,336,"API"]],"Comments":[]}
{"id":60511,"text":"ID:48488777\nPost:\nText: In the example the difference is in floating point computation as pointed out. In general there might also be a truncation in expect depending on the integration tolerance. \nText: The mean and some other moments have for many distribution an analytical solution in which case we usually get a precise estimate. \nText: expect is a general function that computes the expectation for arbitrary (*) functions through summation in the discrete case and numerical integration in the continuous case. This accumulates floating point noise but also depends on the convergence criteria for the numerical integration and will, in general, be less precise than an analytically computed moment. \nText: (*) There might be numerical problems in the integration for some \"not nice\" functions, which can happen for example with default settings in si.quad \nAPI:\nscipy.integrate.quad\n","label":[[856,863,"Mention"],[870,890,"API"]],"Comments":[]}
{"id":60512,"text":"ID:48583089\nPost:\nText: The spectral resolution is determined by the number of points used in the FFT, which is controlled by the nperseg parameter. To increase the resolution you would increase the number of input points per FFT computation. For example, increasing the number of points from the default 256 to double the resolution (i.e. 512 points) you would call spectrogram like so: \nCode: f, t, Sxx = signal.spectrogram(x, fs, nperseg=512)\n\nText: Note that you could also use: \nCode: f, t, Sxx = signal.spectrogram(x, fs, nfft=512)\n\nText: to use more points in the FFT, but not more input point per segment (i.e. zero padding each segment). This would essentially produce a spectrogram where the additional frequency points are interpolated. It wouldn't increase the resolution (i.e. two tones with very similar frequencies wouldn't be any more distinguishable), but the additional points would make the result appear more smooth. \nAPI:\nscipy.signal.spectrogram\n","label":[[367,378,"Mention"],[943,967,"API"]],"Comments":[]}
{"id":60513,"text":"ID:48672158\nPost:\nText: The documentation says: \nText: extrapolate to out-of-bounds points based on first and last intervals \nText: For example, all it does for x < -3 is to use the same formula as it used for -3 < x < -2, the leftmost interval between knots. Similarly, the formula for x > 4 is the same that was used for 3 < x < 4. \nText: These will be some cubic polynomials that interpolate two values that they were supposed to interpolate, but cannot be expected to follow any large scale pattern in the function. \nText: Simply put, this extrapolation is useless. Splines are not, and were never meant to be, useful as extrapolation tools. \nText: InterpolatedUnivariateSpline has a slightly more sensible option for extrapolation, which is by nearest boundary value (extending the graph by horizontal lines). But if you want something that actually follows the large scale behavior of the data, don't look in nterpolatv module: check out curve_fit from optimization instead. \nAPI:\nscipy.interpolate\n","label":[[915,925,"Mention"],[987,1004,"API"]],"Comments":[]}
{"id":60514,"text":"ID:48675599\nPost:\nText: Scipy provides a least square curve fit method that supports custom defined functions. Here is an example for the first model: \nCode: import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n#custom fit function - first slope steeper than second slope\ndef two_lin(x, m1, n1, m2, n2):\n    return np.min([m1 * x + n1, m2 * x + n2], axis = 0)\n\n#x\/y data points\nx = np.asarray([0, 1, 2,  3,  4,  5,  6,  7,  8,  9,  10])\ny = np.asarray([2, 4, 8, 12, 14, 18, 20, 21, 22, 23,  24])\n#initial guess for a steep rising and plateau phase\nstart_values = [3, 0, 0, 3]\n#curve fitting\nfit_param, pcov = curve_fit(two_lin, x, y, p0 = start_values)\n\n#output of slope\/intercept for both parts\nm1, n1, m2, n2 = fit_param\nprint(m1, n1, m2, n2)\n#calculating sum of squared residuals as parameter for fit quality\nr = y - two_lin(x, *fit_param)\nprint(np.sum(np.square(r)))\n\n#point, where the parts intersect \nif m1 != m2:\n    x_intersect = (n2 - n1) \/ (m1 - m2)\n    print(x_intersect)\nelse:\n    print(\"did not find two linear components\")\n\n#plot data and fit function\nx_fit = np.linspace(-1, 11, 100)  \nplt.plot(x, y, 'o', label='data')\nplt.plot(x_fit, two_lin(x_fit, *fit_param), '--', label='fit')\n\nplt.axis([-2, 12, 0, 30])\nplt.legend()\nplt.show()\n\nText: More information about curve_fit can be found in the reference guide. For polynomials, numpy provides standard functions with numpy.polyfit and numpy.poly1d, but you still have to provide the expected degree. \nText: The sum of squared residuals can be used to compare the accuracy of different fit functions. \nAPI:\nscipy.optimize.curve_fit\n","label":[[1315,1324,"Mention"],[1607,1631,"API"]],"Comments":[]}
{"id":60515,"text":"ID:48725696\nPost:\nText: The main problem in your program was a misunderstanding, how opt.curve_fit is designed and its assumption of the fit function: \nCode:  ydata = f(xdata, *params) + eps\n\nText: This means that the fit function has to have the array for the x values as the first parameter followed by the function parameters in no particular order and must return an array for the y values. Here is an example, how to do this: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.optimize \n\n#t has to be the first parameter of the fit function \ndef fit(t, a, f, p, o):\n    return a * np.sin(2*np.pi*t*f + p) + o\n\nampl = 1\nfreq = 2\nphase = np.pi\/2\noffset = 0.5\nt = np.arange(0,10,0.01)\n\n#is the same as fit(t, ampl, freq, phase, offset)\nfunc = np.sin(2*np.pi*t*freq + phase) + offset\n\nfastfft = np.fft.fft(func)\nfreq_array = np.fft.fftfreq(len(t),t[0]-t[1])\n\nmax_value_index = np.argmax(abs(fastfft))\nfrequency = abs(freq_array[max_value_index])\n\nguess = (0.9, frequency, np.pi\/4, 0.1)\n#renamed the covariance matrix\nparams, pcov = scipy.optimize.curve_fit(fit, t, func, p0=guess)\na, f, p, o = params\n\n#calculate the fit plot using the fit function\nplt.plot(t, func, 'r-', t, fit(t, *params), 'b-')\nplt.show()\n\nText: As you can see, I have also changed the way the fit function for the plot is calculated. You don't need another function - just utilise the fit function with the parameter list, the fit procedure gives you back. The other problem was that you called the covariance array fit - overwriting the previously defined function fit. I fixed that as well. P.S.: Of course now you only see one curve, because the perfect fit covers your data points. \nAPI:\nscipy.optimize.curve_fit\n","label":[[85,98,"Mention"],[1685,1709,"API"]],"Comments":[]}
{"id":60516,"text":"ID:48764861\nPost:\nText: In the regular eigenvalue decomposition, the statement that the left and right eigenvectors are orthogonal can be restated as the product evecs_l.conj().T @ evecs_r is diagonal. That statement is not true in general for the generalized eigenvalue problem Mx = Nx. There, it becomes the product evecs_l.conj().T @ N @ evecs_r is diagonal. (We need to write evecs_l.conj().T instead of just evecs_l because eig returns the conjugate of the left eigenvectors in the columns of the matrix.) \nText: For example, \nCode: In [38]: M\nOut[38]: \narray([[ 1.,  2.],\n       [ 4.,  5.]])\n\nIn [39]: N\nOut[39]: \narray([[ 0.2,  0.1],\n       [ 0.7,  0.8]])\n\nIn [40]: evals, evecs_l, evecs_r = scipy.linalg.eig(M, N, left=True, right=True)\n    ...: \n\nIn [41]: np.set_printoptions(precision=3, linewidth=200, suppress=True)\n\nIn [42]: evecs_l.conj().T @ N @ evecs_r\nOut[42]: \narray([[ 0.701, -0.   ],\n       [ 0.   ,  0.08 ]])\n\nAPI:\nscipy.linalg.eig\n","label":[[430,433,"Mention"],[937,953,"API"]],"Comments":[]}
{"id":60517,"text":"ID:48879759\nPost:\nText: Use ppf (percent point function) of tdist \nCode: >>> from scipy import stats\n>>> stats.t(df=5).ppf((0.025, 0.975))\narray([-2.57058184,  2.57058184])\n\nAPI:\nscipy.stats.t\n","label":[[60,65,"Mention"],[179,192,"API"]],"Comments":[]}
{"id":60518,"text":"ID:48916959\nPost:\nText: Your problem is a nicely structured transportation problem. It can be tackled in various ways. \nText: If you want to solve it with linear programming, you can use scipy.optimize.linprog. Encoding the variables is a little more difficult with multi dimensional decision variables. \nText: With sp.optimize.linprog you could model and solve you problem like this: \nCode: import random\nimport numpy as np\nimport scipy.optimize\n\nLANES = 30\nCARRIERS = 6\n\ncost = np.random.rand(LANES, CARRIERS) # c\ndemand = np.random.rand(LANES) # b_eq\ncapacity = [250, 300, 500, 750, 100, 200] # b_ub\n\nA_eq = np.zeros(LANES*CARRIERS*LANES).reshape(LANES, LANES*CARRIERS)\n# Constraint for each lane, sum over the available carriers\nfor l in range(LANES):\n    for var in range(l*CARRIERS, l*CARRIERS+CARRIERS):\n        A_eq[l, var] = 1\n\nA_ub = np.zeros(CARRIERS*LANES*CARRIERS).reshape(CARRIERS, LANES*CARRIERS)\n# Constraint for each carrier, sum over the lanes\nfor c in range(CARRIERS):\n    for var in range(c, LANES*CARRIERS, CARRIERS):\n        A_ub[c, var] = 1\n\nprint(scipy.optimize.linprog(cost.flatten(), A_eq=A_eq, b_eq=demand, \n    A_ub=A_ub, b_ub=capacity, options={\"maxiter\": 10000}))\n\nText: We need a total of LANES*CARRIERS variables, which can be represented in a one-dimensional array. The variable that expresses how much is transported on lane l with carrier c has the index l*LANES + c. Under this assumption the constraints can be added. As the full problem matrix has LANES*CARRIERS*(LANES+CARRIERS) elements, the linprog function may not be suited for the problem size. You can increase the maxiter parameter, but you could run into other issues like numerical problems, though I did not read the source. \nText: A faster and more robust free solver is bundled with PuLP. You can install PuLP with easy_install pulp. The problem can also be expressed in a more natural way, as PuLP has convenience functions for declaring variable dictionaries. While commercial solvers are faster than the one bundled with PuLP, your problem is a pure linear program and relatively \"easy\" even with 3000 lanes and 6 carriers. \nText: In PuLP it can be implemented in a more natural way: \nCode: from pulp import *\nimport numpy as np\nfrom itertools import product\n\nLANES = 30\nCARRIERS = 6\n\ncost = 100 * np.random.rand(LANES, CARRIERS) # c\ndemand = 10 * np.random.rand(LANES) # b_eq\ncapacity = [250, 300, 500, 750, 100, 200] # b_ub\n\nprob = LpProblem(\"Transportation\",LpMinimize)\nx = LpVariable.dicts(\"Route\", product(range(LANES), range(CARRIERS)), 0, None)\n\nprob += lpSum(cost[l, c] * x[l, c] for l in range(LANES) for c in range(CARRIERS))\n\nfor l in range(LANES):\n    prob += lpSum(cost[l, c] * x[l, c] for c in range(CARRIERS)) == demand[l]\n\nfor c in range(CARRIERS):\n    prob += lpSum(cost[l, c] * x[l, c] for l in range(LANES)) <= capacity[c]\n\nprob.solve()\n\n# Get optimal solution\nif LpStatus[prob.status] == \"Optimal\":\n    x = {(l, c): value(x[l, c]) for l in range(LANES) for c in range(CARRIERS)}\nelse:\n    print(\"Optimization failed.\")\n\nAPI:\nscipy.optimize.linprog\n","label":[[316,335,"Mention"],[3049,3071,"API"]],"Comments":[]}
{"id":60519,"text":"ID:48926671\nPost:\nText: Once you've made a meshgrid R is no longer a single value but an array. \nText: Furthermore, quad returns a tuple of the value and the estimated error. \nText: It's maybe a question of personal taste but I like to use lambda only for anonymous functions. Otherwise, I just get confused. \nText: My solution is probably not the fastest but it's good enough given the current parameters I think. \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.integrate import quad\nfrom scipy.special import jn\n\ndef integrand(x, r): \n    return np.exp(-x**2) * jn(0, r*x) * x\n\ndef intensity(r):\n    output = np.zeros_like(r)\n    for i, ri in enumerate(r.flat):\n        output.flat[i] = quad(lambda x: integrand(x, ri), 0, 5)[0]\n    return output\n\nrho = np.linspace(0, 1.25, 50)\np = np.linspace(0, 2*np.pi, 50)\nR, P = np.meshgrid(rho, p)\nZ = intensity(R)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nX, Y = R*np.cos(P), R*np.sin(P)\n\nsurf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r)\n\nAPI:\nscipy.integrate.quad\n","label":[[116,120,"Mention"],[1075,1095,"API"]],"Comments":[]}
{"id":60520,"text":"ID:48927891\nPost:\nText: The SciPy documentation of opt.newton recommends using opt.brentq for intervals [a,b] where the function changes sign. For monotone functions such as described, a=0 and b can be found by trying large enough numbers. \nCode: import scipy.optimize\n\nf1 = (lambda a: 6.75304970913061 * a**2.37142857142857 - 1.91006495309903)\nf1(0) # -1.91006495309903\nf1(1) # 4.84298475603158\nscipy.optimize.brentq(f1,0.,1.) # 0.5871176550428887\n\nf2 = (lambda a: 0.672716686237341 * a **0.0624999999999993 + 0.87283444645141 * a ** 0.134615384615384 - 1.34775906114245)\nf2(0) # -1.34775906114245\nf2(1) # 0.19779207154630107\nscipy.optimize.brentq(f2,0.,1.) # 0.2624501197238087\n\nAPI:\nscipy.optimize.newton\nscipy.optimize.brentq\n","label":[[51,61,"Mention"],[79,89,"Mention"],[686,707,"API"],[708,729,"API"]],"Comments":[]}
{"id":60521,"text":"ID:49089230\nPost:\nText: First, ss module is in sp.stats not scipy.stats.stats as of scipy 0.17. Also, it is deprecated, but you seem to have figured that out. \nText: Second, in the current linear_model.py on github, the ss package is no longer imported: \nCode: from scipy.linalg import toeplitz\nfrom scipy import stats\nfrom scipy import optimize\n\nText: What version is your statsmodels? Sounds like you may need to update it. \nAPI:\nscipy.stats\n","label":[[47,55,"Mention"],[432,443,"API"]],"Comments":[]}
{"id":60522,"text":"ID:49109870\nPost:\nText: I only checked the Python version and there's indeed an error in the implementation. \nText: Namely, your testfunc, ie the target function of root-finding brentq routine, behaves non-deterministically. During a root-finding run (i.e. one call to brentq() until it returns), brentq needs to call the same callback multiple times until convergence is reached. However, each time brentq calls this callback, the target equation changes as r gets a new pseudo-random value. As a result, the root-finding routine cannot converge to your desired solution. \nText: What you need to do instead, conceptually, is to first generate a sample of uniform random variables, and apply the same, deterministic transformation (i.e. the inverse distribution function) to them. Of course you don't need to do root-solving, as you can use the ppf (percentile function, i.e. inverse of cdf) method of stajs random variable classes. \nText: As a proof of concept, you can run the following code with the (unnecessarily expensive and not very accurate) transformation method on an array of standard uniform sample: \nCode: import numpy\nimport numpy.random\nfrom scipy.optimize import brentq\nfrom scipy.stats import norm\n\n# Setup\nn = 10000\nnumpy.random.seed(0x5eed)\nran_array = numpy.random.uniform(size=n)\n\nsol_array = numpy.empty_like(ran_array)\n\n\n# Target function for root-finding: F(x) - p = 0 where p is the probability level\n# for which the quantile is to be found\ndef targetfunc(x, p):\n    return norm.cdf(x, 5, 2) - p\n\n\nfor i in range(n):\n    sol_array[i] = brentq(targetfunc, -100.0, 100.0, args=(ran_array[i],))\nprint(\"mean = %10f\" % sol_array.mean())\nprint(\"std  = %10f\" % sol_array.std())\n\nText: Output: \nText: mean = 5.011041 std = 2.009365 \nAPI:\nscipy.stats\n","label":[[902,907,"Mention"],[1756,1767,"API"]],"Comments":[]}
{"id":60523,"text":"ID:49282684\nPost:\nText: I believe that binom can take advantage of broadcasting in the way that you're looking for. \nCode: # Binomial PMF: Pr(X=k) = choose(n, k) * p**k * (1-p)**(n-k)\n# Probability of getting exactly k successes in n trials\n\n>>> from scipy.stats import binom\n\n>>> n = np.arange(1, N+1, dtype=np.int64)\n>>> dist = binom(p=0.25, n=n)\n>>> M = dist.pmf(k=np.arange(N+1, dtype=np.int64)[:, None])\n\n>>> M.round(2)\narray([[0.75, 0.56, 0.42, 0.32, 0.24, 0.18, 0.13, 0.1 , 0.08, 0.06],\n       [0.25, 0.38, 0.42, 0.42, 0.4 , 0.36, 0.31, 0.27, 0.23, 0.19],\n       [0.  , 0.06, 0.14, 0.21, 0.26, 0.3 , 0.31, 0.31, 0.3 , 0.28],\n       [0.  , 0.  , 0.02, 0.05, 0.09, 0.13, 0.17, 0.21, 0.23, 0.25],\n       [0.  , 0.  , 0.  , 0.  , 0.01, 0.03, 0.06, 0.09, 0.12, 0.15],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.02, 0.04, 0.06],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.02],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])\n\nText: Here, the rows are k (0-indexed) and the columns are n (1-indexed): \nCode: >>> from math import factorial as fac\n>>> def manual_pmf(p, n, k):\n...     return fac(n) \/ (fac(k) * fac(n - k)) * p**k * (1-p)**(n-k)\n\n>>> manual_pmf(p=0.25, n=3, k=2)\n0.140625  # (2, 2) in M because M's columns are effectively 1-indexed \n\nText: You could also start n at zero to get an array that is 0-indexed on both rows and columns: \nCode: >>> n = np.arange(N+1, dtype=np.int64)  # M.shape is (11, 11)\n\nAPI:\nscipy.stats.binom\n","label":[[39,44,"Mention"],[1680,1697,"API"]],"Comments":[]}
{"id":60524,"text":"ID:49317875\nPost:\nText: You can directly call the method gaaussian_ke which is also used by pandas internally. This method returns the desired function. You can then call one of the methods from scipy.integrate to calculate areas under the kernel density estimate, e.g. \nCode: from scipy import stats, integrate\n\nkde = stats.gaussian_kde(a[0])\n\n# Calculate the integral of the kde between 10 and 20:\nxmin, xmax = 10, 20\nintegral, err = integrate.quad(kde, xmin, xmax)\n\nx = np.linspace(-5,20,100)\nx_integral = np.linspace(xmin, xmax, 100)\n\nplt.plot(x, kde(x), label=\"KDE\")\nplt.fill_between(x_integral, 0, kde(x_integral),\n                 alpha=0.3, color='b', label=\"Area: {:.3f}\".format(integral))\nplt.legend()\n\nAPI:\nscipy.stats.gaussian_kde\n","label":[[57,69,"Mention"],[718,742,"API"]],"Comments":[]}
{"id":60525,"text":"ID:49527145\nPost:\nText: The error is raised inside the function quad because d is a numpy.array and not a scalar. The function given to opt.curve_fit take the independent variable (x_linear in your case) as first argument. \nText: The quick and dirty fix is to modify curvefit to compute the definite integral for each value in d: \nCode: def curvefit(xs,a,b,c):\n    return [quad(f,0,x,args=(a,b,c))[0] for x in xs]\n\nAPI:\nscipy.integrate.quad\nscipy.optimize.curve_fit\n","label":[[64,68,"Mention"],[136,149,"Mention"],[420,440,"API"],[441,465,"API"]],"Comments":[]}
{"id":60526,"text":"ID:49561155\nPost:\nText: This error is most likely happening because loadmat cannot find the file of interest. Because you're using Windows, the path you're defining is not quite correct. You need to delineate the directory separator \\ with two backslashes: \\\\. \nText: In other words: \nCode: data = sio.loadmat('C:\\\\Users\\\\Martin\\\\Desktop\\\\Biophysics PhD\\\\Results\\\\180321_agonists_spreading_conditions\\\\180321_agonists_spreading_conditions\\\\Compare_ADPdexBSA.mat')\n\nAPI:\nscipy.io.loadmat\n","label":[[68,75,"Mention"],[470,486,"API"]],"Comments":[]}
{"id":60527,"text":"ID:49577395\nPost:\nText: sp.stats.pareto is not a class. It is an instance of class: \nCode: scipy.stats.distributions.pareto_gen\n\nText: We can build a similar interface for our own class like: \nText: Code: \nCode: import scipy.stats as stats\n\nclass pareto(stats.distributions.pareto_gen):\n\n    def __new__(cls, *args, **kwargs):\n        # get a `pareto` instance\n        self = stats.distributions.pareto_gen(a=1.0, name=\"pareto\")\n\n        # call the instance with desired setup\n        return self(*args, **kwargs)\n\n    def __init__(self, *args, **kwargs):\n        # already called __init__()\n        pass\n\nText: Test Code: \nCode: u = stats.pareto(2)\nprint(u.cdf(1))\n\nu = pareto(2)\nprint(u.cdf(1))\n\nText: Results: \nCode: 0.0\n0.0\n\nAPI:\nscipy.stats.pareto\n","label":[[24,39,"Mention"],[734,752,"API"]],"Comments":[]}
{"id":60528,"text":"ID:49709180\nPost:\nText: For the calculations in hconvolve with mode='full' or mode='same' to be properly defined, the data in the first argument is (effectively) extended with zeros. Your FFT calculation, on the other hand, does circular convolution, which corresponds to using the periodic extension of the data. To see the consequences of this difference, consider how the first point of the result is calculated. \nText: (It is helpful to have in mind the usual \"sliding window\" view of convolution, such as shown at http:\/\/mathworld.wolfram.com\/Convolution.html or https:\/\/en.wikipedia.org\/wiki\/Convolution#Visual_explanation. In your case, the sliding window is gg.) \nText: For convolve with mode='same', you can visualize the calculation of the first point by aligning the right half of gg over the left end of ff, and summing the elementwise product of those two signals. ff is very small at its left end, so this calculation is very close to 0. Subsequent points of the convolution remain zero until the sliding window starts encountering larger values of ff. So the \"interesting\" part of the result is in the middle of the convolution. \nText: For the first point of the FFT calculation, imagine the right end of gg aligned with the left end of ff. Again take the sum of the elementwise product. There are two big differences here. First, gg is not shifted by half its length like it is with mode='same' in scipy.signal.convolve. Second, the values that gg is multiplied by are not all zero--they are the periodic extension of ff, so in this \"sliding window\" visualization, we have the rectangular window aligned directly over the center of the double pulse (in the periodic extension). Because of the symmetry of gg and the antisymmetry of ff, this first value is 0. As gg slides right, the symmetry is broken, the positive pulse dominates the calculation, and nontrivial values are computed. Once the window passes the double pulse, the values of the convolution become very small. They become very big again near the end of the convolution, when the rectangular pulse encounters the other side of the double pulse. \nText: To get your FFT calculcation to match convolve calculation, you can adjust the phase of the rectangular pulse in gg. For example (assuming Nx is even). For example, if you add this line \nCode: gg2 = np.roll(gg, -(Nx\/\/2 - 1))\n\nText: and use gg2 in place of gg in the calculation of tfg: \nCode: tfg = np.fft.rfft(gg2)             # DFT of the kernel\n\nText: then conv_dfts and conv_pure agree. There are others ways you can tweak things to get the results to align as you expected. The main point of this answer is to explain why the results that you calculated were different. \nAPI:\nscipy.signal.convolve\nscipy.signal.convolve\nscipy.signal.convolve\n","label":[[48,57,"Mention"],[682,690,"Mention"],[2170,2178,"Mention"],[2713,2734,"API"],[2735,2756,"API"],[2757,2778,"API"]],"Comments":[]}
{"id":60529,"text":"ID:49732825\nPost:\nText: 1. Passing extra arguments through quad \nText: The quad docs say: \nText: If the user desires improved integration performance, then f may be a sp.LowLevelCallable with one of the signatures: double func(double x) double func(double x, void *user_data) double func(int n, double *xx) double func(int n, double *xx, void *user_data) The user_data is the data contained in the scipy.LowLevelCallable. In the call forms with xx, n is the length of the xx array which contains xx[0] == x and the rest of the items are numbers contained in the args argument of quad. \nText: Therefore to pass an extra argument to integrand through quad, you are better of using the double func(int n, double *xx) signature. \nText: You can write a decorator to your integrand function to transform it to a LowLevelCallable like so: \nCode: import numpy as np\nimport scipy.integrate as si\nimport numba\nfrom numba import cfunc\nfrom numba.types import intc, CPointer, float64\nfrom scipy import LowLevelCallable\n\n\ndef jit_integrand_function(integrand_function):\n    jitted_function = numba.jit(integrand_function, nopython=True)\n    \n    @cfunc(float64(intc, CPointer(float64)))\n    def wrapped(n, xx):\n        return jitted_function(xx[0], xx[1])\n    return LowLevelCallable(wrapped.ctypes)\n\n@jit_integrand_function\ndef integrand(t, *args):\n    a = args[0]\n    return np.exp(-t\/a) \/ t**2\n\ndef do_integrate(func, a):\n    \"\"\"\n    Integrate the given function from 1.0 to +inf with additional argument a.\n    \"\"\"\n    return si.quad(func, 1, np.inf, args=(a,))\n\nprint(do_integrate(integrand, 2.))\n>>>(0.326643862324553, 1.936891932288535e-10)\n\nText: Or if you don't want the decorator, create the LowLevelCallable manually and pass it to quad. \nText: 2. Wrapping the integrand function \nText: I am not sure if the following would meet your requirements but you could also wrap your integrand function to achieve the same result: \nCode: import numpy as np\nfrom numba import cfunc\nimport numba.types\n\ndef get_integrand(*args):\n    a = args[0]\n    def integrand(t):\n        return np.exp(-t\/a) \/ t**2\n    return integrand\n\nnb_integrand = cfunc(numba.float64(numba.float64))(get_integrand(2.))\n\nimport scipy.integrate as si\ndef do_integrate(func):\n    \"\"\"\n    Integrate the given function from 1.0 to +inf.\n    \"\"\"\n    return si.quad(func, 1, np.inf)\n\nprint(do_integrate(get_integrand(2)))\n>>>(0.326643862324553, 1.936891932288535e-10)\nprint(do_integrate(nb_integrand.ctypes))\n>>>(0.326643862324553, 1.936891932288535e-10)\n\nText: 3. Casting from voidptr to a python type \nText: I don't think this is possible yet. From this discussion in 2016, it seems that voidptr is only here to pass a context to a C callback. \nText: The void * pointer case would be for APIs where foreign C code does not every try to dereference the pointer, but simply passes it back to the callback as way for the callback to retain state between calls. I don't think it is particularly important at the moment, but I wanted to raise the issue. \nText: And trying the following: \nCode: numba.types.RawPointer('p').can_convert_to(\n    numba.typing.context.Context(), CPointer(numba.types.Any)))\n>>>None\n\nText: doesn't seem encouraging either! \nAPI:\nscipy.integrate.quad\nscipy.LowLevelCallable\n","label":[[59,63,"Mention"],[167,186,"Mention"],[3209,3229,"API"],[3230,3252,"API"]],"Comments":[]}
{"id":60530,"text":"ID:49744655\nPost:\nText: Looks like a bit of source-diving is needed; fortunately, it does not require copying a lot of source code. This is how basic fit works: \nCode: from scipy.stats import cauchy\ndata = [0, 3, 4, 4, 5, 9]\nres = cauchy.fit(data)   # (3.9798237305661255, 0.9205374643383732)\n\nText: and this is how it is modified to return OptimizeResult: \nCode: from scipy.optimize import minimize\nargs = cauchy._fitstart(data)\nx0, func, restore, args = cauchy._reduce_func(args, {})\nres = minimize(func, x0, args=(data,), method='BFGS')\n\nText: Now res is \nCode:       fun: 14.337039523098689\n hess_inv: array([[ 0.23321703, -0.0117229 ],\n       [-0.0117229 ,  0.36807373]])\n      jac: array([ 0.0000000e+00, -1.1920929e-07])\n  message: 'Optimization terminated successfully.'\n     nfev: 32\n      nit: 5\n     njev: 8\n   status: 0\n  success: True\n        x: array([3.9798262 , 0.92055376])\n\nText: where you may recognize the parameters as being res.x. The function being minimized is \"penalized NNLF\" (nonnegative likelihood function). \nText: By the way, \nText: For all its optimizations, SciPy returns an object called OptimizeResult \nText: is an over-generalization. This is true for minimize method. The sp.stats.fit uses fmin by default, which returns no such thing. \nText: So, if want identical output to fmin, that can be arranged, but there'll be no extra information there. \nCode: from scipy.optimize import fmin\nargs = cauchy._fitstart(data)\nx0, func, restore, args = cauchy._reduce_func(args, {})\nres = fmin(func, x0, args=(data,))\nprint(tuple(res))   #(3.9798237305661255, 0.9205374643383732)\n\nText: Using minimize with method='Nelder-Mead' has essentially the same effect. You do get a bit of extra information, but given that the method is simplex-based (no derivatives being computed), this information is of limited use. \nCode: res = minimize(func, x0, args=(data,), method='Nelder-Mead')\nprint(res)\nprint(tuple(res.x))\n\nText: prints \nCode:  final_simplex: (array([[3.97982373, 0.92053746],\n       [3.97983057, 0.92060317],\n       [3.97977536, 0.92059568]]), array([14.33703952, 14.33703953, 14.33703953]))\n           fun: 14.337039523477827\n       message: 'Optimization terminated successfully.'\n          nfev: 58\n           nit: 31\n        status: 0\n       success: True\n             x: array([3.97982373, 0.92053746])\n(3.9798237305661255, 0.9205374643383732)\n\nAPI:\nscipy.stats.fit\n","label":[[1208,1220,"Mention"],[2386,2401,"API"]],"Comments":[]}
{"id":60531,"text":"ID:49806536\nPost:\nText: The function sp.cluster.vq.vq does what you want. \nText: Here's a modified version of your code that demonstrates vq: \nCode: import numpy as np\nfrom scipy.cluster.vq import vq\nimport matplotlib.pyplot as plt\n\n\n# `points1` is the set A described in the question.\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,3)\n                               for y in np.linspace(-1,1,3)])\n\n# `points2` is the set B.  In this example, there are 5 points in B.\nN = 5\nnp.random.seed(1357924)\npoints2 = 2*np.random.rand(N, 2) - 1\n\n# For each point in points1, find the closest point in points2:\ncode, dist = vq(points1, points2)\n\n\nplt.plot(points1[:,0], points1[:,1], 'b*')\nplt.plot(points2[:,0], points2[:,1], 'rh')\n\nfor i, j in enumerate(code):\n    plt.plot([points1[i,0], points2[j,0]],\n             [points1[i,1], points2[j,1]], 'k', alpha=0.4)\n\nplt.grid(True, alpha=0.25)\nplt.axis('equal')\nplt.show()\n\nText: The script produces the following plot: \nAPI:\nscipy.cluster.vq.vq\n","label":[[37,53,"Mention"],[970,989,"API"]],"Comments":[]}
{"id":60532,"text":"ID:49924670\nPost:\nText: Just to complete the question with the answer provided in the comment above: \nText: The issue was not due to the large number of points, but the fact that I had such large values on my y axis. Since the default initial values are 1, my values of around 1000 were too large. To fix that an initial guess for the line fit was used for parameter p0. From the docs for curve_fit it looks like: \nText: p0 : None, scalar, or N-length sequence, optional Initial guess for the parameters. If None, then the initial values will all be 1 (if the number of parameters for the function can be determined using introspection, otherwise a ValueError is raised). \nText: So my final code ended up looking like this: \nCode: from scipy import optimize\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15], dtype=float)\ny = np.array([500, 700, 900, 1100, 1300, 1500, 2892, 4281, 5670, 7059, 8447, 9836, 11225, 12614, 14003])\n\ndef piecewise_linear(x, x0, y0, k1, k2):\n    return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])\n\np, e = optimize.curve_fit(piecewise_linear, x, y, p0=(10, -2500, 0, -500))\nxd = np.linspace(-5, 30, 100)\nplt.plot(x, y, \".\")\nplt.plot(xd, piecewise_linear(xd, *p))\nplt.show()\n\nAPI:\nscipy.optimize.curve_fit\n","label":[[389,398,"Mention"],[1309,1333,"API"]],"Comments":[]}
{"id":60533,"text":"ID:50063416\nPost:\nText: From the docstring of librosa.core.load: \nText: Load an audio file as a floating point time series. Audio will be automatically resampled to the given rate (default sr=22050). To preserve the native sampling rate of the file, use sr=None. \nText: read does not automatically resample the data, and the samples are not converted to floating point if they are integers in the file. \nAPI:\nscipy.io.wavfile.read\n","label":[[270,274,"Mention"],[409,430,"API"]],"Comments":[]}
{"id":60534,"text":"ID:50076060\nPost:\nText: The most likely explanation is that your library does not work well with lists of lists, and especially where the sublists have different lengths. You should check if this is indeed the case. \nText: As per the docs, savemat is designed for a dictionary of arrays, which is not what you have provided: \nText: Save a dictionary of names and arrays into a MATLAB-style .mat file. This saves the array objects in the given dictionary to a MATLAB- style .mat file. \nText: What you can do is restructure your data: \nCode: dic = {\"A\": np.array([1, 2, 3]),\n       \"B\": np.array([1, 2, 4]),\n       \"C\": np.array([6, 7, 8, 9]),\n       \"D\": np.array([1])}\n\nText: Remember that a numpy array with rows of different length will become dtype=Object and virtually unusable for most vectorised functions. It probably won't work well with scipy either. \nText: A painful solution is to perform the conversion when you load the data again: \nCode: import numpy as np\nfrom operator import itemgetter\n\nlst = [[np.array([[1, 2, 3]]), np.array([[1, 2, 4]]),\n        np.array([[6, 7, 8, 9]]), np.array([[1]])]]\n\nres = list(map(list, (map(itemgetter(0), map(list, lst[0])))))\n\n[[1, 2, 3], [1, 2, 4], [6, 7, 8, 9], [1]]\n\nText: There is no native function composition in Python, but the above logic can be made more readable with 3rd party library toolz: \nCode: from operator import itemgetter\nfrom toolz import compose\n\nres = list(map(compose(list, itemgetter(0), list), lst[0]))\n\nAPI:\nscipy.io.savemat\n","label":[[240,247,"Mention"],[1483,1499,"API"]],"Comments":[]}
{"id":60535,"text":"ID:50079482\nPost:\nText: curve_fit makes it really easy to fit data points to your custom function: \nCode: import numpy as np\nfrom scipy.optimize import curve_fit\n\n#definition of the function \ndef myfunc(x, a, b, c):\n    return a + b * np.cos(x - c)\n\n#sample data\nx_data = np.arange(5)\ny_data = 2.34 + 1.23 * np.cos(x_data + .23)\n\n#the actual curve fitting procedure, a, b, c are stored in popt\npopt, _pcov = curve_fit(myfunc, x_data, y_data)\nprint(popt)\n\n#the rest is just a graphic representation of the data points and the fitted curve\nfrom matplotlib import pyplot as plt\n\nx_fit = np.linspace(-1, 6, 1000)\ny_fit = myfunc(x_fit, *popt)\n\nplt.plot(x_data, y_data, \"ro\", label = \"data points\")\nplt.plot(x_fit, y_fit, \"b\", label = \"fitted curve\\na = {}\\nb = {}\\nc = {}\".format(*popt))\n\nplt.legend()\nplt.show()\n\nText: Output: \nCode: [ 2.34  1.23 -0.23]\n\nText: Edit: \nText: Your question update introduces several problems. First, your x-values are in degree, while np.cos expects values in radians. Therefore, we better convert the values with np.deg2rad. The reverse function would be np.rad2deg. Second, it is a good idea to fit for different frequencies as well, let's introduce an additional parameter for that. Third, fits are usually quite sensitive to initial guesses. You can provide a parameter p0 in scipy for that. Fourth, you changed the resolution of the fitted curve to the low resolution of your data points, hence it looks so undersampled. If we address all these problems: \nCode: import numpy as np\nfrom scipy.optimize import curve_fit\n\n#sample data\nx_data = [0, 60, 120, 180, 240, 300]\ny_data = [25, 40, 70, 30, 10, 15]\n\n#definition of the function with additional frequency value d\ndef myfunc(x, a, b, c, d):\n    return a + b * np.cos(d * np.deg2rad(x) - c)\n#initial guess of parameters a, b, c, d\np_initial = [np.average(y_data), np.average(y_data), 0, 1]\n\n#the actual curve fitting procedure, a, b, c, d are stored in popt\npopt, _pcov = curve_fit(myfunc, x_data, y_data, p0 = p_initial)\nprint(popt)\n#we have to convert the phase shift back into degrees\nprint(np.rad2deg(popt[2]))\n\n#graphic representation of the data points and the fitted curve\nfrom matplotlib import pyplot as plt\n#define x_values for a smooth curve representation\nx_fit = np.linspace(np.min(x_data), np.max(x_data), 1000)\ny_fit = myfunc(x_fit, *popt)\n\nplt.plot(x_data, y_data, \"ro\", label = \"data\")\nplt.plot(x_fit, y_fit, \"b\", label = \"fit\")\nplt.xlabel(r'$\\theta$ (degrees)');\nplt.ylabel(r'$f(\\theta)$');\n\nplt.legend()\nplt.show()\n\nText: we get this output: \nCode: [34.31293761 26.92479369  2.20852009  1.18144319]\n126.53888003953764\n\nAPI:\nscipy.optimize.curve_fit\n","label":[[24,33,"Mention"],[2626,2650,"API"]],"Comments":[]}
{"id":60536,"text":"ID:50109254\nPost:\nText: st.pearsonr computes the p value using the t distribution. (You can check the source code in the file stats.py on github.) This should definitely be mentioned in the docstring. \nText: Here's an example. First, import pearsonr and scipy's implementation of the t distribution: \nCode: In [334]: from scipy.stats import pearsonr, t as tdist\n\nText: Define x and y for this example: \nCode: In [335]: x = np.array([0, 1, 2, 3, 5, 8, 13])\n\nIn [336]: y = np.array([1.2, 1.4, 1.6, 1.7, 2.0, 4.1, 6.6])\n\nText: Compute r and p for this data: \nCode: In [337]: r, p = pearsonr(x, y)\n\nIn [338]: r\nOut[338]: 0.9739566302403544\n\nIn [339]: p\nOut[339]: 0.0002073053505382502\n\nText: Now compute p again, by first computing the t statistic, and then finding twice the survival function for that t value: \nCode: In [340]: df = len(x) - 2\n\nIn [341]: t = r * np.sqrt(df\/(1 - r**2))\n\nIn [342]: 2*tdist.sf(t, df)  # This is the p value.\nOut[342]: 0.0002073053505382502\n\nText: We get the same p value, as expected. \nText: I don't know the source of the statement \"The p-values are not entirely reliable but are probably reasonable for datasets larger than 500 or so\". If anyone knows a citable reference, it should be added to the pearsonr docstring. \nAPI:\nscipy.stats.pearsonr\n","label":[[24,35,"Mention"],[1255,1275,"API"]],"Comments":[]}
{"id":60537,"text":"ID:50146722\nPost:\nText: ewntropy assumes that the distributions are 1-dimensional. Looking at the docstring, you can see: \nCode: S = -sum(pk * log(pk), axis=0)\n\nText: which means it sums over the first axis. Giving it an array of shape (m, n) will give you a result of shape (n,), which is like treating each row of your arrays as a separate pair of distributions. \nText: But the definition of entropy doesn't care about the dimensionality of the distributions. It's just about the probabilities of an event, which in your case is a single element of a or b. So you can do: \nCode: div = scipy.stats.entropy(a.ravel(), qk=b.ravel(), base=None)\n\nText: and you'll get a single value for the KL divergence. \nAPI:\nscipy.stats.entropy\n","label":[[24,32,"Mention"],[709,728,"API"]],"Comments":[]}
{"id":60538,"text":"ID:50353752\nPost:\nText: x is between 0 and 15 and between 30 and 50 \nText: This would make the model infeasible. There is no such x. You probably mean: \nText: x is between 0 and 15 OR between 30 and 50 \nText: This is non-convex, so standard local solvers have troubles with this. It is often modeled with an extra binary variable: \nCode: 30   x  15(1-) + 50   \n  {0,1}\n\nText: Of course, this assumes you can handle binary variables (SLSQP can't). Models with binary variables and nonlinear constraints (or objective function) are called MINLP models (Mixed Integer Non-linear Programming). Solvers for these type of models are readily available. \nText: Some other approaches that may work: \nText: Solve the problem twice. Once with 0  x  15 and once with 30  x  50. Then pick the best solution. Use the basinhopping global solver to help your way out of a local optimum. This is not a rigorous algorithm (no guarantees), but it can help. \nText: Some approaches that typically don't work: \nText: instead of a binary variable   {0,1} use a continuous variable   [0,1] and add the constraint (1-)=0. Typically this will get you stuck. The polynomial approach in the other answer: this is also non-convex and not really suited for SLSQP. You will get stuck in a local optimum. Add a penalty to the objective if x  [15,30]. This also does not work with a local solver. \nAPI:\nscipy.optimize.basinhopping\n","label":[[814,826,"Mention"],[1388,1415,"API"]],"Comments":[]}
{"id":60539,"text":"ID:50453759\nPost:\nText: You might consider using lmfit (https:\/\/lmfit.github.io\/lmfit-py) for this. Lmfit provides a higher-level interface to curve fitting and makes fitting parameters first class python objects. Among other things, this easily allows fixing some parameters, and setting bounds on parameters in a more pythonic manner than what curve_fit uses. In particular for your question, lmfit parameters also support using mathematical expressions as constraint expressions for all parameters. \nText: To turn your model function piecewise_linear() into an Model for curve-fitting with lmfit you would do something like \nCode: from lmfit import Model\n\n# make a model\nmymodel = Model(piecewise_linear)\n\n# create parameters and set initial values\n# note that parameters are *named* from the \n# names of arguments of your model function\nparams = mymodel.make_params(t0=0, t1=1, b=3, m1=2, m2=2)\n\n# now, you can place bounds on parameters, maybe like\nparams['b'].min = 0\nparams['m1'].min = 0\n\n# but what you want is an inequality constraint, so\n#   1. add a new parameter 'tdiff'\n#   2. constrain t1 = t0 + tdiff\n#   3. set a minimum value of 0 for tdiff\nparams.add('tdiff', value=1, min=0)\nparams['t1'].expr = 't0 + tdiff'\n\n# now perform the fit\nresult = mymodel.fit(yy, params, t=t)\n\n# print out results\nprint(result.fit_report())\n\nText: You can read in the lmfit docs or on other SO questions how to extract other information from the fit result. \nAPI:\nscipy.optimize.curve_fit\n","label":[[346,355,"Mention"],[1459,1483,"API"]],"Comments":[]}
{"id":60540,"text":"ID:50644922\nPost:\nText: This is not a MIMO system, it is a SISO system but you have multiple inputs. \nText: You can create a MIMO system and apply your inputs all at once which will be computed channel by channel but simultaneously. Moreover, you can't use sp.signal.lsim for MIMO systems yet. You can use other options such as python-control (if you have slycot extension otherwise again no MIMO) or harold if you have Python 3.6 or greater (disclaimer: I'm the author). \nCode: import numpy as np\nfrom harold import *\nimport matplotlib.pyplot\nnbr_inputs = 5\nt_in = np.arange(0,10,0.2)\ndim = (nbr_inputs, len(t_in))\n\nx = np.cumsum(np.random.normal(0,2e-3, dim), axis=1)\n\n# Forming a 1x5 system, common denominator will be completed automatically\nH = Transfer([[[1, 3, 3]]*nbr_inputs], [1, 2, 1])\n\nText: The keyword per_channel=True applies first input to first channel, second input to second and so on. Otherwise combined response is returned. You can check the shapes by playing around with it to see what I mean. \nCode: # Notice it is x.T below -> input shape = <num samples>, <num inputs>\ny, t = simulate_linear_system(H, x.T, t_in, per_channel=True)\n\nplt.plot(t, y)\n\nText: This gives \nAPI:\nscipy.signal.lsim\n","label":[[257,271,"Mention"],[1195,1212,"API"]],"Comments":[]}
{"id":60541,"text":"ID:50806457\nPost:\nText: betainc is based on incbet in scipy\/special\/cephes\/incbet.c, a function from the Cephes Math Library. \nText: I found this by searching for betainc in the SciPy Github repository, which lead me to scipy\/special\/functions.json, which lead me to incbet. \nAPI:\nscipy.special.betainc\n","label":[[24,31,"Mention"],[281,302,"API"]],"Comments":[]}
{"id":60542,"text":"ID:50845387\nPost:\nText: Generally speaking, the tools you need are a combination of tf.map_fn and tf.py_func. \nText: tf.py_func allows you to wrap a standard python function into a tensorflow op that is inserted into your graph. tf.map_fn allows you to call a function repeatedly on the batch samples, when the function cannot operate on the whole batch  as it is often the case with image functions. \nText: In the present case, I would probably advise to use zoom on the basis that it can operate directly on the 4D tensor, which makes things simpler. On the other hand, it takes as input zoom factors, not sizes, so we need to compute them. \nCode: import tensorflow as tf\n\nsess = tf.InteractiveSession()\n\n# unimportant -- just a way to get an input tensor\nbatch_size = 13\nim_size = 7\nnum_channel=5\nx = tf.eye(im_size)[None,...,None] + tf.zeros((batch_size, 1, 1, num_channel))\nnew_size = 17\n\nfrom scipy import ndimage\nnew_x = tf.py_func(\n  lambda a: ndimage.zoom(a, (1, new_size\/im_size, new_size\/im_size, 1)),\n  [x], [tf.float32], stateful=False)[0]\nprint(new_x.eval().shape)\n# (13, 17, 17, 5)\n\nText: You could use other functions (e.g. OpenCV's cv2.resize, Scikit-image's transform.image, Scipy's misc.imresize) but none can operate directly on 4D tensors and therefore are more verbose to use. You may still want to use them if you want an interpolation other than zoom's spline-based interpolation. \nText: However, be aware of the following things: \nText: Python functions are executed on the host. So, if you are executing your graph on a device like a graphics card, it needs to stop, copy the tensor to host memory, call your function, then copy the result back on the device. This can completely ruin your computation time if memory transfers are important. Gradients do not pass through python functions. If your node is used, say, in an upscaling part of a network, layers upstream will not receive any gradient (or only part of it, if you have skip connections), which would compromise your training. \nText: For those two reasons, I would advise to apply this kind of resampling to inputs only, when preprocessed on CPU and gradients are not used. \nText: If you do want to use this upscale node for training on the device, then I see no alternative as to either stick with the buggy tf.image.resize_image, or to write your own. \nAPI:\nscipy.ndimage.zoom\n","label":[[461,465,"Mention"],[2348,2366,"API"]],"Comments":[]}
{"id":60543,"text":"ID:50889332\nPost:\nText: The Matlab function wblinv computes the inverse cumulative distribution function for a Weibull distribution. The corresponding distribution in scipy that you want is scipy.stats.weibull_min, and the method to compute the inverse of the CDF (also known as the percent point function or the quantile function) is scipy.stats.weibull_min.ppf. The distribution invweibull is a different probability distribution. \nText: For example, \nCode: In [10]: from scipy import stats\n\nIn [11]: p = 0.9978\n\nIn [12]: scale_param = 3.5666\n\nIn [13]: shape_param = 0.4936\n\nIn [14]: stats.weibull_min.ppf(p, shape_param, scale=scale_param)\nOut[14]: 139.97751836430132\n\nAPI:\nscipy.stats.invweibull\n","label":[[381,391,"Mention"],[677,699,"API"]],"Comments":[]}
{"id":60544,"text":"ID:51029521\nPost:\nText: To visualize the problem better, I will refer to the 2x2 dimensions of the array as the rows and columns, and the 3 dimension as depth. I will refer to vectors along the 3rd dimension as \"pixels\" (pixels have length 3), and planes along the first two dimensions as \"channels\". \nText: Your loop is accumulating a set of pixels selected by the mask idx == i, and taking the median of each channel within that set. The result is an Nx3 array, where N is the number of distinct incides that you have. \nText: One day, generalized ufuncs will be ubiquitous in numpy, and np.median will be such a function. On that day, you will be able to use reduceat magic1 to do something like \nCode: unq, ind = np.unique(idx, return_inverse=True)\nnp.median.reduceat(dat.reshape(-1, dat.shape[-1]), np.r_[0, np.where(np.diff(unq[ind]))[0]+1])\n\nText: 1 See Applying operation to unevenly split portions of numpy array for more info on the specific type of magic. \nText: Since this is not currently possible, you can use median instead. This version allows you to compute medians over a set of labeled areas in an array, which is exactly what you have with idx. This method assumes that your index array contains N densely packed values, all of which are in range(N). Otherwise the reshaping operations will not work properly. \nText: If that is not the case, start by transforming idx: \nCode: _, ind = np.unique(idx, return_inverse=True)\nidx = ind.reshape(idx.shape)\n\nText: OR \nCode: idx = np.unique(idx, return_inverse=True)[1].reshape(idx.shape)\n\nText: Since you are actually computing a separate median for each region and channel, you will need to have a set of labels for each channel. Flesh out idx to have a distinct set of indices for each channel: \nCode: chan = dat.shape[-1]\noffset = idx.max() + 1\nindex = np.stack([idx + i * offset for i in range(chan)], axis=-1)\n\nText: Now index has an identical set of regions defined in each channel, which you can use in scipy.ndimage.median: \nCode: out = scipy.ndimage.median(dat, index, index=range(offset * chan)).reshape(chan, offset).T\n\nText: The input labels must be densely packed from zero to offset * chan for index=range(offset * chan) to work properly, and the reshape operation to have the right number of elements. The final transpose is just an artifact of how the labels are arranged. \nText: Here is the complete product, along with an IDEOne demo of the result: \nCode: import numpy as np\nfrom scipy.ndimage import median\n\ndat = np.arange(12).reshape(2, 2, 3)\nidx = np.array([[0, 0], [1, 2]])\n\ndef summarize(dat, idx):\n    idx = np.unique(idx, return_inverse=True)[1].reshape(idx.shape)\n    chan = dat.shape[-1]\n    offset = idx.max() + 1\n    index = np.stack([idx + i * offset for i in range(chan)], axis=-1)\n    return median(dat, index, index=range(offset * chan)).reshape(chan, offset).T\n\nprint(summarize(dat, idx))\n\nAPI:\nscipy.ndimage.median\n","label":[[1023,1029,"Mention"],[2892,2912,"API"]],"Comments":[]}
{"id":60545,"text":"ID:51032354\nPost:\nText: entropy computes the entropy of a discrete distribution. The values are expected to be probabilities, not probability densities. (In fact, if the sum of the values in the input is not 1, the values are scaled so the sum is 1. This is mentioned in the docstring.) If that is what you have (e.g. the 2-d array p holds the joint distribution probabilities of a bivariate discrete distribution) you can simply pass the flattened array to scipy.stats.entropy; e.g. entropy(p.ravel(), base=2). \nText: The function st.entropy does not compute the same mathematical quantity as the entropy method of a scipy continuous distribution such as scipy.stats.multivariate_normal. The entropy method computes the differential entropy of the distribution. See the wikipedia article on differential entropy for a discussion of the difference between the discrete entropy computed by sp.stats.entropy and the differential entropy computed by the entropy method of the scipy distributions. \nAPI:\nscipy.stats.entropy\nscipy.stats.entropy\nscipy.stats.entropy\n","label":[[24,31,"Mention"],[532,542,"Mention"],[889,905,"Mention"],[1000,1019,"API"],[1020,1039,"API"],[1040,1059,"API"]],"Comments":[]}
{"id":60546,"text":"ID:51100284\nPost:\nText: I have no idea why st.mode is so slow. Anyway, you can get a much faster result using np.bincount: \nCode: # create random frame\n>>> a = np.random.randint(0, 256, (800, 500)).astype(np.int8)\n>>> \n# add row offsets to make bincount create separate bins for each row\n>>> counts = np.bincount((a.view(np.uint8) + 256 * np.arange(800)[:, None]).ravel(), minlength=256*800).reshape(800, 256)\n# find the mode\n>>> values = np.argmax(counts, axis=1)\n# discard the counts for all other values\n>>> counts = counts[np.arange(800), values]\n# convert modes back to original dtype\n>>> values = values.astype(np.uint8).view(np.int8)\n\nAPI:\nscipy.stats.mode\n","label":[[43,50,"Mention"],[647,663,"API"]],"Comments":[]}
{"id":60547,"text":"ID:51248586\nPost:\nText: Take a look at eig and scipy.sparse.linalg.eigs. \nCode: import scipy.linalg as la\nimport scipy.sparse.linalg as sla\n\n# Matlab: [V, D] = eig(A, -B)\nD, V = la.eig(A, -B)\n\n# Matlab: [V, D]= eigs(A, -B, 60, SM')\nD, V = sla.eigs(A, 60, -B, which='SM')\n\nText: Note that you will not, in general, get exactly the same results. The eigenvalues might be in a different order, and the eigenvectors might have a different scaling (eigenvectors are not unique). \nAPI:\nscipy.linalg.eig\n","label":[[39,42,"Mention"],[481,497,"API"]],"Comments":[]}
{"id":60548,"text":"ID:51250399\nPost:\nText: The documentation of dist.euclidean states, that only 1D-vectors are allowed as inputs. Thus you must loop over your arrays like: \nCode: distances = np.empty(b.shape[0])\nfor i in range(b.shape[0]):\n    distances[i] = scipy.spatial.distance.euclidean(a, b[i])\n\nText: If you want to have a vectorized implementation, you need to write your own function. Perhaps using np.vectorize with a correct signature will also work, but this is in fact also just a short-hand for a for-loop and will thus have the same performance as a simple for-loop. \nText: As stated in my comment to hannes wittingham's solution, I'll post a one-liner which is focussing on performance: \nCode: distances = ((b - a)**2).sum(axis=1)**0.5\n\nText: Writing out all the calculations reduces the number of separate functions calls and thus assignments of the intermediate results to new arrays. Thus it is about 22% faster than using the solution of hannes wittingham for an array shape of b.shape == (20, 3) and about 5% faster for an array shape of b.shape == (20000, 3): \nCode: a = np.array([1, 1, 1,])\nb = np.random.rand(20, 3)\n%timeit ((b - a)**2).sum(axis=1)**0.5\n# 5.37 s  140 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)\n%timeit euclidean_distances(a, b)\n# 6.89 s  345 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)\n\nb = np.random.rand(20000, 3)\n%timeit ((b - a)**2).sum(axis=1)**0.5\n# 588 s  43.2 s per loop (mean  std. dev. of 7 runs, 1 loop each)\n%timeit euclidean_distances(a, b)\n# 616 s  36.3 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n\nText: But your are losing the flexibility of being able to easily change to distance calculation routine. When using the scipy.spatial.distance module, you can change the calculation routing by simply calling another method. \nText: To improve the calculation performance even further, you can use a jit (just in time) compiler like numba for your functions: \nCode: import numba as nb\n@nb.njit\ndef euc(a, b):\n    return ((b - a)**2).sum(axis=1)**0.5\n\nText: This reduces the time needed to do the calculations by about 70% for small arrays and by about 60% for large arrays. Unluckily the axis keyword for np.linalg.norm is not yet supported by numba. \nAPI:\nscipy.spatial.distance.euclidean\n","label":[[45,59,"Mention"],[2249,2281,"API"]],"Comments":[]}
{"id":60549,"text":"ID:51259773\nPost:\nText: Maximum Likelihood Estimation (MLE) is one of the most important procedure to obtain point estimates for parameters of a distribution. This is what you need to start with. \nText: Analytical Solution: \nText: Multinational distribution is an extension to binomial distribution for which MLE can be obtained analytically. Refer this math stack exchange post (MLE for Multinomial Distribution) for full analytical solution. The procedure starts with defining a likelihood function, L(p) conditioned on observed data x(i), where p and x are the probabilities and observed occurrences for k classes\/ categories and i= 0,1,...k. Its a measure of likelihood of observing a set of observations (x) given parameter set (p): \nText: L(p) equals: \nText: The main idea is to maximize the likelihood function value over the range of parameters (p). Given the total observations n (i.e. sum of occurrences for all categories), the point estimates equals: \nCode: a.values\/a.values.sum()                        # point estimates for p = x\/n\n\n# array([[0.        ], [0.02941176], [0.05882353], [0.08823529], \n#        [0.05882353], [0.02941176], [0.17647059], [0.        ], \n#        [0.02941176], [0.02941176], [0.20588235], [0.29411765]])\n\nText: Numerical Solution: \nText: The above results can also be numerically obtained using scipy.optimize.minimize. Notice that L(p) is a product of factorial and exponential terms. The factorial term is a constant and does not depends on the parameter values (p), therefore not considered for optimization. For the exponential terms, it is better to perform a log transformation to simplify the objective function; common practice for MLE, as log is a monotone increasing function. Also, as the sp.optimize.minimize is used for minimization, we will use the negative of our log transformed likelihood function. Note that maximizing a function value is equal to minimizing its negative value. \nCode: import pandas as pd\nimport numpy as np\nimport scipy.optimize as sciopt\n\n# bounds for parameters to lie between (0,1), \n# absolute zero (0) for lower bound avoided as log takes an infinite value \nbnds = [(0.001e-12,1) for i in range(12)]\n\n# Initializing parameters value for optimization\ninit_parameters = np.asarray([0.1 for i in range(12)])\n\n# Negative Log Likelihood Function\nneg_log_lik = lambda p: -np.sum([a.values[i]*np.log(p[i]) for i in range(12)])\n\n# Constraint sum(p) = 1\ncons = {'type': 'eq', 'fun': lambda p:  (sum([p[i] for i in range(12)]) - 1) }\n\n# Minimizing neg_log_lik\nresults = sciopt.minimize(neg_log_lik, x0 = init_parameters, \n                          method='SLSQP', bounds= bnds, constraints= cons)\n\nresults.x                                    # point estimates for p\n\n#   array([1.00000000e-15, 2.94179308e-02, 5.88243586e-02, 8.82394605e-02,\n#          5.88243586e-02, 2.94059735e-02, 1.76454713e-01, 1.00000000e-15,\n#          2.94134577e-02, 2.94135714e-02, 2.05849197e-01, 2.94156978e-01])\n\nText: Refer minimize documentation for details on above implementation. \nAPI:\nscipy.optimize.minimize\nscipy.optimize.minimize\n","label":[[1742,1762,"Mention"],[2980,2988,"Mention"],[3046,3069,"API"],[3070,3093,"API"]],"Comments":[]}
{"id":60550,"text":"ID:51489457\nPost:\nText: Yes, the quantity called alpha in interval would be called 1-alpha in statistics textbooks. You are not missing anything. It's just a suboptimal name choice. \nText: The only discussion of the name of that parameter that I found concerned a different name collision: \nText: Sigh. Given that both interval and levy_stable are quite esoteric,[...] \nAPI:\nscipy.stats.rv_continuous.interval\n","label":[[58,66,"Mention"],[375,409,"API"]],"Comments":[]}
{"id":60551,"text":"ID:51575962\nPost:\nText: You can also use si.odeint for this kind of task which might be easier to set up: \nCode: import numpy as np\nfrom scipy.integrate import odeint\n\n\ndef twoBody(y, t, mu):\n    \"\"\"\n    Two Body function returns the derivative of the state space variables.\nINPUTS:\n    --- t ---\n        A scalar time value.\n\n    --- y ---\n        A 6x1 array of the state space of a particle in 3D space\nOUTPUTS:\n    --- ydot ---\n        The derivative of y for the two-body problem\n\n\"\"\"\n\n    r = np.sqrt(y[0]**2 + y[1]**2 + y[2]**2)\n\n    ydot = np.empty((6,))\n\n    ydot[0] = y[3]\n    ydot[1] = y[4]\n    ydot[2] = y[5]\n    ydot[3] = (-mu\/(r**3))*y[0]\n    ydot[4] = (-mu\/(r**3))*y[1]\n    ydot[5] = (-mu\/(r**3))*y[2]\n\n    return ydot\n\n\n# In m and m\/s\n# first three are the (x, y, z) position\n# second three are the velocities in those same directions respectively\nY0 = np.array([-5614924.5443320004,\n               -2014046.755686,\n               2471050.0114869997,\n               -673.03650300000004,\n               582.41158099999996,\n               1247.7034980000001])\n\nmu = 3.986004418 * 10**14\n\nsolution = odeint(twoBody, Y0, np.linspace(0., 351., 100), args=(mu, ))\n\nText: I cannot judge whether the output is correct, but it seems to integrate well. \nAPI:\nscipy.integrate.odeint\n","label":[[41,50,"Mention"],[1265,1287,"API"]],"Comments":[]}
{"id":60552,"text":"ID:51610392\nPost:\nText: Here is a simpler method using si.dblquad instead of nquad: \nText: Return the double (definite) integral of func(y, x) from x = a..b and y = gfun(x)..hfun(x). \nCode: from  scipy.integrate import dblquad\n\ndef func(u2, u3, gamma):\n    return (1-1\/(1+gamma-u3-u2))*(1\/(1+u2)**2)*(1\/(1+u3)**2)\n\n\ngamma = 10\n\ndef gfun(u3):\n    return 0\n\ndef hfun(u3):\n    return gamma-u3\n\ndblquad(func, 0, gamma, gfun, hfun, args=(gamma,))\n\nText: It seems that gfun and hfun do not accept the extra arguments, so gamma has to be a global variable. \nText: Using nquad, after many trial and error: \nCode: from  scipy.integrate import nquad\n\ndef func(u2, u3, gamma):\n    return (1-1\/(1+gamma-u3-u2))*(1\/(1+u2)**2)*(1\/(1+u3)**2)\n\ndef range_u3(gamma):\n    return (0, gamma)\n\ndef range_u2(u3, gamma):\n    return (0, gamma-u3)\n\ngamma = 10\nnquad(func, [range_u2, range_u3], args=(gamma,) )\n\nText: Useful quote from the source code of tplquad: \nCode: # nquad will hand (y, x, t0, ...) to ranges0\n# nquad will hand (x, t0, ...) to ranges1\n\nText: And from the nquad documentation, the order of the variables is reversed (same for dblquad): \nText: Integration is carried out in order. That is, integration over x0 is the innermost integral, and xn is the outermost \nText: Generic case with k nested integrations: \nCode: from  scipy.integrate import nquad\nimport numpy as np\n\ndef func(*args):\n    gamma = args[-1]\n    var = np.array(args[:-1])\n\n    return (1-1\/(1+gamma-np.sum(var)))*np.prod(((1+var)**-2))\n\ndef range_func(*args):\n    gamma = args[-1]\n    return (0, gamma-sum(args[:-1]))\n\ngamma, k = 10, 2\nnquad(func, [range_func]*k, args=(gamma,) )\n\nAPI:\nscipy.integrate.dblquad\n","label":[[55,65,"Mention"],[1646,1669,"API"]],"Comments":[]}
{"id":60553,"text":"ID:51734783\nPost:\nText: odeint solves the initial value problem. The problem that you describe is a two-point boundary value problem. For that, you can use solve_bvp \nText: You could also take a look at scikits.bvp1lg and scikits.bvp_solver, although it looks like bvp_solver hasn't been updated in a long time. \nText: For example, here is how you could use scipy.integrate.solve_bvp. I changed the parameters so the solution does not decay so fast and has a lower frequency. With b = 0.25, the decay is rapid enough that (100)  0 for all solutions where (0) = 0 and |(0)| is on the order of 1. \nText: The function bc will be passed the values of [(t), (t)] at t=0 and t=100. It must return two values that are the \"residuals\" of the boundary conditions. That just means it must compute values that must be 0. In your case, just return y0[1] (which is (0)) and y1[0] (which is (100)). (If the boundary condition at t=0 had been, say (0) = 1, the first element of the return value of bc would be y0[1] - 1.) \nCode: import numpy as np\nfrom scipy.integrate import solve_bvp, odeint\nimport matplotlib.pyplot as plt\n\n\ndef pend(t, y, b, c):\n    theta, omega = y\n    dydt = [omega, -b*omega - c*np.sin(theta)]\n    return dydt\n\n\ndef bc(y0, y1, b, c):\n    # Values at t=0:\n    theta0, omega0 = y0\n\n    # Values at t=100:  \n    theta1, omega1 = y1\n\n    # These return values are what we want to be 0:\n    return [omega0, theta1]\n\n\nb = 0.02\nc = 0.08\n\nt = np.linspace(0, 100, 201)\n\n# Use the solution to the initial value problem as the initial guess\n# for the BVP solver. (This is probably not necessary!  Other, simpler\n# guesses might also work.)\nystart = odeint(pend, [1, 0], t, args=(b, c,), tfirst=True)\n\n\nresult = solve_bvp(lambda t, y: pend(t, y, b=b, c=c),\n                   lambda y0, y1: bc(y0, y1, b=b, c=c),\n                   t, ystart.T)\n\n\nplt.figure(figsize=(6.5, 3.5))\nplt.plot(result.x, result.y[0], label=r'$\\theta(t)$')\nplt.plot(result.x, result.y[1], '--', label=r'$\\omega(t)$')\nplt.xlabel('t')\nplt.grid()\nplt.legend(framealpha=1, shadow=True)\nplt.tight_layout()\n\nplt.show()\n\nText: Here's the plot of the result, where you can see that (0) = 0 and (100) = 0. \nText: Note that the solution to the boundary value problem is not unique. If we modify the creation ystart to \nCode: ystart = odeint(pend, [np.pi, 0], t, args=(b, c,), tfirst=True)\n\nText: a different solution is found, as seen in the following plot: \nText: In this solution, the pendulum starts out almost at the inverted position (result.y[0, 0] = 3.141592653578858). It starts to fall very slowly; gradually it falls faster, and it reaches the straight down position at t = 100. \nText: The trivial solution (t)  0 and (t)  0 also satisfies the boundary conditions. \nAPI:\nscipy.integrate.solve_bvp\n","label":[[156,165,"Mention"],[2758,2783,"API"]],"Comments":[]}
{"id":60554,"text":"ID:51905076\nPost:\nText: Scaling \nText: The implementation in calc_old uses the output from np.fft.fft directly without any scaling. \nText: On the other hand the implementation calc_new uses sectrogram which ultimately uses np.fft.rfft but also scales the results based on the received scaling and return_onesided arguments. More specifically: \nText: For the default return_onesided=True (since you haven't provided an explicit value in calc_new), the value of each bin is doubled to count the total energy including the symmetric bin. For the provided scaling='spectrum', the values are further scaled by the factor 1.0\/win.sum()**2. For the selected Hann window, that correspond to 4\/N**2 where N=bf.WINDOW_LEN is the window length. \nText: You may thus expect the new implementation cald_new to give you a result that is scaled by an overall factor of 8\/bf.WINDOW_LEN**2 compared with calc_old. Alternatively if you want your second implementation to give the same scaling as calc_old you should multiply the result of spectrogram by 0.125 * bf.WINDOW_LEN**2. \nText: Number of frequency bins \nText: Given an even number of points nperseg, your initial implementation calc_old keeps only nperseg\/\/2 frequency bins. \nText: On the other hand the complete non-redundant half spectrum should give you nperseg\/\/2 + 1 frequency bin (there are nperseg-2 bins with corresponding symmetry plus 2 asymmetric bins at 0Hz and the Nyquist rate, so keeping the non-redundant part leave you with (nperseg-2)\/\/2 + 2 == nperseg\/\/2 + 1). That is what spectrogram returns. \nText: In other words your initial implementation calc_old is missing the Nyquist frequency bin. \nText: Number of time steps \nText: The implementation in calc_old skips the last time step if there are less than bf.WINDOW_LEN samples left for the last time step computation. It would not skip these samples only occurs whenever len(raw_data)-bf.WINDOW_STRIDE_LEN is an exact multiple of bf.WINDOW_LEN. I'm guessing that isn't the case with your specific input sequence. \nText: In contrast, spectrogram pads the data with extra samples if required such that all inputs samples are used during the spectrogram computation, and that may result in one extra time step compared with your calc_old implementation. \nAPI:\nscipy.signal.spectrogram\nscipy.signal.spectrogram\nscipy.signal.spectrogram\nscipy.signal.spectrogram\n","label":[[190,200,"Mention"],[1020,1031,"Mention"],[1533,1544,"Mention"],[2043,2054,"Mention"],[2267,2291,"API"],[2292,2316,"API"],[2317,2341,"API"],[2342,2366,"API"]],"Comments":[]}
{"id":60555,"text":"ID:51906388\nPost:\nText: The st.lognorm lognormal distribution is parameterised in a slightly unusual way, in order to be consistent with the other continuous distributions. The first argument is the shape parameter, which is your sigma. That's followed by the loc and scale arguments, which allow shifting and scaling of the distribution. Here you want loc=0.0 and scale=exp(mu). So to compute the mean, you want to do something like: \nCode: >>> import numpy as np\n>>> from scipy.stats import lognorm\n>>> mu = 0.4104857306\n>>> sigma = 3.4070874277012617\n>>> lognorm.mean(sigma, 0.0, np.exp(mu))\n500.0000010889041\n\nText: Or more clearly: pass the scale parameter by name, and leave the loc parameter at its default of 0.0: \nCode: >>> lognorm.mean(sigma, scale=np.exp(mu))\n500.0000010889041\n\nText: As @coldspeed says in his comment, your expected value for the standard deviation doesn't look right. I get: \nCode: >>> lognorm.std(sigma, scale=np.exp(mu))\n165831.2402402415\n\nText: and I get the same value calculating by hand. \nText: To double check that these parameter choices are indeed giving the expected lognormal, I created a sample of a million deviates and looked at the mean and standard deviation of the log of that sample. As expected, those give me back values that look roughly like your original mu and sigma: \nCode: >>> samples = lognorm.rvs(sigma, scale=np.exp(mu), size=10**6)\n>>> np.log(samples).mean()  # should be close to mu\n0.4134644116056518\n>>> np.log(samples).std(ddof=1)  # should be close to sigma\n3.4050012251732285\n\nText: In response to the edit: you've got the formula for the variance of a lognormal slightly wrong - you need to replace the exp(sigma**2 - 1) term with (exp(sigma**2) - 1). If you do that, and rerun the fsolve computation, you get: \nCode: sigma = 0.9444564779275075\nmu = 5.768609079062494\n\nText: And with those values, you should get the expected mean and standard deviation: \nCode: >>> from scipy.stats import lognorm\n>>> import numpy as np\n>>> sigma = 0.9444564779275075\n>>> mu = 5.768609079062494\n>>> lognorm.mean(sigma, scale=np.exp(mu))\n499.9999999949592\n>>> lognorm.std(sigma, scale=np.exp(mu))\n599.9999996859631\n\nText: Rather than using fsolve, you could also solve analytically for sigma and mu, given the desired mean and standard deviation. This gives you more accurate results, more quickly: \nCode: >>> mean = 500.0\n>>> var = 600.0**2\n>>> sigma = np.sqrt(np.log1p(var\/mean**2))\n>>> mu = np.log(mean) - 0.5*sigma*sigma\n>>> mu, sigma\n(5.768609078769636, 0.9444564782482624)\n>>> lognorm.mean(sigma, scale=np.exp(mu))\n499.99999999999966\n>>> lognorm.std(sigma, scale=np.exp(mu))\n599.9999999999995\n\nAPI:\nscipy.stats.lognorm\n","label":[[28,38,"Mention"],[2655,2674,"API"]],"Comments":[]}
{"id":60556,"text":"ID:51944636\nPost:\nText: iqr doesn't seem to follow the recursive algorithm documented in Wikipedia. Instead it simply does np.percentile(x, 75) - np.percentile(x, 25) This is not exclusive of the median, it is inclusive, so you get (32 + 33)\/2 - (25 + 28)\/2 = 6 \nText: If you want to use the algorithm in wikipedia you'd need to do something like: \nCode: def iqr_(m):\n    m = np.array(m)\n    n = m.size\/\/2\n    m_ = np.partition(m.ravel(), n + 1)\n    return np.median(m_[n + m.size%2:]) - np.median(m_[:n])\n\niqr_([23,25,28,28,32,33,35])\n8.0\n\nText: EDIT: On the talk page of wikipedia it is brought up that the algorithm presented is not definitive, and in fact the method of iqr is also acceptable. See the three methods for determining quartiles Here \nAPI:\nscipy.stats.iqr\nscipy.stats.iqr\n","label":[[24,27,"Mention"],[674,677,"Mention"],[757,772,"API"],[773,788,"API"]],"Comments":[]}
{"id":60557,"text":"ID:52003654\nPost:\nText: As newbie already said, use sp.optimize.linprog if you want to solve a LP (linear program), i.e. your objective function and your constraints are linear. If either the objective or one of the constraints isn't linear, we are facing a NLP (nonlinear optimization problem), which can be solved by scipy.optimize.minimize: \nCode: minimize(obj_fun, x0=xinit, bounds=bnds, constraints=cons)\n\nText: where obj_fun is your objective function, xinit a initial point, bnds a list of tuples for the bounds of your variables and cons a list of constraint dicts. \nText: Here's an example. Suppose we want to solve the following NLP: \nText: Since all constraints are linear, we can express them by a affin-linear function A*x-b such that we have the inequality A*x >= b. Here A is a 3x2 matrix and b the 3x1 right hand side vector: \nCode: import numpy as np\nfrom scipy.optimize import minimize\n\nobj_fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\nA = np.array([[1, -2], [-1, -2], [-1, 2]])\nb = np.array([-2, -6, -2])\nbnds = [(0, None) for i in range(A.shape[1])]  # x_1 >= 0, x_2 >= 0\nxinit = [0, 0] \n\nText: Now the only thing left to do is defining the constraints, each one has to be a dict of the form \nCode: {\"type\": \"ineq\", \"fun\": constr_fun}\n\nText: where constr_fun is a callable function such that constr_fun >= 0. Thus, we could define each constraint \nCode: cons = [{'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n        {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n        {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2}]\n\nText: and we'd be done. However, in fact, this can be quite cumbersome for many constraints. Instead, we can pass all constraints directly by: \nCode: cons = [{\"type\": \"ineq\", \"fun\": lambda x: A @ x - b}]\n\nText: where @ denotes the matrix multiplication operator. Putting all together \nCode: res = minimize(obj_fun, x0=xinit, bounds=bnds, constraints=cons)\nprint(res)\n\nText: yields \nCode:      fun: 0.799999999999998\n     jac: array([ 0.79999999, -1.59999999])\n message: 'Optimization terminated successfully.'\n    nfev: 16\n     nit: 4\n    njev: 4\n  status: 0\n success: True\n       x: array([1.39999999, 1.69999999])\n\nText: Likewise, you could use a LinearConstraint object: \nCode: from scipy.optimize import LinearConstraint\n\n# lb <= A <= ub. In our case: lb = b, ub = inf\nlincon = LinearConstraint(A, b, np.inf*np.ones(3))\n\n# rest as above\nres = minimize(obj_fun, x0=xinit, bounds=bnds, constraints=(lincon,))\n\nText: Edit: To answer your new question: \nCode: # b1    <= A * x   <==>   -b1 >= -A*x        <==>   A*x - b1 >= 0\n# A * x <= b2      <==>    A*x - b2 <= 0     <==>  -Ax + b2 >= 0\ncons = [{\"type\": \"ineq\", \"fun\": lambda x: A @ x - b1}, {\"type\": \"ineq\", \"fun\": lambda x: -A @ x + b2}]\nsol=minimize(obj,x0,constraints=cons)\nprint(sol)\n\nAPI:\nscipy.optimize.linprog\n","label":[[52,71,"Mention"],[2822,2844,"API"]],"Comments":[]}
{"id":60558,"text":"ID:52035987\nPost:\nText: Very interesting question. As said by a_guest, you will have to fit to the two regions separately. However, I think you probably also want the two regions to connect smoothly at the point t0, the point where we switch from one model to the other. In order to do this, we need to add the constraint that y1 == y2 at the point t0. \nText: In order to do this with scipy, look at opt.minimize with the SLSQP method. However, I wrote a scipy wrapper to make this kind of thing easier, called symfit. I will show you how to do this with symfit, because I think it's better suited to the task, but with this example you should also be able to implement it with pure scipy if you prefer. \nCode: from symfit import parameters, variables, Fit, Piecewise, exp, Eq\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nt, y = variables('t, y')\nm, c, d, k, t0 = parameters('m, c, d, k, t0')\n\n# Help the fit by bounding the switchpoint between the models\nt0.min = 0.6\nt0.max = 0.9\n\n# Make a piecewise model\ny1 = m * t + c\ny2 = d * exp(- k * t)\nmodel = {y: Piecewise((y1, t <= t0), (y2, t > t0))}\n\n# As a constraint, we demand equality between the two models at the point t0\n# to do this, we substitute t -> t0 and demand equality using `Eq`\nconstraints = [Eq(y1.subs({t: t0}), y2.subs({t: t0}))]\n\n# Read the data\ntdata, ydata = np.genfromtxt('Experimental Data.csv', delimiter=',', skip_header=1).T\n\nfit = Fit(model, t=tdata, y=ydata, constraints=constraints)\nfit_result = fit.execute()\nprint(fit_result)\n\nplt.scatter(tdata, ydata)\nplt.plot(tdata, fit.model(t=tdata, **fit_result.params).y)\nplt.show()\n\nAPI:\nscipy.optimize.minimize\n","label":[[400,412,"Mention"],[1616,1639,"API"]],"Comments":[]}
{"id":60559,"text":"ID:52141580\nPost:\nText: For this task correlate2d is your friend. \nText: Demo \nText: I wrapped your code in a function named naive_correlation: \nCode: import numpy as np\n\ndef naive_correlation(image, kernel):\n    image_padded = np.zeros((image.shape[0] + 2, image.shape[1] + 2))\n    image_padded[1:-1, 1:-1] = image\n    out = np.zeros_like(image)\n    for x in range(image.shape[1]):image\n        for y in range(image.shape[0]):\n            out[y, x] = (kernel * image_padded[y:y + 3, x:x + 3]).sum()\n    return out\n\nText: Notice that your snippet throws an error because out is not initialized. \nCode: In [67]: from scipy.signal import correlate2d\n\nIn [68]: img = np.array([[3, 9, 5, 9],\n    ...:                 [1, 7, 4, 3],\n    ...:                 [2, 1, 6, 5]])\n    ...: \n\nIn [69]: kernel = np.array([[0, 1, 0],\n    ...:                    [0, 0, 0],\n    ...:                    [0, -1, 0]])\n    ...: \n\nIn [70]: res1 = correlate2d(img, kernel, mode='same')\n\nIn [71]: res1\nOut[71]: \narray([[-1, -7, -4, -3],\n       [ 1,  8, -1,  4],\n       [ 1,  7,  4,  3]])\n\nIn [72]: res2 = naive_correlation(img, kernel)\n\nIn [73]: np.array_equal(res1, res2)\nOut[73]: True\n\nText: If you wish to perform convolution rather than correlation you could use convolve2d. \nText: Edit \nText: Is this what you are looking for? \nCode: def explicit_correlation(image, kernel):\n    hi, wi= image.shape\n    hk, wk = kernel.shape\n    image_padded = np.zeros(shape=(hi + hk - 1, wi + wk - 1))    \n    image_padded[hk\/\/2:-hk\/\/2, wk\/\/2:-wk\/\/2] = image\n    out = np.zeros(shape=image.shape)\n    for row in range(hi):\n        for col in range(wi):\n            for i in range(hk):\n                for j in range(wk):\n                    out[row, col] += image_padded[row + i, col + j]*kernel[i, j]\n    return out\n\nAPI:\nscipy.signal.correlate2d\n","label":[[38,49,"Mention"],[1788,1812,"API"]],"Comments":[]}
{"id":60560,"text":"ID:52244382\nPost:\nText: Implementation \nText: FFT convolution can be relatively easily implemented in tensorflow. The following follows fftconvolve quite strictly \nCode: import tensorflow as tf\n\ndef _centered(arr, newshape):\n    # Return the center newshape portion of the array.\n    currshape = tf.shape(arr)[-2:]\n    startind = (currshape - newshape) \/\/ 2\n    endind = startind + newshape\n    return arr[..., startind[0]:endind[0], startind[1]:endind[1]]\n\ndef fftconv(in1, in2, mode=\"full\"):\n    # Reorder channels to come second (needed for fft)\n    in1 = tf.transpose(in1, perm=[0, 3, 1, 2])\n    in2 = tf.transpose(in2, perm=[0, 3, 1, 2])\n\n    # Extract shapes\n    s1 = tf.convert_to_tensor(tf.shape(in1)[-2:])\n    s2 = tf.convert_to_tensor(tf.shape(in2)[-2:])\n    shape = s1 + s2 - 1\n\n    # Compute convolution in fourier space\n    sp1 = tf.spectral.rfft2d(in1, shape)\n    sp2 = tf.spectral.rfft2d(in2, shape)\n    ret = tf.spectral.irfft2d(sp1 * sp2, shape)\n\n    # Crop according to mode\n    if mode == \"full\":\n        cropped = ret\n    elif mode == \"same\":\n        cropped = _centered(ret, s1)\n    elif mode == \"valid\":\n        cropped = _centered(ret, s1 - s2 + 1)\n    else:\n        raise ValueError(\"Acceptable mode flags are 'valid',\"\n                         \" 'same', or 'full'.\")\n\n    # Reorder channels to last\n    result = tf.transpose(cropped, perm=[0, 2, 3, 1])\n    return result\n\nText: Example \nText: A quick example of applying a gaussian smoothing with width 20 pixels to the standard \"face\" image is as follows: \nCode: if __name__ == '__main__':\n    from scipy import misc\n    import matplotlib.pyplot as plt\n    from tensorflow.python.ops import array_ops, math_ops\n    session = tf.InteractiveSession()\n\n    # Create gaussian\n    std = 20\n    grid_x, grid_y = array_ops.meshgrid(math_ops.range(3 * std),\n                                        math_ops.range(3 * std))\n    grid_x = tf.cast(grid_x[None, ..., None], 'float32')\n    grid_y = tf.cast(grid_y[None, ..., None], 'float32')\n\n    gaussian = tf.exp(-((grid_x - 1.5 * std) ** 2 + (grid_y - 1.5 * std) ** 2) \/ std ** 2)\n    gaussian = gaussian \/ tf.reduce_sum(gaussian)\n\n    face = misc.face(gray=False)[None, ...].astype('float32')\n\n    # Apply convolution\n    result = fftconv(face, gaussian, 'same')\n    result_r = session.run(result)\n\n    # Show results\n    plt.figure('face')\n    plt.imshow(face[0, ...] \/ 256.0)\n\n    plt.figure('convolved')\n    plt.imshow(result_r[0, ...] \/ 256.0)\n\nAPI:\nscipy.signal.fftconvolve\n","label":[[136,147,"Mention"],[2471,2495,"API"]],"Comments":[]}
{"id":60561,"text":"ID:52357822\nPost:\nText: Here is example code using the opt.differential_evolution genetic algorithm, with your data and equation. This scipy module uses the Latin Hypercube algorithm to ensure a thorough search of parameter space and so requires bounds within which to search - in this example, those bounds are based on the data maximum and minimum values. For other problems you might need to supply different search bounds if you know what range of parameter values to expect. \nCode: import numpy, scipy, matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom scipy.optimize import differential_evolution\nimport warnings\n\n# power law function\ndef func_power_law(x,a,b,c):\n    return a*(x**b)+c\n\ntest_X = [1.0,2,3,4,5,6,7,8,9,10]\ntest_Y =[3.0,1.5,1.2222222222222223,1.125,1.08,1.0555555555555556,1.0408163265306123,1.03125, 1.0246913580246915,1.02]\n\n\n# function for genetic algorithm to minimize (sum of squared error)\ndef sumOfSquaredError(parameterTuple):\n    warnings.filterwarnings(\"ignore\") # do not print warnings by genetic algorithm\n    val = func_power_law(test_X, *parameterTuple)\n    return numpy.sum((test_Y - val) ** 2.0)\n\n\ndef generate_Initial_Parameters():\n    # min and max used for bounds\n    maxX = max(test_X)\n    minX = min(test_X)\n    maxY = max(test_Y)\n    minY = min(test_Y)\n    maxXY = max(maxX, maxY)\n\n    parameterBounds = []\n    parameterBounds.append([-maxXY, maxXY]) # seach bounds for a\n    parameterBounds.append([-maxXY, maxXY]) # seach bounds for b\n    parameterBounds.append([-maxXY, maxXY]) # seach bounds for c\n\n    # \"seed\" the numpy random number generator for repeatable results\n    result = differential_evolution(sumOfSquaredError, parameterBounds, seed=3)\n    return result.x\n\n# generate initial parameter values\ngeneticParameters = generate_Initial_Parameters()\n\n# curve fit the test data\nfittedParameters, pcov = curve_fit(func_power_law, test_X, test_Y, geneticParameters)\n\nprint('Parameters', fittedParameters)\n\nmodelPredictions = func_power_law(test_X, *fittedParameters) \n\nabsError = modelPredictions - test_Y\n\nSE = numpy.square(absError) # squared errors\nMSE = numpy.mean(SE) # mean squared errors\nRMSE = numpy.sqrt(MSE) # Root Mean Squared Error, RMSE\nRsquared = 1.0 - (numpy.var(absError) \/ numpy.var(test_Y))\nprint('RMSE:', RMSE)\nprint('R-squared:', Rsquared)\n\nprint()\n\n\n##########################################################\n# graphics output section\ndef ModelAndScatterPlot(graphWidth, graphHeight):\n    f = plt.figure(figsize=(graphWidth\/100.0, graphHeight\/100.0), dpi=100)\n    axes = f.add_subplot(111)\n\n    # first the raw data as a scatter plot\n    axes.plot(test_X, test_Y,  'D')\n\n    # create data for the fitted equation plot\n    xModel = numpy.linspace(min(test_X), max(test_X))\n    yModel = func_power_law(xModel, *fittedParameters)\n\n    # now the model as a line plot\n    axes.plot(xModel, yModel)\n\n    axes.set_xlabel('X Data') # X axis data label\n    axes.set_ylabel('Y Data') # Y axis data label\n\n    plt.show()\n    plt.close('all') # clean up after using pyplot\n\ngraphWidth = 800\ngraphHeight = 600\nModelAndScatterPlot(graphWidth, graphHeight)\n\nAPI:\nscipy.optimize.differential_evolution\n","label":[[55,81,"Mention"],[3153,3190,"API"]],"Comments":[]}
{"id":60562,"text":"ID:52366706\nPost:\nText: #1. All Distances \nText: only using numpy \nText: The naive method is: \nCode: D = np.sqrt(np.sum((X[:, None, :] - Y[None, :, :])**2, axis = -1))\n\nText: However this takes up a lot of memory creating an (i, j, n)-shaped intermediate matrix, and is very slow \nText: However, thanks to a trick from @Divakar (eucl_dist package, wiki), we can use a bit of algebra and np.einsum to decompose as such: (X - Y)**2 = X**2 - 2*X*Y + Y**2 \nCode: D = np.sqrt(                                #  (X - Y) ** 2   \nnp.einsum('ij, ij ->i', X, X)[:, None] +    # = X ** 2        \\\nnp.einsum('ij, ij ->i', Y, Y)          -    # + Y ** 2        \\\n2 * X.dot(Y.T))                             # - 2 * X * Y\n\nText: Y is X \nText: Similar to above: \nCode: XX = np.einsum('ij, ij ->i', X, X)\nD = np.sqrt(XX[:, None] + XX - 2 * X.dot(X.T))\n\nText: Beware that floating-point imprecision can make the diagonal terms deviate very slightly from zero with this method. If you need to make sure they are zero, you'll need to explicitly set it: \nCode: np.einsum('ii->i', D)[:] = 0 \n\nText: Any Package \nText: sp.spatial.distance.cdist is the most intuitive builtin function for this, and far faster than bare numpy \nCode: from scipy.spatial.distance import cdist\nD = cdist(X, Y)\n\nText: cdist can also deal with many, many distance measures as well as user-defined distance measures (although these are not optimized). Check the documentation linked above for details. \nText: Y is X \nText: For self-referring distances, pdist works similar to cdist, but returns a 1-D condensed distance array, saving space on the symmetric distance matrix by only having each term once. You can convert this to a square matrix using squareform \nCode: from scipy.spatial.distance import pdist, squareform\nD_cond = pdist(X)\nD = squareform(D_cond)\n\nText: #2. K Nearest Neighbors (KNN) \nText: Only using numpy \nText: We could use np.argpartition to get the k-nearest indices and use those to get the corresponding distance values. So, with D as the array holding the distance values obtained above, we would have - \nCode: if k == 1:\n    k_i = D.argmin(0)\nelse:\n    k_i = D.argpartition(k, axis = 0)[:k]\nk_d = np.take_along_axis(D, k_i, axis = 0)\n\nText: However we can speed this up a bit by not taking the square roots until we have reduced our dataset. np.sqrt is the slowest part of calculating the Euclidean norm, so we don't want to do that until the end. \nCode: D_sq = np.einsum('ij, ij ->i', X, X)[:, None] +\\\n       np.einsum('ij, ij ->i', Y, Y) - 2 * X.dot(Y.T)\nif k == 1:\n    k_i = D_sq.argmin(0)\nelse:\n    k_i = D_sq.argpartition(k, axis = 0)[:k]\nk_d = np.sqrt(np.take_along_axis(D_sq, k_i, axis = 0))\n\nText: Now, np.argpartition performs indirect partition and doesn't necessarily give us the elements in sorted order and only makes sure that the first k elements are the smallest ones. So, for a sorted output, we need to use argsort on the output from previous step - \nCode: sorted_idx = k_d.argsort(axis = 0)\nk_i_sorted = np.take_along_axis(k_i, sorted_idx, axis = 0)\nk_d_sorted = np.take_along_axis(k_d, sorted_idx, axis = 0)\n\nText: If you only need, k_i, you never need the square root at all: \nCode: D_sq = np.einsum('ij, ij ->i', X, X)[:, None] +\\\n       np.einsum('ij, ij ->i', Y, Y) - 2 * X.dot(Y.T)\nif k == 1:\n    k_i = D_sq.argmin(0)\nelse:\n    k_i = D_sq.argpartition(k, axis = 0)[:k]\nk_d_sq = np.take_along_axis(D_sq, k_i, axis = 0)\nsorted_idx = k_d_sq.argsort(axis = 0)\nk_i_sorted = np.take_along_axis(k_i, sorted_idx, axis = 0)\n\nText: X is Y \nText: In the above code, replace: \nCode: D_sq = np.einsum('ij, ij ->i', X, X)[:, None] +\\\n       np.einsum('ij, ij ->i', Y, Y) - 2 * X.dot(Y.T)\n\nText: with: \nCode: XX = np.einsum('ij, ij ->i', X, X)\nD_sq = XX[:, None] + XX - 2 * X.dot(X.T))\n\nText: Any Package \nText: KD-Tree is a much faster method to find neighbors and constrained distances. Be aware the while KDTree is usually much faster than brute force solutions above for 3d (as long as oyu have more than 8 points), if you have n-dimensions, KDTree only scales well if you have more than 2**n points. For discussion and more advanced methods for high dimensions, see Here \nText: The most recommended method for implementing KDTree is to use scipy's KDTree or cKDTree \nCode: from scipy.spatial import KDTree\nX_tree = KDTree(X)\nk_d, k_i = X_tree.query(Y, k = k)\n\nText: Unfortunately scipy's KDTree implementation is slow and has a tendency to segfault for larger data sets. As pointed out by @HansMusgrave here, pykdtree increases the performance a lot, but is not as common an include as scipy and can only deal with Euclidean distance currently (while the KDTree in scipy can handle Minkowsi p-norms of any order) \nText: X is Y \nText: Use instead: \nCode: k_d, k_i = X_tree.query(X, k = k)\n\nText: Arbitrary metrics \nText: A BallTree has similar algorithmic properties to a KDTree. I'm not aware of a parallel\/vectorized\/fast BallTree in Python, but using scipy we can still have reasonable KNN queries for user-defined metrics. If available, builtin metrics will be much faster. \nCode: def d(a, b):\n    return max(np.abs(a-b))\n\ntree = sklearn.neighbors.BallTree(X, metric=d)\nk_d, k_i = tree.query(Y)\n\nText: This answer will be wrong if d() is not a metric. The only reason a BallTree is faster than brute force is because the properties of a metric allow it to rule out some solutions. For truly arbitrary functions, brute force is actually necessary. \nText: #3. Radius search \nText: Only using numpy \nText: The simplest method is just to use boolean indexing: \nCode: mask = D_sq < r**2\nr_i, r_j = np.where(mask)\nr_d = np.sqrt(D_sq[mask])\n\nText: Any Package \nText: Similar to above, you can use kd.query_ball_point \nCode: r_ij = X_tree.query_ball_point(Y, r = r)\n\nText: or spsp.KDTree.query_ball_tree \nCode: Y_tree = KDTree(Y)\nr_ij = X_tree.query_ball_tree(Y_tree, r = r)\n\nText: Unfortunately r_ij ends up being a list of index arrays that are a bit difficult to untangle for later use. \nText: Much easier is to use cKDTree's sparse_distance_matrix, which can output a coo_matrix \nCode: from scipy.spatial import cKDTree\nX_cTree = cKDTree(X)\nY_cTree = cKDTree(Y)\nD_coo = X_cTree.sparse_distance_matrix(Y_cTree, r = r, output_type = `coo_matrix`)\nr_i = D_coo.row\nr_j = D_coo.column\nr_d = D_coo.data\n\nText: This is an extraordinarily flexible format for the distance matrix, as it stays an actual matrix (if converted to csr) can also be used for many vectorized operations. \nAPI:\nscipy.spatial.distance.cdist\nscipy.spatial.distance.pdist\nscipy.spatial.KDTree\nscipy.spatial.cKDTree\nscipy.spatial.KDTree.query_ball_point\nscipy.spatial.KDTree.query_ball_tree\n","label":[[1097,1122,"Mention"],[1507,1512,"Mention"],[4243,4249,"Mention"],[4253,4260,"Mention"],[5688,5707,"Mention"],[5766,5793,"Mention"],[6472,6500,"API"],[6501,6529,"API"],[6530,6550,"API"],[6551,6572,"API"],[6573,6610,"API"],[6611,6647,"API"]],"Comments":[]}
{"id":60563,"text":"ID:52452520\nPost:\nText: I think I know why this might be happening. It is possible to pass initial shape parameter estimates when fitting, see the documentation for fit where it states \"Starting value(s) for any shape-characterizing arguments (those not provided will be determined by a call to _fitstart(data)). No default value.\" Here is some extremely ugly, functional, code using my pyeq3 statistical distribution fitter which internally attempts to use different estimates, fit them, and return the parameters for best nnlf of the different fits. This example code does not show the behavior you observe, and gives the same shape parameters regardless of scaling. You would need to install pyeq3 with \"pip3 install pyeq3\" to run this code. The pyeq3 code is designed for text input from a web interface on zunzun.com, so hold you nose - here is the example code: \nCode: import numpy as np\n\n#Set up arrays of values to fit curve to \nsample=np.random.rand(1,30) #Random set of decimal values \nsmallVals = sample*1e-5     #Scale to smaller values \n\n#If the above is not creating different values, this instance of random numbers has:\nbugArr = np.array([0.25322987, 0.81952358, 0.94497455, 0.36295543, 0.72272746, 0.49482558,0.65674877, 0.40876558, 0.64952248, 0.23171052, 0.24645658, 0.35359126,0.27578928, 0.24820775, 0.69789187, 0.98876361, 0.22104156,0.40019593,0.0756707,  0.12342556, 0.3601186,  0.54137089,0.43477705, 0.44622486,0.75483338, 0.69766687, 0.1508741,  0.75428996, 0.93706003, 0.1191987])\nbugArr_small = bugArr*1e-5\n\n#This array of random numbers gives the same shape parameter regardless \nfineArr = np.array([0.7449611,  0.82376693, 0.32601009, 0.18544293, 0.56779629, 0.30495415,\n        0.04670362, 0.88106521, 0.34013959, 0.84598841, 0.24454428, 0.57981437,\n        0.57129427, 0.8857514,  0.96254429, 0.64174078, 0.33048637, 0.17124045,\n        0.11512589, 0.31884749, 0.48975204, 0.87988863, 0.86898236, 0.83513966,\n        0.05858769, 0.25889509, 0.13591874, 0.89106616, 0.66471263, 0.69786708])\nfineArr_small = fineArr*1e-5\n\nbugArr_str = ''\nfor i in range(len(bugArr)):\n    bugArr_str += str(bugArr[i]) + '\\n'\nbugArr_small_str = ''\nfor i in range(len(bugArr_small)):\n    bugArr_small_str += str(bugArr_small[i]) + '\\n'\nfineArr_str = ''\nfor i in range(len(fineArr)):\n    fineArr_str += str(fineArr[i]) + '\\n'\nfineArr_small_str = ''\nfor i in range(len(fineArr_small)):\n    fineArr_small_str += str(fineArr_small[i]) + '\\n'\nimport pyeq3\n\nsimpleObject_bugArr = pyeq3.IModel.IModel()\nsimpleObject_bugArr._dimensionality = 1\npyeq3.dataConvertorService().ConvertAndSortColumnarASCII(bugArr_str, simpleObject_bugArr, False)\nsolver = pyeq3.solverService()\nresult_bugArr = solver.SolveStatisticalDistribution('genextreme', simpleObject_bugArr.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')\nsimpleObject_bugArr_small = pyeq3.IModel.IModel()\nsimpleObject_bugArr_small._dimensionality = 1\npyeq3.dataConvertorService().ConvertAndSortColumnarASCII(bugArr_small_str, simpleObject_bugArr_small, False)\nsolver = pyeq3.solverService()\nresult_bugArr_small = solver.SolveStatisticalDistribution('genextreme', simpleObject_bugArr_small.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')\n\nsimpleObject_fineArr = pyeq3.IModel.IModel()\nsimpleObject_fineArr._dimensionality = 1\npyeq3.dataConvertorService().ConvertAndSortColumnarASCII(fineArr_str, simpleObject_fineArr, False)\nsolver = pyeq3.solverService()\nresult_fineArr = solver.SolveStatisticalDistribution('genextreme', simpleObject_fineArr.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')\n\nsimpleObject_fineArr_small = pyeq3.IModel.IModel()\nsimpleObject_fineArr_small._dimensionality = 1\npyeq3.dataConvertorService().ConvertAndSortColumnarASCII(fineArr_small_str, simpleObject_fineArr_small, False)\nsolver = pyeq3.solverService()\nresult_fineArr_small = solver.SolveStatisticalDistribution('genextreme', simpleObject_fineArr_small.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')\n\nprint('ba',result_bugArr[1]['fittedParameters'])\nprint('ba_s',result_bugArr_small[1]['fittedParameters'])\nprint()\nprint('fa',result_fineArr[1]['fittedParameters'])\nprint('fa_s',result_fineArr_small[1]['fittedParameters'])\n\nAPI:\nscipy.stats.rv_continuous.fit\n","label":[[165,168,"Mention"],[4226,4255,"API"]],"Comments":[]}
{"id":60564,"text":"ID:52592811\nPost:\nText: It is not possible to obtain the value of chi^2 from curve_fit directly without manual calculations. It is possible to get additional output from curve_fit besides popt and pcov by providing the argument full_output=True, but the additional output does not contain the value of chi^2. (The additional output is documented e.g. at leastsq here). \nText: In the case where sigma is a MxM array, the definition of the chi^2 function minimized by curve_fit is slightly different. In this case, curve_fit minimizes the function r.T @ inv(sigma) @ r, where r = ydata - f(xdata, *popt), instead of chisq = sum((r \/ sigma) ** 2) in the case of one dimensional sigma, see the documentation of the parameter sigma. So you should also be able to calculate chi^2 in your case by using r.T @ inv(sigma) @ r with your optimized parameters. \nText: An alternative would be to use another package, for example lmfit, where the value of chi square can be directly obtained from the fit result: \nCode: from lmfit.models import GaussianModel\n\nmodel = GaussianModel()\n\n# create parameters with initial guesses:\nparams = model.make_params(center=9, amplitude=40, sigma=1)  \n\nresult = model.fit(n, params, x=centers)\nprint(result.chisqr)\n\nAPI:\nscipy.optimize.curve_fit\n","label":[[77,86,"Mention"],[1244,1268,"API"]],"Comments":[]}
{"id":60565,"text":"ID:52595447\nPost:\nText: A fuller traceback would be nice. My guess is that seaborn.distplot is using staas to calculate something. The error occurs in \nCode: def _compute_qth_percentile(sorted, per, interpolation_method, axis):\n    ....\n    indexer = [slice(None)] * sorted.ndim\n    ...\n    indexer[axis] = slice(i, i + 2)\n    ...\n    return np.add.reduce(sorted[indexer] * weights, axis=axis) \/ sumval\n\nText: So in this last line, the list indexer is used to slice sorted. \nCode: In [81]: x = np.arange(12).reshape(3,4)\nIn [83]: indexer = [slice(None), slice(None,2)]\nIn [84]: x[indexer]\n\/usr\/local\/bin\/ipython3:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  #!\/usr\/bin\/python3\nOut[84]: \narray([[0, 1],\n       [4, 5],\n       [8, 9]])\nIn [85]: x[tuple(indexer)]\nOut[85]: \narray([[0, 1],\n       [4, 5],\n       [8, 9]])\n\nText: Using a list of slices works, but the plan is to depreciate in the future. Indexes that involve several dimensions are supposed to be tuples. The use of lists in the context is an older style that is being phased out. \nText: So the scipy developers need to fix this. This isn't something end users should have to deal with. But for now, don't worry about the futurewarning. It doesn't affect the calculations or plotting. There is a way of suppressing future warnings, but I don't know it off hand. \nText: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated use `arr[tuple(seq)]` instead of `arr[seq]` \nAPI:\nscipy.stats\n","label":[[101,106,"Mention"],[1697,1708,"API"]],"Comments":[]}
{"id":60566,"text":"ID:52692882\nPost:\nText: Contingency tables are used in statistics to summarize the relationship between several categorical variables. \nText: In your example, The Contingency table between the two variables Genderand Married is a Frequency table of these variables presented simultaneously. \nText: A chi-squared test conducted on a contingency table can test whether or not a relationship exists between variables. These effects are defined as relationships between rows and columns. \nText: chi2_contingency computes -by default- Pearsons chi-squared statistic. \nText: Moreover,we are interested in the Sig(2-Tailed) which is the p-value in your example. \nText: The p-value is the evidence against a null hypothesis. The smaller the p-value, the strong the evidence that you should reject the null hypothesis. \nText: And the null hypothesis in your case is the dependence of the observed frequencies in the contingency table. \nText: Choosing Significant Level -alpha as 5%; your p-value is 4.502328957824834e-19 is much less than .05 indicating that the rows and columns of the contingency table are independent. Generally this means that it is worthwhile to interpret the cells in the contingency table. \nText: In this particular case it means that being Male or Female (i.e. Gender) is not distributed similarly across the different levels of Marital Status (i.e. Married, Not-Married). \nText: So, being married may be the status of one gender more than the other! \nText: Update \nText: According to your comment, I see you have some doubts about this test. \nText: This test basically tells you if the relationship between variables is Significant (i.e. may represent the population) or came by chance! \nText: So if you have high level of Significance (high p-value), that means there's a significant dependency between the variables! \nText: Now, if Gender and Married are both features in your model, that may lead to over-fitting and features redundancy. Then, you may want to choose one of them. \nText: But if Gender or Married is the dependent variable (like y), then it's good they have significant relationship. \nText: Extra bonus: Sometimes one of the features become temporarily a dependent variable during Data Imputation (when you have missing values). \nAPI:\nscipy.stats.chi2_contingency\n","label":[[491,507,"Mention"],[2271,2299,"API"]],"Comments":[]}
{"id":60567,"text":"ID:52702937\nPost:\nText: I had similar problem. Found solution on the net that seems to be also faster than sp.signal.resample (https:\/\/github.com\/nwhitehead\/swmixer\/blob\/master\/swmixer.py). It is based on np.interp function. Added also sp.signal.resample_poly for comparison (which is not very best in this case). \nCode: import scipy.signal \nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# DISCLAIMER: This function is copied from https:\/\/github.com\/nwhitehead\/swmixer\/blob\/master\/swmixer.py, \n#             which was released under LGPL. \ndef resample_by_interpolation(signal, input_fs, output_fs):\n\n    scale = output_fs \/ input_fs\n    # calculate new length of sample\n    n = round(len(signal) * scale)\n\n    # use linear interpolation\n    # endpoint keyword means than linspace doesn't go all the way to 1.0\n    # If it did, there are some off-by-one errors\n    # e.g. scale=2.0, [1,2,3] should go to [1,1.5,2,2.5,3,3]\n    # but with endpoint=True, we get [1,1.4,1.8,2.2,2.6,3]\n    # Both are OK, but since resampling will often involve\n    # exact ratios (i.e. for 44100 to 22050 or vice versa)\n    # using endpoint=False gets less noise in the resampled sound\n    resampled_signal = np.interp(\n        np.linspace(0.0, 1.0, n, endpoint=False),  # where to interpret\n        np.linspace(0.0, 1.0, len(signal), endpoint=False),  # known positions\n        signal,  # known data points\n    )\n    return resampled_signal\n\nx = np.linspace(0, 10, 256, endpoint=False)\ny = np.cos(-x**2\/6.0)\nyre = scipy.signal.resample(y,20)\nxre = np.linspace(0, 10, len(yre), endpoint=False)\n\nyre_polyphase = scipy.signal.resample_poly(y, 20, 256)\nyre_interpolation = resample_by_interpolation(y, 256, 20)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x,y,'b', xre,yre,'or-')\nplt.plot(xre, yre_polyphase, 'og-')\nplt.plot(xre, yre_interpolation, 'ok-')\nplt.legend(['original signal', 'scipy.signal.resample', 'scipy.signal.resample_poly', 'interpolation method'], loc='lower left')\nplt.show()\n\nText: CARE! This method, however, seems to perform some unwanted low-pass filtering. \nCode: x = np.linspace(0, 10, 16, endpoint=False)\ny = np.random.RandomState(seed=1).rand(len(x))\nyre = scipy.signal.resample(y, 18)\nxre = np.linspace(0, 10, len(yre), endpoint=False)\n\nyre_polyphase = scipy.signal.resample_poly(y, 18, 16)\nyre_interpolation = resample_by_interpolation(y, 16, 18)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x,y,'b', xre,yre,'or-')\nplt.plot(xre, yre_polyphase, 'og-')\nplt.plot(xre, yre_interpolation, 'ok-')\nplt.legend(['original signal', 'scipy.signal.resample', 'scipy.signal.resample_poly', 'interpolation method'], loc='lower left')\nplt.show()\n\nText: Still, this is the best result I got, but I hope someone will provide something better. \nAPI:\nscipy.signal.resample\nscipy.signal.resample_poly\n","label":[[107,125,"Mention"],[236,259,"Mention"],[2731,2752,"API"],[2753,2779,"API"]],"Comments":[]}
{"id":60568,"text":"ID:52799188\nPost:\nText: To sample from a distribution in sp.stats use the .rvs method. \nText: Example: \nCode: >>> from scipy import stats\n>>> \n>>> n = 3\n>>> mn = np.random.random(n)\n>>> cov = np.random.random((2*n, n)) - 0.5\n>>> cov = cov.T@cov\n>>> \n>>> frzn = stats.multivariate_normal(mn, cov)\n>>> frzn\n<scipy.stats._multivariate.multivariate_normal_frozen object at 0x7f156ea782b0>\n>>> frzn.rvs(n)\narray([[ 1.38391348,  0.65518546, -0.79541539],\n       [ 0.36422157, -0.49308578,  0.94995824],\n       [-0.73152442, -0.06003768, -0.28373662]])\n\nAPI:\nscipy.stats\n","label":[[57,65,"Mention"],[552,563,"API"]],"Comments":[]}
{"id":60569,"text":"ID:52976809\nPost:\nText: You could use lqr together with scipy.sparse.block_diag. I'm just not sure it will be any faster. \nText: Example: \nCode: >>> import numpy as np\n>>> from scipy.sparse import block_diag\n>>> from scipy.sparse import linalg as sprsla\n>>> \n>>> x = np.random.random((3,5,4))\n>>> y = np.random.random((3,5))\n>>> \n>>> for A, b in zip(x, y):\n...     print(np.linalg.lstsq(A, b))\n... \n(array([-0.11536962,  0.22575441,  0.03597646,  0.52014899]), array([0.22232195]), 4, array([2.27188101, 0.69355384, 0.63567141, 0.21700743]))\n(array([-2.36307163,  2.27693405, -1.85653264,  3.63307554]), array([0.04810252]), 4, array([2.61853881, 0.74251282, 0.38701194, 0.06751288]))\n(array([-0.6817038 , -0.02537582,  0.75882223,  0.03190649]), array([0.09892803]), 4, array([2.5094637 , 0.55673403, 0.39252624, 0.18598489]))\n>>> \n>>> sprsla.lsqr(block_diag(x), y.ravel())\n(array([-0.11536962,  0.22575441,  0.03597646,  0.52014899, -2.36307163,\n        2.27693405, -1.85653264,  3.63307554, -0.6817038 , -0.02537582,\n        0.75882223,  0.03190649]), 2, 15, 0.6077437777160813, 0.6077437777160813, 6.226368324510392, 106.63227777368986, 1.3277892240815807e-14, 5.36589277249043, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\nAPI:\nscipy.sparse.linalg.lsqr\n","label":[[38,41,"Mention"],[1246,1270,"API"]],"Comments":[]}
{"id":60570,"text":"ID:53002457\nPost:\nText: Have you tried to add import scipy.interpolate.interpnd in your base program (not in the setup script)? What happens then? According to the experiences I have made so far, one needs to add scipy to the packages list in order to let it be included correctly by cx_Freeze. But then one also needs to add kdtree to the excludes list due to this issue. Try thus to use the following options in your setup script: build_exe_options = { \"packages\": [\"os\",\"textwrap\",\"msvcrt\",\"warnings\",\"time\",\"datetime\",\"platform\",\"sklearn\",\"operator\",\"nltk.tokenize\",\"stop_words\",\"pandas\",\"nltk.stem.porter\",\"sklearn.feature_extraction.text\",\"sklearn.decomposition\",\"progressbar\",\"numpy\",\"packaging\",\"asyncio\",\"scipy\" ], \"includes\": [\"appdirs\",\"packaging.version\",\"packaging.specifiers\",\"packaging.requirements\",\"pyLDAvis.sklearn\",\"pyLDAvis.urls\",\"scipy.sparse.csgraph._validation\"], \"excludes\" : [\"tkinter\",\"sqlite3\",\"scipy.spatial.cKDTree\"], \"include_msvcr\" : True } Additional remark: the build_exe option \"include_msvcr\": True seems to have no effect with cx_Freeze versions 5.0.2, 5.1.1, and 6.0b1. See this issue, my post there (jpeg13) contains some more details. You might need to add the MSVCR DLLs manually using the build_exe option include_files. \nAPI:\nscipy.spatial.cKDTree\n","label":[[326,332,"Mention"],[1268,1289,"API"]],"Comments":[]}
{"id":60571,"text":"ID:53036574\nPost:\nText: You need 2d interpolation over scattered data. I'd default to using griddata in this case, but you seem to want a callable interpolator, whereas griddata needs a given set of points onto which it will interpolate. \nText: Not to worry: griddata with 2d cubic interpolation uses a CloughTocher2DInterpolator. So we can do exactly that: \nCode: import numpy as np\nimport scipy.interpolate as interp\n\nx = [100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0, 100.0, 75.0, 50.0, 0.0]\ny = [300.0, 300.0, 300.0, 300.0, 500.0, 500.0, 500.0, 500.0, 700.0, 700.0, 700.0, 700.0, 1000.0, 1000.0, 1000.0, 1000.0, 1500.0, 1500.0, 1500.0, 1500.0, 2000.0, 2000.0, 2000.0, 2000.0, 3000.0, 3000.0, 3000.0, 3000.0, 5000.0, 5000.0, 5000.0, 5000.0, 7500.0, 7500.0, 7500.0, 75000.0, 10000.0, 10000.0, 10000.0, 10000.0]\nz = [100.0, 95.0, 87.5, 77.5, 60.0, 57.0, 52.5, 46.5, 40.0, 38.0, 35.0, 31.0, 30.0, 28.5, 26.25, 23.25, 23.0, 21.85, 20.125, 17.825, 17.0, 16.15, 14.875, 13.175, 13.0, 12.35, 11.375, 10.075, 10.0, 9.5, 8.75, 7.75, 7.0, 6.65, 6.125, 5.425, 5.0, 4.75, 4.375, 3.875]\n\ninterpolator = interp.CloughTocher2DInterpolator(np.array([x,y]).T, z)\n\nText: Now you can call this interpolator with 2 coordinates to give you the corresponding interpolated data point: \nCode: >>> interpolator(x[10], y[10]) == z[10]\nTrue\n>>> interpolator(2, 300)\narray(77.81343)\n\nText: Note that you'll have to stay inside the convex hull of the input points, otherwise you'll get nan (or whatever is passed as the fill_value keyword to the interpolator): \nCode: >>> interpolator(2, 30)\narray(nan)\n\nText: Extrapolation is usually meaningless anyway, and your input points are scattered in a bit erratic way: \nText: So even if extrapolation was possible I wouldn't believe it. \nText: Just to demonstrate how the resulting interpolator is constrained to the convex hull of the input points, here's a surface plot of your data on a gridded mesh we create just for plotting: \nCode: import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# go linearly in the x grid\nxline = np.linspace(min(x), max(x), 30)\n# go logarithmically in the y grid (considering y distribution)\nyline = np.logspace(np.log10(min(y)), np.log10(max(y)), 30)\n# construct 2d grid from these\nxgrid,ygrid = np.meshgrid(xline, yline)\n# interpolate z data; same shape as xgrid and ygrid\nz_interp = interpolator(xgrid, ygrid)\n\n# create 3d Axes and plot surface and base points\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(xgrid, ygrid, z_interp, cmap='viridis',\n                vmin=min(z), vmax=max(z))\nax.plot(x, y, z, 'ro')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\n\nplt.show()\n\nText: Here's the output from two angles (it's better to rotate around interactively; such stills don't do the 3d representation justice): \nText: There are two main features to note: \nText: The surface nicely fits the red points, which is expected from interpolation. Fortunately the input points are nice and smooth so everything goes well with interpolation. (The fact that the red points are usually hidden by the surface is only due to how pyplot's renderer mishandles the relative position of complex 3d objects) The surface is cut (due to nan values) along the convex hull of the input points, so even though our gridded arrays define a rectangular grid we only get a cut of the surface where interpolation makes sense. \nAPI:\nscipy.interpolate.griddata\n","label":[[92,100,"Mention"],[3588,3614,"API"]],"Comments":[]}
{"id":60572,"text":"ID:53245038\nPost:\nText: I don't think you want to evaluate the hermite polynomials symbolically. Instead, why not use numpy.polynomial.hermite.hermval? Also maybe use spec.factorial to calculate the factorial. \nCode: import numpy as np\nfrom numpy.polynomial.hermite import hermval\nfrom scipy.special import factorial\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\n\ndef psi(n, x, t):\n    A = (np.pi ** -0.25) * (1 \/ np.sqrt((2 ** n) * factorial(n)))\n    E = n + 0.5\n    nar = np.zeros(n+1)\n    nar[-1] = 1\n    f = A * hermval(x,nar) * np.exp(-(x ** 2) \/ 2) * np.cos(E * t)\n    return f\n\n\ndef animar(f, x0=0, xf=1, dx=0.01, t0=0, tf=1, dt=0.01, ym=-2, yM=2):\n    nf = int((xf - x0) \/ dx + 1)\n    nt = int((tf - t0) \/ dt + 1)\n    x = np.linspace(x0, xf, nf)\n    t = np.linspace(t0, tf, nt)\n\n    fig, ax = plt.subplots()\n\n    ax.set_xlim((x0, xf))\n    ax.set_ylim((ym, yM))\n\n    line, = ax.plot([], [], lw=2)\n\n    def init():\n        line.set_data([], [])\n        return line,\n\n    def animate(i):\n        y = f(x, i)\n        line.set_data(x, y)\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                   frames=5 * t, interval=20, blit=True)\n    plt.show()\n    return anim\n\n\nF = lambda x,t: psi(3, x, t)\nanim = animar(F, x0=-3, xf=3, tf=3, ym=-1, yM=1)\n\nAPI:\nscipy.special.factorial\n","label":[[167,181,"Mention"],[1338,1361,"API"]],"Comments":[]}
{"id":60573,"text":"ID:53252167\nPost:\nText: expon has two parameters, the location loc and the scale scale. (In fact, all the univariate continuous distributions in SciPy include these two parameters.) Neither of the two versions that you show include the location parameter loc. You can think of them as having fixed loc=0; when you use the fit method, you can achieve this by using the argument floc=0. See Generating random numbers given required distribution and empirical sampling for an example. \nText: SciPy's scale corresponds to  in your second version of the distribution, or 1\/ in the first version. \nAPI:\nscipy.stats.expon\n","label":[[24,29,"Mention"],[599,616,"API"]],"Comments":[]}
{"id":60574,"text":"ID:53290845\nPost:\nText: A sympy function isn't a Python function as opt.newton expects it. To convert: \nCode: pyfunction0 = sympy.lambdify(x, function0)\n\nAPI:\nscipy.optimize.newton\n","label":[[68,78,"Mention"],[159,180,"API"]],"Comments":[]}
{"id":60575,"text":"ID:53338192\nPost:\nText: You'll get your expected result with numpy.percentile if you set interpolation=midpoint: \nCode: x = numpy.array([4.1, 6.2, 6.7, 7.1, 7.4, 7.4, 7.9, 8.1])\nq1_x = numpy.percentile(x, 25, interpolation='midpoint')\nq3_x = numpy.percentile(x, 75, interpolation='midpoint')\nprint(q3_x - q1_x)\n\nText: This outputs: \nCode: 1.2000000000000002\n\nText: Setting interpolation=midpoint also makes iqr give the result you wanted: \nCode: from scipy.stats import iqr\n\nx = numpy.array([4.1, 6.2, 6.7, 7.1, 7.4, 7.4, 7.9, 8.1])\nprint(iqr(x, rng=(25,75), interpolation='midpoint'))\n\nText: which outputs: \nCode: 1.2000000000000002\n\nText: See the interpolation parameter in the linked docs for more info on what the option actually does. \nAPI:\nscipy.stats.iqr\n","label":[[407,410,"Mention"],[746,761,"API"]],"Comments":[]}
{"id":60576,"text":"ID:53401495\nPost:\nText: First, your f sometimes returns an int, and if no explicit output dtype is specified, numpy.vectorize guesses the output dtype by calling the underlying function on the first element of the input. That means that some of the f results in the derivative calculation are being coerced to integers, throwing off the results. \nText: You can't just call numpy.vectorize on a function that doesn't handle arrays and assume everything will work out. You still have to pay attention to things like dtype and the other quirks in the docs, and it'll never be as fast as a function written to handle vectorized operation naturally. \nText: The other problem is that, as explicitly stated in the documentation, \nText: scipy.misc.derivative(func, x0, dx=1.0, n=1, args=(), order=3) Find the n-th derivative of a function at a point. Given a function, use a central difference formula with spacing dx to compute the n-th derivative at x0. \nText: smp.derivative uses a central difference formula, with a default spacing of 1. This step size is much larger than the size of the \"wiggles\" in your graph. You will have to specify a smaller step size to get useful derivative results. \nAPI:\nscipy.misc.derivative\n","label":[[955,969,"Mention"],[1195,1216,"API"]],"Comments":[]}
{"id":60577,"text":"ID:53466472\nPost:\nText: Solution \nText: Here's a complete working example for fitting points of the form (x, 0): \nCode: from scipy.spatial.distance import cdist\nfrom scipy.optimize import minimize\n\n# set up a test set of 100 points to fit against\nn = 100\nxyTestset = np.random.rand(n,2)*10\n\ndef fun(x, xycomp):\n        # x is a vector, assumed to be of size 1\n        # cdist expects a 2D array, so we reshape xy into a 1x2 array\n        xy = np.array((x[0], 0)).reshape(1, -1)\n        return cdist(xy, xycomp).max()\n\nfit = minimize(fun, x0=0, args=xyTestset)\nprint(fit.x)\n\nText: which outputs: \nCode: [5.06807808]\n\nText: This means, roughly speaking, that the minimization is finding the centroid of the set of random test points, as expected. If you wanted to do a 2D fitting to points of the form (x, y) instead, you can do: \nCode: from scipy.spatial.distance import cdist\nfrom scipy.optimize import minimize\n\n# set up a test set of 100 points to fit against\nn = 100\nxyTestset = np.random.rand(n,2)*10\n\ndef fun(x, xycomp):\n        # x is a vector, assumed to be of size 2\n        return cdist(x.reshape(1, -1), xycomp).max()\n\nfit = minimize(fun, x0=(0, 0), args=xyTestset)\nprint(fit.x)\n\nText: which outputs: \nCode: [5.21292828 5.01491085]\n\nText: which, again, is roughly the centroid of the 100 random points in xyTestset, as you'd expect. \nText: Complete explanation \nText: The problem that you're running into is that minimize has very specific expectations about the form of its first argument fun. fun is supposed to be a function that takes x as its first argument, where x is a 1D vector of the values to be minimized over. fun can also take additional arguments. These have to be passed into minimize via the args parameter, and their values are constant (ie they won't change over the course of the minimization). \nText: Also, you should be aware that your case of fitting (x, 0) can be simplified. It's effectively a 1D problem, so you all you need to do is calculate the x distances between the points. You can completely ignore the y distances and still get the same results. \nText: Additionally, you don't need a minimization to solve the problem you stated. The point that minimizes the distance to the farthest point (which is the same as saying \"minimizing the distance to all points\") is the centroid. The coordinates of the centroid are the means of each coordinate in your set of points, so if your points are stored in an Nx2 array xydata you can calculate the centroid by just doing: \nCode: xydata.mean(axis=1)\n\nAPI:\nscipy.optimize.minimize\n","label":[[1423,1431,"Mention"],[2540,2563,"API"]],"Comments":[]}
{"id":60578,"text":"ID:53484320\nPost:\nText: You can compute the condensed distance matrix of your objects and pass it to liknage to compute the linkage matrix. Then pass the linkage matrix to, say, fcluster or scipy.cluster.hierarchy.dendrogram. \nText: For example, \nCode: from scipy.cluster.hierarchy import linkage, dendrogram\n\nn = len(objects)\ncondensed_dist = [my_distance_metric(objects[j], objects[k])\n                      for j in range(n)\n                          for k in range(j+1, n)]\n\nZ = linkage(condensed_dist)\ndendrogram(Z)\n\nAPI:\nscipy.cluster.hierarchy.linkage\nscipy.cluster.hierarchy.fcluster\n","label":[[101,108,"Mention"],[178,186,"Mention"],[527,558,"API"],[559,591,"API"]],"Comments":[]}
{"id":60579,"text":"ID:53490554\nPost:\nText: plot method for your maxima, df3_max, should do the trick: \nCode: df3_max.plot()\n\nText: To apply the filter, you can use the butter function for a lowpass filter. \nCode: b, a = signal.butter(5, 0.1)\ny2 = signal.filtfilt(b, a, df3_max.values)\ndf3_max_filt = pd.DataFrame(y2, index=df3_max.index)\n\ndf3_max_filt.plot()\n\nAPI:\nscipy.signal.butter\n","label":[[149,155,"Mention"],[346,365,"API"]],"Comments":[]}
{"id":60580,"text":"ID:53567440\nPost:\nText: I just realised I was looking at it the wrong way. Solution can be found here: Fit plane to a set of points in 3D: opt.minimize vs lstsq \nAPI:\nscipy.optimize.minimize\nscipy.linalg.lstsq\n","label":[[139,151,"Mention"],[155,160,"Mention"],[167,190,"API"],[191,209,"API"]],"Comments":[]}
{"id":60581,"text":"ID:53697404\nPost:\nText: Fast solution \nText: Here's how you can convert the output of the fast solution based on spsp.Voronoi that you linked to into a Numpy array of arbitrary width and height. Given the set of regions, vertices that you get as output from the voronoi_finite_polygons_2d function in the linked code, here's a helper function that will convert that output to an array: \nCode: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\ndef vorarr(regions, vertices, width, height, dpi=100):\n    fig = plt.Figure(figsize=(width\/dpi, height\/dpi), dpi=dpi)\n    canvas = FigureCanvas(fig)\n    ax = fig.add_axes([0,0,1,1])\n\n    # colorize\n    for region in regions:\n        polygon = vertices[region]\n        ax.fill(*zip(*polygon), alpha=0.4)\n\n    ax.plot(points[:,0], points[:,1], 'ko')\n    ax.set_xlim(vor.min_bound[0] - 0.1, vor.max_bound[0] + 0.1)\n    ax.set_ylim(vor.min_bound[1] - 0.1, vor.max_bound[1] + 0.1)\n\n    canvas.draw()\n    return np.frombuffer(canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)\n\nText: Testing it out \nText: Here's a complete example of vorarr in action: \nCode: from scipy.spatial import Voronoi\n\n# get random points\nnp.random.seed(1234)\npoints = np.random.rand(15, 2)\n\n# compute Voronoi tesselation\nvor = Voronoi(points)\n\n# voronoi_finite_polygons_2d function from https:\/\/stackoverflow.com\/a\/20678647\/425458\nregions, vertices = voronoi_finite_polygons_2d(vor)\n\n# convert plotting data to numpy array\narr = vorarr(regions, vertices, width=1000, height=1000)\n\n# plot the numpy array\nplt.imshow(arr)\n\nText: Output: \nText: As you can see, the resulting Numpy array does indeed have a shape of (1000, 1000), as specified in the call to vorarr. \nText: If you want to fix up your existing code \nText: Here's how you could alter your current code to work with\/return a Numpy array: \nCode: import math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_voronoi_diagram(width, height, centers_x, centers_y):\n    arr = np.zeros((width, height, 3), dtype=int)\n    imgx,imgy = width, height\n    num_cells=len(centers_x)\n\n    nx = centers_x\n    ny = centers_y\n\n    randcolors = np.random.randint(0, 255, size=(num_cells, 3))\n\n    for y in range(imgy):\n        for x in range(imgx):\n            dmin = math.hypot(imgx-1, imgy-1)\n            j = -1\n            for i in range(num_cells):\n                d = math.hypot(nx[i]-x, ny[i]-y)\n                if d < dmin:\n                    dmin = d\n                    j = i\n            arr[x, y, :] = randcolors[j]\n\n    plt.imshow(arr.transpose(1, 0, 2))\n    plt.scatter(cx, cy, c='w', edgecolors='k')\n    plt.show()\n    return arr\n\nText: Example usage: \nCode: np.random.seed(1234)\n\nwidth = 500\ncx = np.random.rand(15)*width\n\nheight = 300\ncy = np.random.rand(15)*height\n\narr = generate_voronoi_diagram(width, height, cx, cy)\n\nText: Example output: \nAPI:\nscipy.spatial.Voronoi\n","label":[[113,125,"Mention"],[2927,2948,"API"]],"Comments":[]}
{"id":60582,"text":"ID:53706019\nPost:\nText: Checkout sp.integrate.ode here. It is more flexible than odeint and helps with what you want to do. \nText: A simple example using a vertical shot, integrated until it touches ground: \nCode: from scipy.integrate import ode, odeint\nimport scipy.constants as SPC\n\ndef f(t, y):\n    return [y[1], -SPC.g]\n\nv0 = 10\ny0 = 0\n\nr = ode(f)\nr.set_initial_value([y0, v0], 0)\n\ndt = 0.1\nwhile r.successful() and r.y[0] >= 0:\n    print('time: {:.3f}, y: {:.3f}, vy: {:.3f}'.format(r.t + dt, *r.integrate(r.t + dt)))\n\nText: Each time you call r.integrate, r will store current time and y value. You can pass them to a list if you want to store them. \nAPI:\nscipy.integrate.ode\n","label":[[33,49,"Mention"],[662,681,"API"]],"Comments":[]}
{"id":60583,"text":"ID:54061450\nPost:\nText: According to interp1d's documentation: \nText: ynew = f(xnew) # use interpolation function returned by interp1d \nText: It returns a function \/ callable object which takes a value x and returns the interpolated value of f(x). In your case \"x\" = t: \nCode: dpdt = (ps-p)\/d + ft(t1)\/Mu1   # pass t1 to ft to obtain interpolated value\n\nText: UPDATE \nText: This new error is due to odeint sampling the function f(t) at values of t beyond the last value of t_points. This is necessary for error correction and there is no option to prevent odeint from doing so. However, we can instead extrapolate f(t) beyond the supplied samples, using InterpolatedUnivariateSpline: from interpolate import InterpolatedUnivariateSpline ... ft = InterpolatedUnivariateSpline(t1, y1, k=1) As with interp1d, this returns a function with the same signature. However, after applying this fix the result becomes: Which is of course incorrect. You have declared hs, cs, ps outside of the function as constants. In-fact they are functions of the alpha* and sigma* variables, so have to be evaluated during each call to equation: def equations(x, t): p = x[0] alphad = x[1] alphas = x[2] sigmad = x[3] sigmas = x[4] hs = (sigmas-(sigmas**(2)-k3*alphas*(2*sigmas-alphas))**(1\/2))\/k3 cs = (alphas-hs)\/2 ps = k4*(hs**2)\/cs dpdt = (ps-p)\/d + ft(t)\/Mu1 dalphaddt = (1\/vd)*(k2-w*(alphad-alphas)) dalphasdt = (1\/vs)*(w*(alphad-alphas)-k2) dsigmaddt = (1\/vd)*(k1-w*(sigmad-sigmas)) dsigmasdt = (1\/vs)*(w*(sigmad-sigmas)-k1-(ps-p)\/d*Mu2) return [dpdt, dalphaddt, dalphasdt, dsigmaddt, dsigmasdt] The result now matches the graph in the exercise... almost. You passed t1 as the horizontal axis variable to odeint. It only has 14 elements which is too few for a smooth output. Pass new_t instead: solve = ig.odeint(equations, [p0, alphad0, alphas0, sigmad0, sigmas0], new_t) The result now exactly matches the expected one! \nAPI:\nscipy.interpolate\n","label":[[689,700,"Mention"],[1911,1928,"API"]],"Comments":[]}
{"id":60584,"text":"ID:54207619\nPost:\nText: If you don't slice the last index (ie do image[:, :, 1]) then everything should work fine: \nCode: import numpy as np\nfrom PIL import Image\nimport scipy.misc as smc\n\nimage = np.array(Image.open(\"FLAIR-148.png\"))\ntest_image = image[:, :, 1]\nsmc.imsave('out.png', test_image)\n\nText: Basically, imsave does't know what to do with an array of shape (M, N, 1). However, it does know that it should save an array of shape (M, N) as a grayscale image. \nText: You may also need to convert the array to uint8 to ensure consistent results. Here's a complete minimal example: \nCode: import scipy.misc as smc\n\n# get the test image as an array\nimg = smc.face()\n\n# slice test image\nimg = img[:, :, 1]\n\n# convert to uint8\nimg = img.astype('uint8')\n\n# save\nsmc.imsave('test.png', img)\n\nText: Output: \nText: Caveat \nText: imsave is deprecated. It is suggested to use imageio.imwrite instead. \nAPI:\nscipy.misc.imsave\nscipy.misc.imsave\n","label":[[315,321,"Mention"],[828,834,"Mention"],[904,921,"API"],[922,939,"API"]],"Comments":[]}
{"id":60585,"text":"ID:54362367\nPost:\nText: sp.fftpack.fft returns both positive & negative frequencies, you only need the positive frequencies which in your code are stored in fourier_transform[1:len(signal)\/2]. See sp.fftpack.fft documentation here \nText: The mathematics of discrete Fourier transforms has to take into account negative frequencies due to the periodic nature of the sine & cosine functions, more information here. \nAPI:\nscipy.fftpack.fft\nscipy.fftpack.fft\n","label":[[24,38,"Mention"],[197,211,"Mention"],[419,436,"API"],[437,454,"API"]],"Comments":[]}
{"id":60586,"text":"ID:54378356\nPost:\nText: tldr; the pmf's of the geometric distribution are different in R and SciPy. \nText: First off, it's good to confirm that generally quantiles calculated in R and Python agree, for example in the case of the normal distribution from sp.stats import norm norm.ppf(0.99) #2.3263478740408408 qnorm(0.99) #[1] 2.326348 For the case of the geometric distribution, the quantile functions differ because the probability mass functions (pmf) are different. In R, the pmf of the geometric distribution is defined as p(1 - p)^k (see help(\"Geometric\")); in Python's SciPy module the geometric distribution is defined as p(1 - p)^(k-1) (see scipy.stats.geom). You can find a summary of key quantities for both definitions in the Wikipedia article. In essence, the ^k definition is \"used for modeling the number of failures until the first success\", where as the ^(k-1) definition relates to \"the probability that the kth trial (out of k trials) is the first success\". See also: Which geometric distribution to use? \nAPI:\nscipy.stats\n","label":[[254,262,"Mention"],[1030,1041,"API"]],"Comments":[]}
{"id":60587,"text":"ID:54616160\nPost:\nText: sp.spatial has many good functions for handling distance computations. \nText: Let's create an array pos of 1000 (x, y) points, similar to what you have in your dataframe. \nCode: import numpy as np\nfrom scipy.spatial import distance_matrix\n\nnum = 1000\npos = np.random.uniform(size=(num, 2))\n\n# Distance threshold\nd = 0.25\n\nText: From here we shall use the distance_matrix function to calculate pairwise distances. Then we use np.argwhere to find the indices of all the pairwise distances less than some threshold d. \nCode: pair_dist = distance_matrix(pos, pos)\n\nids = np.argwhere(pair_dist < d)\n\nText: ids now contains the \"ID's of all pairs of points that are within a cutoff distance \"d\" of each other\", as you desired. \nText: Shortcomings \nText: Of course, this method has the shortcoming that we always compute the distance between each point and itself (returning a distance of 0), which will always be less than our threshold d. However, we can exclude self-comparisons from our ids with the following fudge: \nCode: pair_dist[np.r_[:num], np.r_[:num]] = np.inf\nids = np.argwhere(pair_dist < d)\n\nText: Another shortcoming is that we compute the full symmetric pairwise distance matrix when we only really need the upper or lower triangular pairwise distance matrix. However, unless this computation really is a bottleneck in your code, I wouldn't worry too much about this. \nAPI:\nscipy.spatial\n","label":[[24,34,"Mention"],[1408,1421,"API"]],"Comments":[]}
{"id":60588,"text":"ID:54726284\nPost:\nText: Although this question is old, I happen to have just ran some tests and then stumbled upon this question. The answer is yes. Internally, scipy seems to converts the array to size M = 2*(N+1). Ideally, M = 2^i, for some integer i. Therefore, N should follow N = 2^i - 1. The following picture shows how timings scale with fft-size. Note that the orange line is much smoother, indicating no unexpected memory overhead. \nText: Green line: N = 2^i Blue line: N = 2^i + 1 Orange line: N = 2^i - 1 \nText: UPDATE After digging some more into the documentation of scipy.fftpack, I found that the above answer is only partly true. According to the documentation, \"SciPys FFTPACK has efficient functions for radix {2, 3, 4, 5}\". This means that instead of efficiently doing arrays of size M = 2^i, it can handle any M = 2^i * 3^j * 5^k (4 is not a prime). The optimum for syfp.dst (or dct) is then M - 1. Finding those numbers can be a little awkward, but luckily there's a function for that, too! \nText: Please note that the above graph is log-log scale, so speedups of 40 or so are not uncommon. Thus, choosing a fast size can make you calculations orders of magnitudes faster! (I found this out the hard way). \nAPI:\nscipy.fftpack.dst\n","label":[[887,895,"Mention"],[1234,1251,"API"]],"Comments":[]}
{"id":60589,"text":"ID:54968424\nPost:\nText: The sp.stats.sem function uses a default value of ddof=1 for the number-of-degrees-of-freedom parameter while numpy.std uses ddof=0 by default. This is also highlighted in the docs: \nText: The default value for ddof is different to the default (0) used by other ddof containing routines, such as np.std and np.nanstd. \nText: Consequently you get: \nCode: >>> print(sem(l))\n1.06458129484\n>>> print(sem(l, ddof=0))\n0.971825315808\n>>> print(sem(l, ddof=1))\n1.06458129484\n\n>>> print(np.std(l)\/np.sqrt(len(l)))\n0.971825315808\n>>> print(np.std(l, ddof=0)\/np.sqrt(len(l)))\n0.971825315808\n>>> print(np.std(l, ddof=1)\/np.sqrt(len(l)))\n1.06458129484\n\nAPI:\nscipy.stats.sem\n","label":[[28,40,"Mention"],[669,684,"API"]],"Comments":[]}
{"id":60590,"text":"ID:55220195\nPost:\nText: There is no need to access the index. sch.dendrogram provides a labels argument which you should use to supply your labels. \nCode: scipy.cluster.hierarchy.dendrogram(Z, labels=labels, ....)\n\nText: Complete code: \nCode: import numpy as np\nimport scipy.cluster.hierarchy as sc\nimport matplotlib.pyplot as plt\n\nZ = np.array([\n   [  2.        ,   9.        ,  20.12172148,   2.        ],\n   [  0.        ,   1.        ,  26.16772232,   2.        ],\n   [ 11.        ,  12.        ,  29.40258214,   2.        ],\n   [ 14.        ,  16.        ,  30.89332011,   3.        ],\n   [  3.        ,   7.        ,  33.70695832,   2.        ],\n   [  5.        ,  13.        ,  34.22180543,   2.        ],\n   [  4.        ,  15.        ,  35.52080322,   3.        ],\n   [ 17.        ,  21.        ,  45.3919152 ,   5.        ],\n   [  6.        ,  20.        ,  45.56339627,   3.        ],\n   [  8.        ,  23.        ,  66.42828305,   4.        ],\n   [ 10.        ,  22.        ,  87.52531145,   6.        ],\n   [ 18.        ,  24.        ,  93.78070161,   7.        ],\n   [ 19.        ,  26.        , 124.09967826,   9.        ],\n   [ 25.        ,  27.        , 160.11685636,  15.        ]])\n\nlabels = ['wood', 'stone', 'flora', 'liquid', 'food', 'metal', 'ceramic', \n          'sky', 'glass', 'paper', 'animal', 'skin', 'fabrics', 'gem', 'ground']\n\n# calculate full dendrogram\nplt.figure()\nplt.title('Hierarchical Clustering Dendrogram for signature data')\nplt.xlabel('sample index')\nplt.ylabel('distance')\nsc.dendrogram(\n    Z,\n    labels=labels,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.tight_layout()\nplt.show()\n\nAPI:\nscipy.cluster.hierarchy.dendrogram\n","label":[[62,76,"Mention"],[1707,1741,"API"]],"Comments":[]}
{"id":60591,"text":"ID:55250374\nPost:\nText: The quad function takes a function as its first input, but you were providing data from the gaussian evaluated at x: \nCode: import numpy as np\nimport scipy\n\nmu = 5\nsigma = 30\nlowerbound = 0.5\nupperbound = np.inf\n\n# generate Gaussian function\ndef gauss(x):\n    return scipy.stats.norm.pdf(x, mu, sigma)\n\n# integrate between bounds\nintegral = scipy.integrate.quad(gauss, lowerbound, upperbound)\nprint(integral)\n\nCode: (0.5596176923702426, 5.087725389583706e-10)\n\nText: If you want to integrate discrete data, sp.integrate.quad is not the tool for the job. Use scipy.integrate.simps instead. \nAPI:\nscipy.integrate.quad\n","label":[[531,548,"Mention"],[619,639,"API"]],"Comments":[]}
{"id":60592,"text":"ID:55326692\nPost:\nText: Numba simply is not a general-purpose library to speed code up. There is a class of problems that can be solved in a much faster way with numba (especially if you have loops over arrays, number crunching) but everything else is either (1) not supported or (2) only slightly faster or even a lot slower. \nText: [...] would it even speed up the code? \nText: SciPy is already a high-performance library so in most cases I would expect numba to perform worse (or rarely: slightly better). You might do some profiling to find out if the bottleneck is really in the code that you jitted, then you could get some improvements. But I suspect the bottleneck will be in the compiled code of SciPy and that compiled code is probably already heavily optimized (so it's really unlikely that you find an implementation that could \"only\" compete with that code). \nText: Is there a way to use jit with quad and curve_fit without manually deleting all try except structures from the scipy code? \nText: As you correctly assumed try and except is simply not supported by numba at this time. \nText: 2.6.1. Language 2.6.1.1. Constructs Numba strives to support as much of the Python language as possible, but some language features are not available inside Numba-compiled functions. The following Python language features are not currently supported: [...] Exception handling (try .. except, try .. finally) \nText: So the answer here is No. \nAPI:\nscipy.integrate.quad\n","label":[[910,914,"Mention"],[1450,1470,"API"]],"Comments":[]}
{"id":60593,"text":"ID:55426623\nPost:\nText: It looks like RectBivariateSpline will do the trick: \nCode: from scipy.interpolate import RectBivariateSpline\nimage = # as given\nindices = # as given\n\nspline = RectBivariateSpline(numpy.arange(M), numpy.arange(N), image)\n\ninterpolated = spline(indices[0], indices[1], grid=False)\n\nText: This gets you the interpolated values, but it doesn't give you nan where you need it. You can get that with where: \nCode: nans = numpy.zeros(interpolated.shape) + numpy.nan\nx_in_bounds = (0 <= indices[0]) & (indices[0] < M)\ny_in_bounds = (0 <= indices[1]) & (indices[1] < N)\nbounded = numpy.where(x_in_bounds & y_in_bounds, interpolated, nans)\n\nText: I tested this with a 2624x2624 image and 100,000 points in indices and all told it took under a second. \nAPI:\nscipy.interpolate.RectBivariateSpline\n","label":[[38,57,"Mention"],[772,809,"API"]],"Comments":[]}
{"id":60594,"text":"ID:55612821\nPost:\nText: One answer for both the questions. \nText: fsolve does not support constraints. You may provide initial estimate as positive values , but that does not guarantee positive roots . However, you can reformulate your problem as optimization problem and minimize the cost function imposing constraints using any optimization function such as scipy.optimize.minimize. \nText: As a minimal example , if you want to find the positive root of the equation x*x -4 , you could do as below. \nCode: scipy.optimize.minimize(lambda x:(x*x-4)**2,x0= [5], bounds =((0,None),))\n\nText: The bounds parameter which takes (min,max) pair can be used to impose the positive constraint on the root. \nText: output : \nCode:  fun: array([1.66882981e-17])\n hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n      jac: array([1.27318954e-07])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n     nfev: 20\n      nit: 9\n   status: 0\n  success: True\n        x: array([2.])\n\nText: Going by this, your code can be modified as below. Just add the bounds , change the function return statement , and change the fsolve to minimize with the bounds. \nCode: import numpy as np\nimport scipy\nfrom scipy.optimize import fsolve\nimport time\n#\n# \"B\" is the energy potentials of the species [C_gr , CO , CO2 , H2 , CH4 , H2O , N2* , SiO2* , H2S]\nB = [-11.0, -309.3632404425132, -613.3667287153355, -135.61840658777166, -269.52018727412405, -434.67499662354476, -193.0773646004259, -980.0, -230.02942769438977]\n# \"a_atoms\" is the number of atoms in the reactants [C, H, O, N*, S, SiO2*]  \n# * Elements that doesn't react. '\na_atoms = [4.27311296e-02, 8.10688756e-02, 6.17738749e-02, 1.32864225e-01, 3.18931655e-05, 3.74477901e-04]\nP_zero = 100.0 # Standard energy pressure\nP_eq = 95.0 # Reaction pressure\n# Standard temperature 298.15K, reaction temperature 940K.\n#\nstart_time = time.time()\ndef GibbsEq(z):\n# Lambda's exponentials:\n    Y1 = z[0]\n    Y2 = z[1] \n    Y3 = z[2]\n    Y4 = z[3] \n    Y5 = z[4] \n    Y6 = z[5]\n# Number of moles in each phase:\n    N1 = z[6]\n    N2 = z[7]\n    N3 = z[8]\n\n    bounds =((0,None),)*9\n# Equations of energy conservation and mass conservation:\n    F = np.zeros(9) \n    F[0] = (P_zero\/P_eq) * N1 * ((B[1] * (Y1 * Y3) + B[2] * (Y1 * Y3**2) + B[4] * (Y1 * Y2**2)) + N2 * (B[0] * Y1)) - a_atoms[0]\n    F[1] = (P_zero\/P_eq) * N1 * (2 * B[3] * Y2**2 + 4 * B[4] * (Y1 * Y2**4) + 2 * B[5] * ((Y2**2) * Y3) + 2 * B[8] * ((Y2**2) * Y5)) - a_atoms[1]\n    F[2] = (P_zero\/P_eq) * N1 * (B[1] * (Y1 * Y3) + 2 * B[2] * (Y1 * Y3**2) + B[5] * ((Y2**2) * Y3)) - a_atoms[2]\n    F[3] = (P_zero\/P_eq) * N1 * (2 * B[6]**2) - a_atoms[3]\n    F[4] = (P_zero\/P_eq) * N1 * (B[8] * ((Y2**2) * Y5)) - a_atoms[4]\n    F[5] = N3 * (B[7] * Y5)  - a_atoms[5]\n# \n    F[6] = (P_zero\/P_eq) * (B[1] * (Y1 * Y3) + B[2] * (Y1 * Y3**2) + B[3] * Y2**2 + B[4] * (Y1 * Y2**4) + B[5] * ((Y2**2) * Y3) + B[6] * Y4 + B[8] * Y5) - 1 \n    F[7] = B[0] * Y1 - 1 \n    F[8] = B[7] * Y6 - 1\n    return (np.sum(F)**2)\n#\nzGuess = np.ones(9)\nz = scipy.optimize.minimize(GibbsEq, zGuess , bounds=bounds)\nend_time = time.time()\ntime_solution = (end_time - start_time)\nprint('Solving time: {} s'.format(time_solution))\n#\n\nprint(z.x)\n\nprint(N_T)\nfor n in N_T:\n    if n < 0:\n        print('Error: there is negative values for mass in the solution!')\n        break \n\nText: output: \nCode: Solving time: 0.012451648712158203 s\n[1.47559173 2.09905553 1.71722403 1.01828262 1.17529548 1.08815712\n 1.00294916 1.00104157 1.08815763]\n\nAPI:\nscipy.optimize.minimize\n","label":[[1126,1134,"Mention"],[3497,3520,"API"]],"Comments":[]}
{"id":60595,"text":"ID:55620490\nPost:\nText: There are two similar functions that can help you: arrgrelmin and scipy.signal.argrelmax. There are search for local min\/max in discrete arrays. You should pass your array and neighbours search radius as order. Your problem can be solved by their combination: \nCode: >>> a = np.asarray([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 , 1, 1, 1, 1], int)\n\n>>> signal.argrelmin(a, order=3)\n(array([4], dtype=int32),)\n\n>>> signal.argrelmax(a, order=3)\n(array([15], dtype=int32),)\n\nText: Then you can just replace these elements. \nAPI:\nscipy.signal.argrelmin\n","label":[[75,85,"Mention"],[565,587,"API"]],"Comments":[]}
{"id":60596,"text":"ID:55645803\nPost:\nText: Here's an example of how to get a speedup using Ray (a library for parallel and distributed Python). You can run the code below after doing pip install ray (on Linux or MacOS). \nText: Running the serial version of the computation below (e.g., doing scipy.sparse.linalg.cg(K, F, tol=1e-11, maxiter=100) 20 times) takes 33 seconds on my laptop. Timing the code below for launching the 20 tasks and getting the results takes 8.7 seconds. My laptop has 4 physical cores, so this is almost a 4x speedup. \nText: I changed your code a lot, but I think I preserved the essence of it. \nCode: import numpy as np\nimport ray\nimport scipy.sparse\nimport scipy.sparse.linalg\n\n# Consider passing in 'num_cpus=psutil.cpu_count(logical=True)'.\nray.init()\n\nnum_elements = 10**7\ndim = 10**4\n\ndata = np.random.normal(size=num_elements)\nrow_indices = np.random.randint(0, dim, size=num_elements)\ncol_indices = np.random.randint(0, dim, size=num_elements)\n\nK = scipy.sparse.csc_matrix((data, (row_indices, col_indices)))\n\n@ray.remote\ndef solve_system(K, F):\n    # Solve the system.\n    return scipy.sparse.linalg.cg(K, F, tol=1e-11, maxiter=100)[0]\n\n# Store the array in shared memory first. This is optional. That is, you could\n# directly pass in K, however, this should speed it up because this way it only\n# needs to serialize K once. On the other hand, if you use a different value of\n# \"K\" for each call to \"solve_system\", then this doesn't help.\nK_id = ray.put(K)\n\n# Time the code below!\n\nresult_ids = []\nfor _ in range(20):\n    F = np.random.normal(size=dim)\n    result_ids.append(solve_system.remote(K_id, F))\n\n# Run a bunch of tasks in parallel. Ray will schedule one per core.\nresults = ray.get(result_ids)\n\nText: The call to ray.init() starts the Ray worker processes. The call to solve_system.remote submits the tasks to the workers. Ray will schedule one per core by default, though you can specify that a particular task requires more resources (or fewer resources) via @ray.remote(num_cpus=2). You can also specify GPU resources and other custom resources. \nText: The call to solve_system.remote immediately returns an ID representing the eventual output of the computation, and the call to ray.get takes the IDs and retrieves the actual results of the computation (so ray.get will wait until the tasks finish executing). \nText: Some notes \nText: On my laptop, sps.linalg.cg seems to limit itself to a single core, but if it doesn't, then you should consider pinning each worker to a specific core to avoid contention between worker processes (you can do this on Linux by doing psutil.Process().cpu_affinity([i]) where i is the index of the core to bind to. If the tasks all take variable amounts of time, make sure that you aren't just waiting for one really slow task. You can check this by running ray timeline from the command line and visualizing the result in chrome:\/\/tracing (in the Chrome web browser). Ray uses a shared memory object store to avoid having to serialize and deserialize the K matrix once per worker. This is an important performance optimization (though it doesn't matter if the tasks take a really long time). This helps primarily with objects that contain large numpy arrays. It doesn't help with arbitrary Python objects. This is enabled by using the Apache Arrow data layout. You can read more in this blog post. \nText: You can see more in the Ray documentation. Note that I'm one of the Ray developers. \nAPI:\nscipy.sparse.linalg.cg\n","label":[[2377,2390,"Mention"],[3455,3477,"API"]],"Comments":[]}
{"id":60597,"text":"ID:55684821\nPost:\nText: Tl;dr: If I write it with the ouput given by the SciPy documentation: Sxx = Zxx ** 2 \nText: Explanation: Spectrogram and Short Time Fourier Transform are two different object, yet they are really close together. \nText: The short-time Fourier transform (STFT), is a Fourier-related transform used to determine the sinusoidal frequency and phase content of local sections of a signal as it changes over time. In practice, the procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment. This reveals the Fourier spectrum on each shorter segment. One then usually plots the changing spectra as a function of time. Wikipedia \nText: On the other hand, \nText: A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. Wikipedia \nText: The spectrogram basically cuts your signal in small windows, and display a range of colors showing the intensity of this or that specific frequency. Exactly as the STFT. In fact it's using the STFT. \nText: Now, for the difference, by definition, the spectrogram is squared magnitude of the short-time Fourier transform (STFT) of the signal s(t): \nText: spectrogram(t, w) = |STFT(t, w)|^2 \nText: The example shown at the bottom of the sig.stft page shows: \nCode: >>> plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=amp)\n\nText: It's working and you can see a color scale. But it's a linear one, because of the abs operation. \nText: In reality, to get the real spectrogram, one should write: \nCode: >>> plt.pcolormesh(t, f, Zxx ** 2, vmin=0, vmax=amp)\n\nAPI:\nscipy.signal.stft\n","label":[[1353,1361,"Mention"],[1673,1690,"API"]],"Comments":[]}
{"id":60598,"text":"ID:55799196\nPost:\nText: I think the problem is that you need to make sure you're consistent about the number of dimensions, and the 'domains' of those dimensions. You won't get good results interpolating into places you have not sampled. I think the errors you're getting are related to trying to compute things in these places. \nText: Here's how to make something like the example in the griddta docs work for a 7-dimensional example. I'm using a much simpler function which just sums the 'features' in the points data: \nCode: import numpy as np\n\ndef func(data):\n    return np.sum(data, axis=1)\n\ngrid = np.mgrid[0:1:5j, 0:1:5j, 0:1:5j, 0:1:5j, 0:1:5j, 0:1:5j, 0:1:5j]\n\npoints = np.random.rand(100, 7)\nvalues = func(points)\n\nText: Notice that the grid covers the entire range of the coordinates points. That is, since each column in points has values in the range 0 to 1, I should make sure I'm making a grid over those same coordinates. \nCode: from scipy.interpolate import griddata\ngrid_z = griddata(points, values, tuple(grid), method='linear')\n\nText: Now I have this: \nCode: >>> grid_z[2, 2, 2, 2, 2, :, :]\narray([[ nan,  nan,  nan,  nan,  nan],\n       [ nan, 3.  , 3.25, 3.5 ,  nan],\n       [ nan, 3.25, 3.5 , 3.75,  nan],\n       [ nan, 3.5 , 3.75, 4.  ,  nan],\n       [ nan,  nan,  nan,  nan,  nan]])\n\nText: Notice that there are a lot of NaNs. If you use nearest as the method, you'll always get a solution, but of course linear interpolation needs two things to interpolate between, so the 'edges' of the hypercube are not valid (and 7-D space has a lot of edge!). \nAPI:\nscipy.interpolate.griddata\n","label":[[389,396,"Mention"],[1579,1605,"API"]],"Comments":[]}
{"id":60599,"text":"ID:55987029\nPost:\nText: Your code had just some confusing variables so I just cleared that out and simplified some lines, now the minimization works correctly. However, the question now is: if the results are correct? and do they make sense? and that is for you to judge: \nCode: import numpy as np \nfrom scipy.optimize import minimize\n\ndef f(w, cov_matrix):\n    return (np.matrix(w) * cov_matrix * np.matrix(w).T)[0,0]\n\ncov_matrix = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]])\np    = [1, 2, 3]\nw0   = [(1\/len(p))  for e in p]\nbnds = tuple((0,1)  for e in w0)\ncons = ({'type': 'eq', 'fun': lambda w:  np.sum(w)-1.0})\n\nres  = minimize(f, w0, \n                args        = cov_matrix, \n                method      = 'SLSQP',\n                constraints = cons, \n                bounds      = bnds)\nweights = res.x\nprint(res)\nprint(weights)\n\nText: Update: \nText: Based on your comments, it seems to me that -maybe- your function has has multiple minima and that's why opt.minimize gets trapped in there. I suggest sp.optimize.basinhopping as an alternative, this would use a random step to go over most of the minima of your function and it will still be fast. Here is the code: \nCode: import numpy as np \nfrom scipy.optimize import basinhopping\n\n\nclass MyBounds(object):\n     def __init__(self, xmax=[1,1], xmin=[0,0] ):\n         self.xmax = np.array(xmax)\n         self.xmin = np.array(xmin)\n\n     def __call__(self, **kwargs):\n         x = kwargs[\"x_new\"]\n         tmax = bool(np.all(x <= self.xmax))\n         tmin = bool(np.all(x >= self.xmin))\n         return tmax and tmin\n\ndef f(w):\n    global cov_matrix\n    return (np.matrix(w) * cov_matrix * np.matrix(w).T)[0,0]\n\ncov_matrix = np.array([[0.000244181, 0.000198035],\n                       [0.000198035, 0.000545958]])\n\np    = ['ABEV3', 'BBDC4']\nw0   = [(1\/len(p))  for e in p]\nbnds = tuple((0,1)  for e in w0)\ncons = ({'type': 'eq', 'fun': lambda w:  np.sum(w)-1.0})\n\nbnds = MyBounds()\nminimizer_kwargs = {\"method\":\"SLSQP\", \"constraints\": cons}\nres  = basinhopping(f, w0, \n                    accept_test  = bnds)\nweights = res.x\nprint(res)\nprint(\"weights: \", weights)\n\nText: Output: \nCode:                         fun: 2.3907094432990195e-09\n lowest_optimization_result:       fun: 2.3907094432990195e-09\n hess_inv: array([[ 2699.43934183, -1184.79396719],\n       [-1184.79396719,  1210.50404805]])\n      jac: array([1.34548553e-06, 2.00122166e-06])\n  message: 'Optimization terminated successfully.'\n     nfev: 60\n      nit: 6\n     njev: 15\n   status: 0\n  success: True\n        x: array([0.00179748, 0.00118076])\n                    message: ['requested number of basinhopping iterations completed successfully']\n      minimization_failures: 0\n                       nfev: 6104\n                        nit: 100\n                       njev: 1526\n                          x: array([0.00179748, 0.00118076])\nweights:  [0.00179748 0.00118076]\n\nAPI:\nscipy.optimize.minimize\nscipy.optimize.basinhopping\n","label":[[1011,1023,"Mention"],[1057,1081,"Mention"],[2950,2973,"API"],[2974,3001,"API"]],"Comments":[]}
{"id":60600,"text":"ID:56097730\nPost:\nText: If you have an arbitrary cloud of (X, Y, Z) points and you want to interpolate the z-coordinate of some (x, y) point, you have a number of different options. The simplest is probably to just use sp.interpolate.interp2d to get the z-value: \nCode: f = interp2d(X.T, Y.T, Z.T)\nz = f(x, y)\n\nText: Since the grid you have appears to be regular, you may be better off using scipy.interpolate.RectBivariateSpline, which has a very similar interface, but is specifically made for regular grids: \nCode: f = RectBivariateSpline(X.T, Y.T, Z.T)\nz = f(x, y)\n\nText: Since you have a regular meshgrid, you can also do \nCode: f = RectBivariateSpline(X[0, :], Y[:, 0], Z.T)\nz = f(x, y)\n\nText: Notice that the dimensions are flipped between the plotting arrays and the interpolation arrays. Plotting treats axis 0 as rows, i.e. Y, while the interpolation functions treat axis 0 as X. Rather than transposing, you could also switch the X and Y inputs, leaving Z intact for a similar end result, e.g.: \nCode: f = RectBivariateSpline(Y, X, Z)\nz = f(y, x)\n\nText: Alternatively, you could change all your plotting code to swap the inputs as well, but that would be too much work at this point. Whatever you do, pick an approach and stick with it. As long as you do it consistently, they should all work. \nText: If you use one of the scipy approaches (recommended), keep the object f around to interpolate any further points you might want. \nText: If you want a more manual approach, you can do something like find the three closest (X, Y, Z) points to (x, y), and find the value of the plane between them at (x, y). For example: \nCode: def interp_point(x, y, X, Y, Z):\n    \"\"\"\n    x, y: scalar coordinates to interpolate at\n    X, Y, Z: arrays of coordinates corresponding to function\n    \"\"\"\n    X = X.ravel()\n    Y = Y.ravel()\n    Z = Z.ravel()\n\n    # distances from x, y to all X, Y points\n    dist = np.hypot(X - x, Y - y)\n    # indices of the nearest points\n    nearest3 = np.argpartition(dist, 2)[:3]\n    # extract the coordinates\n    points = np.stack((X[nearest3], Y[nearest3], Z[nearest3]))\n    # compute 2 vectors in the plane\n    vecs = np.diff(points, axis=0)\n    # compute normal to plane\n    plane = np.cross(vecs[0], vecs[1])\n    # rhs of plane equation\n    d = np.dot(plane, points [:, 0])\n    # The final result:\n    z = (d - np.dot(plane[:2], [x, y])) \/ plane[-1]\n    return z\n\nprint(interp_point(x, y, X.T, Y.T, Z.T))\n\nText: Since your data is on a regular grid, it might be easier to do something like bilinear interpolation on the quad surrounding (x, y): \nCode: def interp_grid(x, y, X, Y, Z):\n    \"\"\"\n    x, y: scalar coordinates to interpolate at\n    X, Y, Z: arrays of coordinates corresponding to function\n    \"\"\"\n    X, Y = X[:, 0], Y[0, :]\n\n    # find matching element\n    r, c = np.searchsorted(Y, y), np.searchsorted(X, x)\n    if r == 0: r += 1\n    if c == 0: c += 1\n    # interpolate\n    z = (Z[r - 1, c - 1] * (X[c] - x) * (Y[r] - y) +\n         Z[r - 1, c] * (x - X[c - 1]) * (Y[r] - y) +\n         Z[r, c - 1] * (X[c] - x) * (y - Y[r - 1]) +\n         Z[r, c] * (x - X[c - 1]) * (y - Y[r - 1])\n    ) \/ ((X[c] - X[c - 1]) * (Y[r] - Y[r - 1]))\n    return z\n\nprint(interpolate_grid(x, y, X.T, Y.T, Z.T))\n\nAPI:\nscipy.interpolate.interp2d\n","label":[[219,242,"Mention"],[3239,3265,"API"]],"Comments":[]}
{"id":60601,"text":"ID:56187054\nPost:\nText: When you save and reload your sparse array you have created an array with one entry; an object, being your sparse array. So A has nothing at [1,1]. You should use save_npz instead. \nText: For example: \nCode: import scipy.sparse as sps\nimport numpy as np\n\nA = sps.csr_matrix((10,10))\nA\n<10x10 sparse matrix of type '<class 'numpy.float64'>'\n    with 0 stored elements in Compressed Sparse Row format>\nnp.save('test_matrix.dat', A)\nB = np.load('test_matrix.dat.npy', allow_pickle=True)\nB\narray(<10x10 sparse matrix of type '<class 'numpy.float64'>'\n    with 0 stored elements in Compressed Sparse Row format>, dtype=object)\nB[1,1]\nIndexError                                Traceback (most recent call last)\n<ipython-input-101-969f8bd5206a> in <module>\n----> 1 B[1,1]\n\nIndexError: too many indices for array\nsps.save_npz('sparse_dat')\nC = sps.load_npz('sparse_dat.npz')\nC\n<10x10 sparse matrix of type '<class 'numpy.float64'>'\n    with 0 stored elements in Compressed Sparse Row format>\nC[1,1]\n0.0\n\nText: Mind you you can still retrieve A from B like so: \nCode: D = B.tolist()\nD\n<10x10 sparse matrix of type '<class 'numpy.float64'>'\n    with 0 stored elements in Compressed Sparse Row format>\nD[1,1]\n0.0\n\nAPI:\nscipy.sparse.save_npz\n","label":[[187,195,"Mention"],[1232,1253,"API"]],"Comments":[]}
{"id":60602,"text":"ID:56189510\nPost:\nText: It's an old question, but with over 1000 views, perhaps there are still people showing up here with similar questions. \nText: sig.remez computes the coefficients of a finite impulse response (FIR) filter. The output is just one set of coefficients. You say you expected two sets of coefficients, which means you expected remez to design an infinite impulse response (IIR) filter, but remez does not do that. \nText: You can apply the filter with a convolution function such as numpy.convolve or scipy.signal.convolve. You can also use scipy.signal.lfilter. lfilter accepts the coefficients of an IIR filter. The first two arguments of lfilter, b and a, are the coefficients of the numerator and denominator of the IIR filter. You can pass an FIR filter to lfilter by setting b to the coefficients returned by remez, and setting a=1. (In other words, an \"IIR\" filter with a trivial denominator is, in fact, an FIR filter.) \nAPI:\nscipy.signal.remez\n","label":[[150,159,"Mention"],[951,969,"API"]],"Comments":[]}
{"id":60603,"text":"ID:56310765\nPost:\nText: It looks like you have a series of questions that come together on this issue. I'll settle it here. \nText: You calculate entropy in the following form of entropy according to your code: \nText: scipy.stats.entropy(pk, qk=None, base=None) Calculate the entropy of a distribution for given probability values. If only probabilities pk are given, the entropy is calculated as S = -sum(pk * log(pk), axis=0). \nText: Tensorflow does not provide a direct API to calculate entropy on each row of the tensor. What we need to do is to implement the above formula. \nCode: import tensorflow as tf\nimport pandas as pd\nfrom scipy.stats import entropy\n\na = [1.1,2.2,3.3,4.4,2.2,3.3]\nres = entropy(pd.value_counts(a))\n\n_, _, count = tf.unique_with_counts(tf.constant(a))\n# [1 2 2 1]\nprob = count \/ tf.reduce_sum(count)\n# [0.16666667 0.33333333 0.33333333 0.16666667]\ntf_res = -tf.reduce_sum(prob * tf.log(prob))\n\nwith tf.Session() as sess:\n    print('scipy version: \\n',res)\n    print('tensorflow version: \\n',sess.run(tf_res))\n\nscipy version: \n 1.329661348854758\ntensorflow version: \n 1.3296613488547582\n\nText: Then we need to define a function and achieve for loop through tf.map_fn in your custom layer according to above code. \nCode: def rev_entropy(self, x, beta,batch):\n    def row_entropy(row):\n        _, _, count = tf.unique_with_counts(row)\n        prob = count \/ tf.reduce_sum(count)\n        return -tf.reduce_sum(prob * tf.log(prob))\n\n    value_ranges = [-10.0, 100.0]\n    nbins = 50\n    new_f_w_t = tf.histogram_fixed_width_bins(x, value_ranges, nbins)\n    rev = tf.map_fn(row_entropy, new_f_w_t,dtype=tf.float32)\n\n    new_f_w_t = x * 1\/(1+rev)*beta\n\n    return new_f_w_t\n\nText: Notes that the hidden layer will not produce a gradient that cannot propagate backwards since entropy is calculated on the basis of statistical probabilistic values. Maybe you need to rethink your hidden layer structure. \nAPI:\nscipy.stats.entropy\n","label":[[178,185,"Mention"],[1927,1946,"API"]],"Comments":[]}
{"id":60604,"text":"ID:56354281\nPost:\nText: Get the windowed summations and divide by the valid members in each window. We can use convolve2d to get both and hence have a solution like so - \nCode: from scipy.signal import convolve2d\n\ndef windowed_average(a, kernel_size, mode='same'):\n    k = np.ones((kernel_size,kernel_size),dtype=int)\n    window_sum = convolve2d(a,k,mode)\n    window_count = convolve2d(np.ones(a.shape, dtype=bool),k,mode)\n    return window_sum\/window_count\n\nText: Alternative #1 \nText: Alternatively, if you want to make use of uniform_filter to get the windowed summations, we can do so and that might be more efficient as well, like so - \nCode: from scipy.ndimage import uniform_filter\n\nn = kernel_size**2\nwindow_sum = uniform_filter(a, kernel_size, mode='constant', cval=0.0)*n\n\nText: Sample runs - \nCode: In [54]: a\nOut[54]: \narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\n\nIn [55]: windowed_average(a, kernel_size=3)\nOut[55]: \narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\n\nIn [56]: b\nOut[56]: \narray([[1., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\nIn [57]: windowed_average(b, kernel_size=3)\nOut[57]: \narray([[0.25      , 0.16666667, 0.        ],\n       [0.16666667, 0.11111111, 0.        ],\n       [0.        , 0.        , 0.        ]])\n\nAPI:\nscipy.signal.convolve2d\n","label":[[111,121,"Mention"],[1403,1426,"API"]],"Comments":[]}
{"id":60605,"text":"ID:56446053\nPost:\nText: smp.imsave has been deprecated in newer Scipy versions. \nText: Change your code to: \nCode: import imageio\nimageio.imwrite('filename.jpg', array)\n\nAPI:\nscipy.misc.imsave\n","label":[[24,34,"Mention"],[175,192,"API"]],"Comments":[]}
{"id":60606,"text":"ID:56462285\nPost:\nText: If we take a look at the scipy.optimization documentation we can see that minimize is listed under local optimization. The main problem is that your problem is non-convex and thus minimize cannot guarantee the proper convergence. As it's also very much non-differentiable, many algorithms won't be suited at all. \nText: scipy.optimize does provide some global optimization algorithms though that can be found on the documentation page under global optimization, namely basinhopping, brute, and differential_evolution. Look at this answer for some short explanation. \nText: Basically you can try brute first, just to see any systematic problems. It's basically a brute force solution and will be slow, but find your minimum. The more sophisticated method would be using differential_evolution. Since your function isn't really smooth, basinhopping might not work, but it's still worth a shot and would probably converge the fastest. \nAPI:\nscipy.optimize.minimize\nscipy.optimize.minimize\n","label":[[98,106,"Mention"],[204,212,"Mention"],[962,985,"API"],[986,1009,"API"]],"Comments":[]}
{"id":60607,"text":"ID:56622513\nPost:\nText: I don't think you necessarily need scipy.optimize.minimize. Since you are minimizing a scalar, you can use minimize_scalar (docs). This can be done like the following: \nCode: from scipy.optimize import minimize_scalar\nimport numpy as np\n\n\n# define vecs\nbasic_vec  = np.array([123, 342, 235, 123,  56, 345, 234, 123, 345,  54, 234]).reshape(11, 1)\nfactor_vec = np.array([234, 345, 453, 345, 456, 457,  23,  45,  56, 567,   5]).reshape(11, 1)\n# define sums\nBaseSum    = np.sum(basic_vec)\nFacSum     = np.sum(factor_vec)\n# define \nf      = lambda x, FacSum: np.abs(BaseSum - FacSum * x)\nresult = minimize_scalar(f, args   = (FacSum,), bounds = (0, FacSum), method = 'bounded')\n# prints\nprint(\"x                    = \", result.x)\nprint(\"BaseSum - FacSum * x = \", f(result.x, FacSum))\n\nText: Output: \nCode: x                    =  0.741461642947231\nBaseSum - FacSum * x =  0.004465840431748802\n\nText: Moreover, I am not even sure why do you even need to use a minimization when you can simply do: \nCode: x = BaseSum\/FacSum\n\nAPI:\nscipy.optimize.minimize_scalar\n","label":[[131,146,"Mention"],[1048,1078,"API"]],"Comments":[]}
{"id":60608,"text":"ID:56630456\nPost:\nText: The documentation for minimize mentions the args parameter: \nText: args : tuple, optional Extra arguments passed to the objective function and its derivatives (fun, jac and hess functions). \nText: You can use it as follows: \nCode: import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import Bounds\n\nbounds = Bounds([2,10],[5,20])\nx0 = np.array([2.5,15])\n\ndef objective(x, *args):\n    a, b, c = args  # or just use args[0], args[1], args[2]\n    x0 = x[0]\n    x1 = x[1]\n    return a*x0 + b*x0*x1 - c*x1*x1\n\n# Pass in a tuple with the wanted arguments a, b, c\nres = minimize(objective, x0, args=(1,-2,3), method='trust-constr',options={'verbose': 1}, bounds=bounds)\n\nAPI:\nscipy.optimize.minimize\n","label":[[46,54,"Mention"],[715,738,"API"]],"Comments":[]}
{"id":60609,"text":"ID:56633692\nPost:\nText: If x has dtype np.int16, then x1 has dtype np.float64. It appears that wrgite attempts to write 64 bit floating values to the file, even though the documentation only mentions 32 bit floating point formats. You can work around the problem by converting x1 to int16, or by normalizing the values in x1 to the range [-1, 1] (or [-0.5, 0.5], or to whatever range you want in [-1, 1]). That is, you can use \nCode: wavfile.write('test_output1.wav', fs, np.round(x1).astype(x.dtype))  # If x has an integer dtype\n\nText: or \nCode: wavfile.write('test_output1.wav', fs, (x1\/2**15).astype(np.float32))\n\nAPI:\nscipy.io.wavfile.write\n","label":[[95,101,"Mention"],[623,645,"API"]],"Comments":[]}
{"id":60610,"text":"ID:56788000\nPost:\nText: make_lsq_spline from interpolate creates a least-squares spline fit with fixed predefined knots. \nAPI:\nscipy.interpolate\n","label":[[45,56,"Mention"],[127,144,"API"]],"Comments":[]}
{"id":60611,"text":"ID:56789548\nPost:\nText: The default interp1d uses linear interpolation, i.e., it simply computes a line between two points. A weighted interpolation does not make much sense mathematically in such scenario - there is only one way in euclidean space to make a straight line between two points. \nText: Depending on your goal, you can look into other methods of interpolation, e.g., B-splines. Then you can use scipy's splrep and set the w argument: \nText: w - Strictly positive rank-1 array of weights the same length as x and y. The weights are used in computing the weighted least-squares spline fit. If the errors in the y values have standard-deviation given by the vector d, then w should be 1\/d. Default is ones(len(x)). \nAPI:\nscipy.interpolate.splrep\n","label":[[416,422,"Mention"],[731,755,"API"]],"Comments":[]}
{"id":60612,"text":"ID:56947411\nPost:\nText: You are relying on the Toeplitz constructor to produce a symmetric matrix, so that the entries below the diagonal are the same as above the diagonal. However, the documentation for scipy.linalg.toeplitz(c, r=None) says not \"transpose\", but \nText: *\"If r is not given, r == conjugate(c) is assumed.\" \nText: so that the resulting matrix is self-adjoint. In this case this means that the entries above the diagonal have their sign switched. \nText: It makes no sense to first construct a dense matrix and then extract a sparse representation. Construct it as sparse tridiagonal matrix from the start, using diags \nCode: A = sparse.diags([ (N-3)*[-alpha], (N-2)*[1+2*alpha], (N-3)*[-alpha]], [-1,0,1], format=\"csc\");\nB = sparse.diags([ (N-3)*[ alpha], (N-2)*[1-2*alpha], (N-3)*[ alpha]], [-1,0,1], format=\"csc\");\n\nAPI:\nscipy.sparse.diags\n","label":[[627,632,"Mention"],[838,856,"API"]],"Comments":[]}
{"id":60613,"text":"ID:58459322\nPost:\nText: I ran into this same problem and found this question while searching for a solution. I ended up finding a solution that uses GridSearchCV and am leaving this answer for anyone else who searches and finds this question. \nText: The cv parameter of the GridSearchCV class can take as its input an iterable yielding (train, test) splits as arrays of indices. You can generate splits that use only data from the positive class in the training folds, and the remaining data in the positive class plus all data in the negative class in the testing folds. \nText: You can use KFold to make the splits \nCode: from sklearn.model_selection import KFold\n\nText: Suppose Xpos is an nXp numpy array of data for the positive class for the OneClassSVM and Xneg is an mXp array of data for known anomalous examples. \nText: You can first generate splits for Xpos using \nCode: splits = KFold(n_splits=5).split(Xpos)\n\nText: This will construct a generator of tuples of the form (train, test) where train is a numpy array of int containing indices for the examples in a training fold and test is a numpy array containing indices for examples in a test fold. \nText: You can then combine Xpos and Xneg into a single dataset using \nCode: X = np.concatenate([Xpos, Xneg], axis=0)\n\nText: The OneClassSVM will make prediction 1.0 for examples it thinks are in the positive class and prediction -1.0 for examples it thinks are anomalous. We can make labels for our data using \nCode: y = np.concatenate([np.repeat(1.0, len(Xpos)), np.repeat(-1.0, len(Xneg))])\n\nText: We can then make a new generator of (train, test) splits with indices for the anomalous examples included in the test folds. \nCode: n, m = len(Xpos), len(Xneg)\n\nsplits = ((train, np.concatenate([test, np.arange(n, n + m)], axis=0)\n          for train, test in splits)\n\nText: You can then pass these splits to GridSearchCV using the data X, y and whatever scoring method and other parameters you wish. \nCode: grid_search = GridSearchCV(estimator, param_grid, cv=splits, scoring=...)\n\nText: Edit: I hadnt noticed that this approach was suggested in the comments of the other answer by Vivek Kumar, and that the OP had rejected it because they didnt believe it would work with their method of choosing the best parameters. I still prefer the approach Ive described because GridSearchCV will automatically handle multiprocessing and provides exception handling and informative warning and error messages. \nText: It is also flexible in the choice of scoring method. You can use multiple scoring methods by passing a dictionary mapping strings to scoring callables and even define custom scoring callables. This is described in the Scikit-learn documentation here. A bespoke method of choosing the best parameters could likely be implemented with a custom scoring function. All of the metrics used by the OP could be included using the dictionary approach described in the documentation. \nText: You can find a real world example here. I'll make a note to change the link when this gets merged into master. \nAPI:\nsklearn.model_selection.KFold\n","label":[[591,596,"Mention"],[3069,3098,"API"]],"Comments":[]}
{"id":60614,"text":"ID:58682437\nPost:\nText: Its a classic problem of Imbalanced data. A couple of simple things that you can try is to upsample the minority class or downsample the majority class and try again. A better way would be to change your algorithm and use a SVC or a Neural Network that could weight the loss for the minority cases highly. \nText: For example, the sklearn classifier has class_weights = 'balanced' parameter that will help in this. It will basically weight the cost for minority classes with the ratio of those guys in the input data. \nText: The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as \nAPI:\nsklearn.svm.SVC\n","label":[[362,372,"Mention"],[692,707,"API"]],"Comments":[]}
{"id":60615,"text":"ID:58867038\nPost:\nText: Indeed, there is no solution. As the number of training vectors increase so does the training time. \nText: Reference: https:\/\/scikit-learn.org\/stable\/modules\/svm.html#complexity \nText: Just for the record, SVMs are great for these problems (see here: https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_digits_classification.html) but when the dataset is huge they become slow. \nText: EDIT 1: In sklearn website there is this: \nText: The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using sklearn.linear_model.LinearSVC or sk.linear_model.SGDClassifier instead, possibly after a Nystrdoem transformer. \nAPI:\nsklearn.linear_model.SGDClassifier\nsklearn.kernel_approximation.Nystroem\n","label":[[709,738,"Mention"],[765,774,"Mention"],[794,828,"API"],[829,866,"API"]],"Comments":[]}
{"id":60616,"text":"ID:58937507\nPost:\nText: I think maybe you need to use pre.StandardScaler instead of normalize. \nText: When I use your method on 5000 random points from an image (uint8 hence the shift to 1 I guess), I get: \nText: When I standardize using StandardScaler on an image, everything else the same apart from X_plot = np.linspace(-3.1, 3.1, 1000), I get: \nText: This matches (more or less because of random sampling) the result from seaborn, which I trust: \nText: I made that with: \nCode: import seaborn as sns\nsns.kdeplot(X, bw=0.1)\n\nText: Sorry, just realized I used a smaller bandwidth than you... but you get the idea. \nAPI:\nsklearn.preprocessing.StandardScaler\n","label":[[54,72,"Mention"],[622,658,"API"]],"Comments":[]}
{"id":60617,"text":"ID:59150723\nPost:\nText: Actually, the documentation of skld.load_files says that the images or any data files must be present in the following hierarchy: \nText: container_folder\/ category_1_folder\/ file_1.txt file_2.txt  file_42.txt category_2_folder\/ file_43.txt file_44.txt  \nText: I think your images are present at the path \/Users\/USer\/Downloads\/C4IMAGES\/. In that case, you will have to create a subfolder like category 1, category 2 (if your data is not categorized, just create a subfolder with any name and put all your images in the subfolder) and put images with coresponding category in the subfolders. \nText: Now, you can pass the argument \/Users\/USer\/Downloads\/C4IMAGES\/ in the function load_files and it should load your data in python list data_dir['data'] in binary format. \nText: You can then convert your images from binary format to numpy array and display your image: \nCode: import io\nimport numpy as np    \nfrom PIL import Image\n\n# decode i'th image using: \nimg = Image.open(io.BytesIO(data_dir.data[i]))\nimg = np.asarray(img)\n\n# display i'th image\nimport matplotlib.pyplot as plt\n\nplt.imshow(img)\nplt.show()\n\nText: Referances: 1. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_files.html 2. Convert image loaded as binary string into numpy array \nAPI:\nsklearn.datasets.load_files\n","label":[[55,70,"Mention"],[1301,1328,"API"]],"Comments":[]}
{"id":60618,"text":"ID:59166569\nPost:\nText: Your only issue is that you need validation data. You can't measure accuracy between the predict(x_test) and a non-existing y_test. Use sk.model_selection.train_test_split to make a validation set based on your training data. You will have a train, validation, and test set. You can evaluate the performance of your model on the validation set. \nCode: from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y)\n\nText: Other remarks: \nText: Accuracy makes no sense here because you're trying to predict on continuous values. Only use accuracy for categorical variables. \nText: At a minimum, this could work: \nCode: import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\ntrain = pd.read_csv('C:\/Users\/pedro\/Documents\/Pedro\/UFMG\/8o periodo\/Python\/Trabalho Final\/train.csv', index_col='sku').fillna(-1)\ntest_data = pd.read_csv('C:\/Users\/pedro\/Documents\/Pedro\/UFMG\/8o '\n                    'periodo\/Python\/Trabalho Final\/test.csv', index_col='sku').fillna(-1)\n\nx, y = train.drop('isBackorder', axis=1), train['isBackorder']\nX_train, X_test, y_train, y_test = train_test_split(x, y)\n\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 10, alpha = 10, n_estimators = 10)\n\nxg_reg.fit(X_train,y_train)\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(xg_reg, X_train, y_train, cv=kfold)\ny_test_pred = xg_reg.predict(X_test)\n\nmse = mean_squared_error(y_test_pred, y_test)\n\ny_pred = xg_reg.predict(X_test)\n\npd.DataFrame(y_pred).to_csv('competitionsubmission.csv',index=False)\n\nAPI:\nsklearn.model_selection.train_test_split\n","label":[[160,195,"Mention"],[1843,1883,"API"]],"Comments":[]}
{"id":60619,"text":"ID:59349069\nPost:\nText: You can just use cross_vkl_prdeict (source) \nCode: from sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_val_predict\n\ndiabetes = datasets.load_diabetes()\n\nX = diabetes.data[:150]\ny = diabetes.target[:150]\n\nlasso = linear_model.Lasso()\ny_pred = cross_val_predict(lasso, X, y, cv=3)\n\nprint(y_pred)\n\nCode: [174.26933996 117.6539241  164.60228641 155.65049088 132.68647979\n 128.49511245 120.76146877 141.069413   164.18904498 182.37394949]\n\nText: It works with classification too, of course. \nAPI:\nsklearn.model_selection.cross_val_predict\n","label":[[41,58,"Mention"],[549,590,"API"]],"Comments":[]}
{"id":60620,"text":"ID:59375911\nPost:\nText: The semantic of _Pipeline is the following: a sequence of transformers (i.e. implementing fit and transform) followed by a final predictor (i.e. implementing fit and predict (optionally predict_proba,decision_function, etc.). \nText: Since all scikit-learn metrics expect only either predict or predict_proba output, it will not be straightforward to do what you like. \nText: I think that the easiest way is to implement your own meta-estimator which will make what you want: \nCode: from sklearn.base import BaseEstimator\nclass PostProcessor(BaseEstimator):\n    def __init__(self, predictor):\n        self.predictor = predictor\n    def fit(self, X, y):\n        self.predictor.fit(X, y)\n    def predict(self, X):\n        y_pred = self.predictor.predict(X)\n        y_pred_proba = self.predictor.predict_proba(X)\n        # do something with those\n        return np.hstack([y_pred, y_pred_proba])\n\nAPI:\nsklearn.pipeline.Pipeline\n","label":[[40,49,"Mention"],[922,947,"API"]],"Comments":[]}
{"id":60621,"text":"ID:59439287\nPost:\nText: from prprocessing import Imputer was deprecated with scikit-learn v0.20.4 and removed as of v0.22.2. See the sklean changelog. \nCode: from sklearn.impute import SimpleImputer\nimport numpy as np\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nText: pip install scikit-learn==0.20.4 or conda install scikit-learn=0.20.4 are not a good options because scikit-learn==0.20.4 is more than 3 years out of date. \nAPI:\nsklearn.preprocessing\n","label":[[29,41,"Mention"],[452,473,"API"]],"Comments":[]}
{"id":60622,"text":"ID:59809853\nPost:\nText: In order to use sk.preprocessing.MinMaxScaler you need first to fit the scaler to the values of your training data. This is done (as you already did) using \nCode: scaler.fit_transform(file_x[list_of_features_to_normalize])\n\nText: After this fit your scaling object scaler has its internal parameters (e.g., min_, scale_ etc.) tuned according to the training data. \nText: Once training is done, and you wish to evaluate your model on new records you only need to apply the scaler without fitting it to the new data: \nCode: val_t = scaler.transform(validation_data)\n\nAPI:\nsklearn.preprocessing.MinMaxScaler\n","label":[[40,69,"Mention"],[594,628,"API"]],"Comments":[]}
{"id":60623,"text":"ID:60001019\nPost:\nText: Result \nText: For one-class SVM, LIBSVM solves a scaled problem that every _i is multiplied by (), where  is the hyper-parameter and  is the number of instances. So the constraint becomes _i1 and _i _i = . \nText: Reason \nText: In Section 2.3 of LIBSVM \nText: Similar to the case of -SVC, in LIBSVM, we solve a scaled version of (7). \nText: where in Section 2.2 (-Support Vector Classification) quoted \nText: In LIBSVM, we solve a scaled version of problem (5) because numerically _i may be too small due to the constraint _i1\/. \nText: So for one-class SVM, LIBSVM solves a scaled problem because numerically _i may be too small due to the constraint _i1\/(). \nText: Verification \nText: Specifically, because the question is about sklearn, I modify the code from here to confirm thought, though from my understanding sksvm.OneClassSVM use LIBSVM in the backend. \nCode: from sklearn.svm import OneClassSVM\nfrom sklearn.datasets import load_boston\n\nX = load_boston()['data'][:, [8, 10]]\nclf = OneClassSVM(nu=0.261, gamma=0.05)\nclf.fit(X)\n\nprint(clf.nu*X.shape[0])\nprint(clf._dual_coef_.sum())\n\nText: gives \nCode: 132.066\n132.06599999999918\n\nAPI:\nsklearn.svm.OneClassSVM\n","label":[[861,878,"Mention"],[1188,1211,"API"]],"Comments":[]}
{"id":60624,"text":"ID:60417104\nPost:\nText: there is not default value for DecisionTreeClassifier spliter param, the default value is best so you can use: \nCode: def decisiontree(data, labels, criterion = \"gini\", splitter = \"best\", max_depth = None): #expects *2d data and 1d labels\n\n    model = sklearn.tree.DecisionTreeClassifier(criterion = criterion, splitter = splitter, max_depth = max_depth)\n    model = model.fit(data,labels)\n\n    return model\n\nAPI:\nsklearn.tree.DecisionTreeClassifier\n","label":[[55,77,"Mention"],[438,473,"API"]],"Comments":[]}
{"id":60625,"text":"ID:60519176\nPost:\nText: These following informations might be helpful: \nText: The type of some of the objects: data[feature]: pandas.Series data[feature].values: numpy.ndarray You can reshape a numpy.ndarray but not a pandas.Series, so you need to use .values to get a numpy.ndarray When you assign a numpy.ndarray to data[feature], automatic type conversion occurs, so data[feature] = data[feature].values.reshape(-1, 1) doesn't seem to do anything. fit_transform takes an array-like(Need to be a 2D array, e.g. pandas.DataFrame or numpy.ndarray) object as argument because OneHotEncoder is designed to fit\/transform multiple features at the same time, input pandas.Series(1D array) will cause error. fit_transform will return sparse matrix(or 2-d array), assign it to a pandas.Series may cause a disaster. \nText: (Not Recommended) If you insist on processing one feature after another: \nCode: for feature in categorical_feats:\n    encoder = OneHotEncoder()\n    tmp_ohe_data = pd.DataFrame(\n        encoder.fit_transform(data[feature].values.reshape(-1, 1)).toarray(),\n        columns=encoder.get_feature_names(),\n    )\n    data = pd.concat([tmp_ohe_data, data], axis=1).drop([feature], axis=1)\n\n\nText: I Recommended do encoding like this: \nCode: encoder = OneHotEncoder()\n\nohe_data = pd.DataFrame(\n    encoder.fit_transform(data[categorical_feats]).toarray(),\n    columns=encoder.get_feature_names(),\n)\nres = pd.concat([ohe_data, data], axis=1).drop(categorical_feats, axis=1)\n\nText: pandas.get_dummies is also a good choice. \nAPI:\nsklearn.preprocessing.OneHotEncoder\n","label":[[575,588,"Mention"],[1534,1569,"API"]],"Comments":[]}
{"id":60626,"text":"ID:60531740\nPost:\nText: You can get closest to what you want with utils and inspect. E.g.you can get a list of all sklearn classes or just classifiers like: \nCode: from sklearn.utils.testing import all_estimators\nall_est = all_estimators(type_filter=None)\nall_classifiers = all_estimators(type_filter=\"classifier\")\n\nText: Then with the help of inspect you can retrieve args of classifiers .fit method like: \nCode: import inspect\nall_classifiers_fit_args = {}\nfor name, clf in all_classifiers:\n    all_classifiers_fit_args[name] = inspect.signature(clf.fit)\n\nText: Finally you can put the info into pandas df: \nCode: df = pd.DataFrame(all_classifiers_fit_args.items(), columns=[\"fit_classifier\", \"args\"])\ndf\n    fit_classifier  args\n0   AdaBoostClassifier  (self, X, y, sample_weight=None)\n1   BaggingClassifier   (self, X, y, sample_weight=None)\n2   BernoulliNB (self, X, y, sample_weight=None)\n3   CalibratedClassifierCV  (self, X, y, sample_weight=None)\n4   CategoricalNB   (self, X, y, sample_weight=None)\n5   CheckingClassifier  (self, X, y, **fit_params)\n6   ClassifierChain (self, X, Y)\n7   ComplementNB    (self, X, y, sample_weight=None)\n8   DecisionTreeClassifier  (self, X, y, sample_weight=None, check_input=T...\n...\n\nText: Alternatively you can access args of the classifiers themselves: \nCode: for name, clf in all_classifiers:\n    all_classifiers_args[name] = inspect.signature(clf)\ndf = pd.DataFrame(all_classifiers_args.items(), columns=[\"classifier\", \"args\"])\ndf\n    classifier  args\n0   AdaBoostClassifier  (base_estimator=None, n_estimators=50, learnin...\n1   BaggingClassifier   (base_estimator=None, n_estimators=10, max_sam...\n2   BernoulliNB (alpha=1.0, binarize=0.0, fit_prior=True, clas...\n3   CalibratedClassifierCV  (base_estimator=None, method='sigmoid', cv=None)\n4   CategoricalNB   (alpha=1.0, fit_prior=True, class_prior=None)\n5   CheckingClassifier  (check_y=None, check_X=None, foo_param=0, expe...\n6   ClassifierChain (base_estimator, order=None, cv=None, random_s...\n7   ComplementNB    (alpha=1.0, fit_prior=True, class_prior=None, ...\n8   DecisionTreeClassifier  (criterion='gini', splitter='best', max_depth=...\n...\n\nAPI:\nsklearn.utils\n","label":[[66,71,"Mention"],[2160,2173,"API"]],"Comments":[]}
{"id":60627,"text":"ID:60531889\nPost:\nText: extract_patches_2d reshapes a 2D image into a collection of overlapping patches. To decompose your image into non-overlapping blocks you could use skimage.util.view_as_blocks. \nText: The following snippet is a possible implementation of your custom functions. Notice that create_patches gets rid of the bottom and right borders of the image if the image size is not an integer multiple of the patch size. I adopted this solution to avoid resizing of the image (an operation which is likely to introduce artifacts). The original image can be reconstructed either from the patch files (not implemented) or directly from the 4D array returned by create_patches. \nCode: import os\nimport numpy as np\nfrom skimage import io, util\n\ndef create_patches(img, folder, patch_width, patch_height):\n    # Trim right and bottom borders if image size is not \n    # an integer multiple of patch size\n    nrows, ncols = patch_height, patch_width   \n    trimmed = img[:img.shape[0]\/\/nrows*nrows, :img.shape[1]\/\/ncols*ncols]   \n    \n    # Create folder to store results if necessary\n    patch_dir = os.path.join(folder, 'Patched Image')\n    if not os.path.isdir(patch_dir):\n        os.mkdir(patch_dir)\n\n    # Generate patches and save them to disk           \n    patches = util.view_as_blocks(trimmed, (nrows, ncols))\n    for i in range(patches.shape[0]):\n        for j in range(patches.shape[1]):\n            patch = patches[i, j, :, :]\n            patch_name = f'patch_{i:02}_{j:02}.png'\n            io.imsave(os.path.join(patch_dir, patch_name), patch)\n\n    return patches\n\n\ndef reconstruct_image(patches):\n    img_height = patches.shape[0]*patches.shape[2]\n    img_width = patches.shape[1]*patches.shape[3]\n    return patches.transpose(0, 2, 1, 3).reshape(img_height, img_width)\n\nText: Demo \nCode: In [134]: import matplotlib.pyplot as plt\n     ...: \n     ...: folder = r'C:\\Users\\User\\path-to-your-folder'\n     ...: filename = 'sample_image.png'\n     ...: \n     ...: original = io.imread(os.path.join(folder, filename))\n     ...: patches = create_patches(original, folder, 50, 50)\n     ...: reconstructed = reconstruct_image(patches)\n     ...: \n     ...: fig, (ax0, ax1) = plt.subplots(1, 2)\n     ...: ax0.imshow(original, cmap='gray')\n     ...: ax0.set_title('Original')\n     ...: ax1.imshow(reconstructed, cmap='gray')\n     ...: ax1.set_title('Reconstructed')\n     ...: plt.show(fig)\n     ...: \n     ...: for patch in os.listdir(os.path.join(folder, 'Patched Image')):\n     ...:     print(patch)\n     ...:     \npatch_00_00.png\npatch_00_01.png\npatch_00_02.png\npatch_00_03.png\n...\npatch_07_05.png\npatch_07_06.png\npatch_07_07.png\npatch_07_08.png\npatch_07_09.png\n\nText: The image used in the example above can be downloaded from here \nAPI:\nsklearn.feature_extraction.image.extract_patches_2d\n","label":[[24,42,"Mention"],[2747,2798,"API"]],"Comments":[]}
{"id":60628,"text":"ID:60553912\nPost:\nText: Here's a slightly roundabout trick that will do it. \nText: Try re-centering your data, i.e. subtract x[-1], y[-1] from all datapoints so that x[-1], y[-1] is now the origin. \nText: Now fit your data using LinearRegression with fit_intercept set to False. This way, the data is fit so that the line is forced to pass through the origin. Because we've re-centered the data, the origin corresponds to x[-1], y[-1]. \nText: When you use the model to make predictions, subtract x[-1] from any datapoint for which you are making a prediction, then add y[-1] to the resulting prediction, and this will give you the same results as forcing your model to pass through x[-1], y[-1]. \nText: This is a little roundabout but it's the simplest way that occurs to me to do it using the sklearn linear regression function (without writing your own). \nAPI:\nsklearn.linear_model.LinearRegression\n","label":[[229,245,"Mention"],[863,900,"API"]],"Comments":[]}
{"id":60629,"text":"ID:60599065\nPost:\nText: You can ignore the deprecation warning, it's only a warning (I wouldn't worry if your code isn't referencing that subpackage, there's probably an import somewhere under the hood inside sklearn.) You could suppress all FutureWarnings, but then you might miss another more important one, on sklearn or another package. So I'd just ignore it for now. But if you want to: import warnings warnings.simplefilter('ignore', FutureWarning) from tree import ... # ... Then turn warnings back on for other packages warnings.filterwarnings('module') # or 'once', or 'always' \nText: See the doc, or How to suppress Future warning from import?, although obviously you replace import pandas with your own import statement. \nAPI:\nsklearn.tree\n","label":[[460,464,"Mention"],[738,750,"API"]],"Comments":[]}
{"id":60630,"text":"ID:60637924\nPost:\nText: Below is a dummy pandas.DataFrame for example: \nCode: import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndf = pd.DataFrame({'X1':[100,120,140,200,230,400,500,540,600,625],\n                       'X2':[14,15,22,24,23,31,33,35,40,40],\n                       'Y':[0,0,0,0,1,1,1,1,1,1]})\n\nText: Here we have 3 columns, X1,X2,Y suppose X1 & X2 are your independent variables and 'Y' column is your dependent variable. \nCode: X = df[['X1','X2']]\ny = df['Y']\n\nText: With train_test_split you are creating 4 portions of data which will be used for fitting & predicting values. \nCode: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4,random_state=42) \n\nX_train, X_test, y_train, y_test\n\nText: Now \nText: 1). X_train - This includes your all independent variables,these will be used to train the model, also as we have specified the test_size = 0.4, this means 60% of observations from your complete data will be used to train\/fit the model and rest 40% will be used to test the model. \nText: 2). X_test - This is remaining 40% portion of the independent variables from the data which will not be used in the training phase and will be used to make predictions to test the accuracy of the model. \nText: 3). y_train - This is your dependent variable which needs to be predicted by this model, this includes category labels against your independent variables, we need to specify our dependent variable while training\/fitting the model. \nText: 4). y_test - This data has category labels for your test data, these labels will be used to test the accuracy between actual and predicted categories. \nText: Now you can fit a model on this data, let's fit LogisticRegression \nCode: logreg = LogisticRegression()\nlogreg.fit(X_train, y_train) #This is where the training is taking place\ny_pred_logreg = logreg.predict(X_test) #Making predictions to test the model on test data\nprint('Logistic Regression Train accuracy %s' % logreg.score(X_train, y_train)) #Train accuracy\n#Logistic Regression Train accuracy 0.8333333333333334\nprint('Logistic Regression Test accuracy %s' % accuracy_score(y_pred_logreg, y_test)) #Test accuracy\n#Logistic Regression Test accuracy 0.5\nprint(confusion_matrix(y_test, y_pred_logreg)) #Confusion matrix\nprint(classification_report(y_test, y_pred_logreg)) #Classification Report\n\nText: You can read more about metrics here \nText: Read more about data split here \nText: Hope this helps:) \nAPI:\nsklearn.model_selection.train_test_split\nsklearn.linear_model.LogisticRegression\n","label":[[647,663,"Mention"],[1845,1863,"Mention"],[2609,2649,"API"],[2650,2689,"API"]],"Comments":[]}
{"id":60631,"text":"ID:60650715\nPost:\nText: KNeihborsClassifier uses all observations from your train data, while as the name suggests sk.ensemble.RandomForestClassifier uses data randomly, so you can expect different results from Random Forest per iteration. Now coming to the question of using it on different systems, this is tricky one, but you can give a try to following approach (though I have not tested this yet). \nText: 1). Fit a Random Forest model on your data with some random_state, let's say random_state = 0 \nText: 2). Import pickle, create a pickle object rf.pkl which will be saved at your current working directory. \nText: 3). Dump the current Random Forest model object in the pickle object. \nCode: import pickle    \npkl = 'rf.pkl'\nwith open(pkl,'wb') as file:\n    pickle.dump(rf,file)\n\nText: 4). Share the pickle object file to another user\/system. \nText: 5). Store the pickle object at some location and set that as working directory. \nText: 6). Open Python on that system, run your python code to read the data. \nText: 7). Instead of creating a new model, load the pickled model using following lines of code: \nCode: with open(pkl,'rb') as file:\n    pkl_model = pickle.load(file)\n\nText: 8). Test if your pickled model works and produces same results as it did on another system. \nText: I haven't tested this approach, but I think you should give a try to this and let me know if this works. Cheers!! \nAPI:\nsklearn.neighbors.KNeighborsClassifier\nsklearn.ensemble.RandomForestClassifier\n","label":[[24,43,"Mention"],[115,149,"Mention"],[1409,1447,"API"],[1448,1487,"API"]],"Comments":[]}
{"id":60632,"text":"ID:60788911\nPost:\nText: Normalization has different meanings depending on the context and sometimes the term is misleading. I think sklearn uses the terms interchangeably, to mean adjusting values measured on different scales to a notionally common scale (e.g., between 0 and 1), rather than change the data such that they follow a Normal distribution (apart from the StandardScaler, which does that). \nText: From my understanding, in sklearn they differ in the input they work on and how, and where they can be used. \nText: I assume that with Normalization you mean sklearn.preprocessing.Normalizer. \nText: So, the main difference is that Normalizer scales samples to unit norm (vector lenght) while StandardScaler scales features to unit variance, after subtracting the mean. Therefore, the former works on the rows, while the latter on the columns. \nText: In particular, \nText: normalize \"scales input vectors individually to unit norm (vector length).'. It can either be applied to rows (by setting the parameter axis to 1) and to features\/columns (by setting the parameter axis to 0). It uses one of the following norms: l1, l2, or max to normalize each non zero sample (or each non-zero feature if the axis is 0). Note: The term norm here refers to the mathematical definition. See here and here for more information. sk.preprocessing.Normalizer \"normalizes samples individually to unit norm.\". It behaves exactly as normalize when axis=1. Differently from normalize, Normalizer performs normalization using the Transformer API (e.g. as part of a preprocessing sklearn.pipeline.Pipeline). StandardScaler \"standardizes features by removing the mean and scaling to unit variance\". It does not use the norm of a vector, rather it computes the z-score for each feature. \nText: This interesting article explore more the differences among them. \nText: Let's use norm='max' for convenience: \nCode: from sklearn.preprocessing import normalize, Normalizer, StandardScaler\n\nX = [[1, 2],\n     [2, 4]]\n\n# Normalize column based on the maximum of each column (x\/max(column))\nnormalize(X, norm='max', axis=0)\n\n# Normalize column based on the maximum of each row (x\/max(row))\nnormalize(X, norm='max', axis=1)\n\n# Normalize with Normalizer (only rows)\nNormalizer(norm='max').fit_transform(X)\n\n# Standardize with StandardScaler (only columns)\nStandardScaler().fit_transform(X)\n\n\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([('normalization_step', normalize())] # NOT POSSIBLE\n\npipe = Pipeline([('normalization_step', Normalizer())] # POSSIBLE\n\npipe = Pipeline([('normalization_step', StandardScaler())] # POSSIBLE\n\npipe.score(X, y) # Assuming y exists\n\n\nText: The aforementioned lines of code would transform the data as follows: \nCode: \n# Normalize with normalize, axis=0 (columns)\n[[0.5, 0.5],\n [1. , 1. ]]\n\n# Normalize with normalize, axis=1 (rows)\n[[0.5, 1],\n [0.5, 1. ]]\n\n# Normalize with Normalizer (rows)\n[[0.5, 1],\n [0.5, 1. ]]\n\n# Standardize with StandardScaler (columns)\n[[-1, -1],\n [1, 1. ]]\n\nAPI:\nsklearn.preprocessing.Normalizer\nsklearn.preprocessing.StandardScaler\nsklearn.preprocessing.normalize\nsklearn.preprocessing.Normalizer\nsklearn.preprocessing.normalize\nsklearn.preprocessing.StandardScaler\n","label":[[640,650,"Mention"],[701,715,"Mention"],[881,890,"Mention"],[1324,1351,"Mention"],[1423,1432,"Mention"],[1595,1609,"Mention"],[3007,3039,"API"],[3040,3076,"API"],[3077,3108,"API"],[3109,3141,"API"],[3142,3173,"API"],[3174,3210,"API"]],"Comments":[]}
{"id":60633,"text":"ID:60922925\nPost:\nText: When you have that many different categories creating a dummy columns is not going to help. I would suggest looking for creating categories for a county or some manageable groups of zip codes. If that does not work i would just label encodefrom preprocessing import LabelEncoder and include the encoded column as a feature to see if it come as an important feature before thinking about more feature engineering. \nAPI:\nsklearn.preprocessing\n","label":[[269,282,"Mention"],[443,464,"API"]],"Comments":[]}
{"id":60634,"text":"ID:61039376\nPost:\nText: As the name suggests, the n_splits parameter is used to specify how many times (basically how many separate splits) you want the splits to happen. \nText: For example, setting n_splits = 3 would make the loop generate 3 different splits (one for each iteration) so you can perform validation more effectively. \nText: Setting n_splits = 1 would mimic what train_test_split would do (along with the stratify parameter mentioned). The documentation has detailed explanations of each parameter for this function. \nAPI:\nsklearn.model_selection.train_test_split\n","label":[[378,394,"Mention"],[538,578,"API"]],"Comments":[]}
{"id":60635,"text":"ID:61099313\nPost:\nText: Suppose this the mapping of your classes: \nCode: {'Product A':0, 'Product B':1, 'Product C':2, 'NO Product':3)}\n\nText: Then from skle.RandomForestClassifier docs, use class_weight as follows: \nCode: rf = RandomForestClassifier(n_estimators = 100, class_weight = {0:1,1:1,2:2,3:1})\n\nText: This will give more weights to 'Product C' \nAPI:\nsklearn.ensemble.RandomForestClassifier\n","label":[[153,180,"Mention"],[361,400,"API"]],"Comments":[]}
{"id":60636,"text":"ID:61131420\nPost:\nText: Got it: tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel() \nText: and multilabel_confusion_matrix \nAPI:\nsklearn.metrics.multilabel_confusion_matrix\n","label":[[113,140,"Mention"],[147,190,"API"]],"Comments":[]}
{"id":60637,"text":"ID:61403351\nPost:\nText: The reason sk.preprocessing.StandardScaler exists is for things like on-demand-data machine learning and such. It is used in pipelines. It does work by itself, but that is a using a sludge hammer on a tac. The way you described is the only way to rescale data as you see fit with your own parameters. My only recommendation would be to use array's; since arrays project their operations to all their entries automatically, so the code looks nicer. \nCode: import numpy\n\ndata = numpy.array([1,2,3,34,2,2,3,43,4,3,2,3,4,4,5,56,6,43,32,2,2])\n\n#Custom mean and std.\nnew_data = (data-10)\/5\n\n#Using the array's mean and std. \nnew_data = (data-data.mean())\/data.std()\n\nAPI:\nsklearn.preprocessing.StandardScaler\n","label":[[35,66,"Mention"],[690,726,"API"]],"Comments":[]}
{"id":60638,"text":"ID:61492678\nPost:\nText: As mentioned in the comments section, I don't think the comparison is fair mainly because the sm.pairwise.cosine_similarity is designed to compare pairwise distance\/similarity of the samples in the given input 2-D arrays. On the other hand, scipy.spatial.distance.cosine is designed to compute cosine distance of two 1-D arrays. \nText: Maybe a more fair comparison is to use scipy.spatial.distance.cdist vs. sklearn.metrics.pairwise.cosine_similarity, where both computes pairwise distance of samples in the given arrays. However, to my surprise, that shows the sklearn implementation is much faster than the scipy implementation (which I don't have an explanation for that currently!). Here is the experiment: \nCode: import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial.distance import cdist\n\nx = np.random.rand(1000,1000)\ny = np.random.rand(1000,1000)\n\ndef sklearn_cosine():\n    return cosine_similarity(x, y)\n\ndef scipy_cosine():\n    return 1. - cdist(x, y, 'cosine')\n\n# Make sure their result is the same.\nassert np.allclose(sklearn_cosine(), scipy_cosine())\n\nText: And here is the timing result: \nCode: %timeit sklearn_cosine()\n10 loops, best of 3: 74 ms per loop\n\n%timeit scipy_cosine()\n1 loop, best of 3: 752 ms per loop\n\nAPI:\nsklearn.metrics.pairwise.cosine_similarity\n","label":[[118,147,"Mention"],[1298,1340,"API"]],"Comments":[]}
{"id":60639,"text":"ID:61603110\nPost:\nText: Before I get to answers, I would like to point out that when you have a program that uses a large set of numbers you should always use numpy.array from numpy library to store that kind of data. I don't know what version of Python, scikit-learn, and SciPy are you using, but I am using Python 3.7.3, scikit-learn 0.21.3, and SciPy 1.3.0. When I ran your code to compare build-times, I got AttributeError: 'list' object has no attribute 'size'. This error is saying that list listOfRandom2DPoints has no attribute size. The problem is that KDTree expects numpy.array which has attribute size. Class scipy.spatial.KDTree works with Python lists but as you can see in the source code of __init__ method of class scipy.spatial.KDTree, first line is self.data = np.asarray(data), which means that data will be converted to numpy.array. \nText: Because of this, I cahanged your lines: \nCode: from random import randint\nlistOfRandom2DPoints = [ [randint(0,dim),randint(0,dim)] for x in range(length)]\n\nText: to: \nCode: import numpy as np\nListOfRandom2DPoints = np.random.randint(0, dim, size=(length, 2))\n\nText: (This change doesn't affect speed comparisons because change is made in setup code.) \nText: Now answers on your questions: \nText: Like you said scikit-learn seems to beet SciPy for the build time. The reason why this happens isn't that scikit-learn has a faster algorithm, but KDTrnee is implemented in Cython (link to source code), and scipy.spatial.KDTree is written in pure Python code (link to source code). (If you don't know what is Cython, an oversimplified explanation would be that Cython makes possible writing C code in Python and main reason for doing that is that C is much faster than Python) SciPy library also has implementation in Cython scipy.spatial.cKDTree (link to source code), it works the same as scipy.spatial.KDTree and if you compare build times of sk.neighbors.KDTree and scipy.spatial.cKDTree: timeit.timeit('scipy.spatial.cKDTree(npListOfRandom2DPoints, leafsize=20)', setup=setup, number=nTimes) timeit.timeit('sklearn.neighbors.KDTree(npListOfRandom2DPoints, leaf_size=20)', setup=setup, number=nTimes) Build times are very similar, and when I ran the code, scipy.spatial.cKDTree was a little bit (around 20%) faster. With query times situation is very similar, scipy.spatial.KDTree (pure Python implementation) is about ten times slower than KDTree (Cython implementation) and scipy.spatial.cKDTree (Cython implementation) is aproximatly as fast as sklearn.neighbors.KDTree. I have tested query times up to N = 10000000, and got the same result as you. Query times stay the same regardless of N (meaning query time for scipy.spatial.KDTree is same for N = 1000 and N = 1000000, and the same thing for query times forsklearn.neighbors.KDTree and scipy.spatial.cKDTree). That is because query (search) time complexity is O(logN) and even for N = 1000000, logN is very small so the difference is too small to measure. Build algorithm of KDTree (__init__ method of class) has time complexity of O(KNlogN) (about scikit-learn Nearest Neighbor Algorithms) so in your case it would be O(2NlogN) which is practically O(NlogN). Based on very similar build times of KDTree and scipy.spatial.cKDTree I assume that the build algorithm of scipy.spatial.cKDTree also has time complexity of O(NlogN). I am no expert on nearest neighbor search algorithms, but based on some online search, I would say that for low-dimensional nearest neighbor search algorithms this as fast as it can be. If you go to nearest neighbor search Wikipedia page you will see that there are exact methods and approximation methods. k-d tree is exact method, it is subtype of space partitioning methods. Of all space partitioning methods (only fast exact methods for nearest neighbor search based on Wikipedia page), k-d tree is the best method in the case of low-dimensional Euclidean space for nearest neighbor search in static context (there isn't a lot of insertions and deletions). Also if you look at approximation methods under greedy search in proximity neighborhood graphs you will see \"Proximity graph methods are considered the current state-of-the-art for the approximate nearest neighbors search.\" When you look at the research article that is cited for this method (Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs) you can see that this method has time complexity of O(NlogN). This means that for low-dimensional spaces k-d tree (exact method) is as fast as approximation methods. For now, we have compared build (construction) time complexity of structures that are used for nearest neighbor searches. All these algorithms have search (query) time complexity of O(logN). So best that we can get is build complexity of O(NlogN) and query complexity of O(logN) which is what we have in k-d tree method. So based on my research I would say that k-d tree is the best structure for low-dimensional nearest neighbor searches. (I think if there was a better (faster) method to do nearest neighbor search, than scikit-learn and SciPy would have implemented that method. Also from a theoretical standpoint, knowing that fastest sorting algorithms have time complexity of O(NlogN), it would be quite surprising to have nearest neighbor search build algorithm with time complexity less than O(NlogN).) Like I said you are comparing KDTref with Cython implementation and scipy.spatial.KDTree with pure Python implementation. In theory KDTree should be faster than scipy.spatial.KDTree, I compared these up to 1000000 and they seem to get closer at large N. For N = 100, scipy.spatial.KDTree is about 10 times slower than KDTree and for N = 1000000, scipy.spatial.KDTree is about twice as slow as sklearn.neighbors.KDTree. I am not sure why is this happening, but I suspect that for big N, memory becomes a bigger problem than the number of operations. I checked re-build time also up to 1000000 and it does increase linearly and that is because the duration of function pickle.loads is linearly proportional to the size of the loading object. For me, pickling of sklearn.neighbors.KDTree, scipy.spatial.KDTree, and scipy.spatial.cKDTree works so I can't reproduce your error. I am guessing that the problem is that you have an older version of SciPy so updating SciPy to the newest version should fix this problem. (If you need more help on this problem you should add some more info to your question. What are your Python and SciPy versions, exact code to reproduce this error, and full error message?) \nAPI:\nsklearn.neighbors.KDTree\nsklearn.neighbors.KDTree\nsklearn.neighbors.KDTree\nsklearn.neighbors.KDTree\nsklearn.neighbors.KDTree\nsklearn.neighbors.KDTree\nsklearn.neighbors.KDTree\nsklearn.neighbors.KDTree\nsklearn.neighbors.KDTree\n","label":[[562,568,"Mention"],[1404,1411,"Mention"],[1903,1922,"Mention"],[2402,2408,"Mention"],[2994,3000,"Mention"],[3216,3222,"Mention"],[5413,5419,"Mention"],[5515,5521,"Mention"],[5701,5707,"Mention"],[6590,6614,"API"],[6615,6639,"API"],[6640,6664,"API"],[6665,6689,"API"],[6690,6714,"API"],[6715,6739,"API"],[6740,6764,"API"],[6765,6789,"API"],[6790,6814,"API"]],"Comments":[]}
{"id":60640,"text":"ID:62761373\nPost:\nText: You cannot pass string to fit() method. Column name needs to be transformed into float. Good method is to use: LabelEncoder \nText: Given above sample of dataset, here is reproducible example how to perform LabelEncoding: \nCode: from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nle = preprocessing.LabelEncoder()\ndata.name = le.fit_transform(data.name)\nX = data.iloc[:, 0:4]\ny = data.iloc[:, 5]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\n\nprint(classifier.coef_,classifier.intercept_)\n\n\nText: resulting model coefficients and intercept: \nCode: [[ 0.09253555  0.09253555 -0.15407024  0.        ]] [-0.1015314]\n\nAPI:\nsklearn.preprocessing.LabelEncoder\n","label":[[135,147,"Mention"],[830,864,"API"]],"Comments":[]}
{"id":60641,"text":"ID:63571555\nPost:\nText: I did this based on some old code of mine that is itself based on scikit-learn working with text. Let me also reference, Scikit-learn 6.2.3 and note that CountVectorizer will be of particular interest as it contains what you want to do with OneHotEncoder and more. From CountVectorizer documentation: \nText: CountVectorizer implements both tokenization and occurrence counting in a single class: \nText: In the example case you provided, you have a total number of 95 words which consist of 22 unique words -- assuming you used all the words which probably isn't what you would want. Put differently, words like \"there, is, a, my, was, I, where and which\" probably can't help you tell a good account from a bogus one but words like \"Nigeria, prince, transfer, bank, penis, or enlargement\" are likely indicative of spam. \nText: So, you'd have 22 dimensions (minus whatever excluded ones) of data before you go to the other columns like age, symbol, etc. That's a lot of useless data (all those 0's for nothing you need) so people either store it as a sparse matrix and\/or use some sort of dimension reduction like Lasso or Ridge. You might think that's exactly what you want to do right now and that you're on the right track. That would be a bit different than what you asked though. And you kinda are except there are a couple of more points to deal with. \nText: First, and I think this is the important one, some of your fields should be suspect as they are user reported (like age) or are useless\/redundant (like the name). No kid goes on a porn or distillery site and says they are 15. No pervy old guy says he's 65 looking to chat with underage kids. Even dating sites where you think people would eventually find out. People lie about their ages. The same goes for names. You can include them if you want but remember the old adage: Garbage In, Garbage Out. \nText: Second, Lasso and Ridge regressions both assign cost functions to help with overfitting models. So, house price base on square footage and zip code makes sense. But when you get down to the last time a property tax assessment was done or the distance to the nearest library you might be thinking \"Really?\" But that's not really something you have. \nText: Putting those two together, in your case you have Text (definitely useful), symbol (a derivative of text), account and age (see the above note), note (probably useful for the time they've been on and active), and label -- your assessment. So, of five fields, only two are likely to be useful in predicting the assessment. All this is to say that while you can use lasso or ridge, you might be better served using Bayes for this task. If you're up for it there are multiple pages that will show they are equivalent under certain conditions [example]. But the reason to consider Bayes is the computational load for this example. \nText: Symbols (part iv) I've been loathe to say this but, from experience, punctuation is not a good indicator. The reason I say loathe is that you might come up with some novel implementation. But a lot a have tried, so the odds are small. Part of this is related to Zipf's Law which has to do with words rather than punctuation. However, if you make punctuation to carry some sort of additional semantic meaning, it is essentially another word. Remember the goal is not to find that a symbol is in spam, rather, the goal is to find if the symbol is a reliable indicator of spam and is sufficiently unique. \nText: But if you really wanted to add punctuation as some sort of indicator, you might need to think of it differently. For example, is just the presence of a question mark enough? Or, is having three or more in a row? Or, a high percentage of characters per {text, email, message, post, etc}? This gets into feature engineering which is part of why I would say you need to think through it. Personally (and from a quick look through my spam folder) I'd look at emoji, foreign characters (e.g., ) and perhaps text effects (bold, underlined, etc). But you then have a separate and second question. With text content, you have probabilistic loadings with say an aggerate measurement: \nText: print(f\"{message} is flagged for consideration at {loading}%. \nText: But amongst those options suggested above you would need to develop some sort of weighting for that feature. You could just append the symbol to each Text field but before TF-IDF. But then you need to use a different approach. You could also assign a weighting to the content and a second one to your engineered feature that would be based off Principle Component Analysis and\/or Confusion Matrix. \nText: For example - Text 34 is known spam: \nText: Nw Skinny Pill Kills Too Much Fat? This Diet is Sweeping The Nation \nText: The Bayesian approach assigns an aggregate probability of 94% spam, well above your threshold of 89%. But it's known spam with a probability of 1(00%). The delta of 6% would be due to what most likely? I'd argue in this case it's the . \nText: The same applies with label. From your train set, you may have zero accounts over 2 years that send spam and 90% come from accounts less than 1 week. \nText: Anyway, on to the code and implementation. \nText: 1. Mung data. \nText: This is supervised so 'Label' is critical by definition. \nText: 2. Train-test split \nText: You didn't mention this but it's worth noting. train_test_split \nText: 3. Tokenizing text with scikit-learn. \nText: This is where what you're specifically asking starts. Turn the corpus (the collection of documents) into a bag-of-words. You said you were using NLTK which is good for academia but I find overly cumbersome. SpacCy is great, Gensim rocks. But I'm using scikit-learn. My code varies a bit from the example in that it shows a bit of what is going on behind the scenes. \nCode: from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(lowercase=True, tokenizer=None, stop_words='english',\n   analyzer='word', max_df=1.0, min_df=1, max_features=None)\ncount_vect.fit(your training data)\n\n# uncomment if you'd like to know the mapping of the columns to the words.\n# count_vect.vocabulary_\n# for key in sorted(count_vect.vocabulary_.keys()):\n#     print(\"{0:<20s} {1}\".format(key, count_vect.vocabulary_[key]))\n\nText: About the training set: \nCode: X_train_counts = count_vect.transform(your training data)\nprint(\"The type of X_train_counts is {0}.\".format(type(X_train_counts)))\nprint(\"The X matrix has {0} rows (documents) and {1} columns (words).\".format(\n        X_train_counts.shape[0], X_train_counts.shape[1]))\n\nText: That will give you something like this: \nCode: The type of X_train_counts is <class 'scipy.sparse.csr.csr_matrix'>.\nThe X matrix has 2257 rows (documents) and 35482 columns (words).\n\nText: 4. Convert them to frequencies (tf or tf-idf). \nText: You have occurrences of words. CountVectorizer is just the number of times each word appears in each document. For each document, we would like to normalize by the number of words. This is the term (or word) frequency. IDF is useful in avoiding underflow errors resulting from dividing the one occurrence you have of a word by the gargantuan data set of words. Which is not true in your case but normally is an issue. \nText: 5. Ok, now you can start training a classifier. \nText: Stick with the Scikit learn example on this, at least for now. They're using naive Bayes and I laid out my reasoning for why I think Lasso and Ridge aren't best suited in this case. But if you want to go with a regression model, you're set up for it too. If you want to add in your other fields (symbol, age, etc) you might consider just appending them to each record. \nText: At this point I have another couple of steps: \nText: 6. Find out the tokens associated with each category as a sniff test. \nText: In general, picking the categories and words associated with each is somewhat of an art. You will probably have to iterate on this. \nCode: feature_words = count_vect.get_feature_names()\nn = 7 #number of top words associated with the category that we wish to see\n\nfor cat in range(len(categories)):\n    print(f\"\\nTarget: {cat}, name: {target_names[cat]}\")\n    log_prob = nb_model.feature_log_prob_[cat]\n    i_topn = np.argsort(log_prob)[::-1][:n]\n    features_topn = [feature_words[i] for i in i_topn]\n    print(f\"Top {n} tokens: \", features_topn)\n\nText: 7. Prediction as a second sniff test. \nText: A new doc or three that you make up going off similar classification. Then: \nCode: X_new_counts = count_vect.transform(docs_new)\nX_new_tfidf = tfidf_transformer.transform(X_new_counts)\npredictions = nb_model.predict(X_new_tfidf)\nprint('Predictions')\nfor doc, category in zip(docs_new, predictions):\n    print(\"{0} => {1}\".format(doc, twenty_train.target_names[category]))\n\nText: 8 & 9. Pipeline and Eval as done in the tutorial. \nText: Additional information from follow on questions. \nText: Answers for some parts of your questions have been incorporated above. Train-test before tokenization or vice versa. I've chosen my words carefully here, so read this part carefully. Currently, it's good practice is to do split then tokenize. The rationale is reproducibility. Others tokenize then split. The rationale is computational efficiency and the term freq would be the same for both. You will see both done all the time. A Data Scientist would test it extensively. What will output look like? It depends on what your problem is, what stage you're at, and how you code it. You seem to just be doing a spam filter. At some point, you'll have a set of loadings that will typically take the form of word: tf-idf loading with several terms\/loadings for each document. You may or may have set at a threshold so you only see the filter results in addition to the model's probabilistic results. What about the other columns? As I said before, 'Label' is critical as this is supervised learning. Age is useless; Name is probably useless unless all names are unique. Fun fact: there are about 150 named 'math' or 'Math' on StackOverflow. Presumable only one has your user number. 'Symbol' is tricky and you should think hard about it. \nText: Last point. There is a reason why this is a field on it's own. There is a reason why books are written on it and why there are multiple article series on it. So, cramming this onto one wall of text, while concise, is probably sub-optimal in that there is so very much not included that you need to know. \nAPI:\nsklearn.model_selection.train_test_split\n","label":[[5380,5396,"Mention"],[10468,10508,"API"]],"Comments":[]}
{"id":60642,"text":"ID:63672016\nPost:\nText: In your code \nCode: from sklearn.linear_model import LinearRegression as lm\n\nx = data_all[combi_list[0][1:]]\ny = data_all[combi_list[0][0]]\n\nlm.fit(x, y)\n\nText: you have not instantiated the LinearRegression class, but you are using the instance method .fit(). You need to instantiate the object before calling .fit(). \nCode: from sklearn.linear_model import LinearRegression\n\nx = data_all[combi_list[0][1:]]\ny = data_all[combi_list[0][0]]\n\nlm = LinearRegression()\nlm.fit(x, y)\n\nText: From an answer to a related question: \nText: You get the seemingly strange error that y is missing because .fit is an instance method, so the first argument to this function is actually self. When you call .fit on an instance, self is passed automatically. If you call .fit on the class (as opposed to the instance), you would have to supply self. \nAPI:\nsklearn.linear_model.LinearRegression\n","label":[[215,231,"Mention"],[863,900,"API"]],"Comments":[]}
{"id":60643,"text":"ID:63777839\nPost:\nText: log_loss assumes that the labels for probabilities are in alphabetical order. This can be seen in the source code. \nText: To flip your labels, you need to rename them in the opposite alphabetical order, e.g. aspam and bham: \nCode: from sklearn.metrics import log_loss\nlog_loss([\"aspam\", \"bham\", \"bham\", \"aspam\"], [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n\nText: Output: ~1.816. \nAPI:\nsklearn.metrics.log_loss\n","label":[[24,32,"Mention"],[410,434,"API"]],"Comments":[]}
{"id":60644,"text":"ID:64184206\nPost:\nText: The MLPClassifier uses (a variant of) Stochastic Gradient Descent (SGD) by default. Your question could be framed more generally as how SGD is used to optimize the parameter values in a supervised learning context. There is nothing specific to Multi-layer Perceptrons (MLP) here. \nText: So with the MLPClassifier we are building a neural network based on a training dataset. Setting early_stopping = True it is possible to use a validation dataset within the training process \nText: Correct, although it should be noted that this validation set is taken away from the original training set. \nText: in order to check whether the network is working on a new set as well. \nText: Not quite. The point of early stopping is to track the validation score during training and stop training as soon as the validation score stops improving significantly. \nText: If early_stopping = False, no validation within [t]he process is done. After one has finished building, we can use the fitted model in order to predict on a third dataset if we wish to. \nText: Correct. \nText: What I was thiking before is, that doing the whole training process a validation dataset is being taken aside anways with validating after every epoch. \nText: As you probably know by now, this is not so. The division of the learning process into epochs is somewhat arbitrary and has nothing to do with validation. \nAPI:\nsklearn.neural_network.MLPClassifier\n","label":[[28,41,"Mention"],[1405,1441,"API"]],"Comments":[]}
{"id":60645,"text":"ID:64596644\nPost:\nText: You have a feature with all negative values: \nCode: df['exp(x005)*log(x000)']\n\nText: returns \nCode: 0     -3630.638503\n1     -2212.931477\n2     -4751.790753\n3     -3754.508972\n4     -3395.387438\n          ...\n501   -2022.382877\n502   -1407.856591\n503   -2998.638158\n504   -1973.273347\n505   -1267.482741\nName: exp(x005)*log(x000), Length: 506, dtype: float64\n\nText: Quoting another answer (https:\/\/stackoverflow.com\/a\/46608239\/5025009): \nText: The error message Input X must be non-negative says it all: Pearson's chi square test (goodness of fit) does not apply to negative values. It's logical because the chi square test assumes frequencies distribution and a frequency can't be a negative number. Consequently, chi2 asserts the input is non-negative. \nText: In many cases, it may be quite safe to simply shift each feature to make it all positive, or even normalize to [0, 1] interval as suggested by EdChum. \nText: If data transformation is for some reason not possible (e.g. a negative value is an important factor), you should pick another statistic to score your features: \nText: f_regression computes ANOVA f-value mutual_info_classif computes the mutual information \nText: Since the whole point of this procedure is to prepare the features for another method, it's not a big deal to pick anyone, the end result usually the same or very close. \nAPI:\nsklearn.feature_selection.chi2\nsklearn.feature_selection.f_regression\nsklearn.feature_selection.mutual_info_classif\n","label":[[739,743,"Mention"],[1112,1124,"Mention"],[1148,1167,"Mention"],[1383,1413,"API"],[1414,1452,"API"],[1453,1498,"API"]],"Comments":[]}
{"id":60646,"text":"ID:64907828\nPost:\nText: I am afraid that this cannot work. If you one-hot encode your categorical data, your missing values will be encoded into a new binary variable and KNNImputer will fail to deal with them because: \nText: it works on each column at a time, not on the full set of one-hot encoded columns there won't any missing to be dealt with anymore \nText: Anyway, you have a couple of options for imputing missing categorical variables using scikit-learn: \nText: you can use SimpleImputer using strategy=\"most_frequent\": this will replace missing values using the most frequent value along each column, no matter if they are strings or numeric data use KNNImpeter with some limitation: you have first to transform your categorical features into numeric ones while preserving the NaN values (see: LabelEncoder that keeps missing values as 'NaN'), then you can use the KNNImputer using only the nearest neighbour as replacement (if you use more than one neighbour it will render some meaningless average). For example: \nCode:     import numpy as np\n    import pandas as pd\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.impute import KNNImputer\n    \n    df = pd.DataFrame({'A': ['x', np.NaN, 'z'], 'B': [1, 6, 9], 'C': [2, 1, np.NaN]})\n    \n    df = df.apply(lambda series: pd.Series(\n        LabelEncoder().fit_transform(series[series.notnull()]),\n        index=series[series.notnull()].index\n    ))\n    \n    imputer = KNNImputer(n_neighbors=1)\n    imputer.fit_transform(df)\n    \n    In:\n        A   B   C\n    0   x   1   2.0\n    1   NaN 6   1.0\n    2   z   9   NaN\n    \n    Out:\n    array([[0., 0., 1.],\n           [0., 1., 0.],\n           [1., 2., 0.]])\n\nText: Use lterativeImputer and replicate a MissForest imputer for mixed data (but you will have to processe separately numeric from categorical features). For example: \nCode:     import numpy as np\n    import pandas as pd\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.experimental import enable_iterative_imputer\n    from sklearn.impute import IterativeImputer\n    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n    \n    df = pd.DataFrame({'A': ['x', np.NaN, 'z'], 'B': [1, 6, 9], 'C': [2, 1, np.NaN]})\n    \n    categorical = ['A']\n    numerical = ['B', 'C']\n    \n    df[categorical] = df[categorical].apply(lambda series: pd.Series(\n        LabelEncoder().fit_transform(series[series.notnull()]),\n        index=series[series.notnull()].index\n    ))\n    \n    print(df)\n    \n    imp_num = IterativeImputer(estimator=RandomForestRegressor(),\n                               initial_strategy='mean',\n                               max_iter=10, random_state=0)\n    imp_cat = IterativeImputer(estimator=RandomForestClassifier(), \n                               initial_strategy='most_frequent',\n                               max_iter=10, random_state=0)\n    \n    df[numerical] = imp_num.fit_transform(df[numerical])\n    df[categorical] = imp_cat.fit_transform(df[categorical])\n    \n    print(df)\n\nAPI:\nsklearn.impute.SimpleImputer\nsklearn.impute.KNNImputer\nsklearn.impute.IterativeImputer\n","label":[[483,496,"Mention"],[661,671,"Mention"],[1695,1711,"Mention"],[3032,3060,"API"],[3061,3086,"API"],[3087,3118,"API"]],"Comments":[]}
{"id":60647,"text":"ID:64927597\nPost:\nText: Your problem is somewhere else. You define label = LabelEncoder() - then you use label.type and get AttributeError: 'str' object has no attribute 'type'. \nText: This means label (at this exact moment) is of type string whereas it should be reported to be <class 'sklearn.preprocessing._label.LabelEncoder'> following label = LabelEncoder()! \nText: When fit'ing labels that way, you get integers already, not strings: no conversion whatsoever needed: \nCode: from sklearn import preprocessing\nimport pandas as pd\nfrom random import random, choice\n\nr = random\nc = choice\n\ndf = pd.DataFrame([[r(),r(),r(),r(),c([\"sitting\",\"standing\"])] for _ in range(6)],\n                  columns=[\"quat1\",\"quat2\",\"quat3\",\"quat4\",\"activity\"])\n\nle = preprocessing.LabelEncoder()\ndf[\"label\"] = le.fit_transform(df[\"activity\"])\n\nprint(df)\nprint(df[\"label\"].dtypes)\n\nText: Output: \nCode:       quat1     quat2     quat3     quat4  activity  label\n0  0.550365  0.051738  0.485262  0.194497  standing      1\n1  0.656460  0.151324  0.131370  0.338022  standing      1\n2  0.512595  0.501235  0.449589  0.302794  standing      1\n3  0.440568  0.043643  0.817394  0.128534   sitting      0\n4  0.364890  0.714289  0.683436  0.731021   sitting      0\n5  0.708488  0.423278  0.624220  0.880735  standing      1\n\nint32\n\nText: To get back to your label, use \nCode: print(le.inverse_transform([0]))   # ['sitting']\n\nText: See Examples on LabelEncoder documentation \nAPI:\nsklearn.preprocessing.LabelEncoder\n","label":[[1426,1438,"Mention"],[1459,1493,"API"]],"Comments":[]}
{"id":60648,"text":"ID:65115098\nPost:\nText: First, a general explanation - think of LDiA as a clustering algorithm, that's going to determine, by default, 10 centroids, based on the frequencies of words in the texts, and it's going to put greater weights on some of those words than others by virtue of proximity to the centroid. Each centroid represents a 'topic' in this context, where the topic is unnamed, but can be sort of described by the words that are most dominant in forming each cluster. \nText: So generally what you're doing with LDA is: \nText: getting it to tell you what the 10 (or whatever) topics are of a given text. or getting it to tell you which centroid\/topic some new text is closest to \nText: For the second scenario, your expectation is that LDiA will output the \"score\" of the new text for each of the 10 clusters\/topics. The index of the highest score is the index of the cluster\/topic to which that new text belongs. \nText: I prefer gensim.models.LdaMulticore, but since you've used the sk.decomposition.LatentDirichletAllocation I'll use that. \nText: Here's some sample code (drawn from here) that runs through this process \nCode: from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.datasets import fetch_20newsgroups\nimport random\n\nn_samples = 2000\nn_features = 1000\nn_components = 10\nn_top_words = 20\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\n    \ndata, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n                             remove=('headers', 'footers', 'quotes'),\n                             return_X_y=True)\nX = data[:n_samples]\n#create a count vectorizer using the sklearn CountVectorizer which has some useful features\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=n_features,\n                                stop_words='english')\nvectorizedX = tf_vectorizer.fit_transform(X)\nlda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=0)\nlda.fit(vectorizedX)\n\n\nText: Now let's try a new text: \nCode: testX = tf_vectorizer.transform([\"I am educated about learned stuff\"])\n#get lda to score this text against each of the 10 topics\nlda.transform(testX)\n\nOut:\narray([[0.54995409, 0.05001176, 0.05000163, 0.05000579, 0.05      ,\n        0.05001033, 0.05000001, 0.05001449, 0.05000123, 0.05000066]])\n\n#looks like the first topic has the high score - now what are the words that are most associated with each topic?\nprint(\"\\nTopics in LDA model:\")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)\n\nOut:\nTopics in LDA model:\nTopic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\nTopic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\nTopic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\nTopic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\nTopic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\nTopic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\nTopic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\nTopic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\nTopic #8: people said did just didn know time like went think children came come don took years say dead told started\nTopic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n\n\n\nText: Seems sensible - the sample text is about education and the word cloud for the first topic is about education. \nText: The pictures below are from another dataset (ham vs spam SMS messages, so only two possible topics) which I reduced to 3 dimensions with PCA, but in case a picture helps, these two (same data from different angles) might give a general sense of what's going on with LDiA. (graphs are from Latent Discriminant Analysis vs LDiA, but the representation is still relevant) \nText: While LDiA is an unsupervised method, to actually use it in a business context you'll likely want to at least manually intervene to give the topics names that are meaningful to your context. e.g. Assigning a subject area to stories on a news aggregation site, choosing amongst ['Business', 'Sports', 'Entertainment', etc] \nText: For further study, perhaps run through something like this: https:\/\/towardsdatascience.com\/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24 \nAPI:\nsklearn.decomposition.LatentDirichletAllocation\n","label":[[995,1037,"Mention"],[5331,5378,"API"]],"Comments":[]}
{"id":60649,"text":"ID:65341177\nPost:\nText: GridSearchCV has nothing to to with kernels. \nText: kernel is a parameter of your estimator (e.g. classifier can use a kernel). GridSearchCV just gives you the option to try different combinations of parameters for your estimator. \nText: Long story short: you have to look at the estimator you use, eg. for sklearn.svm.SVC: \nCode: kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n    Specifies the kernel type to be used in the algorithm.\n    It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n    a callable.\n    If none is given, 'rbf' will be used. If a callable is given it is\n    used to pre-compute the kernel matrix from data matrices; that matrix\n    should be an array of shape ``(n_samples, n_samples)``.\n\nAPI:\nsklearn.svm.SVC\n","label":[[122,132,"Mention"],[792,807,"API"]],"Comments":[]}
{"id":60650,"text":"ID:65381429\nPost:\nText: The comment by @4.Pi.n solved my problem: \nText: It's exactly as your professor says, The most common way is to storing k-models, then averaging there predictions, ex. y_pred = (pred_1 + pred_2 + ... + pred_k) \/ k, or you might use ms.cross_val_predict \nAPI:\nsklearn.model_selection.cross_val_predict\n","label":[[256,276,"Mention"],[283,324,"API"]],"Comments":[]}
{"id":60651,"text":"ID:65852308\nPost:\nText: You just need to inherit from sk.base.BaseEstimator as well as the transformermixin : ). The type error says: \nText: it does not seem to be a scikit-learn estimator \nText: So you just need to make it one :D. The code below should work. \nCode: import numpy as np\nimport sklearn\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.linear_model import LinearRegression\n\nclass MyTransform(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n    def fit(self, *_, **__):\n        return self\n\n    def transform(self, X):\n        return np.array(X)*10\n\n    def inverse_transform(self, X):\n        return np.array(X)\/10\n\n\ndef MyLinearRegression():\n    return TransformedTargetRegressor(\n        regressor=LinearRegression(),\n        transformer=MyTransform()\n    )\n\n\n\nmodel = MyLinearRegression()\nmodel.fit(X=[[1], [2], [3]], y=[1, 2, 3])\n\nAPI:\nsklearn.base.BaseEstimator\n","label":[[54,75,"Mention"],[885,911,"API"]],"Comments":[]}
{"id":60652,"text":"ID:66203211\nPost:\nText: First, lng_loss applies natural logarithm (math.log or numpy.log) to probabilities, not base-2 logarithm. \nText: Second, you obviously got -0.0 because of multiplying log probabilities to zeros in y_true. For a binary case, log-loss is \nCode: -logP(y_true, y_pred) = -(y_true*log(y_pred) + (1-y_true)*log(1-y_pred))\n\nText: Third, you forgot to take an average of log-losses in your code. \nCode: from math import log\n\ndef bin_cross_entropy(p, q):\n    n = len(p)\n    return -sum(p[i]*log(q[i]) + (1-p[i])*log(1-q[i]) for i in range(n)) \/ n\n\nbin_cross_entropy(y_true, y_pred)  # 0.6931471805599453\n\nAPI:\nsklearn.metrics.log_loss\n","label":[[31,39,"Mention"],[625,649,"API"]],"Comments":[]}
{"id":60653,"text":"ID:66252262\nPost:\nText: You can certainly use your \"extra\" features like is_it_capital?, is_it_upper?, and contains_num?. It seems you're struggling with how exactly to combine the two seemingly disparate feature sets. You could use something like sk.pipeline.FeatureUnion or ColumnTransformer to apply your different encoding strategies to each set of features. There's no reason you couldn't use your extra features in combinations with whatever a text-feature extraction method (e.g. your BoW approach) would produce. \nCode: df = pd.DataFrame({'text': ['this is some text', 'this is some MORE text', 'hi hi some text 123', 'bananas oranges'], 'is_it_upper': [0, 1, 0, 0], 'contains_num': [0, 0, 1, 0]})\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import ColumnTransformer\n\ntransformer = ColumnTransformer([('text', CountVectorizer(), 'text')], remainder='passthrough')\nX = transformer.fit_transform(df)\n\nprint(X)\n[[0 0 0 1 0 0 1 1 1 0 0]\n [0 0 0 1 1 0 1 1 1 1 0]\n [1 0 2 0 0 0 1 1 0 0 1]\n [0 1 0 0 0 1 0 0 0 0 0]]\nprint(transformer.get_feature_names())\n['text__123', 'text__bananas', 'text__hi', 'text__is', 'text__more', 'text__oranges', 'text__some', 'text__text', 'text__this', 'is_it_upper', 'contains_num']\n\nText: More on your specific example: \nCode: X=df[['Text','is_it_capital?', 'is_it_upper?', 'contains_num?']]\ny=df['Label']\n\n# Need to use DenseTransformer to properly concatenate results\n# from CountVectorizer and other transformer steps\nfrom sklearn.base import TransformerMixin\nclass DenseTransformer(TransformerMixin):\n    def fit(self, X, y=None, **fit_params):\n        return self\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([\n     ('vectorizer', CountVectorizer()), \n     ('to_dense', DenseTransformer()), \n])\n\ntransformer = ColumnTransformer([('text', pipeline, 'Text')], remainder='passthrough')\n\nX_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.25, random_state=40)\n\nX_train = transformer.fit_transform(X_train)\nX_test = transformer.transform(X_test)\n\ndf_train = pd.concat([X_train, y_train], axis=1)\ndf_test = pd.concat([X_test, y_test], axis=1)\n\nAPI:\nsklearn.pipeline.FeatureUnion\nsklearn.compose.ColumnTransformer\n","label":[[248,272,"Mention"],[276,293,"Mention"],[2230,2259,"API"],[2260,2293,"API"]],"Comments":[]}
{"id":60654,"text":"ID:66382784\nPost:\nText: You should use ColumnTransformer: \nCode: >>> import pandas as pd\n>>> from sklearn.compose import ColumnTransformer\n>>> from sklearn.decomposition import PCA\n>>> df = pd.DataFrame({'c1': [1, 2, 3, 4],\n...                    'c2': [3., 5.5, 8., 10.5],\n...                    'c_to_preserve': [-5, -3, 6, 10]})\n>>> featurizer = ColumnTransformer([('pca', PCA(n_components=1), ['c1', 'c2']),\n...                                 ('preserve', 'passthrough', ['c_to_preserve'])])\n>>> featurizer.fit_transform(df)\narray([[ 4.03887361, -5.        ],\n       [ 1.3462912 , -3.        ],\n       [-1.3462912 ,  6.        ],\n       [-4.03887361, 10.        ]])\n\nText: Check ColumnTransformer for more information. \nAPI:\nsklearn.compose.ColumnTransformer\n","label":[[684,701,"Mention"],[730,763,"API"]],"Comments":[]}
{"id":60655,"text":"ID:66523050\nPost:\nText: Okay, months later I have got the answer ! \nText: You can use StackingRegressor , which allows you to use the output of a given estimator as the input of antoher estimator. \nText: In the case of my problem, a custom estimator easy to code would work. \nAPI:\nsklearn.ensemble.StackingRegressor\n","label":[[86,103,"Mention"],[281,315,"API"]],"Comments":[]}
{"id":60656,"text":"ID:66674553\nPost:\nText: This was answered over at stats.SE a few years ago: Sample weights scaling in SVC See also the documentation: \nText: sample_weight : array-like of shape (n_samples,), default=None Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points. \nText: To summarize, sample weights are incorporated into the SVM algorithm in sklearn by modifying the regularization parameter C. You can compensate for AdaBoost by increasing the parameter C in your underlying SVC. \nAPI:\nsklearn.svm.SVC\n","label":[[102,105,"Mention"],[544,559,"API"]],"Comments":[]}
{"id":60657,"text":"ID:66761365\nPost:\nText: Mainly, it is done for the sake of the re-usability. Rather than duplicating the code already implemented for StratifiedShuffleSplit, train_test_split just calls that class. For the same reason, when stratify=False, it uses the model_selection.ShuffleSplit class (see source code). \nText: Please note that duplicating code is considered a bad practice, because it assumed to inflate maintenance costs, but also considered defect-prone as inconsistent changes to code duplicates can lead to unexpected behavior. Here a reference if you'd like to learn more. \nText: Besides, although they perform the same task, they cannot be always used in the same contexts. For example, train_test_split cannot be used within a Random or Grid search with RandomizedSearchCV or sklearn.model_selection.GridSearchCV. The StratifiedShuffleSplit does. The reason is that the former is not \"an iterable yielding (train, test) splits as arrays of indices\". While the latter has a method split that yields (train, test) splits as array of indices. More info here (see parameter cv). \nAPI:\nsklearn.model_selection.RandomizedSearchCV\n","label":[[764,782,"Mention"],[1091,1133,"API"]],"Comments":[]}
{"id":60658,"text":"ID:66840196\nPost:\nText: Found the answer through trial and error. Answering my own question in case anyone was thinking like I did and needs clarity. \nText: Yes, if you use training data that spans the problem space, it is the same as running ridge regression in python using the equations. sklearn does what it says in the documentation. You need to use fit_intercept=True to get Ridge to fit the Y intercept of your problem, otherwise it is assumed to be zero. \nText: If you use the default, fit_intercept=False, and your problem does NOT have a Y-intercept of zero, you will of course, get a bad solution. \nText: This might lead a novice like me to the impression that you haven't supplied enough training data, which is incorrect. \nAPI:\nsklearn.linear_model.Ridge\n","label":[[381,386,"Mention"],[741,767,"API"]],"Comments":[]}
{"id":60659,"text":"ID:66873764\nPost:\nText: The most important feature does not necessarily mean that it will be the one used to make the first split. In fact, DecisionTreeClassifier uses entropy to decide which feature to use when making a split, so unless SelectKBest does this too, there is no need for both methods to reach the same conclusions in the same order. Even the same feature will reduce entropy differently in different stages of a tree classifier. \nText: As a side note, trees do not always consider all features when making nodes. Take a look at max_features here. This means that, depending on your random-state and max_features hyper parameters, your tree may or may not have considered worst_concave_points when making the first split. \nAPI:\nsklearn.tree.DecisionTreeClassifier\n","label":[[140,162,"Mention"],[742,777,"API"]],"Comments":[]}
{"id":60660,"text":"ID:66938313\nPost:\nText: You can use the warnings-module to temporarily suppress warnings. Either all warnings or specific warnings. \nText: In this case scikit-learn is raising a ConvergenceWarning so I suggest suppressing exactly that type of warning. That warning-class is located in ConvergenceWarning so import it beforehand and use the context-manager catch_warnings and the function simplefilter to ignore the warning, i.e. not print it to the screen: \nCode: import warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n    \n    optimizer_function_that_creates_warning()\n\n\nText: You can also ignore that specific warning globally to avoid using the context-manager: \nCode: import warnings\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n\noptimizer_function_that_creates_warning()\n\nText: I suggest using the context-manager though since you are sure about where you suppress warnings. This way you will not suppress warnings from unexpected places. \nAPI:\nsklearn.exceptions.ConvergenceWarning\n","label":[[285,303,"Mention"],[1075,1112,"API"]],"Comments":[]}
{"id":60661,"text":"ID:68055797\nPost:\nText: As per scikit-learn documentation for accuracy_score: \nText: for multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true. \nText: This means that each label will look something like [0,0,1,0] and will need identical match for a single Positive (so y_pred will need to be [0,0,1,0] as well), and anything that isn't [0,0,1,0] will result in a single Negative. \nText: In your manual function, you count each partial match separately: if y_true is [0,0,1,0] and y_pred is [0,1,0,0], you count this as 2 True Negatives (in position 0 and 3), 1 False Positive (position 1) and 1 False Negative (position 2). With the formula you use for accuracy, this results in ac = (0+2)\/(0+2+1+1), which gives 50% accuracy, while accuracy_score will be 0%. \nText: If you want to replicate scikit-learn accuracy_score manually, you would need to first check each member of y_array[i], and only then label it as one of the TP,TN,FP,FN. \nText: However seeing as you're dealign with multilabel classification, as per link above, you might want to check out sklearn.metrics.jaccard_score, sm.hamming_loss or zero_one_loss \nAPI:\nsklearn.metrics.accuracy_score\nsklearn.metrics.hamming_loss\nsklearn.metrics.zero_one_loss\n","label":[[848,862,"Mention"],[1202,1217,"Mention"],[1221,1234,"Mention"],[1241,1271,"API"],[1272,1300,"API"],[1301,1330,"API"]],"Comments":[]}
{"id":60662,"text":"ID:68210503\nPost:\nText: It looks like your column transformer is not selecting the categorical and numerical columns. You can fix that by using make_column_selector to select data based on their types. \nText: You can use it as follow: \nCode: from sklearn.compose import make_column_selector\npreprocessing = ColumnTransformer(\n    [('cat', categorical_encoder, make_column_selector(dtype_include=object)),\n     ('num', 'passthrough', make_column_selector(dtype_exclude=object))])\n\nAPI:\nsklearn.compose.make_column_selector\n","label":[[144,164,"Mention"],[485,521,"API"]],"Comments":[]}
{"id":60663,"text":"ID:68222368\nPost:\nText: As touched upon in the help page, the core of hdbscan is 1) calculating the mutual reachability distance and 2) applying the single linkage algorithm. Since you do not have that many data points and your distance metric is pre-computed, you can see your clustering is decided by the single linkage: \nCode: import numpy as np\nimport hdbscan\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.array([[0.0, 0.741, 0.344, 1.0, 0.062, 0.084],\n [0.741, 0.0, 0.648, 0.592, 0.678, 0.657],\n [0.344, 0.648, 0.0, 0.648, 0.282, 0.261],\n [1.0, 0.592, 0.655, 0.0, 0.937, 0.916],\n [0.062, 0.678, 0.282, 0.937, 0.0, 0.107],\n [0.084, 0.65, 0.261, 0.916, 0.107, 0.0]])\n\nclusterer = hdbscan.HDBSCAN(min_cluster_size=2,min_samples=1,\n                            metric='precomputed').fit(x)\nclusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)\n\nText: The results will be: \nCode: clusterer.labels_\n\n[0 1 0 1 0 0]\n\nText: Because the minimum number of clusters has to be 2. So the only way the achieve this is to have element 0,2,4,5 together. \nText: One quick solution is to simply cut the tree and get the cluster you intended: \nCode: clusterer.single_linkage_tree_.get_clusters(0.15, min_cluster_size=2)\n\n[ 0 -1 -1 -1  0  0]\n\nText: Or you simply use something from AgglomerativeClustering since you are not relying on hdbscan to calculate the distance metrics. \nAPI:\nsklearn.cluster.AgglomerativeClustering\n","label":[[1294,1317,"Mention"],[1396,1435,"API"]],"Comments":[]}
{"id":60664,"text":"ID:68408000\nPost:\nText: What is meant by bandwidth in scipy.stats.gaussian_kde and KernelDensity is not the same. Scipy.stats.gaussian_kde uses a bandwidth factor https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.gaussian_kde.html. For a 1-D kernel density estimation the following formula is applied: \nText: the bandwidth of KernelDensity = bandwidth factor of the scipy.stats.gaussian_kde * standard deviation of the sample \nText: For your estimation this probably means that your standard deviation equals 4. \nText: I would like to refer to Getting bandwidth used by SciPy's gaussian_kde function for more information. \nAPI:\nsklearn.neighbors.KernelDensity\nsklearn.neighbors.KernelDensity\n","label":[[83,96,"Mention"],[341,354,"Mention"],[643,674,"API"],[675,706,"API"]],"Comments":[]}
{"id":60665,"text":"ID:68421721\nPost:\nText: As per sk.pipeline.Pipeline documentation: \nText: **fit_paramsdict of string -> object Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p. \nText: This means that the parameters passed this way are directly passed to s step .fit() method. If you check PolynomialFeatures documentation, degree argument is used in construction of the PolynomialFeatures object, not in its .fit() method. \nText: If you want to try different hyperparameters for estimators\/transformators within a pipeline, you could use GridSearchCV as shown here. Here's an example code from the link: \nCode: from sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest\npipe = Pipeline([\n    ('select', SelectKBest()),\n    ('model', calibrated_forest)])\nparam_grid = {\n    'select__k': [1, 2],\n    'model__base_estimator__max_depth': [2, 4, 6, 8]}\nsearch = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)\n\nAPI:\nsklearn.pipeline.Pipeline\n","label":[[31,51,"Mention"],[1008,1033,"API"]],"Comments":[]}
{"id":60666,"text":"ID:68422801\nPost:\nText: Solution with MultiLabelBinarizer \nText: Assuming sequences is an array of integers with maximum possible value upto dimension-1, we can use MultiLabelBinarizer from preprocessing to replicate the behaviour of the function vectorize_sequences \nCode: from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer(classes=range(dimension))\nmlb.fit_transform(sequences)\n\nText: Solution with Numpy broadcasting \nText: Assuming sequences is an array of integers with maximum possible value upto dimension-1 \nCode: (np.array(sequences)[:, :, None] == range(dimension)).any(1).view('i1')\n\nText: Worked out example \nCode: >>> sequences\n[[4, 1, 0], \n [4, 0, 3],\n [3, 4, 2]]\n\n>>> dimension = 10\n>>> mlb = MultiLabelBinarizer(classes=range(dimension))\n>>> mlb.fit_transform(sequences)\n\narray([[1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0, 0, 0, 0]])\n\n\n>>> (np.array(sequences)[:, :, None] == range(dimension)).any(1).view('i1')\n\narray([[0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0]])\n\nAPI:\nsklearn.preprocessing\n","label":[[190,203,"Mention"],[1139,1160,"API"]],"Comments":[]}
{"id":60667,"text":"ID:68896933\nPost:\nText: The short answer in your case (using the LinearRegression model) is no, it is not possible to add one or two more examples and train without adding this to the original training set and fitting it all at the same time. Under the hood, the model is simply using Ordinary Least Squares (described here) which requires the complete matrix of training data on which to fit your model. However, this algorithm is very fast and in the case of ~ hundreds of training examples, it would be very quick to re-calculate the parameters of the model with each new couple examples. \nAPI:\nsklearn.linear_model.LinearRegression\n","label":[[65,81,"Mention"],[598,635,"API"]],"Comments":[]}
{"id":60668,"text":"ID:69153103\nPost:\nText: The second message is a warning, which is raised by the sk.metrics.precision_score function. To get rid of the warning you have to explicitly specify the desired behaviour via the 'zero_divison' argument. Note that the precision is calculated using the formula precision = Tp \/ (Tp + Fp). The warning tells you that a division by zero is being performed, which is an invalid operation. This means that your predicted values contain no positive label and all labels are predicted as negative. \nText: The first warning is raised for the same reason but the function doesn't offer an argument to specify what to do in case of a zero division. You might have to check for the values yourself before calling the function to avoid the warning. \nAPI:\nsklearn.metrics.precision_score\n","label":[[80,106,"Mention"],[768,799,"API"]],"Comments":[]}
{"id":60669,"text":"ID:69171277\nPost:\nText: The implementation in the OP is not the correct way to determine, or plot a linear model. As such, the question about determining the angle to plot the line is bypassed, and a more rigorous approach to plotting the regression line is shown. A regression line can be added by converting the datetime dates to ordinal. The model can be calculated with sklearn, or added to the plot with seaborn.regplot, as show below. Plot the full data with pandas.DataFrame.plot Tested in python 3.8.11, pandas 1.3.2, matplotlib 3.4.3, seaborn 0.11.2, sklearn 0.24.2 \nText: Imports and Data \nCode: import yfinance as yf\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# download the data\ndf = yf.download('aapl', '2015-01-01', '2021-01-01')\n\n# convert the datetime index to ordinal values, which can be used to plot a regression line\ndf.index = df.index.map(pd.Timestamp.toordinal)\n\n# display(df.iloc[:5, [4]])\n        Adj Close\nDate             \n735600  24.782110\n735603  24.083958\n735604  24.086227\n735605  24.423975\n735606  25.362394\n\n# convert the regression line start date to ordinal\nx1 = pd.to_datetime('2019-01-02').toordinal()\n\n# data slice for the regression line\ndata=df.loc[x1:].reset_index()\n\nText: Plot a Regression Line with seaborn \nText: Using seaborn.regplot no calculations are required to add the regression line to the line plot of the data. Convert the x-axis labels to datetime format Play around with the xticks and labels if you need the endpoints adjusted. \nCode: # plot the Adj Close data\nax1 = df.plot(y='Adj Close', c='k', figsize=(15, 6), grid=True, legend=False,\n              title='Adjusted Close with Regression Line from 2019-01-02')\n\n# add a regression line\nsns.regplot(data=data, x='Date', y='Adj Close', ax=ax1, color='magenta', scatter_kws={'s': 7}, label='Linear Model', scatter=False)\n\nax1.set_xlim(df.index[0], df.index[-1])\n\n# convert the axis back to datetime\nxticks = ax1.get_xticks()\nlabels = [pd.Timestamp.fromordinal(int(label)).date() for label in xticks]\nax1.set_xticks(xticks)\nax1.set_xticklabels(labels)\n\nax1.legend()\n\nplt.show()\n\nText: Calculate the Linear Model \nText: Use LinearRegression to calculate any desired points from the linear model, and then plot the corresponding line with matplotlib.pyplot.plot In regards to your other question, Extending the trendline of a stock chart to the right, you would calculate the model over a specified range, and then extend the line by predicting y1 and y2, given x1 and x2. This answer shows how to convert the ordinal axis values back to a date format. \nCode: # create the model\nmodel = LinearRegression()\n\n# extract x and y from dataframe data\nx = data[['Date']]\ny = data[['Adj Close']]\n\n# fit the mode\nmodel.fit(x, y)\n\n# print the slope and intercept if desired\nprint('intercept:', model.intercept_)\nprint('slope:', model.coef_)\n\nintercept: [-90078.45713565]\nslope: [[0.1222514]]\n\n# calculate y1, given x1\ny1 = model.predict(np.array([[x1]]))\n\nprint(y1)\narray([[28.27904095]])\n\n# calculate y2, given the last date in data\nx2 = data.Date.iloc[-1]\ny2 = model.predict(np.array([[x2]]))\n\nprint(y2)\narray([[117.40030862]])\n\n# this can be added to `ax1` with\nax1 = df.plot(y='Adj Close', c='k', figsize=(15, 6), grid=True, legend=False,\n              title='Adjusted Close with Regression Line from 2019-01-02')\nax1.plot([x1, x2], [y1[0][0], y2[0][0]], label='Linear Model', c='magenta')\nax1.legend()\n\nText: Angle of the Slope \nText: This is an artifact of the aspect of the axes, which is not equal for x and y. When the aspect is equal, see that the slope is 7.0 deg. \nCode: x = x2 - x1\ny = y2[0][0] - y1[0][0]\nslope = y \/ x\n\nprint(round(slope, 7) == round(model.coef_[0][0], 7))\n[out]:\nTrue\n\nangle = round(np.rad2deg(np.arctan2(y, x)), 1)\nprint(angle)\n[out]:\n7.0\n\n# given the existing plot\nax1 = df.plot(y='Adj Close', c='k', figsize=(15, 6), grid=True, legend=False,\n              title='Adjusted Close with Regression Line from 2019-01-02')\nax1.plot([x1, x2], [y1[0][0], y2[0][0]], label='Linear Model', c='magenta')\n\n# make the aspect equal\nax1.set_aspect('equal', adjustable='box')\n\nAPI:\nsklearn.linear_model.LinearRegression\n","label":[[2231,2247,"Mention"],[4197,4234,"API"]],"Comments":[]}
{"id":60670,"text":"ID:69672181\nPost:\nText: You are performing your ColumnTransformer based on columns defined in the cleaned_data DataFrame instead of columns defined in x_train. \nText: You can either modify your categorical and numerical features by computing them from x_train as follows: \nCode:  numerical_features = x_train.select_dtypes(include=['int64', 'float64']).columns\n    categorical_features = x_train.select_dtypes(include=['object']).columns\n\nText: Or even better, by using make_column_selector to perform the selection as follows: \nCode: from sklearn.compose import make_column_selector\npreprocessor_pipeline = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, make_column_selector(dtype_exclude=object)),\n            ('cat', categorical_transformer, make_column_selector(dtype_include=object))\n        ])\n\nAPI:\nsklearn.compose.make_column_selector\n","label":[[470,490,"Mention"],[844,880,"API"]],"Comments":[]}
{"id":60671,"text":"ID:69845267\nPost:\nText: You can replicate the results of scipy.stats.linregress using LinearRegression as follows: \nCode: import numpy as np\nfrom scipy.stats import linregress\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# input data\ny = np.array([76.6, 118.6, 200.8, 362.3, 648.9])\nx = np.array([10, 20, 40, 80, 160])\n\n# scipy linear regression\nslope, intercept, r_value, p_value, std_err = linregress(x, y)\ny_pred = intercept + slope * x\n\nmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=True)\nrmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=False)\n\nprint('scipy intercept: {:.6f}'.format(intercept))\nprint('scipy slope: {:.6f}'.format(slope))\nprint('scipy MSE: {:.6f}'.format(mse))\nprint('scipy RMSE: {:.6f}'.format(rmse))\n# scipy intercept: 45.058333\n# scipy slope: 3.812608\n# scipy MSE: 49.793366\n# scipy RMSE: 7.056441\n\n# sklearn linear regression\nreg = LinearRegression().fit(x.reshape(- 1, 1), y)\ny_pred = reg.predict(x.reshape(- 1, 1))\n\nmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=True)\nrmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=False)\n\nprint('sklearn intercept: {:.6f}'.format(reg.intercept_))\nprint('sklearn slope: {:.6f}'.format(reg.coef_[0]))\nprint('sklearn MSE: {:.6f}'.format(mse))\nprint('sklearn RMSE: {:.6f}'.format(rmse))\n# sklearn intercept: 45.058333\n# sklearn slope: 3.812608\n# sklearn MSE: 49.793366\n# sklearn RMSE: 7.056441\n\nAPI:\nsklearn.linear_model.LinearRegression\n","label":[[86,102,"Mention"],[1457,1494,"API"]],"Comments":[]}
{"id":60672,"text":"ID:70215369\nPost:\nText: This is probably because you are using an older scikit-learn version than the one this code was written for. \nText: get_feature_names_out is a method of the class TfiidfVectorizer since scikit-learn 1.0. Previously, there was a similar method called get_feature_names. \nText: So you should update your scikit-learn package, or use the old method (not recommended). \nAPI:\nsklearn.feature_extraction.text.TfidfVectorizer\n","label":[[187,203,"Mention"],[395,442,"API"]],"Comments":[]}
{"id":60673,"text":"ID:70279497\nPost:\nText: Point is that you're using predict() rather than predict_proba()\/decision_function() to define your y_hat. This means - considering that the threshold vector is defined by the number of distinct values in y_hat (see here for reference), that you'll have few thresholds per class only on which tpr and fpr are computed (which in turn implies that your curves are evaluated at few points only). Indeed, consider what the doc says to pass to y_scores in roc_curve(), either prob estimates or decision values. In the example from sklearn, decision values are used to compute the scores. Given that you're considering a RandomForestClassifier(), considering probability estimates in your y_hat should be the way to go. What's the point then of label-binarizing the output? The standard definition for ROC is in terms of binary classification. To pass to a multiclass problem, you have to convert your problem into binary by using OneVsAll approach, so that you'll have n_class number of ROC curves. (Observe, indeed, that as SVC() handles multiclass problems in a OvO fashion by default, in the example they had to force to use OvA by applying OneVsRestClassifier constructor; with a RandomForestClassifier you don't have such problem as that's inherently multiclass, see here for reference). In these terms, once you switch to predict_proba() you'll see there's no much sense in label binarizing predictions. # all imports import numpy as np import matplotlib.pyplot as plt from itertools import cycle from sklearn import svm, datasets from sklearn.metrics import roc_curve, auc from sklearn.model_selection import train_test_split from pre import label_binarize from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier # dummy dataset X, y = make_classification(10000, n_classes=5, n_informative=10, weights=[.04, .4, .12, .5, .04]) train, test, ytrain, ytest = train_test_split(X, y, test_size=.3, random_state=42) # random forest model model = RandomForestClassifier() model.fit(train, ytrain) yhat = model.predict_proba(test) def plot_roc_curve(y_test, y_pred): n_classes = len(np.unique(y_test)) y_test = label_binarize(y_test, classes=np.arange(n_classes)) # Compute ROC curve and ROC area for each class fpr = dict() tpr = dict() roc_auc = dict() thresholds = dict() for i in range(n_classes): fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], y_pred[:, i], drop_intermediate=False) roc_auc[i] = auc(fpr[i], tpr[i]) # Compute micro-average ROC curve and ROC area fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel()) roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"]) # First aggregate all false positive rates all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) # Then interpolate all ROC curves at this points mean_tpr = np.zeros_like(all_fpr) for i in range(n_classes): mean_tpr += np.interp(all_fpr, fpr[i], tpr[i]) # Finally average it and compute AUC mean_tpr \/= n_classes fpr[\"macro\"] = all_fpr tpr[\"macro\"] = mean_tpr roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"]) # Plot all ROC curves #plt.figure(figsize=(10,5)) plt.figure(dpi=600) lw = 2 plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]), color=\"deeppink\", linestyle=\":\", linewidth=4,) plt.plot(fpr[\"macro\"], tpr[\"macro\"], label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]), color=\"navy\", linestyle=\":\", linewidth=4,) colors = cycle([\"aqua\", \"darkorange\", \"darkgreen\", \"yellow\", \"blue\"]) for i, color in zip(range(n_classes), colors): plt.plot(fpr[i], tpr[i], color=color, lw=lw, label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i]),) plt.plot([0, 1], [0, 1], \"k--\", lw=lw) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\"False Positive Rate\") plt.ylabel(\"True Positive Rate\") plt.title(\"Receiver Operating Characteristic (ROC) curve\") plt.legend() \nText: Eventually, consider that roc_curve() has also a drop_intermediate parameter meant for dropping suboptimal thresholds (it might be useful to know). \nAPI:\nsklearn.preprocessing\n","label":[[1657,1660,"Mention"],[4104,4125,"API"]],"Comments":[]}
{"id":60674,"text":"ID:70486259\nPost:\nText: I am not an expert with scikit-learn but it seems that one of the requirements of various objects used by this framework is that they can be cloned by calling the clsone method. This appears to be something that the existing XGBRegressor class does, so is something your subclass of XGBRegressor must also do. \nText: What may help is to pass any other unexpected keyword arguments as a **kwargs parameter. In your constructor, kwargs will contain a dict of all of the other keyword parameters that weren't assigned to other constructor parameters. You can pass this dict of parameters on to the call to the superclass constructor by referring to them as **kwargs again: this will cause Python to expand them out: \nCode: class XGBoostQuantileRegressor(XGBRegressor):\n    def __init__(self, quant_alpha, max_depth=3, **kwargs):\n        self.quant_alpha = quant_alpha\n        super().__init__(max_depth=max_depth, **kwargs)\n\n    # other methods unchanged and omitted for brevity.\n\nText: I have answered a question from you previously, and I will reiterate here two points I made in that answer. \nText: Firstly, I am not a data scientist. I have never worked with scikit-learn before, so I have not tested the code I posted above. \nText: Secondly, this is another situation where I believe you should prefer composition over inheritance. You have chosen to use inheritance, and you have hit a problem because of that choice. If your class did not inherit from XGBRegressor but instead had simply created an XGBRegressor and stored it in an attribute, e.g. using a line self.xgb_regressor = XGBRegressor(max_depth=max_depth), and the calls to predict and fit had called self.xgb_regressor.predict and self.xgb_regressor.fit, you wouldn't have had this problem. \nAPI:\nsklearn.base.clone\n","label":[[187,193,"Mention"],[1786,1804,"API"]],"Comments":[]}
{"id":60675,"text":"ID:70670856\nPost:\nText: As you might see from the source code (within the call to _binary_clf_curve(), in turn called by roc_curve() here) the number of thresholds is actually defined by the number of distinct predictions_test (scores, in principle). From your output, however, I would suppose predictions_test might be the output of .predict() (perhaps of a multiclass classification problem? - in which case by the way you'll need to extend the ROC curve definition to deal with multiclass setting) rather than of .predict_proba() or .decision_function() as roc_curve requires. \nText: Moreover, be aware that roc_curve also has a parameter drop_intermediate (default to True) which, in some cases, might drop suboptimal thresholds. \nText: Eventually, I'd suggest the following posts: \nText: Plotting the ROC curve for a multiclass problem for the ROC curve extension to a multiclass setting; sm.roc_curve only shows 5 fprs, tprs, thresholds or sklearn's roc_curve() function returns thresholds and fpr of different dimensions for a better understanding of the implications of the parameter drop_intermediate=True. \nAPI:\nsklearn.metrics.roc_curve\n","label":[[894,906,"Mention"],[1122,1147,"API"]],"Comments":[]}
{"id":60676,"text":"ID:70900442\nPost:\nText: (EDIT: Edited for NMAPE instead of NMAE) \nText: You can use make_scorer over a custom function to get what you need. Here is some helper code below. \nText: The definition of NMAPE is defined based on the formula from this post. It's simply the negative of the below equation - \nCode: import numpy as np\nfrom sklearn.metrics import make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n#define custom function which returns single output as metric score\ndef NMAPE(y_true, y_pred): \n    return 1 - np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n#make scorer from custome function\nnmape_scorer = make_scorer(NMAPE)\n\n#dummy data\nX = np.random.random((1000,10))\ny = np.random.random(1000,)\n\n#cross validation score on model\nreg = LinearRegression()\ncross_val_score(reg, X, y, scoring=nmape_scorer, cv=5)\n\nCode: array([-4453.67565485,  -148.201211  ,  -222.92820259,  -185.27855064,\n        -657.27927049])\n\nAPI:\nsklearn.metrics.make_scorer\n","label":[[84,95,"Mention"],[996,1023,"API"]],"Comments":[]}
{"id":60677,"text":"ID:70934371\nPost:\nText: As quickly sketched in the comment there are a couple of considerations to be done on your example: \nText: method .fit_transform() generally returns either a sparse matrix or a numpy array. Returning a sparse matrix serves the purpose of saving memory; think to the example where you one-hot-encode a categorical attribute with many categories. You'll end up having a matrix with many columns and a single non-zero entry per row; with a sparse matrix you can store the location of the non-zero element only. In these situation you can call .toarray() on the output of .fit_transform() to get a numpy array back to be passed to the pd.DataFrame constructor. Actually, on a five-rows dataset similar to the one you provided df = pd.DataFrame({ 'department': ['operations', 'operations', 'support', 'logistics', 'sales'], 'review': [0.577569, 0.751900, 0.722548, 0.675158, 0.676203], 'projects': [3, 3, 3, 4, 3], 'salary': ['low', 'medium', 'medium', 'low', 'high'], 'satisfaction': [0.626759, 0.751900, 0.722548, 0.675158, 0.676203], 'bonus': [0, 0, 0, 0, 1], 'avg_hrs_month': [180.866070, 182.708149, 184.416084, 188.707545, 179.821083], 'left': [0, 0, 1, 0, 0] }) ord_features = [\"salary\"] ordinal_transformer = OrdinalEncoder() cat_features = [\"department\"] categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\") ct = ColumnTransformer(transformers=[ (\"ord\", ordinal_transformer, ord_features), (\"cat\", categorical_transformer, cat_features), ]) I can't reproduce your issue (namely, I directly obtain a numpy array), but basically pd.DataFrame(ct.fit_transform(df).toarray()) should work for your case. This is the output you would get: As you can see, with respect to your expected output, this only contains the transformed (ordinally encoded) salary column as first column and the transformed (one-hot-encoded) department column from the second to the last column. That's because, as you can see within the docs, parameter remainder is set to 'drop' by default, which implies that all columns which are not subject to transformation are dropped. To avoid this, you should set it to 'passthrough'; this will help you to transform the columns you need and keep the other untouched. ct = ColumnTransformer(transformers=[ (\"ord\", ordinal_transformer, ord_features), (\"cat\", categorical_transformer, cat_features )], remainder='passthrough' ) This would be the output of your pd.DataFrame(ct.fit_transform(df).toarray()) in such a case: Again, as you can see also column order is not the one you would expect after the transformation. Long story short, that's because in a ColumnTransformer \nText: The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right to the output of the transformers. \nText: I would aggest reading Preserve column order after applying sk.compose.ColumnTransformer at this proposal. \nText: Eventually, for what concerns column names you should probably apply a custom solution passing what you want directly to the columns parameter to be passed to the pd.DataFrame constructor. Indeed, OrdinalEncoder (differently from OneHotEncoder) does not provide a .get_feature_names_out() method that makes it generally easy to pass columns=ct.get_feature_names_out() to the pd.DataFrame constructor. See ColumnTransformer & Pipeline with OHE - Is the OHE encoded field retained or removed after ct is performed? for an example of its usage. \nText: Update 10\/2022 - sklearn version 1.2.dev0 \nText: With sklearn version 1.2.0 it will be possible to solve the problem of returning a DataFrame when transforming a ColumnTransformer instance much more easily. Such version has not been released yet, but you can test the following in dev (version 1.2.dev0), by installing the nightly builds as such: \nCode: pip install --pre --extra-index https:\/\/pypi.anaconda.org\/scipy-wheels-nightly\/simple scikit-learn -U\n\nText: The ColumnTransformer (and other transformers as well) now exposes a .set_output() method which gives the possibility to configure a transformer to output pandas DataFrames, by passing parameter transform='pandas' to it. \nText: Therefore, the example becomes: \nCode: import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\ndf = pd.DataFrame({\n    'department': ['operations', 'operations', 'support', 'logistics', 'sales'],\n    'review': [0.577569, 0.751900, 0.722548, 0.675158, 0.676203],\n    'projects': [3, 3, 3, 4, 3],\n    'salary': ['low', 'medium', 'medium', 'low', 'high'],\n    'satisfaction': [0.626759, 0.751900, 0.722548, 0.675158, 0.676203],\n    'bonus': [0, 0, 0, 0, 1],\n    'avg_hrs_month': [180.866070, 182.708149, 184.416084, 188.707545, 179.821083],\n    'left': [0, 0, 1, 0, 0]\n})\n\nord_features = [\"salary\"]\nordinal_transformer = OrdinalEncoder()\n\ncat_features = [\"department\"]\ncategorical_transformer = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n\nct = ColumnTransformer(transformers=[\n    (\"ord\", ordinal_transformer, ord_features),\n    (\"cat\", categorical_transformer, cat_features )],\n    remainder='passthrough'\n)\n\nct.set_output('pandas')\ndf_pandas = ct.fit_transform(df)\ndf_pandas\n\nText: The output also becomes much easier to read as it has proper column names (indeed, at each step, the transformers of which ColumnTransformer is made of do have the attribute feature_names_in_; so you don't lose column names anymore while transforming the input). \nText: Last note. Observe that the example now requires parameter sparse_output=False to be passed to the OneHotEncoder instance in order to work. \nAPI:\nsklearn.compose.ColumnTransformer\n","label":[[3100,3128,"Mention"],[6054,6087,"API"]],"Comments":[]}
{"id":60678,"text":"ID:71550429\nPost:\nText: I've found out that in SGDRegressor MAE loss is a special case of 'epsilon_insensitive' loss with epsilon equal to 0. And according to source code of this loss we simply apply sign(x) function to difference of ground truth and predicted values in order to calculate derivative. \nAPI:\nsklearn.linear_model.SGDRegressor\n","label":[[47,59,"Mention"],[308,341,"API"]],"Comments":[]}
{"id":60679,"text":"ID:71657793\nPost:\nText: In the case you don't want to use pandas.DataFrame in your REST API endpoint, just don't train your model with the DataFrame but convert your data to a numpy array first: \nCode: >>> df\n                    TEXT_1                TEXT_2    NUM_1  NUM_2\n0  This is the first text.      The second text.  300.000   23.3\n1  Here is the third text.  And the fourth text.    2.334   29.0\n>>> df.to_numpy()\narray([['This is the first text.', 'The second text.', 300.0, 23.3],\n       ['Here is the third text.', 'And the fourth text.', 2.334, 29.0]],\n      dtype=object)\n\nText: Then, make changes in how you define the model. I'd suggest to combine preprocessing and predicting steps using _Pipeline into a single model like this: \nCode: from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = Pipeline(steps=[\n    ('transformer', ColumnTransformer(\n        transformers=[\n            ('TEXT_1', CountVectorizer(analyzer='word', stop_words='english'), 0),\n            ('TEXT_2', CountVectorizer(analyzer='word', stop_words='english'), 1),\n        ],\n        remainder='passthrough',\n    )),\n    ('predictor', RandomForestClassifier()),\n])\n\nText: Note, here we are using indices instead of names to reference texts when defining transformers for the ColumnTransformer instance. Once we've transformed the initial DataFrame to a numpy array, the TEXT_1 feature is located at 0, and the TEXT_2 at 1 in a data row. Here is how you can use the model: \nCode: from joblib import dump, load\n\nX = df.to_numpy()\nmodel.fit(X, y)\ndump(model, 'model.joblib')\n\n...\n\nmodel = load('model.joblib')\nresults = model.predict(data)\n\nText: As a result, you don't have to convert your incoming data to the DataFrame in order to make a prediction. \nAPI:\nsklearn.pipeline.Pipeline\n","label":[[704,713,"Mention"],[1893,1918,"API"]],"Comments":[]}
{"id":60680,"text":"ID:72279542\nPost:\nText: The sklearn.datasets.mldata module was deprecated in version 0.20 and will be removed in version 0.22. You can use the from sklearn.datasets import fetch_openml function instead. \nText: For more information, see the sk.datasets.fetch_openml documentation: \nText: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.fetch_openml.html \nAPI:\nsklearn.datasets.fetch_openml\n","label":[[240,264,"Mention"],[378,407,"API"]],"Comments":[]}
{"id":60681,"text":"ID:72507230\nPost:\nText: If I Understand Carefully (IIUC), you can use the min-max scaling formula: \nText: You can apply this formula to your dataframe with implemented MinMaxScaler like below: \nCode: from sklearn.preprocessing import MinMaxScaler\n\ndf2 = df.pivot(columns='ticker', values='value')\n# ticker   eur_jpy   usd_cad\n# 0       3.204568  0.021455\n# 1       1.144708  0.013810\n# ...\n# 48      1.906116  0.002058\n# 49      1.136424  0.022451\n\ndf2[['min_max_scl_eur_jpy', 'min_max_scl_usd_cad']] = MinMaxScaler().fit_transform(df2[['eur_jpy', 'usd_cad']])\nprint(df2)\n\nText: Output: \nCode: ticker   eur_jpy   usd_cad  min_max_scl_eur_jpy  min_max_scl_usd_cad\n0       3.204568  0.021455             0.827982             0.896585\n1       1.144708  0.013810             0.264398             0.567681\n2       2.998154  0.004580             0.771507             0.170540\n3       1.916517  0.003275             0.475567             0.114361\n4       0.955089  0.009206             0.212517             0.369558\n5       3.036463  0.019500             0.781988             0.812471\n6       1.240505  0.006575             0.290608             0.256373\n7       1.224260  0.020711             0.286163             0.864584\n8       3.343022  0.020564             0.865864             0.858280\n9       2.710383  0.023359             0.692771             0.978531\n10      1.218328  0.008440             0.284540             0.336588\n11      2.005472  0.022898             0.499906             0.958704\n12      2.056680  0.016429             0.513916             0.680351\n13      1.010388  0.005553             0.227647             0.212368\n14      3.272408  0.000620             0.846543             0.000149\n15      2.354457  0.018608             0.595389             0.774092\n16      3.297936  0.017484             0.853528             0.725720\n17      2.415297  0.009618             0.612035             0.387285\n18      0.439263  0.000617             0.071386             0.000000\n19      3.335262  0.005988             0.863740             0.231088\n20      2.767412  0.013357             0.708375             0.548171\n21      0.830678  0.013824             0.178478             0.568255\n22      1.056041  0.007806             0.240138             0.309336\n23      1.497400  0.023858             0.360896             1.000000\n24      0.629698  0.014088             0.123489             0.579604\n25      3.758559  0.020663             0.979556             0.862509\n26      0.964214  0.010302             0.215014             0.416719\n27      3.680324  0.023647             0.958150             0.990918\n28      3.169445  0.017329             0.818372             0.719059\n29      1.898905  0.017892             0.470749             0.743299\n30      3.322663  0.020508             0.860293             0.855869\n31      2.735855  0.010578             0.699741             0.428591\n32      2.264645  0.017853             0.570816             0.741636\n33      2.613166  0.021359             0.666173             0.892456\n34      1.976168  0.001568             0.491888             0.040928\n35      3.076169  0.013663             0.792852             0.561335\n36      3.330470  0.013048             0.862429             0.534891\n37      3.600527  0.012340             0.936318             0.504426\n38      0.653994  0.008665             0.130137             0.346288\n39      0.587896  0.013134             0.112052             0.538567\n40      0.178353  0.011326             0.000000             0.460781\n41      3.727127  0.016738             0.970956             0.693658\n42      1.719622  0.010939             0.421696             0.444123\n43      0.460177  0.021131             0.077108             0.882665\n44      3.124722  0.010328             0.806136             0.417826\n45      1.011988  0.007631             0.228085             0.301799\n46      3.833281  0.003896             1.000000             0.141076\n47      3.289872  0.017223             0.851322             0.714495\n48      1.906116  0.002058             0.472721             0.062020\n49      1.136424  0.022451             0.262131             0.939465\n\nAPI:\nsklearn.preprocessing.MinMaxScaler\n","label":[[168,180,"Mention"],[4119,4153,"API"]],"Comments":[]}
{"id":60682,"text":"ID:72659154\nPost:\nText: Whenever you come across things like this, unpack the code a bit and look at what each piece is giving you. \nText: In this case, the docs for euclidean_distances  which I assume you are using (please include this sort of information in your questions!)  also tell you what you need to know: \nText: Returns: distances: ndarray of shape (n_samples_X, n_samples_Y) \nText: So the function euclidean_distances(X, Y) returns a 2D array of all the distances between the points in X and the points in Y. Your X is all your data, and your Y is just one point: the centroid of the cluster. Because Y is only one point, your resulting distance matrix has only one column. Like this: \nCode: from sklearn.metrics import euclidean_distances\nimport numpy as np\n\nX = np.array([[1, 3], [2, 5], [0, 4]])\neuclidean_distances(X, [[0, 0]])\n\nText: This gives: \nCode: array([[3.16227766],\n       [5.38516481],\n       [4.        ]])\n\nText: So the index [:, 0] is getting this column. In fact, you could skip the indexing because np.max() doesn't care: it's just going to give you the max of the entire array. So you could reduce your code to: \nCode: radius = euclidean_distances(X, [center]).max()\n\nAPI:\nsklearn.metrics.pairwise.euclidean_distances\n","label":[[166,185,"Mention"],[1206,1250,"API"]],"Comments":[]}
{"id":60683,"text":"ID:72686959\nPost:\nText: I see in your question, that you use groupby on ['id','plat'] columns then I write the answer with groupby and use apply on it and create the dataframe for mean_absolute_percentage_error for columns that you want. \nCode: from sklearn.metrics import mean_absolute_percentage_error\n\ncols = [['d3_d30'], ['d7_d30', 'd14_d30']]\nlst = []\ndef f_mape(x):\n    dct = {}\n    for col in cols:\n        for c in col:\n            dct[f'real_{c}'] = mean_absolute_percentage_error(x['d30_real'], x[c])\n    lst.append(dct)\n\ndf1.groupby(['id', 'plat']).apply(lambda x: f_mape(x))\nprint(pd.DataFrame(lst))\n\nText: Output: \nCode:    real_d3_d30  real_d7_d30  real_d14_d30\n0     0.083333     0.166667      0.000000\n1     0.000000     0.066667      0.000000\n2     0.071429     0.071429      0.071429\n3     0.052632     0.000000      0.052632\n\nAPI:\nsklearn.metrics.mean_absolute_percentage_error\n","label":[[180,210,"Mention"],[850,896,"API"]],"Comments":[]}
{"id":60684,"text":"ID:72726582\nPost:\nText: Model score is a measure of the model certainty of the outcome. However, it's not necessarily the same as probability: it does not mean 83% people with 0.83 score leaving yet. Logistic regression scores are probabilities by design, but for random forest behaviour is implementation defined. If you seek to integrate your scores into business metrics directly, you'll need to calibrate your model first (using e.g. aalibratedClassifierCV or isotonic regression). \nAPI:\nsklearn.calibration.CalibratedClassifierCV\n","label":[[438,460,"Mention"],[492,534,"API"]],"Comments":[]}
{"id":60685,"text":"ID:72728033\nPost:\nText: The ms.KFold function is a utility that provides the folds but does not actually perform k-fold validation. You have to implement this yourself! \nText: See documentation description: \nText: Provides train\/test indices to split data in train\/test sets. Split dataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining folds form the training set. \nAPI:\nsklearn.model_selection.KFold\n","label":[[28,36,"Mention"],[452,481,"API"]],"Comments":[]}
{"id":60686,"text":"ID:74064311\nPost:\nText: There are a couple of things going on here that we need to disentangle. First, what happened to sparseness? Second, how do you generate sparse faces using the sklearn function? \nText: Where did the sparseness go? \nText: The skd.NMF function went through a major change from versions 0.16 to 0.19. There are multiple ways to implement nonnetative matrix factorization. \nText: Before 0.16, NMF used projected gradient descent as described in Hoyer 2004, and included a sparseness parameter (which as OP noted let you adjust the sparseness of the resulting W basis). \nText: Because of various limitations outlined in this extremely thorough issue at sklearn's github repo, it was decided to move on to two additional methods: \nText: Release 0.16: coordinate descent (PR here which was in version 0.16) Release 0.19: multiplicative update (PR here which was in version 0.19) \nText: This was a pretty major undertaking, and the upshot is we now have a great deal more freedom in terms of error functions, initialization, and regularization. You can read about that at the issue. The objective function is now: \nText: You can read more details\/explanation at the docs, but to note a few things relevant to the question: \nText: The solver param which takes in mu for multiplicative update or cd for coordinate descent. The older projected gradient descent method (with the sparseness parameter) is deprecated. As you can see in the objective function, there are weights for regularizing W and for H (alpha_W and alpha_H respectively). In theory if you want to reign in W, you should increase alpha_W. You can regularize using the L1 or L2 norm, and the ratio between the two is set by l1_ratio. The larger you make l1_ratio, the more you weight the L1 norm over L2 norm. Note: the L1 norm tends to generate more sparse parameter sets, while the L2 norm tends to generate small parameter sets, so in theory if you want sparseness, then set your l1_ratio high. \nText: How to generate sparse faces? \nText: The examination of the objective function suggests what to do. Crank up alpha_W and l1_ratio. But also note that the Lee and Seung paper used multiplicative update (mu), so if you wanted to reproduce their results, I would recommend setting solver to mu, setting alpha_W high, and l1_ratio high, and see what happens. \nText: In the OP's question, they implicitly used the cd solver (which is the default), and set alpha_W=0.01 and l1_ratio=0, which I wouldn't necessarily expect to create a sparse basis set. \nText: But things are actually not that simple. I tried some initial runs of coordinate descent with high l1_ratio and alpha_W and found very low sparseness. So to quantify some of this, I did a grid search, and used a sparseness measure. \nText: Quantifying sparseness is itself a cottage industry (e.g., see this post, and the paper cited there). I used Hoyer's measure of sparsity, adapted from the one used in the nimfa package: \nCode: def sparseness_hoyer(x):\n    \"\"\"\n    The sparseness of array x is a real number in [0, 1], where sparser array\n    has value closer to 1. Sparseness is 1 iff the vector contains a single\n    nonzero component and is equal to 0 iff all components of the vector are \n    the same\n        \n    modified from Hoyer 2004: [sqrt(n)-L1\/L2]\/[sqrt(n)-1]\n    \n    adapted from nimfa package: https:\/\/nimfa.biolab.si\/\n    \"\"\"\n    from math import sqrt # faster than numpy sqrt \n    eps = np.finfo(x.dtype).eps if 'int' not in str(x.dtype) else 1e-9\n    \n    n = x.size\n\n    # measure is meant for nmf: things get weird for negative values\n    if np.min(x) < 0:\n        x -= np.min(x)\n        \n    # patch for array of zeros\n    if np.allclose(x, np.zeros(x.shape), atol=1e-6):\n        return 0.0\n    \n    L1 = abs(x).sum()\n    L2 = sqrt(np.multiply(x, x).sum())\n    sparseness_num = sqrt(n) - (L1 + eps) \/ (L2 + eps)\n    sparseness_den = sqrt(n) - 1\n    \n    return sparseness_num \/ sparseness_den\n\nText: What this measures actually quantifies is sort of complicated, but roughly a sparse image is one with only a few pixels active, a non-sparse image has lots of pixels active. If we run PCA on the faces example from the OP, we can see the sparseness values is low around 0.04 for the eigenfaces: \nText: Sparsifying using coordinate descent? \nText: If we run NMF using the params used in the OP (using coordinate descent, with low W_alpha and l1_ratio, except with 200 components), the sparseness values are again low: \nText: If you look at the histogram of sparseness values this is verified: \nText: Different, but not super impressive, compared with PCA. \nText: I next did a grid search through W_alpha and l1_ratio space, varying them between 0 and 1 (at 0.1 step increments). I found that sparsity was not maximized when they were 1. Surprisingly, contrary to theoretical expectations, I found that sparsity was only high when l1_ratio was 0 and it dropped of precipitously above 0. And within this slice of parameters, sparsity was maximized when alpha_W was 0.9: \nText: Intuitively, this is a huge improvement. There is still a lot of variation in the distribution of sparseness values, but they are much higher: \nText: However, maybe in order to replicate the Lee and Seung results, and better control sparseness, we should be using multiplicative update (which is what they used). Let's try that next. \nText: Sparsifying using multiplicative update \nText: For the next attempt, I used multiplicative update, and this behaved much more as expected, with sparse, parts-based representations emerging: \nText: You can see the drastic difference, and this is reflected in the histogram of sparseness values: \nText: Note the code to generate this is below. \nText: One final interesting thing to note: the sparseness values with this method seem to increase with the component number. I plotted sparseness as a function of component, and this is (roughly) born out, and was born out consistently over all my runs of the algorithm: \nText: I have not seen this discussed elsewhere, so thought I'd mention it. \nText: Code to generate sparse representation of faces using the mu NMF algorithm: \nCode: from sklearn.datasets import fetch_olivetti_faces \nimport matplotlib.pyplot as plt \nimport numpy as np\nfrom sklearn.decomposition import NMF\n\nfaces, _ = fetch_olivetti_faces(return_X_y=True) \n\nnum_nmf_components = 200\nalph_W = 0.9  # cd: .9, mu: .9\nL1_ratio = 0.9 # cd: 0, L1_ratio: 0.9\n\ntry:\n    del estimator\nexcept:\n    print(\"first run\")\n    \nestimator = NMF(num_nmf_components, \n                init='nndsvdar', # nndsvd\n                solver='mu', \n                max_iter=50,\n                alpha_W=alph_W,\n                alpha_H=0, zeros\n                l1_ratio=L1_ratio,\n                shuffle=True)\n\nH = estimator.fit_transform(faces)\nW = estimator.components_\n\n# plot the basis faces\nn_row, n_col = 5, 7 # how many faces to plot\nimage_shape = (64, 64)\nn_samples, n_features = faces.shape\nplt.figure(figsize=(10,12))\nfor face_id, face in enumerate(W[:n_row*n_col]):\n    plt.subplot(n_row, n_col, face_id+1)\n    face_sparseness = sparseness_hoyer(face)\n    plt.imshow(face.reshape(image_shape), cmap='gray')\n    plt.title(f\"{face_sparseness: 0.2f}\")\n    plt.axis('off')\nplt.suptitle('NMF', fontsize=16, y=1)\nplt.tight_layout()\n\nAPI:\nsklearn.decomposition.NMF\n","label":[[248,255,"Mention"],[7305,7330,"API"]],"Comments":[]}
{"id":60687,"text":"ID:74435190\nPost:\nText: Check type of tree model \nText: As @Alexander Santos suggests, you can use the method from this answer to check which module your class belongs to. As far as I can tell, the tree based models are either a part of tree or sklearn.ensemble._tree modules. \nCode: # Method 1: check if object type has __module__ attribute\nmodule = getattr(clf, '__module__', '')\n\nif module.startswith('sklearn.tree') or module.startswith('sklearn.ensemble._tree'):\n    print(\"clf is a tree model\")\n\nText: Alternatively, a less python-esque method is to convert the type to a string and perform the same comparison. \nCode: # Method 2: convert type to string\ntype_ = str(type(clf))\n\nif \"sklearn.tree\" in type_ or \"sklearn.ensemble._tree\" in type_:\n    print(\"Clf is probably a tree model\")\n\nText: You can obviously rewrite this more efficiently if you need to test against many more than just two modules. \nText: Alternative 'hack' \nText: By inspecting the methods of DecisionTree, RandomForest and ExtraTrees regressor and classifiers using dir(clf), it appears all the models you want to test for have methods such as: \nText: min_samples_leaf min_weight_fraction_leaf max_leaf_nodes \nText: So if you really needed one check to validate your model type, you can inspect the model's methods: \nCode: attributes = dir(clf)\n\ncheck_for_list = ['min_samples_leaf', 'min_weight_fraction_leaf', 'max_leaf_nodes']\n\nverdict = False\nfor check in check_for_list:\n    if check in attributes:\n        verdict = True\n        break\n\nif verdict:\n    print(\"clf is probably a tree-based model.\")\n\n\nAPI:\nsklearn.tree\n","label":[[237,241,"Mention"],[1587,1599,"API"]],"Comments":[]}
{"id":60688,"text":"ID:74511330\nPost:\nText: Ok, here is a solution using 250 data points of GOOG stock Close historical data. I have explained the code with comments. Please feel free to ask if there is something vague in there. As you can see, I use pandas and within that library is a convenience function \"rolling\" that computes, among other things, rolling means. I split the data set by hand, but it can also be done by e.g. sk.model_selection.train_test_split \nCode: import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Read data from file\ndf = pd.read_csv(\"GOOG.csv\")\n\n# Calculate 10 day rolling mean and drop first 10 rows because we cannot calculate rolling mean for them\n# shift moves the averages one step ahead so day 10 gets moving average of days 0-9, etc...\ndf[\"Rolling_10d_close\"] = df['Close'].rolling(10).mean().shift(1)\ndf = df.dropna()\n\n# Split data into training and validation sets\ntraining_last_row = int(len(df) * 0.8)\ntraining_data = df.iloc[:training_last_row]\nvalidation_data = df.iloc[training_last_row:]\n\n# Train model on training set of data\nx = training_data[\"Rolling_10d_close\"].to_numpy().reshape(-1, 1)\ny = training_data[\"Close\"].to_numpy().reshape(-1, 1)\n\nreg = LinearRegression().fit(x, y)\nprint(reg.coef_, reg.intercept_)\n# prints [[0.95972717]] [4.14010503]\n\n# Test the performance of predictions on the validation data set\nx_pred = validation_data[\"Rolling_10d_close\"].to_numpy().reshape(-1, 1)\ny_pred = validation_data[\"Close\"].to_numpy().reshape(-1, 1)\n\nprint(reg.score(x_pred, y_pred))\n# prints 0.02467230502090556\n\nAPI:\nsklearn.model_selection.train_test_split\n","label":[[410,445,"Mention"],[1582,1622,"API"]],"Comments":[]}
{"id":60689,"text":"ID:74650195\nPost:\nText: You can use all_estimators from sk.utils \nCode: from sklearn.utils import all_estimators\n\ndef get_all_regressors_sklearn():\n\n    estimators = all_estimators(type_filter='regressor')\n\n    all_regs = []\n    for name, RegClass in estimators:\n        print('Appending', name)\n        try:\n            reg = RegClass()\n            all_regs.append(reg)\n        except Exception as e:\n            pass\n    return all_regs\n\nall_regs = get_all_regressors_sklearn()\nprint(all_regs)\n\nText: Gives: \nCode: [ARDRegression(), AdaBoostRegressor(), BaggingRegressor(), BayesianRidge(), CCA(), DecisionTreeRegressor(), DummyRegressor(), ElasticNet(), ElasticNetCV(), ExtraTreeRegressor(), ExtraTreesRegressor(), GammaRegressor(), GaussianProcessRegressor(), GradientBoostingRegressor(), HistGradientBoostingRegressor(), HuberRegressor(), IsotonicRegression(), KNeighborsRegressor(), KernelRidge(), Lars(), LarsCV(), Lasso(), LassoCV(), LassoLars(), LassoLarsCV(), LassoLarsIC(), LinearRegression(), LinearSVR(), MLPRegressor(), MultiTaskElasticNet(), MultiTaskElasticNetCV(), MultiTaskLasso(), MultiTaskLassoCV(), NuSVR(), OrthogonalMatchingPursuit(), OrthogonalMatchingPursuitCV(), PLSCanonical(), PLSRegression(), PassiveAggressiveRegressor(), PoissonRegressor(), QuantileRegressor(), RANSACRegressor(), RadiusNeighborsRegressor(), RandomForestRegressor(), Ridge(), RidgeCV(), SGDRegressor(), SVR(), TheilSenRegressor(), TransformedTargetRegressor(), TweedieRegressor()]\n\nAPI:\nsklearn.utils\n","label":[[56,64,"Mention"],[1485,1498,"API"]],"Comments":[]}
{"id":60690,"text":"ID:74886775\nPost:\nText: In sk_confusion_matrix we have a parameter called labels with default value None. The documentation of labels tells us: \nText: List of labels to index the matrix. This may be used to reorder or select a subset of labels. If None is given, those that appear at least once in y_true or y_pred are used in sorted order. \nText: So to assign proper index to your classes, pass them sequentially to labels Say for example positive = 1, negative = 0 \nCode: from sklearn.metrics import confusion_matrix as cm\n>>> y_test = [1, 0, 0]\n>>> y_pred = [1, 0, 0]\n>>> cm(y_test, y_pred, labels=[1,0])\narray([[1, 0],\n       [0, 2]])\n\n              Pred\n             |  pos=1 | neg=0 |\n         ___________________\nActual  pos=1|  TP=1  | FN=0 |\n        neg=0|  FP=0  | TN=2 |\n\nText: Note: The TP,TN,FP and FN have changed places by passing labels as [1,0]. TP means both predicted and actual value are positive. TN means both predicted and actual value are negative.Same analysis can be done for FP and FN. \nText: If we dont pass any value to labels, the y_true and y_pred values will be used in sorted order i.e [0,1]. \nCode: >>> y_test = [1, 0, 0]\n>>> y_pred = [1, 0, 0]\n>>> cm(y_test, y_pred)\narray([[2, 0],\n       [0, 1]])\n                 Pred\n             |  neg=0 | pos=1 |\n         ___________________\nActual  neg=0|  TN=2  | FP=0 |\n        pos=1|  FN=0  | TN=1 |\n\nText: This will become even more clear if we use more than 2 labels. Cat=1, Dog=2, Mouse=3 If you want the order to be Cat, Mouse, and Dog then labels=[1,3,2] \nCode: >>> y_test = [1, 2, 3]\n>>> y_pred = [1, 3, 2]\n>>> cm(y_test, y_pred, labels=[1,3,2])\narray([[1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0]])\n\n                Pred\n          |  1  |  3  |  2 |\n          __________________\nActual  1 |   1 |  0  |  0 |\n        3 |   0 |  0  |  1 |\n        2 |   0 |  1  |  0 |\n\nText: If you want some other order like Dog,Mouse, and Cat then labels=[2,3,1] \nCode: >>> cm(y_test, y_pred, labels=[2,3,1])\narray([[0, 1, 0],\n       [1, 0, 0],\n       [0, 0, 1]])\n \n\nAPI:\nsklearn.metrics.confusion_matrix\n","label":[[27,46,"Mention"],[2041,2073,"API"]],"Comments":[]}
{"id":60691,"text":"ID:15006495\nPost:\nText: In this case, where the DataFrame is long but not too wide, you can simply slice it:\n Code: >>> df = pd.DataFrame({\"A\": range(1000), \"B\": range(1000)})\n>>> df\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1000 entries, 0 to 999\nData columns:\nA    1000  non-null values\nB    1000  non-null values\ndtypes: int64(2)\n>>> df[:5]\n   A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n\n Text: ix is deprecated.\n Text: If it's both wide and long, I tend to use .ix:\n Code: >>> df = pd.DataFrame({i: range(1000) for i in range(100)})\n>>> df.ix[:5, :10]\n   0   1   2   3   4   5   6   7   8   9   10\n0   0   0   0   0   0   0   0   0   0   0   0\n1   1   1   1   1   1   1   1   1   1   1   1\n2   2   2   2   2   2   2   2   2   2   2   2\n3   3   3   3   3   3   3   3   3   3   3   3\n4   4   4   4   4   4   4   4   4   4   4   4\n5   5   5   5   5   5   5   5   5   5   5   5\n\n\nAPI:\npandas.DataFrame\npandas.DataFrame.ix\n","label":[[48,57,"Mention"],[475,478,"Mention"],[895,911,"API"],[912,931,"API"]],"Comments":[]}
{"id":60692,"text":"ID:7787535\nPost:\nText: For whatever it's worth, you code works fine on my system even without the for loop to set the label colors.  Just as a reference, here's a stand-alone example trying to follow essentially exactly what you posted:\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some data\nnum = 200\nx = np.linspace(501, 1200, num)\nyellow_data, green_data = np.random.random((2,num))\ngreen_data -= np.linspace(0, 3, yellow_data.size)\n\n# Plot the yellow data\nplt.fill_between(x, yellow_data, 0, color='yellow')\nplt.yticks([0.0, 0.5, 1.0], color='yellow')\n\n# Plot the green data\nax2 = plt.twinx()\nax2.plot(x, green_data, 'g-')\nplt.yticks([-4, -3, -2, -1, 0, 1], color='green')\n\nplt.show()\n\n Text: My guess is that your problem is mostly coming from mixing up references to different objects.   I'm guessing that your code is a bit more complex, and that when you call plt.yticks, ax2 is not the current axis.  You can test that idea by explicitly calling sca(ax2) (set the current axis to ax2) before calling yticks and see if that changes things.  \n Text: Generally speaking, it's best to stick to either entirely the matlab-ish state machine interface or the OO interface, and don't mix them too much.  (Personally, I prefer just sticking to the OO interface. Use pyplot to set up figure objects and for show, and use the axes methods otherwise. To each his own, though.)\n Text: At any rate, with matplotlib >= 1.0, the tick_params function makes this a bit more convenient.  (I'm also using plt.subplots here, which is only in >= 1.0, as well.)\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some data\nyellow_data, green_data = np.random.random((2,2000))\nyellow_data += np.linspace(0, 3, yellow_data.size)\ngreen_data -= np.linspace(0, 3, yellow_data.size)\n\n# Plot the data\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\n\nax1.plot(yellow_data, 'y-')\nax2.plot(green_data, 'g-')\n\n# Change the axis colors...\nax1.tick_params(axis='y', labelcolor='yellow')\nax2.tick_params(axis='y', labelcolor='green')\n\nplt.show()\n\n Text: The equivalent code for older versions of matplotlib would look more like this:\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some data\nyellow_data, green_data = np.random.random((2,2000))\nyellow_data += np.linspace(0, 3, yellow_data.size)\ngreen_data -= np.linspace(0, 3, yellow_data.size)\n\n# Plot the data\nfig = plt.figure()\nax1 = fig.add_subplot(1,1,1)\nax2 = ax1.twinx()\n\nax1.plot(yellow_data, 'y-')\nax2.plot(green_data, 'g-')\n\n# Change the axis colors...\nfor ax, color in zip([ax1, ax2], ['yellow', 'green']):\n    for label in ax.yaxis.get_ticklabels():\n        label.set_color(color)\n\nplt.show()\n\n\nAPI:\nmatplotlib.pyplot.yticks\nmatplotlib.pyplot.sca\nmatplotlib.pyplot.yticks\nmatplotlib.pyplot\nmatplotlib.axes\nmatplotlib.axes.Axes.tick_params\nmatplotlib.pyplot.subplots","label":[[893,903,"Mention"],[980,988,"Mention"],[1034,1040,"Mention"],[1291,1297,"Mention"],[1349,1353,"Mention"],[1447,1458,"Mention"],[1519,1531,"Mention"],[2697,2721,"API"],[2722,2743,"API"],[2744,2768,"API"],[2769,2786,"API"],[2787,2802,"API"],[2803,2835,"API"],[2836,2862,"API"]],"Comments":[]}
{"id":60693,"text":"ID:4562455\nPost:\nText: The way I usually do that is by creating a regular list, then append my stuff into it, and finally transform the list to a numpy array as follows :\n Code: import numpy as np\nbig_array = [] #  empty regular list\nfor i in range(5):\n    arr = i*np.ones((2,4)) # for instance\n    big_array.append(arr)\nbig_np_array = np.array(big_array)  # transformed to a numpy array\n\n Text: of course your final object takes twice the space in the memory at the creation step, but appending on python list is very fast, and creation using np.array() also.\n\nAPI:\nnumpy.array\n","label":[[544,554,"Mention"],[567,578,"API"]],"Comments":[]}
{"id":60694,"text":"ID:5557136\nPost:\nText: numpy has a sinc() function, which is the normalised form of your function, i.e.\n Code: F = lambda x: sin(pi*x) \/ (pi*x)\n\n Text: It handles the case for x == 0.0 correctly,\n Code: In [16]: x = numpy.linspace(-1,1,11)\n\nIn [17]: print x\n[-1.  -0.8 -0.6 -0.4 -0.2  0.   0.2  0.4  0.6  0.8  1. ]\n\n Text: To \"unnormalize\" do,\n Code: In [22]: s = numpy.sinc(x\/numpy.pi)\n\nIn [23]: print s.round(2)\n[ 0.84  0.9   0.94  0.97  0.99  1.    0.99  0.97  0.94  0.9   0.84]\n\n\nAPI:\nnumpy.sinc\n","label":[[35,41,"Mention"],[489,499,"API"]],"Comments":[]}
{"id":60695,"text":"ID:8729186\nPost:\nText: The link posted by Jose has been updated and pylab now has a tight_layout() function that does this automatically (in matplotlib version 1.1.0).\n Text: http:\/\/matplotlib.org\/api\/pyplot_api.html#matplotlib.pyplot.tight_layout\n Text: http:\/\/matplotlib.org\/users\/tight_layout_guide.html#plotting-guide-tight-layout\n\nAPI:\npylab.tight_layout\n","label":[[84,98,"Mention"],[341,359,"API"]],"Comments":[]}
{"id":60696,"text":"ID:1969296\nPost:\nText: Using scipy.interpolate.interp1d\n Text: You can also use scipy.interpolate package to do such conversions (if you don't mind dependency on SciPy):\n Code: >>> from scipy.interpolate import interp1d\n>>> m = interp1d([1,512],[5,10])\n>>> m(256)\narray(7.4951076320939336)\n\n Text: or to convert it back to normal float from 0-rank scipy array:\n Code: >>> float(m(256))\n7.4951076320939336\n\n Text: You can do also multiple conversions in one command easily:\n Code: >>> m([100,200,300])\narray([ 5.96868885,  6.94716243,  7.92563601])\n\n Text: As a bonus, you can do non-uniform mappings from one range to another, for intance if you want to map [1,128] to [1,10], [128,256] to [10,90] and [256,512] to [90,100] you can do it like this:\n Code: >>> m = interp1d([1,128,256,512],[1,10,90,100])\n>>> float(m(400))\n95.625\n\n Text: interp1d creates piecewise linear interpolation objects (which are callable just like functions).\n Text: Using numpy.interp\n Text: As noted by ~unutbu, numpy.interp is also an option (with less dependencies):\n Code: >>> from numpy import interp\n>>> interp(256,[1,512],[5,10])\n7.4951076320939336\n\n\nAPI:\nscipy.interpolate.interp1d\nscipy.interpolate\nscipy.interpolate.interp1d\nnumpy.interp\nnumpy.interp\n","label":[[837,845,"Mention"],[1184,1210,"API"]],"Comments":[]}
{"id":60697,"text":"ID:9149619\nPost:\nText: Actually, it is as simple as setting major and minor separately:\n Code: In [9]: plot([23, 456, 676, 89, 906, 34, 2345])\nOut[9]: [<matplotlib.lines.Line2D at 0x6112f90>]\n\nIn [10]: yscale('log')\n\nIn [11]: grid(b=True, which='major', color='b', linestyle='-')\n\nIn [12]: grid(b=True, which='minor', color='r', linestyle='--')\n\n Text: The gotcha with minor grids is that you have to have minor tick marks turned on too.  In the above code this is done by yscale('log'), but it can also be done with plt.minorticks_on().\n\nAPI:\nmatplotlib.pyplot.yscale\nmatplotlib.pyplot.minorticks_on\n","label":[[473,486,"Mention"],[517,536,"Mention"],[544,568,"API"],[569,600,"API"]],"Comments":[]}
{"id":60698,"text":"ID:6930405\nPost:\nText: numpy.loadtxt is a function, not a module. That's why you can't import loadtxt:\n Code: In [33]: import numpy\nIn [34]: numpy.loadtxt\nOut[34]: <function loadtxt at 0x9f8bca4>\n\n\nAPI:\nnumpy.loadtxt\nnumpy.loadtxt\n","label":[[94,101,"Mention"],[217,230,"API"]],"Comments":[]}
{"id":60699,"text":"ID:15723905\nPost:\nText: You can use the astype method to cast a Series (one column):\n Code: df['col_name'] = df['col_name'].astype(object)\n\n Text: Or the entire DataFrame: df = df.astype(object)\n\n Text: Update\n Text: Since version 0.15, you can use the category datatype in a Series\/column:\n Code: df['col_name'] = df['col_name'].astype('category')\n\n Text: Note: pd.Factor was been deprecated and has been removed in favor of pd.Categorical.\n\nAPI:\npandas.Series.astype\npandas.Series\npandas.DataFrame\npandas.DataFrame.astype\npandas.Factor\npandas.Categorical","label":[[40,46,"Mention"],[64,70,"Mention"],[161,170,"Mention"],[177,194,"Mention"],[363,372,"Mention"],[426,440,"Mention"],[448,468,"API"],[469,482,"API"],[483,499,"API"],[500,523,"API"],[524,537,"API"],[538,556,"API"]],"Comments":[]}
{"id":60700,"text":"ID:4366379\nPost:\nText: NumPy provides fromfile() to read binary data.\n Code: a = numpy.fromfile(\"filename\", dtype=numpy.float32)\n\n Text: will create a one-dimensional array containing your data.  To access it as a two-dimensional Fortran-ordered n x m matrix, you can reshape it:\n Code: a = a.reshape((n, m), order=\"FORTRAN\")\n\n Text: [EDIT: The reshape() actually copies the data in this case (see the comments).  To do it without cpoying, use\n Code: a = a.reshape((m, n)).T\n\n Text: Thanks to Joe Kingtion for pointing this out.]\n Text: But to be honest, if your matrix has several gigabytes, I would go for a HDF5 tool like h5py or PyTables.  Both of the tools have FAQ entries comparing the tool to the other one.  I generally prefer h5py, though PyTables seems to be more commonly used (and the scopes of both projects are slightly different).\n Text: HDF5 files can be written from most programming language used in data analysis.  The list of interfaces in the linked Wikipedia article is not complete, for example there is also an R interface.  But I actually don't know which language you want to use to write the data...\n\nAPI:\nnumpy.fromfile\nnumpy.ndarray.reshape\n","label":[[38,48,"Mention"],[345,354,"Mention"],[1134,1148,"API"],[1149,1170,"API"]],"Comments":[]}
{"id":60701,"text":"ID:3662537\nPost:\nText: You could use np.roll to make shifted copies of a, then use boolean logic on the masks to identify the spots to be filled in:\n Code: import numpy as np\nimport numpy.ma as ma\n\na = np.arange(100).reshape(10,10)\nfill_value=-99\na[2:4,3:8] = fill_value\na[8,8] = fill_value\na = ma.masked_array(a,a==fill_value)\nprint(a)\n\n# [[0 1 2 3 4 5 6 7 8 9]\n#  [10 11 12 13 14 15 16 17 18 19]\n#  [20 21 22 -- -- -- -- -- 28 29]\n#  [30 31 32 -- -- -- -- -- 38 39]\n#  [40 41 42 43 44 45 46 47 48 49]\n#  [50 51 52 53 54 55 56 57 58 59]\n#  [60 61 62 63 64 65 66 67 68 69]\n#  [70 71 72 73 74 75 76 77 78 79]\n#  [80 81 82 83 84 85 86 87 -- 89]\n#  [90 91 92 93 94 95 96 97 98 99]]\n\nfor shift in (-1,1):\n    for axis in (0,1):        \n        a_shifted=np.roll(a,shift=shift,axis=axis)\n        idx=~a_shifted.mask * a.mask\n        a[idx]=a_shifted[idx]\n\nprint(a)\n\n# [[0 1 2 3 4 5 6 7 8 9]\n#  [10 11 12 13 14 15 16 17 18 19]\n#  [20 21 22 13 14 15 16 28 28 29]\n#  [30 31 32 43 44 45 46 47 38 39]\n#  [40 41 42 43 44 45 46 47 48 49]\n#  [50 51 52 53 54 55 56 57 58 59]\n#  [60 61 62 63 64 65 66 67 68 69]\n#  [70 71 72 73 74 75 76 77 78 79]\n#  [80 81 82 83 84 85 86 87 98 89]\n#  [90 91 92 93 94 95 96 97 98 99]]\n\n Text: If you'd like to use a larger set of nearest neighbors, you could perhaps do something like this:\n Code: neighbors=((0,1),(0,-1),(1,0),(-1,0),(1,1),(-1,1),(1,-1),(-1,-1),\n           (0,2),(0,-2),(2,0),(-2,0))\n\n Text: Note that the order of the elements in neighbors is important. You probably want to fill in missing values with the nearest neighbor, not just any neighbor. There's probably a smarter way to generate the neighbors sequence, but I'm not seeing it at the moment.\n Code: a_copy=a.copy()\nfor hor_shift,vert_shift in neighbors:\n    if not np.any(a.mask): break\n    a_shifted=np.roll(a_copy,shift=hor_shift,axis=1)\n    a_shifted=np.roll(a_shifted,shift=vert_shift,axis=0)\n    idx=~a_shifted.mask*a.mask\n    a[idx]=a_shifted[idx]\n\n Text: Note that np.roll happily rolls the lower edge to the top, so a missing value at the top may be filled in by a value from the very bottom. If this is a problem, I'd have to think more about how to fix it. The obvious but not very clever solution would be to use if statements and feed the edges a different sequence of admissible neighbors...\n\nAPI:\nnumpy.roll\nnumpy.roll\n","label":[[37,44,"Mention"],[1968,1975,"Mention"],[2307,2317,"API"],[2318,2328,"API"]],"Comments":[]}
{"id":60702,"text":"ID:3499042\nPost:\nText: If you want to use numpy, you must define darr to be a numpy array, not a list:\n Code: import numpy as np\ndarr = np.array([1, 3.14159, 1e100, -2.71828])\nprint(darr.min())\n\n Text: darr.argmin() will give you the index corresponding to the minimum.\n Text: The reason you were getting an error is because argmin is a method understood by numpy arrays, but not by Python lists.\n\nAPI:\nnumpy.ndarray.argmin\nnumpy.ndarray.argmin\n","label":[[202,215,"Mention"],[325,331,"Mention"],[403,423,"API"],[424,444,"API"]],"Comments":[]}
{"id":60703,"text":"ID:8482798\nPost:\nText: One solution would be to use the plt.legend function, even if you don't want an actual legend.  You can specify the placement of the legend box by using the loc keyterm.  More information can be found at this website but I've also included an example showing how to place a legend:\n Code: ax.scatter(xa,ya, marker='o', s=20, c=\"lightgreen\", alpha=0.9)\nax.scatter(xb,yb, marker='o', s=20, c=\"dodgerblue\", alpha=0.9)\nax.scatter(xc,yc marker='o', s=20, c=\"firebrick\", alpha=1.0)\nax.scatter(xd,xd,xd, marker='o', s=20, c=\"goldenrod\", alpha=0.9)\nline1 = Line2D(range(10), range(10), marker='o', color=\"goldenrod\")\nline2 = Line2D(range(10), range(10), marker='o',color=\"firebrick\")\nline3 = Line2D(range(10), range(10), marker='o',color=\"lightgreen\")\nline4 = Line2D(range(10), range(10), marker='o',color=\"dodgerblue\")\nplt.legend((line1,line2,line3, line4),('line1','line2', 'line3', 'line4'),numpoints=1, loc=2) \n\n Text: Note that because loc=2, the legend is in the upper-left corner of the plot.  And if the text overlaps with the plot, you can make it smaller by using legend.fontsize, which will then make the legend smaller.\n\nAPI:\nmatplotlib.pyplot.legend\nmatplotlib.pyplot.legend.fontsize\n","label":[[56,66,"Mention"],[1089,1104,"Mention"],[1153,1177,"API"],[1178,1211,"API"]],"Comments":[]}
{"id":60704,"text":"ID:8167527\nPost:\nText: The problem can be avoided by using the same axes for each plot, with ax.cla() called to clear the plot after each iteration.\n Code: import pylab as plt\nimport scipy as sp\nimport matplotlib.patches as patches\n\nsp.random.seed(100)\npatch = patches.Circle((.75,.75),radius=.25,fc='none')\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\ndef doplot(x,y,patch,count):\n    ax.set_xlim(-0.2,1.2)\n    ax.set_ylim(-0.2,1.2)\n    x = sp.random.random(100)\n    y = sp.random.random(100)\n    im = ax.scatter(x,y)\n    ax.add_patch(patch)\n    im.set_clip_path(patch)\n    plt.savefig(str(count) + '.png')\n    ax.cla()\n\nfor count in xrange(4):\n    doplot(x,y,patch,count)\n\n\nAPI:\nmatplotlib.axes.Axes.cla\n","label":[[93,101,"Mention"],[683,707,"API"]],"Comments":[]}
{"id":60705,"text":"ID:12417434\nPost:\nText: They are not zero. pandas probably does some formatting while printing DataFrame\/Series so they look like zero.\n Text: By the way, you don't need converters. read_table correctly identifies them as float64:\n Code: In [117]: df = pandas.read_table('gradStat_mmn.tdf')\n\nIn [118]: df.ix[0:10]\nOut[118]:\n    Subject Group Local Global  Attn  mean\n0         1  DSub     S      S  Attn     0\n1         1  DSub     S      S  Dist     0\n2         1  DSub     D      S  Attn     0\n3         1  DSub     D      S  Dist     0\n4         1  DSub     S      D  Attn     0\n5         1  DSub     S      D  Dist     0\n6         1  DSub     D      D  Attn     0\n7         1  DSub     D      D  Dist     0\n8         2  ASub     S      S  Attn     0\n9         2  ASub     S      S  Dist     0\n10        2  ASub     D      S  Attn     0\n\nIn [119]: df['mean'].dtype\nOut[119]: dtype('float64')\n\nIn [120]: df['mean'][0]\nOut[120]: 3.2529000000000002e-22\n\n\nAPI:\npandas.DataFrame\npandas.Series\npandas.read_table\n","label":[[95,104,"Mention"],[105,111,"Mention"],[182,192,"Mention"],[960,976,"API"],[977,990,"API"],[991,1008,"API"]],"Comments":[]}
{"id":60706,"text":"ID:20652740\nPost:\nText: The current scikit-learn Pipeline API is not well suited for supervised learning with unsupervised pre-training. Implementing your own wrapper class is probably the best way to go forward for that case.\n\nAPI:\nsklearn.pipeline.Pipeline\n","label":[[49,57,"Mention"],[233,258,"API"]],"Comments":[]}
{"id":60707,"text":"ID:1273815\nPost:\nText: Alas, setmember1d as it exists in numpy is broken when either array has duplicated elements (as A does here).  Download this version, call it e.g sem.py somewhere on your sys.path, add to it a first line import numpy as nm, and THEN this finally works:\n Code: >>> import sem\n>>> print sem.setmember1d(A.reshape(A.size), v).reshape(A.shape)\n[[False True True]\n [True True False]\n [True False False]]\n\n Text: Note the difference wrt @Aants' similar answer: this version has the second row of the resulting bool array correct, while his version (using the setmember1d that comes as part of numpy) incorrectly has the second row as all Trues.\n\nAPI:\nnumpy.setmember1d\nnumpy.setmember1d\n","label":[[29,40,"Mention"],[576,587,"Mention"],[668,685,"API"],[686,703,"API"]],"Comments":[]}
{"id":60708,"text":"ID:5000818\nPost:\nText: Ok, here's my stab at it.  This works, but there might be a simpler approach.  This solution uses some matplotlib event-handling to trigger a new set_xlim() every time it notices the mouse in motion.  The trigger event 'motion_notify_event' could be eliminated if dynamic synchronous zooming isn't required.  \n Text: Bonus: this works for any number of subplots.\n Code: from matplotlib import pyplot\nimport numpy\n\nx = numpy.linspace(0,10,100)\ny = numpy.sin(x)*(1+x)\n\nfig = pyplot.figure()\nax1 = pyplot.subplot(121)\nax1.plot(x,y)\nax2 = pyplot.subplot(122)\nax2.plot(x,y)\n\nax1.old_xlim = ax1.get_xlim()  # store old values so changes\nax2.old_xlim = ax2.get_xlim()  # can be detected\n\ndef re_zoom(event):\n    zoom = 1.0\n    for ax in event.canvas.figure.axes: # get the change in scale\n        nx = ax.get_xlim()\n        ox = ax.old_xlim\n        if ox != nx:                    # of axes that have changed scale\n            zoom = (nx[1]-nx[0])\/(ox[1]-ox[0])\n\n    for ax in event.canvas.figure.axes: # change the scale\n        nx = ax.get_xlim()\n        ox = ax.old_xlim\n        if ox == nx:                    # of axes that need an update\n            mid = (ox[0] + ox[1])\/2.0\n            dif = zoom*(ox[1] - ox[0])\/2.0\n            nx = (mid - dif, mid + dif)\n            ax.set_xlim(*nx)\n        ax.old_xlim = nx\n    if zoom != 1.0:\n        event.canvas.draw()             # re-draw the canvas (if required)\n\npyplot.connect('motion_notify_event', re_zoom)  # for right-click pan\/zoom\npyplot.connect('button_release_event', re_zoom) # for rectangle-select zoom\npyplot.show()\n\n\nAPI:\nmatplotlib.axes.Axes.set_xlim\n","label":[[169,179,"Mention"],[1603,1632,"API"]],"Comments":[]}
{"id":60709,"text":"ID:5328669\nPost:\nCode: import matplotlib.pyplot as plt\nimport numpy as np\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nhist, bins = np.histogram(x, bins=50)\nwidth = 0.7 * (bins[1] - bins[0])\ncenter = (bins[:-1] + bins[1:]) \/ 2\nplt.bar(center, hist, align='center', width=width)\nplt.show()\n\n Text: The object-oriented interface is also straightforward:\n Code: fig, ax = plt.subplots()\nax.bar(center, hist, align='center', width=width)\nfig.savefig(\"1.png\")\n\n Text: If you are using custom (non-constant) bins, you can pass compute the widths using np.diff, pass the widths to ax.bar and use ax.set_xticks to label the bin edges:\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nbins = [0, 40, 60, 75, 90, 110, 125, 140, 160, 200]\nhist, bins = np.histogram(x, bins=bins)\nwidth = np.diff(bins)\ncenter = (bins[:-1] + bins[1:]) \/ 2\n\nfig, ax = plt.subplots(figsize=(8,3))\nax.bar(center, hist, align='center', width=width)\nax.set_xticks(bins)\nfig.savefig(\"\/tmp\/out.png\")\n\nplt.show()\n\n\nAPI:\nnumpy.diff\nmatplotlib.axes.Axes.bar\nmatplotlib.axes.Axes.set_xticks\n","label":[[562,569,"Mention"],[590,596,"Mention"],[605,618,"Mention"],[1068,1078,"API"],[1079,1103,"API"],[1104,1135,"API"]],"Comments":[]}
{"id":60710,"text":"ID:21331792\nPost:\nText: When you are processing the test data, you used fit_transform(X_test) which actually recomputes another PCA transformation on the test data. You should be using transform(X_test), so that the test data undergoes the same transformation as the training data.\n Text: The portion of code will look something like (thanks ogrisel for the whiten tip):\n Code: estimator = PCA(n_components=350, whiten=True)\nX_train_pca = estimator.fit_transform(X_train)\nX_test_pca = estimator.transform(X_test)\n\n Text: Try and see if it helps?\n\nAPI:\nsklearn.decomposition.PCA.fit_transform\nsklearn.decomposition.PCA\nsklearn.decomposition.PCA.transform\n","label":[[72,93,"Mention"],[128,131,"Mention"],[185,202,"Mention"],[552,591,"API"],[592,617,"API"],[618,653,"API"]],"Comments":[]}
{"id":60711,"text":"ID:9174498\nPost:\nText: You can use standard Python exception handling to trap the error raised by curve_fit in cases where the optimization fails to find a solution. So something like:\n Code: try:\n    popt,pcov = scipy.optimize.curve_fit(f, xdata, ydata, p0=None, sigma=None)\n\nexcept RuntimeError:\n    print(\"Error - curve_fit failed\")\n\n Text: That construct will let you catch and handle the error condition raised by curve_fit without having your program abort.\n\nAPI:\nscipy.optimize.curve_fit\nscipy.optimize.curve_fit\n","label":[[98,107,"Mention"],[419,428,"Mention"],[470,494,"API"],[495,519,"API"]],"Comments":[]}
{"id":60712,"text":"ID:2618478\nPost:\nText: I changed your code to actually return values (and used randint instead of rand - isn't that what you meant?) like this...\n Code: def use_multinomial(length, num_points):\n    probs = ones(length)\/float(length)\n    return multinomial(1, probs, num_points)\n\ndef use_rand(length, num_points):\n    return [randint(1,length) for _ in range(num_points)]\n\n Text: Then I tried my own version, using numpy.random.randint to generate a numpy array of random points on the string:\n Code: def use_np_randint(length, num_point):\n    return nprandint(1, length, num_points)\n\n Text: The results:\n Code: Multinomial took: 13.6279997826 seconds\nRand took: 0.185000181198 seconds\nNP randint took: 0.00100016593933 seconds\n\n Text: Multinomial is obviously really slow comparitively, but is that even what you want? I thought you said you wanted a uniform distribution? Using numpy's randint is clearly the fastest of the bunch.\n\nAPI:\nnumpy.random.randint\nnumpy.random.multinomial\nnumpy.random.randint\n","label":[[735,746,"Mention"],[887,894,"Mention"],[959,983,"API"],[984,1004,"API"]],"Comments":[]}
{"id":60713,"text":"ID:11203825\nPost:\nText: liblinear (the backing implementation of sklearn.linear_model.LogisticRegression) will host its own copy of the data because it is a C++ library whose internal memory layout cannot be directly mapped onto a pre-allocated sparse matrix in scipy such as scipy.sparse.csr_matrix or scipy.sparse.csc_matrix.\n Text: In your case I would recommend to load your data as a scipy.sparse.csr_matrix and feed it to a sklearn.linear_model.SGDClassifier (with loss='log' if you want a logistic regression model and the ability to call the predict_proba method). SGDClassifier will not copy the input data if it's already using the scipy.sparse.csr_matrix memory layout.\n Text: Expect it to allocate a dense model of 800 * (80000 + 1) * 8 \/ (1024 ** 2) = 488MB in memory (in addition to the size of your input dataset).\n Text: Edit: how to optimize the memory access for your dataset\n Text: To free memory after dataset extraction you can:\n Code: x_vectorizer = CountVectorizer(binary = True, analyzer = features)\nx = x_vectorizer.fit_transform(x)\nfrom sklearn.externals import joblib\njoblib.dump(x.tocsr(), 'dataset.joblib')\n\n Text: Then quit this python process (to force complete memory deallocation) and in a new process:\n Code: x_csr = joblib.load('dataset.joblib')\n\n Text: Under linux \/ OSX you could memory map that even more efficiently with:\n Code: x_csr = joblib.load('dataset.joblib', mmap_mode='c')\n\n\nAPI:\nsklearn.linear_model.LogisticRegression\nscipy.sparse.csr_matrix\nscipy.sparse.csc_matrix\nscipy.sparse.csr_matrix\nsklearn.linear_model.SGDClassifier\nsklearn.linear_model.SGDClassifier.predict_proba\nsklearn.linear_model.SGDClassifier\nscipy.sparse.csr_matrix\n","label":[[550,563,"Mention"],[573,586,"Mention"],[1575,1623,"API"],[1624,1658,"API"]],"Comments":[]}
{"id":60714,"text":"ID:14513503\nPost:\nText: The set_index method returns a new DataFrame by default, rather than applying this inplace (in fact, most pandas functions are similar). It has an inplace argument:\n Code: s.set_index('Date', inplace=True)\ns.plot()\n\n Text: which works as you intended!\n Text: Note: to convert the Index to a DatetimeIndex you can use to_datetime:\n Code: s.index = s.index.to_datetime()\n\n Text: .\n Text: Which is to say, s remained unchanged by you .set_index('Date'):\n Code: In [63]: s = pd.read_csv('spy.csv', na_values=[\" \"])\n\nIn [64]: s.set_index('Date')\nOut[64]: \n<class 'pandas.core.frame.DataFrame'>\nIndex: 5033 entries, 1993-01-29 00:00:00 to 2013-01-23 00:00:00\nData columns:\nOpen         5033  non-null values\nHigh         5033  non-null values\nLow          5033  non-null values\nClose        5033  non-null values\nVolume       5033  non-null values\nAdj Close    5033  non-null values\ndtypes: float64(5), int64(1)\n\nIn [65]: s\nOut[65]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 5033 entries, 0 to 5032\nData columns:\nDate         5033  non-null values\nOpen         5033  non-null values\nHigh         5033  non-null values\nLow          5033  non-null values\nClose        5033  non-null values\nVolume       5033  non-null values\nAdj Close    5033  non-null values\ndtypes: float64(5), int64(1), object(1)\n\n\nAPI:\npandas.DataFrame.set_index\npandas.DataFrame\npandas.Index\npandas.DatetimeIndex\npandas.Index.to_datetime\npandas.DataFrame.set_index\n","label":[[28,37,"Mention"],[59,68,"Mention"],[304,309,"Mention"],[315,328,"Mention"],[341,352,"Mention"],[455,473,"Mention"],[1332,1358,"API"],[1359,1375,"API"],[1376,1388,"API"],[1389,1409,"API"],[1410,1434,"API"],[1435,1461,"API"]],"Comments":[]}
{"id":60715,"text":"ID:13082746\nPost:\nText: I think using SGDClassifier instead of LinearSVC for this kind of data would be a good idea, as it is much faster. For the vectorization, I suggest you look into the hash transformer PR.\n Text: For the multiprocessing: You can distribute the data sets across cores, do partial_fit, get the weight vectors, average them, distribute them to the estimators, do partial fit again.\n Text: Doing parallel gradient descent is an area of active research, so there is no ready-made solution there.\n Text: How many classes does your data have btw? For each class, a separate will be trained (automatically). If you have nearly as many classes as cores, it might be better and much easier to just do one class per core, by specifying n_jobs in SGDClassifier.\n\nAPI:\nsklearn.linear_model.SGDClassifier\nsklearn.svm.LinearSVC\nsklearn.linear_model.SGDClassifier.partial_fit\nsklearn.linear_model.SGDClassifier\n","label":[[38,51,"Mention"],[63,72,"Mention"],[293,304,"Mention"],[757,770,"Mention"],[778,812,"API"],[813,834,"API"],[835,881,"API"],[882,916,"API"]],"Comments":[]}
{"id":60716,"text":"ID:2265506\nPost:\nText: You can simply do:\n Code: import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0, 7, 0.01)\n    \nplt.subplot(2, 1, 1)\nplt.plot(x, np.sin(x))\n    \nplt.subplot(2, 2, 3)\nplt.plot(x, np.cos(x))\n    \nplt.subplot(2, 2, 4)\nplt.plot(x, np.sin(x)*np.cos(x))\n\n Text: i.e., the first plot is really a plot in the upper half (the figure is only divided into 2x1 = 2 cells), and the following two smaller plots are done in a 2x2=4 cell grid.\nThe third argument to subplot() is the position of the plot inside the grid (in the direction of reading in English, with cell 1 being in the top-left corner):\nfor example in the second subplot (subplot(2, 2, 3)), the axes will go to the third section of the 2x2 matrix i.e, to the bottom-left corner.\n\nAPI:\nmatplotlib.pyplot.subplot\nmatplotlib.pyplot.subplot\n","label":[[486,495,"Mention"],[659,675,"Mention"],[772,797,"API"],[798,823,"API"]],"Comments":[]}
{"id":60717,"text":"ID:12830184\nPost:\nText: \nClearly, assumed_to_be_the_feature_ids_of_the_top_k_features cannot correspond to the feature-id values - since (see below) the feature-id values in my input file start from 1.\n\n Text: Actually, they are. The SVMlight format loader will detect that your input file has one-based indices and will subtract one from every index so as not to waste a column. If that's not what you want, then pass zero_based=True to load_svmlight_file to pretend that it's actually zero-based and insert an extra column; see its documentation for details.\n\nAPI:\nsklearn.datasets.load_svmlight_file\n","label":[[438,456,"Mention"],[567,602,"API"]],"Comments":[]}
{"id":60718,"text":"ID:14283423\nPost:\nText: I don't think so. The HTMLFormatter used by DataFrame.to_html helps to pretty render a DataFrame in a IPython HTML Notebooks I think.\n Text: The method does not parse each element of your DataFrame, i.e. recognizes an URI pattern to write <a href=\"URI\">Content<\/a> or something else.\n Text: I don't think that (1) it's planned and (2) it's not the purpose of this method. Maybe you can add an issue to the GitHub pandas issues page.\n\nAPI:\npandas.DataFrame.to_html\npandas.DataFrame\n","label":[[68,85,"Mention"],[111,120,"Mention"],[463,487,"API"],[488,504,"API"]],"Comments":[]}
{"id":60719,"text":"ID:5059279\nPost:\nText: The size of the data is independent of the pixel dimensions of the final image.\n Text: Since you say you don't need a high-resolution image, you can generate the image quicker by down-sampling your data. If your data is in the form of a numpy array, a quick and dirty way would be to take every nth column and row with data[::n,::n]. \n Text: You can control the output image's pixel dimensions with fig.set_size_inches and plt.savefig's dpi parameter:\n Code: import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\ndata=np.arange(300).reshape((10,30))\nplt.imshow(data[::2,::2],cmap=cm.Greys)\n\nfig=plt.gcf()\n# Unfortunately, had to find these numbers through trial and error\nfig.set_size_inches(5.163,3.75)  \nax=plt.gca()\nextent=ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n\nplt.savefig('\/tmp\/test.png', dpi=400,\n            bbox_inches=extent)\n\n\nAPI:\nmatplotlib.figure.Figure.set_size_inches\nmatplotlib.pyplot.savefig\n","label":[[422,441,"Mention"],[446,459,"Mention"],[919,959,"API"],[960,985,"API"]],"Comments":[]}
{"id":60720,"text":"ID:13788201\nPost:\nText: There is little point in changing the index itself - since you can just generate using date_range with the desired frequency parameter as in your question. \n Text: I assume what you are trying to do is change the frequency of a Time Series that contains data, in which case you can use resample (documentation). For example if you have the following time series:\n Code: dt_index = pd.date_range('2012-1-1 00:00.001',periods=3, freq='1ms')\nts = pd.Series(randn(3), index=dt_index)\n\n\n2012-01-01 00:00:00           0.594618\n2012-01-01 00:00:00.001000    0.874552\n2012-01-01 00:00:00.002000   -0.700076\nFreq: L\n\n Text: Then you can change the frequency to seconds using resample, specifying how you want to aggregate the values (mean, sum etc.):\n Code: ts.resample('S', how='sum')\n\n2012-01-01 00:00:00    0.594618\n2012-01-01 00:00:01    0.174475\nFreq: S\n\n\nAPI:\npandas.date_range\npandas.Series.resample\npandas.Series.resample\n","label":[[111,121,"Mention"],[310,318,"Mention"],[690,698,"Mention"],[881,898,"API"],[899,921,"API"],[922,944,"API"]],"Comments":[]}
{"id":60721,"text":"ID:7904652\nPost:\nText: Your data was generated with mu=0.07 and sigma=0.89.\nYou are testing this data against a normal distribution with mean 0 and standard deviation of 1.\n Text: The null hypothesis (H0) is that the distribution of which your data is a sample is equal to the standard normal distribution with mean 0, std deviation 1.\n Text: The small p-value is indicating that a test statistic as large as D would be expected with probability p-value.\n Text: In other words, (with p-value ~8.9e-22) it is highly unlikely that H0 is true.\n Text: That is reasonable, since the means and std deviations don't match.\n Text: Compare your result with:\n Code: In [22]: import numpy as np\nIn [23]: import scipy.stats as stats\nIn [24]: stats.kstest(np.random.normal(0,1,10000),'norm')\nOut[24]: (0.007038739782416259, 0.70477679457831155)\n\n Text: To test your data is gaussian, you could shift and rescale it so it is normal with mean 0 and std deviation 1:\n Code: data=np.random.normal(mu,sigma,10000)\nnormed_data=(data-mu)\/sigma\nprint(stats.kstest(normed_data,'norm'))\n# (0.0085805670733036798, 0.45316245879609179)\n\n Text: Warning: (many thanks to user333700 (aka scipy developer Josef Perktold)) If you don't know mu and sigma, estimating the parameters makes the p-value invalid:\n Code: import numpy as np\nimport scipy.stats as stats\n\nmu = 0.3\nsigma = 5\n\nnum_tests = 10**5\nnum_rejects = 0\nalpha = 0.05\nfor i in xrange(num_tests):\n    data = np.random.normal(mu, sigma, 10000)\n    # normed_data = (data - mu) \/ sigma    # this is okay\n    # 4915\/100000 = 0.05 rejects at rejection level 0.05 (as expected)\n    normed_data = (data - data.mean()) \/ data.std()    # this is NOT okay\n    # 20\/100000 = 0.00 rejects at rejection level 0.05 (not expected)\n    D, pval = stats.kstest(normed_data, 'norm')\n    if pval < alpha:\n        num_rejects += 1\nratio = float(num_rejects) \/ num_tests\nprint('{}\/{} = {:.2f} rejects at rejection level {}'.format(\n    num_rejects, num_tests, ratio, alpha))     \n\n Text: prints\n Code: 20\/100000 = 0.00 rejects at rejection level 0.05 (not expected)\n\n Text: which shows that stats.kstest may not reject the expected number of null hypotheses\nif the sample is normalized using the sample's mean and standard deviation\n Code: normed_data = (data - data.mean()) \/ data.std()    # this is NOT okay\n\n\nAPI:\nscipy.stats.kstest\n","label":[[2100,2112,"Mention"],[2326,2344,"API"]],"Comments":[]}
{"id":60722,"text":"ID:10453861\nPost:\nText: Try using ax.grid(True, which='both') to position your grid lines on both major and minor ticks, as suggested here. \n Text: EDIT: Or just set your ticks manually, like this:\n Code: import matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot([1,2,3,14],'ro-')\n\n# set your ticks manually\nax.xaxis.set_ticks([1.,2.,3.,10.])\nax.grid(True)\n\nplt.show()\n\n\nAPI:\nmatplotlib.axes.Axes.grid\n","label":[[34,61,"Mention"],[404,429,"API"]],"Comments":[]}
{"id":60723,"text":"ID:4150486\nPost:\nText: Maybe try something like:\n Code: import matplotlib.pyplot as plt\nimport numpy\nfrom scipy import stats\ndata = [1.5]*7 + [2.5]*2 + [3.5]*8 + [4.5]*3 + [5.5]*1 + [6.5]*8\ndensity = stats.kde.gaussian_kde(data)\nx = numpy.arange(0., 8, .1)\nplt.plot(x, density(x))\nplt.show()\n\n Text: You can easily replace gaussian_kde() by a different kernel density estimate.\n\nAPI:\nscipy.stats.gaussian_kde\n","label":[[323,337,"Mention"],[384,408,"API"]],"Comments":[]}
{"id":60724,"text":"ID:8727021\nPost:\nText: Use plt.xlim and plt.ylim to set the domain and range.\nSet figsize to indirectly control the pixel resolution of the final image. (figsize sets the size of the figure in inches; the default dpi is 100.)\nYou can also control the dpi in the call to plt.savefig.\n Text: With figsize = (10, 10) and dpi = 100, the image will have resolution 1000x1000.\n Text: For example,\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nx, y = np.random.random((2,10000))\nplt.plot(x, y, ',')\nfigname = '\/tmp\/test.pdf'\nxmin, xmax = 0, 1\nymin, ymax = 0, 1\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nplt.savefig(figname)\n\n Text: Your pdf viewer should be able to zoom in any region so individual points can be distinguished.\n\nAPI:\nmatplotlib.pyplot.xlim\nmatplotlib.pyplot.ylim\nmatplotlib.pyplot.savefig\n","label":[[27,35,"Mention"],[40,48,"Mention"],[270,281,"Mention"],[740,762,"API"],[763,785,"API"],[786,811,"API"]],"Comments":[]}
{"id":60725,"text":"ID:9692312\nPost:\nText: @Hooked has already answered most of your question, but I was in the middle of writing this up when he answered, so I'll post it in the hopes that it's still useful...\n Text: You're trying to jump through a few too many hoops.  You don't need watershed_ift.\n Text: You use scipy.ndimage.label to differentiate separate objects in a boolean array and scipy.ndimage.find_objects to find the bounding box of each object.\n Text: Let's break things down a bit.\n Code: import numpy as np\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\n\ndef draw_circle(grid, x0, y0, radius):\n    ny, nx = grid.shape\n    y, x = np.ogrid[:ny, :nx]\n    dist = np.hypot(x - x0, y - y0)\n    grid[dist < radius] = True\n    return grid\n\n# Generate 3 circles...\na = np.zeros((512, 512), dtype=np.bool)\ndraw_circle(a, 100, 200, 30)\ndraw_circle(a, 400, 350, 20)\ndraw_circle(a, 200, 260, 20)\n\n# Label the objects in the array. \nlabels, numobjects = ndimage.label(a)\n\n# Now find their bounding boxes (This will be a tuple of slice objects)\n# You can use each one to directly index your data. \n# E.g. a[slices[0]] gives you the original data within the bounding box of the\n# first object.\nslices = ndimage.find_objects(labels)\n\n#-- Plotting... -------------------------------------\nfig, ax = plt.subplots()\nax.imshow(a)\nax.set_title('Original Data')\n\nfig, ax = plt.subplots()\nax.imshow(labels)\nax.set_title('Labeled objects')\n\nfig, axes = plt.subplots(ncols=numobjects)\nfor ax, sli in zip(axes.flat, slices):\n    ax.imshow(labels[sli], vmin=0, vmax=numobjects)\n    tpl = 'BBox:\\nymin:{0.start}, ymax:{0.stop}\\nxmin:{1.start}, xmax:{1.stop}'\n    ax.set_title(tpl.format(*sli))\nfig.suptitle('Individual Objects')\n\nplt.show()\n\n Text: Hopefully that makes it a bit clearer how to find the bounding boxes of the objects.\n\nAPI:\nscipy.ndimage.watershed_ift\nscipy.ndimage.label\nscipy.ndimage.find_objects\n","label":[[266,279,"Mention"],[1820,1847,"API"]],"Comments":[]}
{"id":60726,"text":"ID:6004738\nPost:\nText: What's happening is that the axis isn't set to \"automatically dimension\" after you call ax.axis().  Calling axis turns off autoscaling (it assumes that if you're manually getting the axis limits, you probably don't want them to change). \n Text: Just add an ax.axis('auto') or an ax.set_autoscale_on() after you plot everything.\n Code: import datetime\nimport matplotlib.pyplot as plt\nx = [datetime.date(2011,2,11),\n     datetime.date(2011,3,11),\n     datetime.date(2011,4,11),\n     datetime.date(2011,5,11),\n     datetime.date(2011,6,11),\n     datetime.date(2011,7,11)]\ny = [23,41,67,72,18,19]\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot_date(x, y, 'kx')\n\n# next task is to broaden the xaxis so that it begins \n# and ends at the start of a month (approximately).  \nxmin, xmax, ymin, ymax = ax.axis() ; print xmin, xmax, ymin, ymax\na1 = xmin - min(x).day + 1\na2 = xmax - max(x).day + 31\n\nax.plot_date(a1, ymin, 'ko')\nax.plot_date(a2, ymin, 'ko')\nax.axis('auto')\n\nplt.show()\n\n\nAPI:\nmatplotlib.axes.Axes.axis\nmatplotlib.axes.Axes.axis\nmatplotlib.axes.Axes.axis\nmatplotlib.axes.Axes.set_autoscale_on\n","label":[[111,120,"Mention"],[131,135,"Mention"],[280,295,"Mention"],[302,323,"Mention"],[1011,1036,"API"],[1037,1062,"API"],[1063,1088,"API"],[1089,1126,"API"]],"Comments":[]}
{"id":60727,"text":"ID:4082392\nPost:\nText: Unless your graphic is huge, many of those 3 million points are going to overlap.\n(A 400x600 image only has 240K dots...)\n Text: So the easiest thing to do would be to take a sample of say, 1000 points, from your data:\n Code: import random\ndelta_sample=random.sample(delta,1000)\n\n Text: and just plot that.\n Text: For example:\n Code: import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\nimport random\n\nfig = plt.figure()\nfig.subplots_adjust(bottom=0.2)\nax = fig.add_subplot(111)\n\nN=3*10**6\ndelta=np.random.normal(size=N)\nvf=np.random.normal(size=N)\ndS=np.random.normal(size=N)\n\nidx=random.sample(range(N),1000)\n\nplt.scatter(delta[idx],vf[idx],c=dS[idx],alpha=0.7,cmap=cm.Paired)\nplt.show()\n\n Text: Or, if you need to pay more attention to outliers, then perhaps you could bin your data using np.histogram, and then compose a delta_sample which has representatives from each bin.\n Text: Unfortunately, when using np.histogram I don't think there is any easy way to associate bins with individual data points. A simple, but approximate solution is to use the location of a point in or on the bin edge itself as a proxy for the points in it:\n Code: xedges=np.linspace(-10,10,100)\nyedges=np.linspace(-10,10,100)\nzedges=np.linspace(-10,10,10)\nhist,edges=np.histogramdd((delta,vf,dS), (xedges,yedges,zedges))\nxidx,yidx,zidx=np.where(hist>0)\nplt.scatter(xedges[xidx],yedges[yidx],c=zedges[zidx],alpha=0.7,cmap=cm.Paired)\nplt.show()\n\n\nAPI:\nnumpy.histogram\nnumpy.histogram\n","label":[[840,852,"Mention"],[960,972,"Mention"],[1480,1495,"API"],[1496,1511,"API"]],"Comments":[]}
{"id":60728,"text":"ID:8028510\nPost:\nText: I have seen lpr -o landscape, but do not have enough experience with it to know if it works for all printers.\n Text: Rather than changing orientation while printing, you can do it when generating the image (if it fits with your workflow). The matplotlib savefig command allows you to specify saving in landscape orientation, but currently only for postscript. That is not a problem, however, since we can easily convert the postscript file to PDF format. Below is an example.\n Text: In Python:\n Code: from pylab import *\nimport numpy as np\n\nx = np.arange(0, 10, 0.1)\ny = np.sin(x)\nplot(x, y)\nxlabel('x')\nylabel('y')\nsavefig('img.eps', orientation='landscape')\n\n Text: I left out the canvas size for convenience and brevity.\n Text: Now we have a file named img.eps. In the shell do the following.\n Code: epstopdf img.eps\n\n Text: Here is what the resulting img.pdf file looks like:\n\n Text: One downside to keep in mind with this approach is that postscript does not like transparency, so if you want transparency this is not the approach for you. To see what I mean take the matplotlib patch_collection.py example. Replace the pylab.show() on the last line with pylab.savefig('patch.pdf'), run it, and then look at the resulting PDF file. It will look like the image in the example. If, however, you do pylab.savefig('patch.eps'), you will see that the objects are all opaque.\n\nAPI:\npylab.savefig\npylab.show\npylab.savefig\npylab.savefig","label":[[277,284,"Mention"],[1404,1417,"API"]],"Comments":[]}
{"id":60729,"text":"ID:9458803\nPost:\nText: I think you don't need to do fftshift(), and you can pass sampling period to fftfreq():\n Code: import scipy\nimport scipy.fftpack\nimport pylab\nfrom scipy import pi\nt = scipy.linspace(0,120,4000)\nacc = lambda t: 10*scipy.sin(2*pi*2.0*t) + 5*scipy.sin(2*pi*8.0*t) + 2*scipy.random.random(len(t))\n\nsignal = acc(t)\n\nFFT = abs(scipy.fft(signal))\nfreqs = scipy.fftpack.fftfreq(signal.size, t[1]-t[0])\n\npylab.subplot(211)\npylab.plot(t, signal)\npylab.subplot(212)\npylab.plot(freqs,20*scipy.log10(FFT),'x')\npylab.show()\n\n Text: from the graph you can see there are two peak at 2Hz and 8Hz.\n\nAPI:\nscipy.fftpack.fftshift\nscipy.fftpack.fftfreq\n","label":[[52,62,"Mention"],[100,109,"Mention"],[609,631,"API"],[632,653,"API"]],"Comments":[]}
{"id":60730,"text":"ID:16409460\nPost:\nText: Allowing pandas to read the second line as data is screwing up the dtype for the columns. Instead of a float dtype, the presence of strings make the dtype of the columns object, and the underlying objects, even the numbers, are strings. This screws up all numerical operations:\n Code: In [8]: obs['latitude']+obs['longitude']\nOut[8]: \n0    degrees_northdegrees_east\n1                -1.82-142.842\n2                 39.87-25.389\n3                27.114-37.704\n\nIn [9]: obs['latitude'][1]\nOut[9]: '-1.82'\n\n Text: So it is imperative that pd.read_csv skip the second line.\n Text: The following is pretty ugly, but given the format of the input, I don't see a better way. \n Code: import pandas as pd\nfrom StringIO import StringIO\n\nx = '''\nlongitude,latitude\ndegrees_east,degrees_north\n-142.842,-1.82\n-25.389,39.87\n-37.704,27.114\n'''\n\ncontent = StringIO(x.strip())\n\ndef read_csv(content):\n    columns = next(content).strip().split(',')\n    units = next(content).strip().split(',')\n    obs = pd.read_table(content, sep=\",\\s*\", header=None)\n    obs.columns = ['{c} ({u})'.format(c=col, u=unit)\n                   for col, unit in zip(columns, units)]\n    return obs\n\nobs = read_csv(content)\nprint(obs)\n#    longitude (degrees_east)  latitude (degrees_north)\n# 0                  -142.842                    -1.820\n# 1                   -25.389                    39.870\n# 2                   -37.704                    27.114\nprint(obs.dtypes)\n# longitude (degrees_east)    float64\n# latitude (degrees_north)    float64\n\n\nAPI:\npandas.read_csv\n","label":[[560,571,"Mention"],[1544,1559,"API"]],"Comments":[]}
{"id":60731,"text":"ID:26716774\nPost:\nText: The to_dict() method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this.\n Text: to_dict() also accepts an 'orient' argument which you'll need in order to output a list of values for each column. Otherwise, a dictionary of the form {index: value} will be returned for each column.\n Text: These steps can be done with the following line:\n Code: >>> df.set_index('ID').T.to_dict('list')\n{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n\n Text: In case a different dictionary format is needed, here are examples of the possible orient arguments. Consider the following simple DataFrame:\n Code: >>> df = pd.DataFrame({'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]})\n>>> df\n        a      b\n0     red  0.500\n1  yellow  0.250\n2    blue  0.125\n\n Text: Then the options are as follows.\n Text: dict - the default: column names are keys, values are dictionaries of index:data pairs\n Code: >>> df.to_dict('dict')\n{'a': {0: 'red', 1: 'yellow', 2: 'blue'}, \n 'b': {0: 0.5, 1: 0.25, 2: 0.125}}\n\n Text: list - keys are column names, values are lists of column data\n Code: >>> df.to_dict('list')\n{'a': ['red', 'yellow', 'blue'], \n 'b': [0.5, 0.25, 0.125]}\n\n Text: series - like 'list', but values are Series\n Code: >>> df.to_dict('series')\n{'a': 0       red\n      1    yellow\n      2      blue\n      Name: a, dtype: object, \n\n 'b': 0    0.500\n      1    0.250\n      2    0.125\n      Name: b, dtype: float64}\n\n Text: split - splits columns\/data\/index as keys with values being column names, data values by row and index labels respectively\n Code: >>> df.to_dict('split')\n{'columns': ['a', 'b'],\n 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],\n 'index': [0, 1, 2]}\n\n Text: records - each row becomes a dictionary where key is column name and value is the data in the cell\n Code: >>> df.to_dict('records')\n[{'a': 'red', 'b': 0.5}, \n {'a': 'yellow', 'b': 0.25}, \n {'a': 'blue', 'b': 0.125}]\n\n Text: index - like 'records', but a dictionary of dictionaries with keys as index labels (rather than a list)\n Code: >>> df.to_dict('index')\n{0: {'a': 'red', 'b': 0.5},\n 1: {'a': 'yellow', 'b': 0.25},\n 2: {'a': 'blue', 'b': 0.125}}\n\n\nAPI:\npandas.DataFrame.to_dict\npandas.DataFrame\npandas.DataFrame\npandas.DataFrame.to_dict\npandas.DataFrame\npandas.Series\n","label":[[28,37,"Mention"],[117,126,"Mention"],[199,208,"Mention"],[244,253,"Mention"],[736,745,"Mention"],[1358,1364,"Mention"],[2297,2321,"API"],[2322,2338,"API"],[2339,2355,"API"],[2356,2380,"API"],[2381,2397,"API"],[2398,2411,"API"]],"Comments":[]}
{"id":60732,"text":"ID:16102866\nPost:\nText: You can take a look at the source code .\n Text: DataFrame has a private function _slice() to slice the DataFrame, and it allows the parameter axis to determine which axis to slice. The __getitem__() for DataFrame doesn't set the axis while invoking _slice(). So the _slice() slice it by default axis 0.\n Text: You can take a simple experiment, that might help you:\n Code: print df._slice(slice(0, 2))\nprint df._slice(slice(0, 2), 0)\nprint df._slice(slice(0, 2), 1)\n\n\nAPI:\npandas.DataFrame\npandas.DataFrame._slice\npandas.DataFrame\npandas.DataFrame.__getitem__\npandas.DataFrame\npandas.DataFrame._slice\npandas.DataFrame._slice\n","label":[[72,81,"Mention"],[105,113,"Mention"],[127,136,"Mention"],[209,222,"Mention"],[227,236,"Mention"],[273,281,"Mention"],[290,298,"Mention"],[496,512,"API"],[513,536,"API"],[537,553,"API"],[554,582,"API"],[583,599,"API"],[600,623,"API"],[624,647,"API"]],"Comments":[]}
{"id":60733,"text":"ID:27905350\nPost:\nText: You could use the fillna method on the DataFrame and specify the method as ffill (forward fill):\n Code: >>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n>>> df.fillna(method='ffill')\n   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n\n Text: This method...\n Text: \npropagate[s] last valid observation forward to next valid\n\n Text: To go the opposite way, there's also a bfill method.\n Text: This method doesn't modify the DataFrame inplace - you'll need to rebind the returned DataFrame to a variable or else specify inplace=True:\n Code: df.fillna(method='ffill', inplace=True)\n\n\nAPI:\npandas.DataFrame.fillna\npandas.DataFrame\npandas.DataFrame\npandas.DataFrame\n","label":[[42,48,"Mention"],[63,72,"Mention"],[459,468,"Mention"],[514,523,"Mention"],[622,645,"API"],[646,662,"API"],[663,679,"API"],[680,696,"API"]],"Comments":[]}
{"id":60734,"text":"ID:3410940\nPost:\nText: The following code might help:\n Code: import numpy as np\n\ndt = np.dtype([('name1', '|S10'), ('name2', '<f8')])\ntuplelist=[\n    ('n1', 1.2),\n    ('n2', 3.4),    \n     ]\narr = np.array(tuplelist, dtype=dt)\n\nprint(arr['name1'])\n# ['n1' 'n2']\nprint(arr['name2'])\n# [ 1.2  3.4]\n\n Text: Your immediate problem was that np.dtype expects the format specifiers to be numpy types, such as '|S10' or '<f8' and not Python types, such as str or float.  If you type help(np.dtype) you'll see many examples of how np.dtypes can be specified. (I've only mentioned a few.)\n Text: Note that np.array expects a list of tuples. It's rather particular about that. \n Text: A list of lists raises TypeError: expected a readable buffer object.\n Text: A (tuple of tuples) or a (tuple of lists) raises ValueError: setting an array element with a sequence.\n\nAPI:\nnumpy.dtype\nhelp\nnumpy.dtype\nnumpy.dtype\nnumpy.array\n","label":[[336,344,"Mention"],[480,488,"Mention"],[522,531,"Mention"],[596,604,"Mention"],[859,870,"API"],[876,887,"API"],[888,899,"API"],[900,911,"API"]],"Comments":[]}
{"id":60735,"text":"ID:11056898\nPost:\nText: What's happening is that matplotlib doesn't actually use datetime objects for plotting.\n Text: Dates are first converted into an internal floating point format. The conversion isn't set up to handle timedeltas (which is arguably an oversight).\n Text: You can basically do exactly what you wanted, you just need to explictly convert the dates to matplotlib's internal format first, and then call ax.xaxis_date().\n Text: As a quick example (Most of this is generating data to plot...):\n Code: import datetime as dt\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndef drange(start, end, interval=dt.timedelta(days=1)):\n    output = []\n    while start <= end:\n        output.append(start)\n        start += interval\n    return output\n\n# Generate a series of dates for plotting...\nedate = drange(dt.datetime(2012, 2, 1), dt.datetime(2012, 6, 15), \n                      dt.timedelta(days=5))\nbdate = drange(dt.datetime(2012, 1, 1), dt.datetime(2012, 5, 15), \n                      dt.timedelta(days=5))\n\n# Now convert them to matplotlib's internal format...\nedate, bdate = [mdates.date2num(item) for item in (edate, bdate)]\n\nypos = range(len(edate))\nfig, ax = plt.subplots()\n\n# Plot the data\nax.barh(ypos, edate - bdate, left=bdate, height=0.8, align='center')\nax.axis('tight')\n\n# We need to tell matplotlib that these are dates...\nax.xaxis_date()\n\nplt.show()\n\n\nAPI:\nmatplotlib.axes.Axes.xaxis_date\n","label":[[419,434,"Mention"],[1404,1435,"API"]],"Comments":[]}
{"id":60736,"text":"ID:12056933\nPost:\nText: Edit 2014-09-30:\n Text: pandas now has a read_sql function. You definitely want to use that instead.\n Text: Original answer:\n Text: I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:\n Code: import decimal\n\nimport pyodbc #just corrected a typo here\nimport numpy as np\nimport pandas\n\ncnn, cur = myConnectToDBfunction()\ncmd = \"SELECT * FROM myTable\"\ncur.execute(cmd)\ndataframe = __processCursor(cur, dataframe=True)\n\ndef __processCursor(cur, dataframe=False, index=None):\n    '''\n    Processes a database cursor with data on it into either\n    a structured numpy array or a pandas dataframe.\n\n    input:\n    cur - a pyodbc cursor that has just received data\n    dataframe - bool. if false, a numpy record array is returned\n                if true, return a pandas dataframe\n    index - list of column(s) to use as index in a pandas dataframe\n    '''\n    datatypes = []\n    colinfo = cur.description\n    for col in colinfo:\n        if col[1] == unicode:\n            datatypes.append((col[0], 'U%d' % col[3]))\n        elif col[1] == str:\n            datatypes.append((col[0], 'S%d' % col[3]))\n        elif col[1] in [float, decimal.Decimal]:\n            datatypes.append((col[0], 'f4'))\n        elif col[1] == datetime.datetime:\n            datatypes.append((col[0], 'O4'))\n        elif col[1] == int:\n            datatypes.append((col[0], 'i4'))\n\n    data = []\n    for row in cur:\n        data.append(tuple(row))\n\n    array = np.array(data, dtype=datatypes)\n    if dataframe:\n        output = pandas.DataFrame.from_records(array)\n\n        if index is not None:\n            output = output.set_index(index)\n\n    else:\n        output = array\n\n    return output\n\n\nAPI:\npandas.read_sql\n","label":[[65,73,"Mention"],[1806,1821,"API"]],"Comments":[]}
{"id":60737,"text":"ID:1903579\nPost:\nText: b[a.argsort()] should do the trick.\n Text: Here's how it works. First you need to find a permutation that sorts a. argsort is a method that computes this:\n Code: >>> a = numpy.array([2, 3, 1])\n>>> p = a.argsort()\n>>> p\n[2, 0, 1]\n\n Text: You can easily check that this is right:\n Code: >>> a[p]\narray([1, 2, 3])\n\n Text: Now apply the same permutation to b.\n Code: >>> b = numpy.array([4, 6, 7])\n>>> b[p]\narray([7, 4, 6])\n\n\nAPI:\nnumpy.ndarray.argsort\nnumpy.ndarray.argsort\n","label":[[25,36,"Mention"],[138,145,"Mention"],[450,471,"API"],[472,493,"API"]],"Comments":[]}
{"id":60738,"text":"ID:3720423\nPost:\nText: You could subclass numpy.ndarray and override the __add__ method, but I think it would be far simpler to just be explicit. For example:\n Code: import numpy as np\nx = np.array([[0,0],[1,1]])\ny = np.array([[1,0],[0,1]])\n\nprint (x + y) % 2\n\n Text: Which yields:\n Code: array([[1, 0],\n       [1, 0]])\n\n\nAPI:\nnumpy.ndarray\nnumpy.ndarray.__add__\n","label":[[73,80,"Mention"],[341,362,"API"]],"Comments":[]}
{"id":60739,"text":"ID:8462660\nPost:\nText: You can use matplotlib.pyplot.xticks to set the locations of the x-axis tick marks.\n Text: Without the code used to generate the question's histogram, I resorted to creating data to produce a similar histogram. In this first example, we have a histogram with the default tick marks.\n Code: from pylab import hist, show\n\nx = [1.1]*29 + [2]*7 + [3.2]*3 + [5]\nhist(x)\nshow()\n\n Text: The objective is to have tick marks at 1, 2, 3, 4, and 5, and the next example does this by using xticks.\n Code: from pylab import hist, show, xticks\n\nx = [1.1]*29 + [2]*7 + [3.2]*3 + [5]\nhist(x)\nxticks(range(1, 6))\nshow()\n\n\nAPI:\nmatplotlib.pyplot.xticks\npylab.xticks\n","label":[[501,507,"Mention"],[658,670,"API"]],"Comments":[]}
{"id":60740,"text":"ID:17142391\nPost:\nText: if you use svm.LinearSVC() as estimator, and .decision_function() (which is like svm.SVC's .predict_proba()) for sorting the results from most probable class to the least probable one. this agrees with .predict() function. Plus, this estimator is faster and gives almost the same results with svm.SVC() \n Text: the only drawback for you might be that .decision_function() gives a signed value sth like between -1 and 3 instead of a probability value. but it agrees with the prediction.\n\nAPI:\nsklearn.svm.LinearSVC\nsklearn.svm.LinearSVC.decision_function\nsklearn.svm.SVC\nsklearn.svm.SVC.predict_proba\nsklearn.svm.SVC.predict\nsklearn.svm.SVC\nsklearn.svm.LinearSVC.decision_function\n","label":[[35,50,"Mention"],[69,89,"Mention"],[105,114,"Mention"],[115,131,"Mention"],[226,236,"Mention"],[317,326,"Mention"],[375,395,"Mention"],[516,537,"API"],[538,577,"API"],[578,593,"API"],[594,623,"API"],[624,647,"API"],[648,663,"API"],[664,703,"API"]],"Comments":[]}
{"id":60741,"text":"ID:14364581\nPost:\nText: If you do the cross validation on a pipeline that wraps both the feature extractor (e.g. CountVectorizer or TfidfVectorizer) and the classifier then everything will work out of the box automatically: features that occur only in the train test set will just be ignored (not mapped to a dimension in the vector representation).\n Text: There is more details about how the vocabulary_ attribute is used to map feature names to dimensions in the documentation on text feature extraction.\n Text: There is also an example that shows how to cross validate a pipeline that comprise a feature extraction component and a classifier.\n Text: Edit: fixed train \/ test typo\n Text: Edit 2: fixed broken link to example.\n\nAPI:\nsklearn.feature_extraction.text.CountVectorizer\nsklearn.feature_extraction.text.TfidfVectorizer\n","label":[[113,128,"Mention"],[132,147,"Mention"],[734,781,"API"],[782,829,"API"]],"Comments":[]}
{"id":60742,"text":"ID:13652027\nPost:\nText: I didn't find a straight-forward way to do it within context of read_csv. However, read_csv returns a DataFrame, which can be filtered by selecting rows by boolean vector df[bool_vec]:\n Code: filtered = df[(df['timestamp'] > targettime)]\n\n Text: This is selecting all rows in df (assuming df is any DataFrame, such as the result of a read_csv call, that at least contains a datetime column timestamp) for which the values in the timestamp column are greater than the value of targettime. Similar question.\n\nAPI:\npandas.read_csv\npandas.read_csv\npandas.DataFrame\npandas.DataFrame\npandas.read_csv\n","label":[[88,96,"Mention"],[107,115,"Mention"],[126,135,"Mention"],[323,332,"Mention"],[358,366,"Mention"],[536,551,"API"],[552,567,"API"],[568,584,"API"],[585,601,"API"],[602,617,"API"]],"Comments":[]}
{"id":60743,"text":"ID:16967324\nPost:\nText: Just use the DataFrame constructor...\n Code: In [6]: tweet_sample = [{'contributers': None, 'truncated': False, 'text': 'foo'}, {'contributers': None, 'truncated': True, 'text': 'bar'}]\n\nIn [7]: df = pd.DataFrame(tweet_sample)\n\nIn [8]: df\nOut[8]:\n  contributers text truncated\n0         None  foo     False\n1         None  bar      True\n\n Text: If you have the file as a JSON you can open it using json.load:\n Code: import json\nwith open('<MyFilePath>\\usTweets0.json', 'r') as f:\n    tweet_sample = json.load(f)\n\n Text: There will be a from_json coming soon to pandas...\n\nAPI:\npandas.DataFrame\njson.load\n","label":[[37,46,"Mention"],[601,617,"API"]],"Comments":[]}
{"id":60744,"text":"ID:9652066\nPost:\nText: I don't think you need that call to xaxis_date(); since you are already providing the x-axis data in a format that matplotlib knows how to deal with.  I also think there's something slightly wrong with your secs formula.  \n Text: We can make use of matplotlib's built-in formatters and locators to: \n Text: \nset the major xticks to a regular interval (minutes, hours, days, etc.)\ncustomize the display using a strftime formatting string\n\n Text: It appears that if a formatter is not specified, the default is to display the year; which is what you were seeing.  \n Text: Try this out:\n Code: import datetime as dt\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter, MinuteLocator\n\nx = [16.7,16.8,17.1,17.4]\ny = [15,17,14,16]\ne = [0.8,1.2,1.1,0.9]\nxtn = []\nfor t in x:\n  h = int(t)\n  m = int((t-int(t))*60)\n  xtn.append(dt.datetime.combine(dt.date(2012,1,1), dt.time(h,m)))\n\ndef larger_alim( alim ):\n  ''' simple utility function to expand axis limits a bit '''\n  amin,amax = alim\n  arng = amax-amin\n  nmin = amin - 0.1 * arng\n  nmax = amax + 0.1 * arng\n  return nmin,nmax\n\nplt.errorbar(xtn,y,yerr=e,fmt='+',elinewidth=2,capsize=0,color='k',ecolor='k')\nplt.gca().xaxis.set_major_locator( MinuteLocator(byminute=range(0,60,10)) )\nplt.gca().xaxis.set_major_formatter( DateFormatter('%H:%M:%S') )\nplt.gca().set_xlim( larger_alim( plt.gca().get_xlim() ) )\nplt.show()\n\n Text: Result:\n Text: FWIW the utility function larger_alim was originally written for this other question:  Is there a way to tell matplotlib to loosen the zoom on the plotted data?\n\nAPI:\nmatplotlib.axes.Axes.xaxis_date\n","label":[[59,71,"Mention"],[1598,1629,"API"]],"Comments":[]}
{"id":60745,"text":"ID:9547928\nPost:\nText: np.delete will take an array of indicies of any size.  You can simply populate your entire array of indicies and perform the delete once, therefore only deallocating and reallocating once.  (not tested. possible typos.)\n Code: bb = np.empty((b.size, 21), dtype=int)\nfor i,v in enumerate(b):\n    bb[i] = v+np.arange(-10,11)\n\nnp.delete(a, bb.flat)  # looks like .flat is optional\n\n Text: Note, if your ranges overlap, you'll get a difference between this and your algorithm:  where yours will remove more items than those originally 10 indices away.\n\nAPI:\nnumpy.delete\n","label":[[23,32,"Mention"],[577,589,"API"]],"Comments":[]}
{"id":60746,"text":"ID:7821917\nPost:\nText: This is a handy trick for unit tests and the like, when you need to do a pixel-to-pixel comparison with a saved plot.\n Text: One way is to use fig.canvas.tostring_rgb and then numpy.fromstring with the approriate dtype.  There are other ways as well, but this is the one I tend to use.\n Text: E.g.\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\n# Make a random plot...\nfig = plt.figure()\nfig.add_subplot(111)\n\n# If we haven't already shown or saved the plot, then we need to\n# draw the figure first...\nfig.canvas.draw()\n\n# Now we can save it to a numpy array.\ndata = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\ndata = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n\n\nAPI:\nmatplotlib.backends.backend_agg.FigureCanvasAgg.tostring_rgb\nnumpy.fromstring\n","label":[[166,189,"Mention"],[730,790,"API"]],"Comments":[]}
{"id":60747,"text":"ID:16967466\nPost:\nText: Note: Now that Series have the dt accessor it's less important that date is the index, though Date\/Time still needs to be a datetime64.\n Text: Update: You can do the groupby more directly (without the lambda):\n Code: In [21]: df.groupby([df[\"Date\/Time\"].dt.year, df[\"Date\/Time\"].dt.hour]).mean()\nOut[21]:\n                     Value\nDate\/Time Date\/Time\n2010      0             60\n          1             50\n          2             52\n          3             49\n\nIn [22]: res = df.groupby([df[\"Date\/Time\"].dt.year, df[\"Date\/Time\"].dt.hour]).mean()\n\nIn [23]: res.index.names = [\"year\", \"hour\"]\n\nIn [24]: res\nOut[24]:\n           Value\nyear hour\n2010 0        60\n     1        50\n     2        52\n     3        49\n\n Text: If it's a datetime64 index you can do:\n Code: In [31]: df1.groupby([df1.index.year, df1.index.hour]).mean()\nOut[31]:\n        Value\n2010 0     60\n     1     50\n     2     52\n     3     49\n\n Text: Old answer (will be slower):\n Text: Assuming Date\/Time was the index* you can use a mapping function in the groupby:\n Code: In [11]: year_hour_means = df1.groupby(lambda x: (x.year, x.hour)).mean()\n\nIn [12]: year_hour_means\nOut[12]:\n           Value\n(2010, 0)     60\n(2010, 1)     50\n(2010, 2)     52\n(2010, 3)     49\n\n Text: For a more useful index, you could then create a MultiIndex from the tuples:\n Code: In [13]: year_hour_means.index = pd.MultiIndex.from_tuples(year_hour_means.index,\n                                                           names=['year', 'hour'])\n\nIn [14]: year_hour_means\nOut[14]:\n           Value\nyear hour\n2010 0        60\n     1        50\n     2        52\n     3        49\n\n Text: * if not, then first use set_index:\n Code: df1 = df.set_index('Date\/Time')\n\n\nAPI:\npandas.Series\nnumpy.datetime64\npandas.DataFrame.groupby\npandas.DataFrame.groupby\npandas.MultiIndex\npandas.DataFrame.set_index\n","label":[[39,45,"Mention"],[148,158,"Mention"],[190,197,"Mention"],[1044,1051,"Mention"],[1311,1321,"Mention"],[1674,1683,"Mention"],[1731,1744,"API"],[1745,1761,"API"],[1762,1786,"API"],[1787,1811,"API"],[1812,1829,"API"],[1830,1856,"API"]],"Comments":[]}
{"id":60748,"text":"ID:15242142\nPost:\nText: BaseEstimator provides among other things a default implementation for the get_params and set_params methods, see [the source code]. This is useful to make the model grid search-able with GridSearchCV for automated parameters tuning and behave well with others when combined in a Pipeline.\n\nAPI:\nsklearn.base.BaseEstimator\nsklearn.base.BaseEstimator.get_params\nsklearn.base.BaseEstimator.set_params\nsklearn.model_selection.GridSearchCV\nsklearn.pipeline.Pipeline\n","label":[[24,37,"Mention"],[99,109,"Mention"],[114,124,"Mention"],[212,224,"Mention"],[304,312,"Mention"],[320,346,"API"],[347,384,"API"],[385,422,"API"],[423,459,"API"],[460,485,"API"]],"Comments":[]}
{"id":60749,"text":"ID:6408525\nPost:\nText: You can use np.nonzero (or ndarray.nonzero) on your boolean array to get corresponding numerical indices, then use these to access the sparse matrix.  Since \"fancy indexing\" on sparse matrices is quite limited compared to dense ndarrays, you need to unpack the rows tuple returned by nonzero and specify that you want to retrieve all columns using the : slice:\n Code: >>> rows.nonzero()\n(array([0, 2]),)\n>>> indices = rows.nonzero()[0]\n>>> indices\narray([0, 2])\n>>> sparse[indices, :]\n<2x100 sparse matrix of type '<type 'numpy.float64'>'\n        with 6 stored elements in LInked List format>\n\n\nAPI:\nnumpy.nonzero\nnumpy.ndarray.nonzero\n","label":[[35,45,"Mention"],[50,65,"Mention"],[623,636,"API"],[637,658,"API"]],"Comments":[]}
{"id":60750,"text":"ID:15140961\nPost:\nText: Suppose you have a csv file that looks like this:\n Code: date,time,milliseconds,value\n20120201,41206,300,1\n20120201,151117,770,2\n\n Text: Then using parse_dates, index_cols and date_parser parameters of read_csv method, one could construct a pandas DataFrame with time index like this:\n Code: import datetime as dt\nimport pandas as pd\nparse = lambda x: dt.datetime.strptime(x, '%Y%m%d %H%M%S %f')\ndf = pd.read_csv('test.csv', parse_dates=[['date', 'time', 'milliseconds']],\n                 index_col=0, date_parser=parse)\n\n Text: This yields:\n Code:                             value\ndate_time_milliseconds           \n2012-02-01 04:12:06.300000      1\n2012-02-01 15:11:17.770000      2\n\n Text: And df.index:\n Code: <class 'pandas.tseries.index.DatetimeIndex'>\n[2012-02-01 04:12:06.300000, 2012-02-01 15:11:17.770000]\nLength: 2, Freq: None, Timezone: None\n\n Text: This answer is based on a similar solution proposed here.\n\nAPI:\npandas.read_csv\npandas.DataFrame\n","label":[[226,234,"Mention"],[272,281,"Mention"],[951,966,"API"],[967,983,"API"]],"Comments":[]}
{"id":60751,"text":"ID:17699049\nPost:\nText: You can use the pivot method for this.\n Text: See the docs: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/reshaping.html#reshaping-by-pivoting-dataframe-objects\n Text: An example:\n Code: In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'a':[0,1,2,3,4,4], 'b':[1,2,2,3,1,3], 'c':[10,10,20,3\n0,40,10]})\n\nIn [3]: df\nOut[3]:\n   a  b   c\n0  0  1  10\n1  1  2  10\n2  2  2  20\n3  3  3  30\n4  4  1  40\n5  4  3  10\n\nIn [4]: df.pivot(index='a', columns='b', values='c')\nOut[4]:\nb   1   2   3\na\n0  10 NaN NaN\n1 NaN  10 NaN\n2 NaN  20 NaN\n3 NaN NaN  30\n4  40 NaN  10\n\n Text: If you want zeros instead of NaN's as in your example, you can use fillna:\n Code: In [5]: df.pivot(index='a', columns='b', values='c').fillna(0)\nOut[5]:\nb   1   2   3\na\n0  10   0   0\n1   0  10   0\n2   0  20   0\n3   0   0  30\n4  40   0  10\n\n\nAPI:\npandas.DataFrame.pivot\npandas.DataFrame.fillna\n","label":[[40,45,"Mention"],[657,663,"Mention"],[836,858,"API"],[859,882,"API"]],"Comments":[]}
{"id":60752,"text":"ID:9610177\nPost:\nText: Yes, you can use the axes() function, as shown in this example from the matplotlib gallery. My favourite way to see if something is possible with matplotlib is to scan through the gallery for something that looks similar to what I want.\n Text: Unfortunately, matplotlib's site appears to have just gone down. There's an older copy on Github.\n\nAPI:\nmatplotlib.axes\n","label":[[44,50,"Mention"],[371,386,"API"]],"Comments":[]}
{"id":60753,"text":"ID:13979142\nPost:\nText: You want to use set_index:\n Code: df1 = df.set_index('date_time')\n\n Text: Which selects the column 'date_time' as an index for the new DataFrame.\n Text: .\n Text: Note: The behaviour you are coming across in the DataFrame constructor is demonstrated as follows:\n Code: df = pd.DataFrame([[1,2],[3,4]])\ndf1 = pd.DataFrame(df, index=[1,2])\n\nIn [3]: df1\nOut[3]: \n    0   1\n1   3   4\n2 NaN NaN\n\n\nAPI:\npandas.DataFrame.set_index\npandas.DataFrame\n","label":[[40,49,"Mention"],[159,168,"Mention"],[420,446,"API"],[447,463,"API"]],"Comments":[]}
{"id":60754,"text":"ID:13839029\nPost:\nText: Pandas 0.15 introduced Categorical Series, which allows a much clearer way to do this:\n Text: First make the month column a categorical and specify the ordering to use.\n Code: In [21]: df['m'] = pd.Categorical(df['m'], [\"March\", \"April\", \"Dec\"])\n\nIn [22]: df  # looks the same!\nOut[22]:\n   a  b      m\n0  1  2  March\n1  5  6    Dec\n2  3  4  April\n\n Text: Now, when you sort the month column it will sort with respect to that list:\n Code: In [23]: df.sort_values(\"m\")\nOut[23]:\n   a  b      m\n0  1  2  March\n2  3  4  April\n1  5  6    Dec\n\n Text: Note: if a value is not in the list it will be converted to NaN.\n Text: An older answer for those interested...\n Text: You could create an intermediary series, and set_index on that:\n Code: df = pd.DataFrame([[1, 2, 'March'],[5, 6, 'Dec'],[3, 4, 'April']], columns=['a','b','m'])\ns = df['m'].apply(lambda x: {'March':0, 'April':1, 'Dec':3}[x])\ns.sort_values()\n\nIn [4]: df.set_index(s.index).sort()\nOut[4]: \n   a  b      m\n0  1  2  March\n1  3  4  April\n2  5  6    Dec\n\n Text: As commented, in newer pandas, Series has a replace method to do this more elegantly:\n Code: s = df['m'].replace({'March':0, 'April':1, 'Dec':3})\n\n Text: The slight difference is that this won't raise if there is a value outside of the dictionary (it'll just stay the same).\n\nAPI:\npandas.DataFrame.set_index\npandas.Series\npandas.Series.replace\n","label":[[732,741,"Mention"],[1074,1080,"Mention"],[1087,1094,"Mention"],[1324,1350,"API"],[1351,1364,"API"],[1365,1386,"API"]],"Comments":[]}
{"id":60755,"text":"ID:13688105\nPost:\nText: There's a bug here: currently cannot pass arguments to str.lstrip and str.rstrip:\n Text: http:\/\/github.com\/pydata\/pandas\/issues\/2411\n Text: EDIT: 2012-12-07 this works now on the dev branch:\n Code: In [8]: df['result'].str.lstrip('+-').str.rstrip('aAbBcC')\nOut[8]: \n1     52\n2     62\n3     44\n4     30\n5    110\nName: result\n\n\nAPI:\npandas.Series.str.lstrip\npandas.Series.str.rstrip\n","label":[[79,89,"Mention"],[94,104,"Mention"],[355,379,"API"],[380,404,"API"]],"Comments":[]}
{"id":60756,"text":"ID:12130021\nPost:\nText: It is possible to get probability estimates from SVMs in scikit-learn by simply setting probability=True when constructing the SVC object. The docs only warn that the probability estimates might not be very good.\n Text: The quintessential probabilistic classifier is logistic regression, so you might give that a try. Note that LR is a linear model though, unlike SVMs which can learn complicated non-linear decision boundaries by using kernels.\n\nAPI:\nsklearn.svm.SVC\n","label":[[151,154,"Mention"],[476,491,"API"]],"Comments":[]}
{"id":60757,"text":"ID:2054156\nPost:\nText: Using numpy, you can use np.average with the axis keyword:\n Code: import numpy as np\nx=np.arange(12)\ny=x.reshape(3,4)\nprint(y)\n# [[ 0  1  2  3]\n#  [ 4  5  6  7]\n#  [ 8  9 10 11]]\nprint(np.average(y,axis=1))\n# [ 1.5  5.5  9.5]\n\n Text: Note that to reshape x, I had to make x start with a length evenly divisible by the group size (in this case 4). \n Text: If the length of x is not evenly divisible by the group size, then could create a masked array and use np.ma.average to compute the appropriate average.\n Text: For example,\n Code: x=np.ma.arange(12)\ny=x.reshape(3,4)\nmask=(x>=10)\ny.mask=mask\nprint(y)\n# [[0 1 2 3]\n#  [4 5 6 7]\n#  [8 9 -- --]]\nprint(np.ma.average(y,axis=1))\n# [1.5 5.5 8.5]\n\n\nAPI:\nnumpy.average\nnumpy.ma.average\n","label":[[48,58,"Mention"],[481,494,"Mention"],[724,737,"API"],[738,754,"API"]],"Comments":[]}
{"id":60758,"text":"ID:15109783\nPost:\nText: You can use numpy's genfromtxt function to retrieve data from the file(http:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.genfromtxt.html)\n Code: import numpy as np\nmydata = np.genfromtxt(filename, delimiter=\",\")\n\n Text: However, if you have textual columns, using genfromtxt is trickier, since you need to specify the data types.\n Text: It will be much easier with the excellent Pandas library (http:\/\/pandas.pydata.org\/)\n Code: import pandas as pd\nmydata = pd.read_csv(filename)\ntarget = mydata[\"Label\"]  #provided your csv has header row, and the label column is named \"Label\"\n\n#select all but the last column as data\ndata = mydata.ix[:,:-1]\n\n\nAPI:\nnumpy.genfromtxt\nnumpy.genfromtxt\n","label":[[44,54,"Mention"],[296,306,"Mention"],[683,699,"API"],[700,716,"API"]],"Comments":[]}
{"id":60759,"text":"ID:4002456\nPost:\nText: scipy.stats.uniform actually uses numpy, here is the corresponding function in stats (mtrand is an alias for numpy.random)\n Code: class uniform_gen(rv_continuous):\n    def _rvs(self):\n        return mtrand.uniform(0.0,1.0,self._size)\n\n Text: scipy.stats has a bit of overhead for error checking and making the interface more flexible. The speed difference should be minimal as long as you don't call uniform.rvs in a loop for each draw. You can get instead all random draws at once, for example (10 million)\n Code: >>> rvs = stats.uniform.rvs(size=(10000, 1000))\n>>> rvs.shape\n(10000, 1000)\n\n Text: Here is the long answer, that I wrote a while ago:\n Text: The basic random numbers in scipy\/numpy are created by\nMersenne-Twister PRNG in numpy.random. The random numbers for\ndistributions in numpy.random are in cython\/pyrex and are pretty fast.\n Text: scipy.stats doesn't have a random number generator, random numbers are\nobtained in one of three ways:\n Text: \ndirectly from numpy.random, e.g. normal, t, ...  pretty fast\nrandom numbers by transformation of other random numbers that are\navailable in numpy.random, also pretty fast because this operates on\nentire arrays of numbers\ngeneric: the only generic generation random number generation is by\nusing the ppf (inverse cdf) to transform uniform random numbers.\nThis is relatively fast if there is an explicit expression for the\nppf, but can be very slow if the ppf has to be calculated\nindirectly. For example if only the pdf is defined, then the cdf is\nobtained through numerical integration and the ppf is obtained through\nan equation solver. So a few distributions are very slow.\n\n\nAPI:\nscipy.stats.uniform\nscipy.stats\nnumpy.random\nscipy.stats\nscipy.stats.uniform.rvs\nnumpy.random\nnumpy.random\nscipy.stats\nnumpy.random\nnumpy.random\nscipy.stats.uniform.ppf\nscipy.stats.uniform.cdf\nscipy.stats.uniform.ppf\nscipy.stats.uniform.ppf\nscipy.stats.uniform.pdf\nscipy.stats.uniform.cdf\nscipy.stats.uniform.ppf\n","label":[[102,107,"Mention"],[423,434,"Mention"],[1284,1287,"Mention"],[1297,1300,"Mention"],[1406,1409,"Mention"],[1439,1442,"Mention"],[1500,1503,"Mention"],[1525,1528,"Mention"],[1579,1582,"Mention"],[1688,1699,"API"],[1725,1748,"API"],[1813,1836,"API"],[1837,1860,"API"],[1861,1884,"API"],[1885,1908,"API"],[1909,1932,"API"],[1933,1956,"API"],[1957,1980,"API"]],"Comments":[]}
{"id":60760,"text":"ID:18409208\nPost:\nText: Just call the plt.xticks and plt.yticks functions.\n Text: First you have to choose where you want your ticks to be on the axis, and then you have to set the labels.\n Text: For example: suppose you have an x axis that spans from 5 to 25 and you want 3 ticks at 8, 15 and 22, and you want labels foo, bar, baz.\n Text: Then you should call:\n Code: # do your plotting first, for example\nx = np.arange(5, 25)\ny = x * x\nplt.plot(x, y)\n# and then the ticks\nplt.xticks([8, 15, 22], ['foo', 'bar', 'baz'])\n# And finally show the plot\nplt.show()\n\n Text: In your case, since your labels ticks are at [0, 1, 2] and you want hello, apple and orange as your labels. You should do:\n Code: plt.xticks([0, 1, 2], ['hello', 'apple', 'orange'])\n\n\nAPI:\nmatplotlib.pyplot.xticks\nmatplotlib.pyplot.yticks\n","label":[[38,48,"Mention"],[53,63,"Mention"],[757,781,"API"],[782,806,"API"]],"Comments":[]}
{"id":60761,"text":"ID:14079641\nPost:\nText: Since pandas in it's current form assumes time series data are arranged with time in the index, not the columns, transposing the DataFrame, at least temporarily, will enable the use of many built-in methods, such as  shift\/diff\/pct_change\/etc.\n Code: In [78]: df = DataFrame(np.random.rand(100, 3) * 100,\n                        columns=['Day1', 'Day2', 'Day3'])\n\nIn [79]: df.head()\nOut[79]: \n        Day1       Day2       Day3\n0  27.113276   0.827977  37.059887\n1  48.817798  19.335033  12.476411\n2  27.001015  18.147742  33.094676\n3  38.428321  95.609824  72.395564\n4  63.626472  36.207677   1.328216\n\nIn [80]: dft = df.T\n\nIn [82]: dft.ix[:, :5]\nOut[82]: \n              0          1          2          3          4          5\nDay1  27.113276  48.817798  27.001015  38.428321  63.626472  25.900132\nDay2   0.827977  19.335033  18.147742  95.609824  36.207677   0.191767\nDay3  37.059887  12.476411  33.094676  72.395564   1.328216  37.011027\n\nIn [89]: dft.pct_change().ix[:, :5]\nOut[89]: \n              0         1         2         3         4           5\nDay1        NaN       NaN       NaN       NaN       NaN         NaN\nDay2  -0.969462 -0.603935 -0.327887  1.488004 -0.430934   -0.992596\nDay3  43.759576 -0.354725  0.823625 -0.242802 -0.963317  191.999688\n\nIn [94]: chg = (dft.pct_change().dropna() < .95).T.astype(int)\n\nIn [95]: chg.head()\nOut[95]: \n   Day2  Day3\n0     1     0\n1     1     1\n2     1     1\n3     0     1\n4     1     1\n\n\nAPI:\npandas.DataFrame\npandas.DataFrame.shift\npandas.DataFrame.diff\npandas.DataFrame.pct_change\n","label":[[153,162,"Mention"],[241,246,"Mention"],[247,251,"Mention"],[252,262,"Mention"],[1471,1487,"API"],[1488,1510,"API"],[1511,1532,"API"],[1533,1560,"API"]],"Comments":[]}
{"id":60762,"text":"ID:9020157\nPost:\nText: apply_along_axis applies the supplied function along 1D slices of the input array, with the slices taken along the axis you specify. So in your example, new_func is applied over each slice of the array along the first axis. It becomes clearer if you use a vector valued function, rather than a scalar, like this:\n Code: In [20]: b = np.array([[1,2,3], [4,5,6], [7,8,9]])\n\nIn [21]: np.apply_along_axis(np.diff,0,b)\nOut[21]: \narray([[3, 3, 3],\n       [3, 3, 3]])\n\nIn [22]: np.apply_along_axis(np.diff,1,b)\nOut[22]: \narray([[1, 1],\n       [1, 1],\n       [1, 1]])\n\n Text: Here, numpy.diff (i.e. the arithmetic difference of adjacent array elements) is applied along each slice of either the first or second axis (dimension) of the input array.\n\nAPI:\nnumpy.apply_along_axis\nnumpy.diff\n","label":[[23,39,"Mention"],[769,791,"API"]],"Comments":[]}
{"id":60763,"text":"ID:6091530\nPost:\nText: As explained in the Scipy documentation, medfilt2 is a median filter. Quoting from the documentation,\n Code: The sample median is the middle array value in a sorted list of neighborhood values\n\n Text: So for your example, the submatrix at position 1,1\n Code: 1 2 3\n3 4 2\n1 7 3\n\n Text: would be sorted into\n Code: 1 1 2 2 3 3 3 4 7\n\n Text: The middle element is 3 so that would be the output of the filter.  There's more detail on Wikipedia. \n\nAPI:\nscipy.signal.medfilt2d\n","label":[[64,72,"Mention"],[471,493,"API"]],"Comments":[]}
{"id":60764,"text":"ID:1614065\nPost:\nText: Use np.intersect1d. \n Code: #!\/usr\/bin\/env python\nimport numpy as np\na = np.array([1,2,3,4,5,6])\nb = np.array([1,4,5])\nc=np.intersect1d(a,b)\nprint(c)\n# [1 4 5]\n\n Text: Note that np.intersect1d gives the wrong answer if a or b have nonunique elements. In that case use\nnp.intersect1d_nu.\n Text: There is also np.setdiff1d, setxor1d, setmember1d, and union1d. See\nNumpy Example List With Doc\n\nAPI:\nnumpy.intersect1d\nnumpy.intersect1d\nnumpy.setdiff1d\nnumpy.setxor1d\nnumpy.setmember1d\nnumpy.union1d\n","label":[[27,41,"Mention"],[201,215,"Mention"],[331,343,"Mention"],[345,353,"Mention"],[355,366,"Mention"],[372,379,"Mention"],[419,436,"API"],[437,454,"API"],[455,470,"API"],[471,485,"API"],[486,503,"API"],[504,517,"API"]],"Comments":[]}
{"id":60765,"text":"ID:17813222\nPost:\nText: You can specify the style of the plotted line when calling df.plot:\n Code: df.plot(x='col_name_1', y='col_name_2', style='o')\n\n Text: The style argument can also be a dict or list, e.g.:\n Code: import numpy as np\nimport pandas as pd\n\nd = {'one' : np.random.rand(10),\n     'two' : np.random.rand(10)}\n\ndf = pd.DataFrame(d)\n\ndf.plot(style=['o','rx'])\n\n Text: All the accepted style formats are listed in the documentation of matplotlib.pyplot.plot.\n\nAPI:\npandas.DataFrame.plot\nmatplotlib.pyplot.plot\n","label":[[83,90,"Mention"],[477,498,"API"]],"Comments":[]}
{"id":60766,"text":"ID:14375841\nPost:\nText: Use pandas:\n Code: import pandas as pd\nprint pd.read_csv('test.csv',sep='|')\n\n Text: out:\n Code:    A   B  C   D  x  F   G  x.1  H\n0  1 NaN  2 NaN  3  4 NaN    5  6\n1  1 NaN  2 NaN  3  4 NaN    5  6\n2  1 NaN  2 NaN  3  4 NaN    5  6\n3  1 NaN  2 NaN  3  4 NaN    5  6\n\n Text: and if you need you can convert it to a dict using to_dict() method:\n Code: {'A': {0: 1, 1: 1, 2: 1, 3: 1},\n 'B': {0: nan, 1: nan, 2: nan, 3: nan},\n 'C': {0: 2, 1: 2, 2: 2, 3: 2},\n 'D': {0: nan, 1: nan, 2: nan, 3: nan},\n 'F': {0: 4, 1: 4, 2: 4, 3: 4},\n 'G': {0: nan, 1: nan, 2: nan, 3: nan},\n 'H': {0: 6, 1: 6, 2: 6, 3: 6},\n 'x': {0: 3, 1: 3, 2: 3, 3: 3},\n 'x.1': {0: 5, 1: 5, 2: 5, 3: 5}}\n\n Text: EDIT:\nIf you need certain names for columns you can do this:\n Code: import pandas as pd\ndf = pd.read_csv('test.csv',sep='|')\ndf.columns = [df.columns[index-1]+'x' if 'x' in name \n              else name for index,name in enumerate(df.columns)]\nprint df\n\n   A   B  C   D  Dx  F   G  Gx  H\n0  1 NaN  2 NaN   3  4 NaN   5  6\n1  1 NaN  2 NaN   3  4 NaN   5  6\n2  1 NaN  2 NaN   3  4 NaN   5  6\n3  1 NaN  2 NaN   3  4 NaN   5  6\n\n Text: If you want to lose the empty cols:\n Code: print df.dropna(axis=1,how='all')\n\n   A  C  Dx  F  Gx  H\n0  1  2   3  4   5  6\n1  1  2   3  4   5  6\n2  1  2   3  4   5  6\n3  1  2   3  4   5  6\n\n\nAPI:\npandas.DataFrame.to_dict\n","label":[[350,359,"Mention"],[1324,1348,"API"]],"Comments":[]}
{"id":60767,"text":"ID:10730365\nPost:\nText: I would strongly recommend using Pandas.  Here I'm using version 0.8 (soon to be released). I think this is close to what you want.\n Code: import pandas as p\nimport numpy as np\nimport matplotlib as plt\n\n# Make up some data:\ntime = p.date_range(start='2011-05-23', end='2012-05-23', freq='min')\nwatts = np.linspace(0, 3.14 * 365, time.size)\nwatts = 38 * (1.5 + np.sin(watts)) + 8 * np.sin(5 * watts)\n\n# Create a time series\nts = p.Series(watts, index=time, name='watts')\n\n# Resample down to 15 minute pieces, using mean values\nts15 = ts.resample('15min', how='mean')\nts15.plot()\n\n Text: Pandas can easily do many other things with your data (like determine your average weekly energy profile). Check out p.read_csv() for reading in your data.\n\nAPI:\npandas.read_csv\n","label":[[727,739,"Mention"],[772,787,"API"]],"Comments":[]}
{"id":60768,"text":"ID:13842286\nPost:\nText: RukTech's answer, df.set_value('C', 'x', 10), is far and away faster than the options I've suggested below. However, it has been slated for deprecation.\n Text: Going forward, the recommended method is .iat\/.at.\n Text: Why df.xs('C')['x']=10 does not work:\n Text: df.xs('C') by default, returns a new dataframe with a copy of the data, so df.xs('C')['x']=10\n\n Text: modifies this new dataframe only.\n Text: df['x'] returns a view of the df dataframe, so \n Code: df['x']['C'] = 10\n\n Text: modifies df itself.\n Text: Warning: It is sometimes difficult to predict if an operation returns a copy or a view. For this reason the docs recommend avoiding assignments with \"chained indexing\".  \n Text: So the recommended alternative is\n Code: df.at['C', 'x'] = 10\n\n Text: which does modify df.\n Code: In [18]: %timeit df.set_value('C', 'x', 10)\n100000 loops, best of 3: 2.9 s per loop\n\nIn [20]: %timeit df['x']['C'] = 10\n100000 loops, best of 3: 6.31 s per loop\n\nIn [81]: %timeit df.at['C', 'x'] = 10\n100000 loops, best of 3: 9.2 s per loop\n\nText: Alternatively, df.xs('C',copy = False)['x']=10 does modify . df.set_value('C','x',10) is the fastest:\n\nAPI:\npandas.DataFrame.set_value\npandas.DataFrame.xs\npandas.DataFrame.xs\npandas.DataFrame.xs\npandas.DataFrame.xs\npandas.DataFrame.set_value","label":[[42,68,"Mention"],[246,264,"Mention"],[287,297,"Mention"],[362,380,"Mention"],[1080,1085,"Mention"],[1126,1138,"Mention"],[1173,1199,"API"],[1200,1219,"API"],[1220,1239,"API"],[1240,1259,"API"],[1260,1279,"API"],[1280,1306,"API"]],"Comments":[]}
{"id":60769,"text":"ID:5267083\nPost:\nText: there is a specific numpy function to do this, np.searchsorted, which is much faster than bisect.\n Code: a=np.arange(1e7)\nc=2e6\n%timeit bisect.bisect(a,c)\n10000 loops, best of 3: 31.6 us per loop\n%timeit np.searchsorted(a,c)\n100000 loops, best of 3: 6.77 us per loop\n\n Text: More remarkably ,it has also a specific keyword side for including or not the last point:\n Code: In [23]: a[:a.searchsorted(66,side='right')]\nOut[23]: array([ 1,  2,  5, 10, 25, 36, 66])\n\nIn [24]: a[:a.searchsorted(66,side='left')]\nOut[24]: array([ 1,  2,  5, 10, 25, 36])\n\n\nAPI:\nnumpy.searchsorted\nbisect.bisect\n","label":[[70,85,"Mention"],[113,119,"Mention"],[578,596,"API"],[597,610,"API"]],"Comments":[]}
{"id":60770,"text":"ID:20180148\nPost:\nText: There is a major conceptual diffrence between those, based on different tasks being addressed:\n Text: Regression: continuous (real-valued) target variable. \n Text: Classification: discrete target variable (classes).\n Text: For a general classification method, term probability of observation being class X may be not defined, as some classification methods, knn for example, do not deal with probabilities.\n Text: However for Random Forest (and some other classification methods), classification is reduced to regression of classes probabilities destibution. Predicted class is taked then as argmax of computed \"probabilities\". In your case, you feed the same input, you get the same result. And yes, it is ok to treat values returned by RandomForestRegressor as probabilities.\n\nAPI:\nsklearn.ensemble.RandomForestRegressor\n","label":[[762,783,"Mention"],[808,846,"API"]],"Comments":[]}
{"id":60771,"text":"ID:9649982\nPost:\nText: \nIs it ok to use the same data in the grid search that I will be using during the real classification?\n\n Text: It is ok to use this data for training (fitting) a classifier. Cross validation, as done by StratifiedKFold, is intended for situations where you don't have enough data to hold out a validation set while optimizing the hyperparameters (the algorithm settings). You can also use if you're too lazy to make a validation set splitter and want to rely on scikit-learn's built-in cross validation :)\n Text: The refit option to GridSearchCV will retrain the estimator on the full training set after finding the optimal settings with cross validation.\n Text: It is, however, senseless to apply a trained classifier to the data you grid searched or trained on, since you already have the labels. If you want to do formal evaluation of a classifier, you should hold out a test set from the very beginning and not touch that again until you've done all your grid searching, validation and fitting.\n\nAPI:\nsklearn.model_selection.StratifiedKFold\nsklearn.model_selection.GridSearchCV\n","label":[[226,241,"Mention"],[556,568,"Mention"],[1028,1067,"API"],[1068,1104,"API"]],"Comments":[]}
{"id":60772,"text":"ID:8459166\nPost:\nText: plt.plot(x,y,'ro-',alpha=0.3)\n Text: if you want to mix up your own color, you can pass a hex value or an rgb tuple http:\/\/matplotlib.sourceforge.net\/api\/colors_api.html\n\nAPI:\nmatplotlib.pyplot.plot\n","label":[[23,31,"Mention"],[199,221,"API"]],"Comments":[]}
{"id":60773,"text":"ID:13434501\nPost:\nText: This question is already resolved, but... \n Text: ...also consider the solution suggested by Wouter in his original comment. The ability to handle missing data, including dropna(), is built into pandas explicitly. Aside from potentially improved performance over doing it manually, these functions also come with a variety of options which may be useful. \n Code: In [24]: df = pd.DataFrame(np.random.randn(10,3))\n\nIn [25]: df.iloc[::2,0] = np.nan; df.iloc[::4,1] = np.nan; df.iloc[::3,2] = np.nan;\n\nIn [26]: df\nOut[26]:\n          0         1         2\n0       NaN       NaN       NaN\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n\n Code: In [27]: df.dropna()     #drop all rows that have any NaN values\nOut[27]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n\n Code: In [28]: df.dropna(how='all')     #drop only if ALL columns are NaN\nOut[28]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n\n Code: In [29]: df.dropna(thresh=2)   #Drop row if it does not have at least two values that are **not** NaN\nOut[29]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n9 -0.310130  0.078891       NaN\n\n Code: In [30]: df.dropna(subset=[1])   #Drop only if NaN in specific column (as asked in the question)\nOut[30]:\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n9 -0.310130  0.078891       NaN\n\n Text: There are also other options (See docs at http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.dropna.html), including dropping columns instead of rows. \n Text: Pretty handy! \n\nAPI:\npandas.DataFrame.dropna\n","label":[[195,203,"Mention"],[2432,2455,"API"]],"Comments":[]}
{"id":60774,"text":"ID:5902579\nPost:\nText: All plot_date does is plot the function and the call ax.xaxis_date().\n Text: All you should need to do is this:\n Code: import numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\n\nx = [datetime.datetime(2010, 12, 1, 10, 0),\n    datetime.datetime(2011, 1, 4, 9, 0),\n    datetime.datetime(2011, 5, 5, 9, 0)]\ny = [4, 9, 2]\n\nax = plt.subplot(111)\nax.bar(x, y, width=10)\nax.xaxis_date()\n\nplt.show()\n\n\nAPI:\nmatplotlib.axes.Axes.plot_date\nmatplotlib.axes.Axes.xaxis_date\n","label":[[27,36,"Mention"],[76,91,"Mention"],[431,461,"API"],[462,493,"API"]],"Comments":[]}
{"id":60775,"text":"ID:47201495\nPost:\nText: This is because of using integer indices (ix selects those by label over -3 rather than position, and this is by design: see integer indexing in pandas \"gotchas\"*).\n Text: *In newer versions of pandas prefer loc or iloc to remove the ambiguity of ix as position or label:\n Text: df.iloc[-3:]\nsee the docs.\n Text: As Wes points out, in this specific case you should just use tail!\n Text: It should also be noted that in Pandas pre-0.14 iloc will raise an IndexError on an out-of-bounds access, while .head() and .tail() will not:\n Text: \n\n\npd.version\n      '0.12.0'\n      df = pd.DataFrame([{\"a\": 1}, {\"a\": 2}])\n      df.iloc[-5:]\n      ...\n      IndexError: out-of-bounds on slice (end)\n      df.tail(5)\n         a\n      0  1\n      1  2\n      Old answer (depreciated method):\n\n\n\n Text: You can use the irows DataFrame method to overcome this ambiguity:\n Text: In [11]: df1.irow(slice(-3, None))\nOut[11]: \n    STK_ID  RPT_Date  TClose   sales  discount\n8      568  20080331   38.75  12.668       NaN\n9      568  20080630   30.09  21.102       NaN\n10     568  20080930   26.00  30.769       NaN\nNote: Series has a similar iget method.\n\nAPI:\npandas.DataFrame.loc\npandas.DataFrame.iloc\npandas.DataFrame.ix\npandas.DataFrame.iloc\npandas.DataFrame.tail\npandas.DataFrame.iloc\npandas.DataFrame.head\npandas.DataFrame.tail\npandas.DataFrame.irows\npandas.DataFrame\npandas.Series\npandas.Series.iget\n","label":[[232,235,"Mention"],[239,243,"Mention"],[271,273,"Mention"],[303,315,"Mention"],[398,402,"Mention"],[459,463,"Mention"],[523,530,"Mention"],[535,542,"Mention"],[826,831,"Mention"],[832,841,"Mention"],[1123,1129,"Mention"],[1144,1148,"Mention"],[1163,1183,"API"],[1184,1205,"API"],[1206,1225,"API"],[1226,1247,"API"],[1248,1269,"API"],[1270,1291,"API"],[1292,1313,"API"],[1314,1335,"API"],[1336,1358,"API"],[1359,1375,"API"],[1376,1389,"API"],[1390,1408,"API"]],"Comments":[]}
{"id":60776,"text":"ID:17181334\nPost:\nText: You need two things, ensure the date column is of dates (rather of strings) and to set the index to these dates.\nYou can do this in one go using to_datetime:\n Code: In [11]: df.index = pd.to_datetime(df.pop('date'))\n\nIn [12]: df\nOut[12]:\n              avg     high   low    qty\ndate\n2013-05-27  16.92    19.00  1.22  71151\n2013-05-30  14.84    19.00  1.22  42939\n2013-06-02   9.19    17.20  1.23   5607\n2013-06-05  23.63  5000.00  1.22   5850\n2013-06-10  13.82    19.36  1.22   5644\n2013-06-15  17.76    24.00  2.02  16969\n\n Text: Then you can call emwa as expected:\n Code: In [13]: pd.ewma(df[\"avg\"], span=60, freq=\"D\")\nOut[13]:\ndate\n2013-05-27    16.920000\n2013-05-28    16.920000\n2013-05-29    16.920000\n2013-05-30    15.862667\n2013-05-31    15.862667\n2013-06-01    15.862667\n2013-06-02    13.563899\n2013-06-03    13.563899\n2013-06-04    13.563899\n2013-06-05    16.207625\n2013-06-06    16.207625\n2013-06-07    16.207625\n2013-06-08    16.207625\n2013-06-09    16.207625\n2013-06-10    15.697743\n2013-06-11    15.697743\n2013-06-12    15.697743\n2013-06-13    15.697743\n2013-06-14    15.697743\n2013-06-15    16.070721\nFreq: D, dtype: float64\n\n Text: and if you set this as a column:\n Code: In [14]: df['ewma'] = pd.ewma(df[\"avg\"], span=60, freq=\"D\")\n\nIn [15]: df\nOut[15]:\n              avg     high   low    qty       ewma\ndate\n2013-05-27  16.92    19.00  1.22  71151  16.920000\n2013-05-30  14.84    19.00  1.22  42939  15.862667\n2013-06-02   9.19    17.20  1.23   5607  13.563899\n2013-06-05  23.63  5000.00  1.22   5850  16.207625\n2013-06-10  13.82    19.36  1.22   5644  15.697743\n2013-06-15  17.76    24.00  2.02  16969  16.070721\n\n\nAPI:\npandas.to_datetime\npandas.ewma\n","label":[[169,180,"Mention"],[573,577,"Mention"],[1662,1680,"API"],[1681,1692,"API"]],"Comments":[]}
{"id":60777,"text":"ID:9914204\nPost:\nText: Basically, figure.colorbar() is good for both images, as long as their are not with too different scales. So you could let matplotlib do it for you... or you manually position your colorbar on axes inside the images. Here is how to control the location of the colorbar:\n Code: import numpy as np\nfrom matplotlib import pyplot as plt\n\nA = np.random.random_integers(0, 10, 100).reshape(10, 10)\nB = np.random.random_integers(0, 10, 100).reshape(10, 10)\n\nfig = plt.figure()\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\n\nmapable = ax1.imshow(A, interpolation=\"nearest\")\ncax = ax2.imshow(A, interpolation=\"nearest\")\n\n# set the tickmarks *if* you want cutom (ie, arbitrary) tick labels:\ncbar = fig.colorbar(cax, ax=None)\n\nfig = plt.figure(2)\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nmapable = ax1.imshow(A, interpolation=\"nearest\")\ncax = ax2.imshow(A, interpolation=\"nearest\")\n# on the figure total in precent l    b      w , height \nax3 = fig.add_axes([0.1, 0.1, 0.8, 0.05]) # setup colorbar axes. \n# put the colorbar on new axes\ncbar = fig.colorbar(mapable,cax=ax3,orientation='horizontal')\n\nplt.show()\n\n Text: Note ofcourse you can position ax3 as you wish, on the side, on the top, where ever, \nas long as it is in the boundaries of the figure. \n Text: I don't know why your line2D is not appearing. \n Text: I added to my code before plt.show() the following and everything is showing:\n Code: from mpl_toolkits.axes_grid1 import anchored_artists\nfrom matplotlib.patheffects import withStroke\ntxt = anchored_artists.AnchoredText(\"SC\",\n                                    loc=2,\n                                    frameon=False,\n                                    prop=dict(size=12))\nif withStroke:\n    txt.txt._text.set_path_effects([withStroke(foreground=\"w\",\n                                               linewidth=3)])\nax1.add_artist(txt)\n\n\n## Draw a line to separate the two different wave lengths, and name each region\nl1 = plt.Line2D([-1,10],[5,5],ls='-',color='black',lineswidth=10)\nax1.add_line(l1)\n\n\nAPI:\nmatplotlib.figure.Figure.colorbar\nmatplotlib.pyplot.show\n","label":[[34,51,"Mention"],[1376,1386,"Mention"],[2058,2091,"API"],[2092,2114,"API"]],"Comments":[]}
{"id":60778,"text":"ID:4091264\nPost:\nText: It is possible to call plt.plot(dates,values) with dates being a list of datetime.datetime objects. The plot will include xticks in a format like '%Y-%m-%d' and as you zoom in, automatically change to one that shows hours, minutes, seconds. \n Text: However, it sounds like you desire more control than this. Perhaps it is not showing the hours, minutes, seconds at the scale you wish.\n Text: In that case, you can set up your own date formatter:\n Code: ax=plt.gca()\nxfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')\nax.xaxis.set_major_formatter(xfmt)\n\n Text: Unfortunately, if you pass datetime.datetime objects to plt.plot, the xticks automatically chosen by matplotlib seems to always have seconds equal to zero. For example, if you run\n Code: import matplotlib.pyplot as plt\nimport matplotlib.dates as md\nimport numpy as np\nimport datetime as dt\nimport time\n\nn=20\nduration=1000\nnow=time.mktime(time.localtime())\ntimestamps=np.linspace(now,now+duration,n)\ndates=[dt.datetime.fromtimestamp(ts) for ts in timestamps]\nvalues=np.sin((timestamps-now)\/duration*2*np.pi)\nplt.subplots_adjust(bottom=0.2)\nplt.xticks( rotation=25 )\nax=plt.gca()\nxfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')\nax.xaxis.set_major_formatter(xfmt)\nplt.plot(dates,values)\nplt.show()\n\n Text: then you get nicely formatted dates, but all the xtick seconds are zero. \n Text: So what's the solution? \n Text: If you convert your timestamps --> datetime.datetime objects --> matplotlib datenums yourself, and pass the datenums to plt.plot, then the seconds are preserved.\n Text: PS. By \"matplotlib datenum\" I mean the kind of number returned by matplotlib.dates.date2num.\n Code: import matplotlib.pyplot as plt\nimport matplotlib.dates as md\nimport numpy as np\nimport datetime as dt\nimport time\n\nn=20\nduration=1000\nnow=time.mktime(time.localtime())\ntimestamps=np.linspace(now,now+duration,n)\ndates=[dt.datetime.fromtimestamp(ts) for ts in timestamps]\ndatenums=md.date2num(dates)\nvalues=np.sin((timestamps-now)\/duration*2*np.pi)\nplt.subplots_adjust(bottom=0.2)\nplt.xticks( rotation=25 )\nax=plt.gca()\nxfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')\nax.xaxis.set_major_formatter(xfmt)\nplt.plot(datenums,values)\nplt.show()\n\n\nAPI:\nmatplotlib.pyplot.plot\ndatetime.datetime\ndatetime.datetime\nmatplotlib.pyplot.plot\ndatetime.datetime\nmatplotlib.pyplot.plot\nmatplotlib.dates.date2num\n","label":[[46,68,"Mention"],[633,641,"Mention"],[1510,1518,"Mention"],[2202,2224,"API"],[2261,2283,"API"],[2302,2324,"API"]],"Comments":[]}
{"id":60779,"text":"ID:5173201\nPost:\nText: The above answers work if you want to create a set out of the elements contained in an ndarray, but if you want to create a set of ndarray objects  or use ndarray objects as keys in a dictionary  then you'll have to provide a hashable wrapper for them. See the code below for a simple example:\n Code: from hashlib import sha1\n\nfrom numpy import all, array, uint8\n\n\nclass hashable(object):\n    r'''Hashable wrapper for ndarray objects.\n\n        Instances of ndarray are not hashable, meaning they cannot be added to\n        sets, nor used as keys in dictionaries. This is by design - ndarray\n        objects are mutable, and therefore cannot reliably implement the\n        __hash__() method.\n\n        The hashable class allows a way around this limitation. It implements\n        the required methods for hashable objects in terms of an encapsulated\n        ndarray object. This can be either a copied instance (which is safer)\n        or the original object (which requires the user to be careful enough\n        not to modify it).\n    '''\n    def __init__(self, wrapped, tight=False):\n        r'''Creates a new hashable object encapsulating an ndarray.\n\n            wrapped\n                The wrapped ndarray.\n\n            tight\n                Optional. If True, a copy of the input ndaray is created.\n                Defaults to False.\n        '''\n        self.__tight = tight\n        self.__wrapped = array(wrapped) if tight else wrapped\n        self.__hash = int(sha1(wrapped.view(uint8)).hexdigest(), 16)\n\n    def __eq__(self, other):\n        return all(self.__wrapped == other.__wrapped)\n\n    def __hash__(self):\n        return self.__hash\n\n    def unwrap(self):\n        r'''Returns the encapsulated ndarray.\n\n            If the wrapper is \"tight\", a copy of the encapsulated ndarray is\n            returned. Otherwise, the encapsulated ndarray itself is returned.\n        '''\n        if self.__tight:\n            return array(self.__wrapped)\n\n        return self.__wrapped\n\n Text: Using the wrapper class is simple enough:\n Code: >>> from numpy import arange\n\n>>> a = arange(0, 1024)\n>>> d = {}\n>>> d[a] = 'foo'\nTraceback (most recent call last):\n  File \"<input>\", line 1, in <module>\nTypeError: unhashable type: 'numpy.ndarray'\n>>> b = hashable(a)\n>>> d[b] = 'bar'\n>>> d[b]\n'bar'\n\n\nAPI:\nnumpy.ndarray\nnumpy.ndarray\nnumpy.ndarray\n","label":[[110,117,"Mention"],[154,161,"Mention"],[179,186,"Mention"],[2320,2333,"API"],[2334,2347,"API"],[2348,2361,"API"]],"Comments":[]}
{"id":60780,"text":"ID:5927270\nPost:\nText: The hist() function returns the information you are looking for:\n Code: n, bins, patches = pl.hist(R, nbins, normed=True)\n\n Text: n is an array of the bar heights, and bins is the array of bin boundaries.  In the given example, len(n) would be 100 and len(bins) would be 101.\n Text: Given some x value, you can use numpy.searchsorted() to find the index of the bin this value belongs to, and then use n[index] to extract the corresponding bar height.\n\nAPI:\nmatplotlib.pyplot.hist\nlen\nlen\nnumpy.searchsorted\n","label":[[27,33,"Mention"],[480,502,"API"]],"Comments":[]}
{"id":60781,"text":"ID:17141755\nPost:\nText: As of the 0.17.0 release, the sort method was deprecated in favor of sort_values.  sort was completely removed in the 0.20.0 release. The arguments (and results) remain the same:\n Code: df.sort_values(['a', 'b'], ascending=[True, False])\n\n Text: You can use the ascending argument of sort:\n Code: df.sort(['a', 'b'], ascending=[True, False])\n\n Text: For example:\n Code: In [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])\n\nIn [12]: df1.sort(['a', 'b'], ascending=[True, False])\nOut[12]:\n   a  b\n2  1  4\n7  1  3\n1  1  2\n3  1  2\n4  3  2\n6  4  4\n0  4  3\n9  4  3\n5  4  1\n8  4  1\n\n Text: As commented by @renadeen\n Text: \nSort isn't in place by default! So you should assign result of the sort method to a variable or add inplace=True to method call.\n\n Text: that is, if you want to reuse df1 as a sorted DataFrame:\n Code: df1 = df1.sort(['a', 'b'], ascending=[True, False])\n\n Text: or\n Code: df1.sort(['a', 'b'], ascending=[True, False], inplace=True)\n\n\nAPI:\npandas.DataFrame.sort\npandas.DataFrame.sort_values\npandas.DataFrame.sort\npandas.DataFrame.sort\npandas.DataFrame\n","label":[[54,58,"Mention"],[93,104,"Mention"],[308,312,"Mention"],[736,740,"Mention"],[852,861,"Mention"],[1007,1028,"API"],[1029,1057,"API"],[1058,1079,"API"],[1080,1101,"API"],[1102,1118,"API"]],"Comments":[]}
{"id":60782,"text":"ID:14059783\nPost:\nText: You can use the DataFrame apply method:\n Code: order_df['Value'] = order_df.apply(lambda row: (row['Prices']*row['Amount']\n                                               if row['Action']=='Sell'\n                                               else -row['Prices']*row['Amount']),\n                                   axis=1)\n\n Text: It is usually faster to use these methods rather than over for loops.\n\nAPI:\npandas.DataFrame\npandas.DataFrame.apply\n","label":[[40,49,"Mention"],[50,55,"Mention"],[429,445,"API"],[446,468,"API"]],"Comments":[]}
{"id":60783,"text":"ID:9398214\nPost:\nText: Sure! Either specify axes.color_cycle in your .matplotlibrc file or set it at runtime using matplotlib.rcParams or matplotlib.rc.\n Text: As an example of the latter:\n Code: import matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\n\n# Set the default color cycle\nmpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"r\", \"k\", \"c\"]) \n\nx = np.linspace(0, 20, 100)\n\nfig, axes = plt.subplots(nrows=2)\n\nfor i in range(10):\n    axes[0].plot(x, i * (x - 10)**2)\n\nfor i in range(10):\n    axes[1].plot(x, i * np.cos(x))\n\nplt.show()\n\n\nAPI:\nmatplotlib.axes.Axes.color_cycle\nmatplotlib.rcParams\nmatplotlib.rc","label":[[44,60,"Mention"],[569,601,"API"]],"Comments":[]}
{"id":60784,"text":"ID:8832212\nPost:\nText: Here are the functions, which reconstructs the dictionaries just use this loadmat instead of scipy.io's loadmat:\n Code: import scipy.io as spio\n\ndef loadmat(filename):\n    '''\n    this function should be called instead of direct spio.loadmat\n    as it cures the problem of not properly recovering python dictionaries\n    from mat files. It calls the function check keys to cure all entries\n    which are still mat-objects\n    '''\n    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n    return _check_keys(data)\n\ndef _check_keys(dict):\n    '''\n    checks if entries in dictionary are mat-objects. If yes\n    todict is called to change them to nested dictionaries\n    '''\n    for key in dict:\n        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n            dict[key] = _todict(dict[key])\n    return dict        \n\ndef _todict(matobj):\n    '''\n    A recursive function which constructs from matobjects nested dictionaries\n    '''\n    dict = {}\n    for strg in matobj._fieldnames:\n        elem = matobj.__dict__[strg]\n        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n            dict[strg] = _todict(elem)\n        else:\n            dict[strg] = elem\n    return dict\n\n\nAPI:\nscipy.io.loadmat\n","label":[[127,134,"Mention"],[1248,1264,"API"]],"Comments":[]}
{"id":60785,"text":"ID:17109187\nPost:\nText: I don't think pandas offers a way to do this in read_csv.\n Text: Perhaps the neatest (in one pass) is to use collections.deque:\n Code: from collections import deque\nfrom StringIO import StringIO\n\nwith open(fname, 'r') as f:\n    q = deque(f, 2)  # replace 2 with n (lines read at the end)\n\nIn [12]: q\nOut[12]: deque(['7,8,9\\n', '10,11,12'], maxlen=2)\n         # these are the last two lines of my csv\n\nIn [13]: pd.read_csv(StringIO(''.join(q)), header=None)\n\n Text: Another option worth trying is to get the number of lines in a first pass and then read the file again, skip that number of rows (minus n) using read_csv...\n\nAPI:\npandas.read_csv\ncollections.deque\npandas.read_csv\n","label":[[72,80,"Mention"],[634,642,"Mention"],[652,667,"API"],[686,701,"API"]],"Comments":[]}
{"id":60786,"text":"ID:10819434\nPost:\nText: DO NOT create a new figure each time with plt.figure(), you'll wind up running out of memory rather quickly. Instead use (for the figure and the axes respectively): plt.clf() plt.cla()\n\n Text: You can run plt.close() to free up the allocation, however there has been some discussion that this method has lead to memory leaks in the past. A quick test shows that in version 1.1.1rc this works without problems, so feel free to use it as an alternative. A useful related question discuses the differences between the methods.\n\nAPI:\nmatplotlib.pyplot.figure\nmatplotlib.pyplot.clf\nmatplotlib.pyplot.cla\nmatplotlib.pyplot.close","label":[[66,78,"Mention"],[189,198,"Mention"],[199,208,"Mention"],[229,240,"Mention"],[554,578,"API"],[579,600,"API"],[601,622,"API"],[623,646,"API"]],"Comments":[]}
{"id":60787,"text":"ID:17963027\nPost:\nText: If the input values were all positive (term frequencies), then the negative values don't actually mean anything. Citing the scikit-learn docs:\n Text: \nSince the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output features value is zero.\n\n Text: The Wikipedia has a little table showing the effect of this scheme, which was invented by Weinberger et al.. Not only does it alleviate the effect of collisions, but it makes your features behave more like Gaussians, which helps some learning algorithms.\n Text: If you want to get rid of the negative values, then pass non_negative=True to the FeatureHasher constructor. The implementation of that option is a bit of a hack, but it makes tf-idf and naive Bayes work again.\n\nAPI:\nsklearn.feature_extraction.FeatureHasher\n","label":[[879,892,"Mention"],[1014,1054,"API"]],"Comments":[]}
{"id":60788,"text":"ID:14508180\nPost:\nText: \"8 train values and 3 values\" is probably best expressed as \"8 features and 3 target variables\" in usual machine learning parlance.\n Text: Both variants should work and yield the similar predictions as RandomForestRegressor has been made to support multi output regression.\n Text: The predictions won't be exactly the same as RandomForestRegressor is a non deterministic algorithm though. But on average the predictive quality of both approaches should be the same.\n Text: Edit: see Andreas answer instead.\n\nAPI:\nsklearn.ensemble.RandomForestRegressor\nsklearn.ensemble.RandomForestRegressor\n","label":[[226,247,"Mention"],[350,371,"Mention"],[537,575,"API"],[576,614,"API"]],"Comments":[]}
{"id":60789,"text":"ID:12238717\nPost:\nText: You should convert them as floats but the way to do it depends on what the integer features actually represent.\n Text: What is the meaning of your integers? Are they category membership indicators (for instance: 1 == sport, 2 == business, 3 == media, 4 == people...) or numerical measures with an order relationship (3 is larger than 2 that is in turn is larger than 1). You cannot say that \"people\" is larger than \"media\" for instance. It is meaningless and would confuse the machine learning algorithm to give it this assumption.\n Text: Categorical features should hence be transformed to explode each feature as several boolean features (with value 0.0 or 1.0) for each possible category. Have a look at the DictVectorizer class in scikit-learn to better understand what I mean by categorical features.\n Text: If there are numerical values just convert them as floats and maybe use the Scaler to have them loosely in the range [-1, 1]. If they span several order of magnitudes (e.g. counts of word occurrences) then taking the logarithm of the counts might yield better results. More documentation on feature preprocessing and examples in this section of the documentation: http:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n Text: Edit: also read this guide that has many more details for features representation and preprocessing: http:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/guide\/guide.pdf\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\n","label":[[735,749,"Mention"],[1430,1471,"API"]],"Comments":[]}
{"id":60790,"text":"ID:16958649\nPost:\nText: Note: sort is deprecated - use sort_values instead\n Text: To sort descending use ascending=False:\n Code: In [6]: df.sort('Bytes', ascending=False)\nOut[6]:\n   0  Bytes      Client Ip\n1  1   2000  192.168.10.12\n0  0   1000   192.168.10.2\n2  2    500   192.168.10.4\n3  3    159  192.168.10.56\n\n Text: To take the first 10 values use .head(10).\n\nAPI:\npandas.DataFrame.sort\npandas.DataFrame.sort_values\npandas.DataFrame.head\n","label":[[30,34,"Mention"],[55,66,"Mention"],[354,363,"Mention"],[371,392,"API"],[393,421,"API"],[422,443,"API"]],"Comments":[]}
{"id":60791,"text":"ID:7379830\nPost:\nText: You need to call legend for the legend to appear. The label kwarg only sets the _label attribute on the artist object in question. It's there for convenience, so that the label in the legend can be clearly associated with the plotting command. It won't add the legend to the plot without explicitly calling ax.legend(...).  Also, you want ax.set_xlim, not ax.xlim to adjust the xaxis limits. Have a look at ax.axis as well.\n Text: It sounds like you want something like this:\n Code: import matplotlib as mpl\nmpl.use('Agg')\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some data\nx = np.arange(0, 22, 2)\nf1, f2, f3, f4 = np.cumsum(np.random.random((4, x.size)) - 0.5, axis=1)\n\n# It's much more convenient to just use pyplot's factory functions...\nfig, ax = plt.subplots()\n\nax.set_title(\"Function performance\",fontsize=14)\nax.set_xlabel(\"code executions\",fontsize=12)\nax.set_ylabel(\"time(s)\",fontsize=12)\nax.grid(True,linestyle='-',color='0.75')\n\ncolors = ['tomato', 'violet', 'blue', 'green']\nlabels = ['Thing One', 'Thing Two', 'Thing Three', 'Thing Four']\nfor func, color, label in zip([f1, f2, f3, f4], colors, labels):\n    ax.plot(x, func, 'o', color=color, markersize=10, label=label)\n\nax.legend(numpoints=1, loc='upper left')\nax.set_xlim([0, x.max() + 1])\n\nfig.savefig('performance.png', dpi=100)\n\n\nAPI:\nmatplotlib.axes.Axes.legend\nmatplotlib.axes.Axes.legend\nmatplotlib.axes.Axes.set_xlim\nmatplotlib.axes.Axes.xlim\nmatplotlib.axes.Axes.axis\n","label":[[40,46,"Mention"],[330,339,"Mention"],[362,373,"Mention"],[379,386,"Mention"],[430,437,"Mention"],[1349,1376,"API"],[1377,1404,"API"],[1405,1434,"API"],[1435,1460,"API"],[1461,1486,"API"]],"Comments":[]}
{"id":60792,"text":"ID:569036\nPost:\nText: You can use the append function.  For rows:\n Code: >>> from numpy import *\n>>> a = array([10,20,30])\n>>> append(a, [[1,2,3]], axis=0)\narray([[10, 20, 30],      \n       [1, 2, 3]])\n\n Text: For columns:\n Code: >>> append(a, [[15],[15]], axis=1)\narray([[10, 20, 30, 15],      \n       [1, 2, 3, 15]])\n\n Text: EDIT\nOf course, as mentioned in other answers, unless you're doing some processing (ex. inversion) on the matrix\/array EVERY time you append something to it, I would just create a list, append to it then convert it to an array.\n\nAPI:\nnumpy.append\n","label":[[38,44,"Mention"],[561,573,"API"]],"Comments":[]}
{"id":60793,"text":"ID:8055823\nPost:\nText: You could smooth your data with a gaussian_filter:\n Code: import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndimage\n\nX, Y = np.mgrid[-70:70, -70:70]\nZ = np.cos((X**2+Y**2)\/200.)+ np.random.normal(size=X.shape)\n\n# Increase the value of sigma to increase the amount of blurring.\n# order=0 means gaussian kernel\nZ2 = ndimage.gaussian_filter(Z, sigma=1.0, order=0)\nfig=plt.figure()\nax=fig.add_subplot(1,2,1)\nax.imshow(Z)\nax=fig.add_subplot(1,2,2)\nax.imshow(Z2)\nplt.show()\n\n Text: The left-side shows the original data, the right-side after gaussian filtering.\n Text: Much of the above code was taken from the Scipy Cookbook, which demonstrates gaussian smoothing using a hand-made gauss kernel. Since scipy comes with the same built in, I chose to use gaussian_filter.\n\nAPI:\nscipy.ndimage.gaussian_filter\nscipy.ndimage.gaussian_filter\n","label":[[57,72,"Mention"],[796,811,"Mention"],[819,848,"API"],[849,878,"API"]],"Comments":[]}
{"id":60794,"text":"ID:14212533\nPost:\nText: In addition to Diego's answer:\n Text: RandomForestClassifier is a classifier to predict class assignment for a discrete number of classes without ordering between the class labels.\n Text: If you want to output continuous, floating point rating, you should try to use a regression model such as RandomForestRegressor instead.\n Text: You might have to clamp the output to the range [0, 6] as there is no guaranty the model will not output predictions such as 6.2 for instance.\n Text: Edit to answer you second point, the predict method expects a list of samples. Hence you should provide it with a list of one sample in your case. Try:\n Code: print rf.predict([testdataset[-1]])\n\n Text: or alternatively:\n Code: print rf.predict(testdataset[-1:])\n\n Text: I wonder why you don't get an error in that case.\n Text: Edit: the ouput does not really make sense: what is the shape of your datasets?\n Code: >>> print np.asarray(train).shape\n\n>>> print np.asarray(target).shape\n\n>>> print np.asarray(testdataset).shape\n\n\nAPI:\nsklearn.ensemble.RandomForestClassifier\nsklearn.ensemble.RandomForestRegressor\nsklearn.ensemble.RandomForestClassifier.predict\n","label":[[62,84,"Mention"],[318,339,"Mention"],[543,550,"Mention"],[1039,1078,"API"],[1079,1117,"API"],[1118,1165,"API"]],"Comments":[]}
{"id":60795,"text":"ID:10338816\nPost:\nText: Any time you call something like Axes3D.clear(), then to enable mouse rotation again you have to call Axes3D.mouse_init(). \n Text: Seems to be undocumented, but it works for me! Source\n\nAPI:\nmpl_toolkits.mplot3d.axes3d.Axes3D.clear\nmpl_toolkits.mplot3d.axes3d.Axes3D.mouse_init\n","label":[[57,71,"Mention"],[126,145,"Mention"],[215,255,"API"],[256,301,"API"]],"Comments":[]}
{"id":60796,"text":"ID:5837352\nPost:\nText: Create your own instance of numpy.random.RandomState() with your chosen seed. Do not use numpy.random.seed() except to work around inflexible libraries that do not let you pass around your own RandomState instance.\n Code: [~]\n|1> from numpy.random import RandomState\n\n[~]\n|2> prng = RandomState(1234567890)\n\n[~]\n|3> prng.randint(-1, 2, size=10)\narray([ 1,  1, -1,  0,  0, -1,  1,  0, -1, -1])\n\n[~]\n|4> prng2 = RandomState(1234567890)\n\n[~]\n|5> prng2.randint(-1, 2, size=10)\narray([ 1,  1, -1,  0,  0, -1,  1,  0, -1, -1])\n\n\nAPI:\nnumpy.random.RandomState\nnumpy.random.seed\nnumpy.random.RandomState\n","label":[[216,227,"Mention"],[594,618,"API"]],"Comments":[]}
{"id":60797,"text":"ID:3532408\nPost:\nText: What's happening is that the xticks actually extend outside of the displayed figure when using matshow.  (I'm not quite sure exactly why this is. I've almost never used matshow, though.)  \n Text: To demonstrate this, look at the output of ax.get_xticks(). In your case, it's array([-1.,  0.,  1.,  2.,  3.,  4.]).  Therefore, when you set the xtick labels, \"ABC\" is at <-1, -1>, and isn't displayed on the figure.\n Text: The easiest solution is just to prepend a blank label to your list of labels, e.g.\n Code: ax.set_xticklabels(['']+alpha)\nax.set_yticklabels(['']+alpha)\n\n Text: As a full example:\n Code: import numpy as np\nimport matplotlib.pyplot as plt\n\nalpha = ['ABC', 'DEF', 'GHI', 'JKL']\n\ndata = np.random.random((4,4))\n\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(data, interpolation='nearest')\nfig.colorbar(cax)\n\nax.set_xticklabels(['']+alpha)\nax.set_yticklabels(['']+alpha)\n\nplt.show()\n\n\nAPI:\nmatplotlib.axes.Axes.get_xticks\n","label":[[262,277,"Mention"],[945,976,"API"]],"Comments":[]}
{"id":60798,"text":"ID:2924168\nPost:\nText: I didn't realize sharex could be passed to add_subplot...\n Code: f = plt.figure()\nax1 = f.add_subplot(211)\nax2 = f.add_subplot(212, sharex=ax1, sharey=ax1)\nax1.imshow(np.random.randn(10,10), interpolation='nearest')\nax2.imshow(np.random.randn(10,10), interpolation='nearest')\nf.canvas.draw()\n\n Text: however, if you try to do the following to set your axis objects for displaying images. ax1.axis('image')\n\n Text: you'll get:\n Code: ValueError: adjustable must be \"datalim\" for shared axes\n\n Text: The matplotlib guys tell me I can use\n Code: ax1.set_adjustable(\"box-forced\")\nax2.set_adjustable(\"box-forced\")\n\n Text: instead, but it doesn't work for me using matplotlib version 0.98.5.3 ... I assume it would with a newer version.  Will update when I hear back.\n\nAPI:\nmatplotlib.figure.Figure.add_subplot\nmatplotlib.axes.Axes.axis","label":[[66,77,"Mention"],[411,428,"Mention"],[791,827,"API"],[828,853,"API"]],"Comments":[]}
{"id":60799,"text":"ID:16158034\nPost:\nText: You can pass the na_values=[\"nan\"] parameter in your read_csv function call.  That will read the string nan values and convert them to the proper np.nan format.\n Text: See here for more info.\n\nAPI:\npandas.read_csv\nnumpy.nan\n","label":[[77,85,"Mention"],[170,176,"Mention"],[222,237,"API"],[238,247,"API"]],"Comments":[]}
{"id":60800,"text":"ID:9855939\nPost:\nText: Try\n Code: sm[np.random.sample(sm.shape[0], K, replace=False), :]\n\n Text: This gets you out an LIL-format matrix with just K of the rows (in the order determined by the random.sample). I'm not sure it's super-fast, but it can't really be worse than manually accessing row by row like you're currently doing, and probably preallocates the results.\n\nAPI:\nnumpy.random.sample\n","label":[[192,205,"Mention"],[376,395,"API"]],"Comments":[]}
{"id":60801,"text":"ID:1598295\nPost:\nText: You can call yourrecarray.resize with a shape which has one more row, then assign to that new row.  Of course. numpy might still have to allocate completely new memory if it just doesn't have room to grow the array in-place, but at least you stand a chance!-)\n Text: Since an example was requested, here comes, modified off the canonical example list...:\n Code: >>> import numpy\n>>> mydescriptor = {'names': ('gender','age','weight'), 'formats': ('S1', 'f4', 'f4')} \n>>> a = numpy.array([('M',64.0,75.0),('F',25.0,60.0)], dtype=mydescriptor)\n>>> print a\n[('M', 64.0, 75.0) ('F', 25.0, 60.0)]\n>>> a.shape\n(2,)\n>>> a.resize(3)\n>>> a.shape\n(3,)\n>>> print a\n[('M', 64.0, 75.0) ('F', 25.0, 60.0) ('', 0.0, 0.0)]\n>>> a[2] = ('X', 17.0, 61.5)\n>>> print a\n[('M', 64.0, 75.0) ('F', 25.0, 60.0) ('X', 17.0, 61.5)]\n\n\nAPI:\nnumpy.recarray.resize\n","label":[[36,55,"Mention"],[834,855,"API"]],"Comments":[]}
{"id":60802,"text":"ID:14822703\nPost:\nText: pandas.Series, has a tolist method:\n Code: In [10]: import pandas as pd\n\nIn [11]: s = pd.Series([0,1,8,9], name = 'BayFail')\n\nIn [12]: s.tolist()\nOut[12]: [0L, 1L, 8L, 9L]\n\n Text: Technical note: In my original answer I said that Series was a subclass of numpy.ndarray and inherited its tolist method. While that's true for Pandas version 0.12 or older, In the soon-to-be-released Pandas version 0.13, Series has been refactored to be a subclass of NDFrame. Series still has a tolist method, but it has no direct relationship to the numpy.ndarray method of the same name.\n\nAPI:\npandas.Series\npandas.Series.tolist\npandas.Series\nnumpy.ndarray\nnumpy.ndarray.tolist\npandas.Series\npandas.Series\npandas.Series.tolist\nnumpy.ndarray\n","label":[[45,51,"Mention"],[254,260,"Mention"],[311,317,"Mention"],[426,432,"Mention"],[482,488,"Mention"],[501,507,"Mention"],[616,636,"API"],[637,650,"API"],[665,685,"API"],[686,699,"API"],[700,713,"API"],[714,734,"API"]],"Comments":[]}
{"id":60803,"text":"ID:1488889\nPost:\nText: leastsq works with vectors so the residual function, func, needs to return a vector of length at least two.  So if you replace return eqn with return [eqn, 0.], your example will work.  Running it gives:\n Code: optimized parameters:  (array([10., 10.]), 2)\n\n Text: which is one of the many correct answers for the minimum of the absolute difference.\n Text: If you want to minimize a scalar function, fmin is the way to go, optimize.fmin(func, Init).\n Text: The issue here is that these two functions, although they look the same for a scalars are aimed at different goals.  leastsq finds the least squared error, generally from a set of idealized curves, and is just one way of doing a \"best fit\".  On the other hand fmin finds the minimum value of a scalar function.\n Text: Obviously yours is a toy example, for which neither of these really makes sense, so which way you go will depend on what your final goal is.\n\nAPI:\nscipy.optimize.leastsq\nscipy.optimize.fmin\nscipy.optimize.fmin\nscipy.optimize.leastsq\nscipy.optimize.fmin\n","label":[[23,30,"Mention"],[423,427,"Mention"],[446,471,"Mention"],[597,604,"Mention"],[740,744,"Mention"],[945,967,"API"],[968,987,"API"],[988,1007,"API"],[1008,1030,"API"],[1031,1050,"API"]],"Comments":[]}
{"id":60804,"text":"ID:13925150\nPost:\nText: Suppose you have this setup:\n Code: import pandas as pd\nimport numpy as np\nimport datetime as DT\n\nnan = np.nan\n\ntrades = pd.DataFrame({'ticker' : ['IBM', 'MSFT', 'GOOG', 'AAPL'],\n                       'date' : pd.date_range('1\/1\/2000', periods = 4), \n                       'cusip' : [nan, nan, 100, nan]\n                       })\ntrades = trades.set_index(['ticker', 'date'])\nprint(trades)\n#                    cusip\n# ticker date             \n# IBM    2000-01-01    NaN\n# MSFT   2000-01-02    NaN\n# GOOG   2000-01-03    100  # <-- We do not want to overwrite this\n# AAPL   2000-01-04    NaN\n\nconfig = pd.DataFrame({'ticker' : ['IBM', 'MSFT', 'GOOG', 'AAPL'],\n                       'date' : pd.date_range('1\/1\/2000', periods = 4),\n                       'cusip' : [1,2,3,nan]})\nconfig = config.set_index(['ticker', 'date'])\n\n# Let's permute the index to show `DataFrame.update` correctly matches rows based on the index, not on the order of the rows.\nnew_index = sorted(config.index)\nconfig = config.reindex(new_index)    \nprint(config)\n#                    cusip\n# ticker date             \n# AAPL   2000-01-04    NaN\n# GOOG   2000-01-03      3\n# IBM    2000-01-01      1\n# MSFT   2000-01-02      2\n\n Text: Then you can update NaN values in trades with values from config using the DataFrame.update method. Note that DataFrame.update matches rows based on indices (which is why set_index was called above).\n Code: trades.update(config, join = 'left', overwrite = False)\nprint(trades)\n\n#                    cusip\n# ticker date             \n# IBM    2000-01-01      1\n# MSFT   2000-01-02      2\n# GOOG   2000-01-03    100 # If overwrite = True, then 100 is overwritten by 3.\n# AAPL   2000-01-04    NaN\n\n\nAPI:\npandas.DataFrame.update\npandas.DataFrame.update\npandas.DataFrame.set_index\n","label":[[1309,1325,"Mention"],[1344,1360,"Mention"],[1405,1414,"Mention"],[1734,1757,"API"],[1758,1781,"API"],[1782,1808,"API"]],"Comments":[]}
{"id":60805,"text":"ID:13984485\nPost:\nText: pd.concat() performs an 'outer' join on the indexes by default and holes can be filled by padding forwards and\/or backwards in time.\n Code: In [17]: pd.concat([DataFrame({'s1': s1}), DataFrame({'s2': s2})]).ffill().bfill()\nOut[17]: \n                 s1   s2\n2012-12-21  9.0e-01 -0.3\n2012-12-22  5.0e-03 -0.3\n2012-12-23 -2.9e-01 -0.3\n2012-12-23 -2.9e-01 -0.3\n2012-12-24 -2.9e-01 -1.8\n2012-12-25 -2.9e-01 -1.4\n\n Text: I should add that ffill() and bfill() are new in pandas 0.10.0.  Prior to that, you can use fillna(method='ffill') and fillna(method='bfill').\n\nAPI:\npandas.concat\npandas.DataFrame.ffill\npandas.DataFrame.bfill\npandas.DataFrame.fillna\npandas.DataFrame.fillna\n","label":[[24,35,"Mention"],[458,465,"Mention"],[470,477,"Mention"],[532,554,"Mention"],[559,581,"Mention"],[589,602,"API"],[603,625,"API"],[626,648,"API"],[649,672,"API"],[673,696,"API"]],"Comments":[]}
{"id":60806,"text":"ID:6715540\nPost:\nText: One good option is to rotate the tick labels.\n Text: In your specific case, you might find it convenient to use figure.autofmt_xdate() (Which will rotate the x-axis labels among other things).\n Text: Alternatively, you could do plt.setp(plt.xticks()[1], rotation=30)  (or various other ways of doing the same thing).\n Text: Also, as a several year later edit, with recent versions of matplotlib, you can call fig.tight_layout() to resize things to fit the labels inside the figure, as @elgehelge notes below.\n\nAPI:\nmatplotlib.figure.Figure.autofmt_xdate\nmatplotlib.pyplot.setp\nmatplotlib.pyplot.xticks\nmatplotlib.figure.Figure.tight_layout\n","label":[[135,157,"Mention"],[251,259,"Mention"],[260,275,"Mention"],[432,450,"Mention"],[538,576,"API"],[577,599,"API"],[600,624,"API"],[625,662,"API"]],"Comments":[]}
{"id":60807,"text":"ID:7534860\nPost:\nText: Maybe your update answers the question. Since fmin uses a downhill gradient algorithm, it searches in a neighborhood of the initial guess for the direction of steepest descent. If you are deep enough into a parameter-region where the function always returns inf then the algorithm can not see which direction to go.\n\nAPI:\nscipy.optimize.fmin\n","label":[[69,73,"Mention"],[345,364,"API"]],"Comments":[]}
{"id":60808,"text":"ID:5073509\nPost:\nText: The pyplot.scatter() function was tailor made for this reason:\n Code: import matplotlib.pyplot as plt\nim = plt.imread(image_name)\nimplot = plt.imshow(im)\n\n# put a blue dot at (10, 20)\nplt.scatter([10], [20])\n\n# put a red dot, size 40, at 2 locations:\nplt.scatter(x=[30, 40], y=[50, 60], c='r', s=40)\n\nplt.show()\n\n Text: See the documentation for more info.\n\nAPI:\nmatplotlib.pyplot.scatter\n","label":[[27,43,"Mention"],[386,411,"API"]],"Comments":[]}
{"id":60809,"text":"ID:20624521\nPost:\nText: The algorithm underlying LinearSVC is very sensitive to extreme values in its input:\n Code: >>> svc = LinearSVC(verbose=1)\n>>> svc.fit(data, groups)\n[LibLinear]....................................................................................................\noptimization finished, #iter = 1000\n\nWARNING: reaching max number of iterations\nUsing -s 2 may be faster (also see FAQ)\n\nObjective value = -0.001256\nnSV = 4\nLinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n     random_state=None, tol=0.0001, verbose=1)\n\n Text: (The warning refers to the LibLinear FAQ, since scikit-learn's LinearSVC is based on that library.)\n Text: You should normalize before fitting:\n Code: >>> from sklearn.preprocessing import scale\n>>> data = scale(data)\n>>> svc.fit(data, groups)\n[LibLinear]...\noptimization finished, #iter = 39\nObjective value = -0.240988\nnSV = 4\nLinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n     random_state=None, tol=0.0001, verbose=1)\n>>> svc.predict(data)\narray([1, 1, 2, 2])\n\n\nAPI:\nsklearn.svm.LinearSVC\nsklearn.svm.LinearSVC\n","label":[[49,58,"Mention"],[697,706,"Mention"],[1196,1217,"API"],[1218,1239,"API"]],"Comments":[]}
{"id":60810,"text":"ID:17702781\nPost:\nText: You've got everything you need. You'll be happy to discover replace: df.replace(d)\n\n\nAPI:\npandas.DataFrame.replace\npandas.DataFrame.replace","label":[[84,91,"Mention"],[93,106,"Mention"],[114,138,"API"],[139,163,"API"]],"Comments":[]}
{"id":60811,"text":"ID:21240646\nPost:\nText: For low-cardinality categorical features it might be appropriate to use a one-hot encoding feature expansion. Have a look at: \n Text: \nhttp:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#encoding-categorical-features\nhttp:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#loading-features-from-dicts\n\n Text: For high cardinality categorical features, you can keep the integer encoding for ExtraTreesClassifier. Even though the algorithm will treat them as regular continuous variables, it does not seem to impact the predictive accuracy too negatively in practice.\n Text: Edit: in any case scikit-learn expect homogeneous floating point type encoding for all the input features. The object dtype is never a valid input type.\n\nAPI:\nsklearn.ensemble.ExtraTreesClassifier\n","label":[[427,447,"Mention"],[769,806,"API"]],"Comments":[]}
{"id":60812,"text":"ID:3320629\nPost:\nText: Note: This answer uses MATLAB syntax, since the question was originally tagged as such. However, even if you're using matplotlib in Python the concept should be the same even if the syntax is slightly different.\n Text: One option you have is to make a polygon that appears to have a hole in it, but really just has two of its edges wrapping around an empty space and touching. You can do this by creating a set of x and y coordinates that track around the edge of the circle, then track from the circle edge to the edge of a bounding square, then track around the edge of that square and back to the circle edge along the same line. Here's an example with a unit circle and a 4 by 4 square centered at the origin:\n Code: theta = linspace(0,2*pi,100);      %# A vector of 100 angles from 0 to 2*pi\nxCircle = cos(theta);              %# x coordinates for circle\nyCircle = sin(theta);              %# y coordinates for circle\nxSquare = [2 2 -2 -2 2 2];         %# x coordinates for square\nySquare = [0 -2 -2 2 2 0];         %# y coordinates for square\nhp = fill([xCircle xSquare],...    %# Plot the filled polygon\n          [yCircle ySquare],'r');\naxis equal                         %# Make axes tick marks equal in size\n\n Text: And here is the figure you should see:\n Text: Notice the line on the right joining the edges of the circle and square. This is where two edges of the red polygon meet and touch each other. If you don't want the edge lines to be visible, you can change their color to be the same as the fill color for the polygon like so: set(hp,'EdgeColor','r');\n\n\nAPI:\nmatplotlib.axes.Axes.set","label":[[1571,1594,"Mention"],[1603,1627,"API"]],"Comments":[]}
{"id":60813,"text":"ID:10272967\nPost:\nText: Things are slowing down because you're adding more and more images and drawing them all each time.\n Text: Either 1) clear the plot between each image (In your case, pylab.cla()), or better yet 2) don't make a new image, just set the data of the existing image to the new data.\n Text: As an example of using cla():\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nimages = np.random.uniform(0, 255, size=(40, 50, 50))\n\nfig, ax = plt.subplots()\n\nfig.show()\nfor image in images:\n    ax.imshow(image)\n    fig.canvas.draw()\n    ax.cla()\n\n Text: And as an example of just setting the data:\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nimages = np.random.uniform(0, 255, size=(40, 50, 50))\n\nfig, ax = plt.subplots()\n\nim = ax.imshow(images[0])\nfig.show()\nfor image in images[1:]:\n    im.set_data(image)\n    fig.canvas.draw()\n\n Text: You'll notice that the second method is considerably faster.\n\nAPI:\npylab.cla\nmatplotlib.axes.Axes.cla","label":[[331,336,"Mention"],[950,974,"API"]],"Comments":[]}
{"id":60814,"text":"ID:1183415\nPost:\nText: You can use the Axes.set_yscale method. That allows you to change the scale after the Axes object is created. That would also allow you to build a control to let the user pick the scale if you needed to.\n Text: The relevant line to add is: ax.set_yscale('log')\n\n Text: You can use 'linear' to switch back to a linear scale. Here's what your code would look like:\n Code: import pylab\nimport matplotlib.pyplot as plt\na = [pow(10, i) for i in range(10)]\nfig = plt.figure()\nax = fig.add_subplot(2, 1, 1)\n\nline, = ax.plot(a, color='blue', lw=2)\n\nax.set_yscale('log')\n\npylab.show()\n\n\nAPI:\nmatplotlib.axes.Axes.set_yscale\nmatplotlib.axes.Axes.set_yscale","label":[[39,54,"Mention"],[263,283,"Mention"],[606,637,"API"],[638,669,"API"]],"Comments":[]}
{"id":60815,"text":"ID:4831249\nPost:\nText: You're missing plt.show() to order matplotlib to show a window with the graph.\n\nAPI:\nmatplotlib.pyplot.show\n","label":[[38,48,"Mention"],[108,130,"API"]],"Comments":[]}
{"id":60816,"text":"ID:14603893\nPost:\nText: You can use join to do the combining:\n Code: >>> import pandas as pd\n>>> df = pd.DataFrame({\"A\": [10,20,30], \"B\": [20, 30, 10]})\n>>> df\n    A   B\n0  10  20\n1  20  30\n2  30  10\n>>> df * 2\n    A   B\n0  20  40\n1  40  60\n2  60  20\n>>> df.join(df*2, rsuffix='1')\n    A   B  A1  B1\n0  10  20  20  40\n1  20  30  40  60\n2  30  10  60  20\n\n Text: where you could replace df*2 with df.apply(your_function) if you liked.\n\nAPI:\npandas.DataFrame.join\npandas.DataFrame.apply\n","label":[[36,40,"Mention"],[396,419,"Mention"],[440,461,"API"],[462,484,"API"]],"Comments":[]}
{"id":60817,"text":"ID:8994954\nPost:\nText: I would use the fill_between method. Look at the Our Favorite Recipes section of the manual for matplotlib for some good examples. They have one that looks like this:\n Text: and another that looks like this:\n\nAPI:\nmatplotlib.pyplot.fill_between\n","label":[[39,51,"Mention"],[237,267,"API"]],"Comments":[]}
{"id":60818,"text":"ID:3877578\nPost:\nText: This is similar to your original approach, and will use less space than unutbu's answer, but I suspect it will be slower.\n Code: >>> import numpy as np\n>>> p = np.array([[1.5, 0], [1.4,1.5], [1.6, 0], [1.7, 1.8]])\n>>> p\narray([[ 1.5,  0. ],\n       [ 1.4,  1.5],\n       [ 1.6,  0. ],\n       [ 1.7,  1.8]])\n>>> nz = (p == 0).sum(1)\n>>> q = p[nz == 0, :]\n>>> q\narray([[ 1.4,  1.5],\n       [ 1.7,  1.8]])\n\n Text: By the way, your line p.delete() doesn't work for me - ndarrays don't have a .delete attribute.\n\nAPI:\nnumpy.ndarray\n","label":[[487,495,"Mention"],[534,547,"API"]],"Comments":[]}
{"id":60819,"text":"ID:15425560\nPost:\nText: There isn't, but if you want to only apply to unique values, just do that yourself.  Get mySeries.unique(), then use your function to pre-calculate the mapped alternatives for those unique values and create a dictionary with the resulting mappings.  Then use pandas map with the dictionary.  This should be about as fast as you can expect.\n\nAPI:\npandas.Series.unique\n","label":[[113,130,"Mention"],[370,390,"API"]],"Comments":[]}
{"id":60820,"text":"ID:5487005\nPost:\nText: You can easily add a second legend by adding the line: ax2.legend(loc=0)\n\n Text: You'll get this:\n Text: But if you want all labels on one legend then you should do something like this:\n Code: import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc('mathtext', default='regular')\n\ntime = np.arange(10)\ntemp = np.random.random(10)*30\nSwdown = np.random.random(10)*100-10\nRn = np.random.random(10)*100-10\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nlns1 = ax.plot(time, Swdown, '-', label = 'Swdown')\nlns2 = ax.plot(time, Rn, '-', label = 'Rn')\nax2 = ax.twinx()\nlns3 = ax2.plot(time, temp, '-r', label = 'temp')\n\n# added these three lines\nlns = lns1+lns2+lns3\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc=0)\n\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20,100)\nplt.show()\n\n Text: Which will give you this:\n\nAPI:\nmatplotlib.axes.Axes.legend","label":[[78,95,"Mention"],[995,1022,"API"]],"Comments":[]}
{"id":60821,"text":"ID:13655271\nPost:\nText: You could use pd.to_datetime:\n Code: In [1]: import pandas as pd\n\nIn [2]: pd.to_datetime('2008-02-27')\nOut[2]: datetime.datetime(2008, 2, 27, 0, 0)\n\n Text: This allows you to \"clean\" the index (or similarly a column) by applying it to the Series:\n Code: df.index = pd.to_datetime(df.index)\n\n Text: or\n Code: df['date_col'] = df['date_col'].apply(pd.to_datetime)\n\n\nAPI:\npandas.to_datetime\n","label":[[38,52,"Mention"],[393,411,"API"]],"Comments":[]}
{"id":60822,"text":"ID:21777511\nPost:\nText: You have to pass an explicit random state to the d-tree constructor:\n Code: >>> DecisionTreeClassifier(random_state=42).get_params()['random_state']\n42\n\n Text: Leaving it at the default value of None means that the fit method will use numpy.random's singleton random state, which is not predictable and not the same across runs.\n\nAPI:\nsklearn.tree.DecisionTreeClassifier.fit\nnumpy.random\n","label":[[239,242,"Mention"],[359,398,"API"]],"Comments":[]}
{"id":60823,"text":"ID:2979118\nPost:\nText: I think this is a bit faster:\n Code: import matplotlib.pyplot as plt\nfrom matplotlib import cm\nfig = plt.figure()\nax = fig.add_axes([0.1,0.1,0.8,0.8])\nfor i in range(200):\n    matrix = complex_calculation()\n    ax.imshow(matrix, cmap=cm.gray)\n    fig.savefig(\"frame{0}.png\".format(i))\n\n Text: plt.imshow calls gca which calls gcf which checks to see if there is a figure; if not, it creates one. By manually instantiating the figure first, you do not need to do all that.\n\nAPI:\nmatplotlib.pyplot.imshow\nmatplotlib.pyplot.gca\nmatplotlib.pyplot.gcf\n","label":[[316,326,"Mention"],[333,336,"Mention"],[349,352,"Mention"],[501,525,"API"],[526,547,"API"],[548,569,"API"]],"Comments":[]}
{"id":60824,"text":"ID:8457520\nPost:\nText: Appears that h = plt.plot(x,y)\n\n Text: returns a list of matplotlib.lines.Line2D objects in h even if there is one element plotted, so h[0].set_alpha() is the method to use.\n\nAPI:\nmatplotlib.pyplot.plot\nmatplotlib.lines.Line2D\nmatplotlib.lines.Line2D.set_alpha","label":[[40,53,"Mention"],[158,174,"Mention"],[203,225,"API"],[250,283,"API"]],"Comments":[]}
{"id":60825,"text":"ID:9284123\nPost:\nText: I think you want to use the Axes method set_color_cycle. As you can imagine, it sets the list of colors that are cycled through when colors are assigned by default, i.e. when no color keyword is provided to the plot call. Here's an extended version of your example:\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nX = np.arange(1990, 1994)\nY = [[  1.50615936e+08,   5.88252480e+07,   2.60363587e+08],\n     [  1.53193798e+08,   5.91663430e+07,   2.63123995e+08],\n     [  1.55704596e+08,   5.94899260e+07,   2.65840188e+08],\n     [  1.58175186e+08,   5.97843680e+07,   2.68559452e+08]]\ncolors = [(0.99609375, 0.3984375, 0.3984375), \n          (0.796875, 0.0, 0.99609375),\n          (0.59765625, 0.99609375, 0.0)]\n\nfig = plt.figure()\nax1 = fig.add_subplot(211)\nax1.set_title('old way')\nax1.plot(X, Y)\n[ax1.lines[i].set_color(color) for i, color in enumerate(colors)]\n\nax2 = fig.add_subplot(212)\nax2.set_title('new way')\nax2.set_color_cycle(colors)\nax2.plot(X, Y)\n\nfig.savefig('manycolors.py')\nplt.show()\n\n Text: This results in two subplots with the identically colored lines:\n\nAPI:\nmatplotlib.axes.Axes\nmatplotlib.axes.Axes.set_color_cycle\n","label":[[51,55,"Mention"],[63,78,"Mention"],[1114,1134,"API"],[1135,1171,"API"]],"Comments":[]}
{"id":60826,"text":"ID:5136064\nPost:\nText: What you want is the grid keyword (if I understood the question correctly):\n Code: fig=figure()\nax = fig.add_subplot(111,projection=\"3d\")\nax.plot(X,Y,Z)\nax.grid(on=False)\nshow()\n\n Text: It would help to see how you are setting up your plot, but at least for me messing around in pylab, ax.grid(on=False) did the trick. This turns off the grid projected onto the sides of the cube. See the mplot3d API for more details:\n Text: http:\/\/matplotlib.sourceforge.net\/mpl_toolkits\/mplot3d\/api.html\n\nAPI:\nmatplotlib.axes.Axes.grid\n","label":[[309,326,"Mention"],[519,544,"API"]],"Comments":[]}
{"id":60827,"text":"ID:20983299\nPost:\nText: I guess you can speed things a bit if you change your code to something like\n Code: for dct in dictionary_list:\n    if 'gene_name' in dct:\n        dct['gene_name'] = str(dct['gene_name'])\n\n Text: I think you can't get away from coercing values to strings, as DictVectorizer uses isinstance(value, six.string_types) as a condition to filter out categorical values in provided data.\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\nisinstance\n","label":[[283,297,"Mention"],[411,452,"API"]],"Comments":[]}
{"id":60828,"text":"ID:14149319\nPost:\nText: You could use get_level_values:\n Code: firsts = df1.index.get_level_values('first')\ndf1['value2'] = df2.loc[firsts].values\n\n Text: Note: you are almost doing a join here (except the df1 is MultiIndex)... so there may be a neater way to describe this...\n Text: .\n Text: In an example (similar to what you have):\n Code: df1 = pd.DataFrame([['a', 'x', 0.123], ['a','x', 0.234],\n                    ['a', 'y', 0.451], ['b', 'x', 0.453]],\n                   columns=['first', 'second', 'value1']\n                   ).set_index(['first', 'second'])\ndf2 = pd.DataFrame([['a', 10],['b', 20]],\n                   columns=['first', 'value']).set_index(['first'])\n\nfirsts = df1.index.get_level_values('first')\ndf1['value2'] = df2.loc[firsts].values\n\nIn [5]: df1\nOut[5]: \n              value1  value2\nfirst second                \na     x        0.123      10\n      x        0.234      10\n      y        0.451      10\nb     x        0.453      20\n\n\nAPI:\npandas.MultiIndex.get_level_values\npandas.MultiIndex\n","label":[[38,54,"Mention"],[213,223,"Mention"],[965,999,"API"],[1000,1017,"API"]],"Comments":[]}
{"id":60829,"text":"ID:7592455\nPost:\nText: As @rocksportrocker implies, you need to take into account that histogram2d returns the edges in addition to the histogram. Another detail is that you probably want to explicitly pass in a range, otherwise one will be chosen for you based on the actual min and max values in your data. You then want to convert the edges to cell centers for the plot. Something like this:\n Code: \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 1000000                     # how many data points\nxmin, xmax = 0.0, 54.0          # distances\nymin, ymax = 0.0, 180.0         # angles\n\n# make up some random data\nxi=np.random.normal(xmax\/2.0, xmax\/4.0, n)\nyi=np.random.normal(ymax\/3.0, ymax\/3.0, n)\n\nZ, xedges, yedges = np.histogram2d(xi,yi, bins=(270,90), range=[[xmin, xmax], [ymin, ymax]])\n\n# find the cell centers from the cell edges\nx = 0.5*(xedges[:-1] + xedges[1:])\ny = 0.5*(yedges[:-1] + yedges[1:])\n\n# promote to 2D arrays\nY, X = np.meshgrid(y, x)\n\nplt.contour(X,Y,Z)\nplt.ylabel('angles')\nplt.xlabel('distance')\nplt.colorbar()\nplt.savefig(\"hist2d.png\")\n\n Text: yields a countour plot like this:\n Text: but personally I wouldn't use contours in this case, since the histogram is likely to be noisy.\n\nAPI:\nnumpy.histogram2d\n","label":[[87,98,"Mention"],[1223,1240,"API"]],"Comments":[]}
{"id":60830,"text":"ID:16923367\nPost:\nText: To delimit by a tab you can use the sep argument of to_csv:\n Code: df.to_csv(file_name, sep='\\t')\n\n Text: To use a specific encoding (e.g. 'utf-8') use the encoding argument:\n Code: df.to_csv(file_name, sep='\\t', encoding='utf-8')\n\n\nAPI:\npandas.DataFrame.to_csv\n","label":[[76,82,"Mention"],[262,285,"API"]],"Comments":[]}
{"id":60831,"text":"ID:10979853\nPost:\nText: Similar as set_data you can use set_text (see here for the documentation: http:\/\/matplotlib.sourceforge.net\/api\/artist_api.html#matplotlib.text.Text.set_text).\n Text: So first \n Code: text = plt.text(x, y, \"Some text\")\n\n Text: and then in the loop:\n Code: text.set_text(\"Some other text\")\n\n Text: In your example it could look like:\n Code: for t in range(10):\n    if t == 0:\n        points, = ax.plot(x, y, marker='o', linestyle='None')\n        text = plt.text(1, 5, \"Loops passed: 0\")\n    else:\n        new_x = ... # x updated\n        new_y = ... # y updated\n        points.set_data(new_x, new_y)\n        text.set_text(\"Loops passed: {0}\".format(t))\n    plt.pause(0.5)\n\n\nAPI:\nmatplotlib.lines.Line2D.set_data\nmatplotlib.text.Text.set_text\n","label":[[35,43,"Mention"],[56,64,"Mention"],[701,733,"API"],[734,763,"API"]],"Comments":[]}
{"id":60832,"text":"ID:11054126\nPost:\nText: Colormap class in matplotlib has nice methods set_over and set_under, which allow to set color to be used for out-of-range values on contour plot.\n Text: But what if I need to set color to be used in a range of values, e.g. white out values from -0.1 to 0.1? Is there a method set_between(colormap, interval, color)? If not, what would be the easiest solution?\n\nAPI:\nmatplotlib.colors.Colormap\nmatplotlib.colors.Colormap.set_over\nmatplotlib.colors.Colormap.set_under\n","label":[[24,32,"Mention"],[70,78,"Mention"],[83,92,"Mention"],[391,417,"API"],[418,453,"API"],[454,490,"API"]],"Comments":[]}
{"id":60833,"text":"ID:3137008\nPost:\nText: For different x-scales use twiny() (think of this as \"shared y-axes\"). An example slightly adapted from the matplotlib documentation:\n Code: import numpy as np\nimport matplotlib.pyplot as plt\n\n# plot f(x)=x for two different x ranges\nx1 = np.linspace(0, 1, 50)\nx2 = np.linspace(0, 2, 50)\nfig = plt.figure()\n\nax1 = fig.add_subplot(111)\nax1.plot(x1, x1,'b--')\n\nax2 = ax1.twiny()\nax2.plot(x2, x2, 'go')\n\nplt.show()\n\n Text: If you just wanted a second axis plot the second data set as invisible.\n Code: ax2.plot(x2, x2, alpha=0)\n\n\nAPI:\nmatplotlib.axes.Axes.twiny\n","label":[[50,57,"Mention"],[555,581,"API"]],"Comments":[]}
{"id":60834,"text":"ID:7982117\nPost:\nText: You could use matplotlib.dates.num2date to convert the nums back into datetime objects. Then call isoformat() to get the date as a string in ISO-8601 format.\n Code: import numpy as np\nimport matplotlib.dates as md\n\ndef num2isodate(num):\n    result=md.num2date(num).isoformat()\n    return result\n\nrawtable = np.loadtxt(\n    'IBTsample.txt', delimiter=',', skiprows=1,\n    converters= {1:md.datestr2num},\n    dtype={'names':['Season','ISO_time','Latitude','Longitude','Enum'],\n           'formats':['uint16','float','f4','f4','uint8']})\n\n Text: Convert the ISO_time dtype to object. This allows the column to hold float values at first, and strings later. Note that astype (below) returns a copy, so calling copy explicitly is no longer needed. Also, since you called copy, I assume holding two copies of the array in memory is not a problem. (If memory were tight, we could write the array line-by-line using the csv module instead of using np.savetxt. But since memory is not a problem, np.savetxt is more convenient.)\n Code: sortable = rawtable.astype({'names':['Season','ISO_time','Latitude','Longitude','Enum'],\n                            'formats':['uint16','object','f4','f4','uint8']})\nsortable = np.sort(sortable, order=('ISO_time'))\nsortable['ISO_time'] = [num2isodate(num) for num in sortable['ISO_time']]\nnp.savetxt('IBTsorted.txt', sortable, fmt='%d,%s,%.1f,%.1f,%d')\n\n Text: PS. I'd recommend never using from module import *, especially when module is numpy. It overwrites a number of Python builtin functions, such as abs, all, any, min, max, sum, round, etc. Not only does that make it harder to call the Python builtins, but also makes it easy to write code which looks right but contains hard-to-find or subtle bugs.\n\nAPI:\nmatplotlib.dates.num2date\ndatetime.datetime.isoformat\nnumpy.ndarray.astype\nnumpy.ndarray.copy\nnumpy.ndarray.copy\nnumpy.savetxt\nnumpy.savetxt\n","label":[[121,132,"Mention"],[687,693,"Mention"],[729,733,"Mention"],[789,793,"Mention"],[963,973,"Mention"],[1010,1020,"Mention"],[1790,1817,"API"],[1818,1838,"API"],[1839,1857,"API"],[1858,1876,"API"],[1877,1890,"API"],[1891,1904,"API"]],"Comments":[]}
{"id":60835,"text":"ID:12163554\nPost:\nText: In both cases you should tune the value of the regularization parameter C using grid search. You cannot compare the results otherwise as a good value for C for one might yield crappy results for the other model.\n Text: For the polynomial kernel you can also grid search the optimal value for the degree (e.g. 2 or 3 or more): in that case you should grid search both C and degree at the same time.\n Text: Edit:\n Text: \nThis has something to do with the uneven distribution of class' instances in my training data, right? Or am I calling the procedure incorrectly?\n\n Text: Check that you have at least 3 samples per class to be able to do StratifiedKFold cross validation with k == 3 (I think this is the default CV used by GridSearchCV for classification). If you have less, don't expect the model to be able to predict anything useful. I would recommend at least 100 samples per class (as a somewhat arbitrary rule of thumb min bound, unless you work on toy problems with less than 10 features and a lot of regularity in the decision boundaries between classes).\n Text: BTW, please always paste the complete traceback in questions \/ bug reports. Otherwise one might not have the necessary info to diagnose the right cause.\n\nAPI:\nsklearn.model_selection.StratifiedKFold\nsklearn.model_selection.GridSearchCV\n","label":[[662,677,"Mention"],[747,759,"Mention"],[1254,1293,"API"],[1294,1330,"API"]],"Comments":[]}
{"id":60836,"text":"ID:16991136\nPost:\nText: This probably means that there is a significant discrepancy between the distribution of the final evaluation data and the development set.\n Text: It would be interesting to measure the over-fitting of your decision trees though: what is the difference between the training score clf.score(X_train, y_train) and the testing score clf.score(X_test, y_test) on your split?\n Text: Also pure decision trees should be considered a toy classifier. They have very poor generalization properties (and can overfit a lot). You should really try ExtraTreesClassifier with increasing numbers for n_estimators. Start with n_estimators=10, then 50, 100, 500, 1000 if the dataset is small enough.\n\nAPI:\nsklearn.tree.DecisionTreeClassifier.score\nsklearn.tree.DecisionTreeClassifier.score\nsklearn.ensemble.ExtraTreesClassifier\n","label":[[303,330,"Mention"],[353,378,"Mention"],[558,578,"Mention"],[711,752,"API"],[753,794,"API"],[795,832,"API"]],"Comments":[]}
{"id":60837,"text":"ID:3721940\nPost:\nText: For what it's worth, you're slightly misunderstanding what imshow() returns, and how matplotlib axes are structured in general... \n Text: An AxesImage object is responsible for the image displayed (e.g. colormaps, data, etc), but not the axis that the image resides in.  It has no control over things like ticks and tick labels.\n Text: What you want to use is the current axis instance.  \n Text: You can access this with gca(), if you're using the pylab interface, or matplotlib.pyplot.gca if you're accessing things through pyplot.   However, if you're using either one, there is an xticks() function to get\/set the xtick labels and locations.\n Text: For example (using pylab):\n Code: import pylab\npylab.figure()\npylab.plot(range(10))\npylab.xticks([2,3,4], ['a','b','c'])\npylab.show()\n\n Text: Using a more object-oriented approach (on a random note, matplotlib's getters and setters get annoying quickly...):\n Code: import matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = fig.add_subplot(1,1,1) # Or we could call plt.gca() later...\nim = ax.imshow(np.random.random((10,10)))\nax.set_xticklabels(['a','b','c','d'])  # Or we could use plt.xticks(...)\n\n Text: Hope that clears things up a bit!\n\nAPI:\nmatplotlib.axes.Axes.imshow\nmatplotlib.image.AxesImage\npylab.gca\nmatplotlib.pyplot.gca\nmatplotlib.pyplot.xticks\n","label":[[82,90,"Mention"],[164,173,"Mention"],[444,449,"Mention"],[607,615,"Mention"],[1239,1266,"API"],[1267,1293,"API"],[1294,1303,"API"],[1326,1350,"API"]],"Comments":[]}
{"id":60838,"text":"ID:13216688\nPost:\nText: This can be accomplished quite simply with the DataFrame method apply.\n Code: In[1]: import pandas as pd; import numpy as np\n\nIn[2]: df = pd.DataFrame(np.arange(40.).reshape((8, 5)), columns=list('abcde')); df\nOut[2]: \n        a   b   c   d   e\n    0   0   1   2   3   4\n    1   5   6   7   8   9\n    2  10  11  12  13  14\n    3  15  16  17  18  19\n    4  20  21  22  23  24\n    5  25  26  27  28  29\n    6  30  31  32  33  34\n    7  35  36  37  38  39\n\nIn[3]: ser = pd.Series(np.arange(8) * 10); ser\nOut[3]: \n    0     0\n    1    10\n    2    20\n    3    30\n    4    40\n    5    50\n    6    60\n    7    70\n\n Text: Now that we have our DataFrame and Series we need a function to pass to apply.\n Code: In[4]: func = lambda x: np.asarray(x) * np.asarray(ser)\n\n Text: We can pass this to df.apply and we are good to go \n Code: In[5]: df.apply(func)\nOut[5]:\n          a     b     c     d     e\n    0     0     0     0     0     0\n    1    50    60    70    80    90\n    2   200   220   240   260   280\n    3   450   480   510   540   570\n    4   800   840   880   920   960\n    5  1250  1300  1350  1400  1450\n    6  1800  1860  1920  1980  2040\n    7  2450  2520  2590  2660  2730\n\n Text: df.apply acts column-wise by default, but it can can also act row-wise by passing axis=1 as an argument to apply.\n Code: In[6]: ser2 = pd.Series(np.arange(5) *5); ser2\nOut[6]: \n    0     0\n    1     5\n    2    10\n    3    15\n    4    20\n\nIn[7]: func2 = lambda x: np.asarray(x) * np.asarray(ser2)\n\nIn[8]: df.apply(func2, axis=1)\nOut[8]: \n       a    b    c    d    e\n    0  0    5   20   45   80\n    1  0   30   70  120  180\n    2  0   55  120  195  280\n    3  0   80  170  270  380\n    4  0  105  220  345  480\n    5  0  130  270  420  580\n    6  0  155  320  495  680\n    7  0  180  370  570  780\n\n Text: This could be done more concisely by defining the anonymous function inside apply\n Code: In[9]: df.apply(lambda x: np.asarray(x) * np.asarray(ser))\nOut[9]: \n          a     b     c     d     e\n    0     0     0     0     0     0\n    1    50    60    70    80    90\n    2   200   220   240   260   280\n    3   450   480   510   540   570\n    4   800   840   880   920   960\n    5  1250  1300  1350  1400  1450\n    6  1800  1860  1920  1980  2040\n    7  2450  2520  2590  2660  2730\n\nIn[10]: df.apply(lambda x: np.asarray(x) * np.asarray(ser2), axis=1)\nOut[10]:\n       a    b    c    d    e\n    0  0    5   20   45   80\n    1  0   30   70  120  180\n    2  0   55  120  195  280\n    3  0   80  170  270  380\n    4  0  105  220  345  480\n    5  0  130  270  420  580\n    6  0  155  320  495  680\n    7  0  180  370  570  780\n\n\nAPI:\npandas.DataFrame\npandas.DataFrame.apply\npandas.DataFrame\npandas.Series\npandas.DataFrame.apply\npandas.DataFrame.apply\npandas.DataFrame.apply\npandas.DataFrame.apply\npandas.DataFrame.apply\n","label":[[71,80,"Mention"],[88,93,"Mention"],[659,668,"Mention"],[673,679,"Mention"],[710,715,"Mention"],[808,816,"Mention"],[1209,1217,"Mention"],[1316,1321,"Mention"],[1891,1896,"Mention"],[2643,2659,"API"],[2660,2682,"API"],[2683,2699,"API"],[2700,2713,"API"],[2714,2736,"API"],[2737,2759,"API"],[2760,2782,"API"],[2783,2805,"API"],[2806,2828,"API"]],"Comments":[]}
{"id":60839,"text":"ID:20352873\nPost:\nText: No, SGDClassifier does not do multilabel classification -- it does multiclass classification, which is a different problem, although both are solved using a one-vs-all problem reduction.\n Text: Then, neither SGD nor OneVsRestClassifier.fit will accept a sparse matrix for y. The former wants an array of labels, as you've already found out. The latter wants, for multilabel purposes, a list of lists of labels, e.g.\n Code: y = [[1], [2, 3], [1, 3]]\n\n Text: to denote that X[0] has label 1, X[1] has labels {2,3} and X[2] has labels {1,3}.\n\nAPI:\nsklearn.linear_model.SGDClassifier\nsklearn.linear_model.SGDClassifier\nsklearn.multiclass.OneVsRestClassifier.fit","label":[[28,41,"Mention"],[232,235,"Mention"],[240,263,"Mention"],[569,603,"API"],[604,638,"API"],[639,681,"API"]],"Comments":[]}
{"id":60840,"text":"ID:10114652\nPost:\nText: Try using merge:\n Code: In [14]: right\nOut[14]: \n    ST_NAME  value2\n0    Oregon   6.218\n1  Nebraska   0.001\n\nIn [15]: merge(left, right)\nOut[15]: \n    ST_NAME  value  value2\n0  Nebraska  2.491   0.001\n1    Oregon  4.685   6.218\n\nIn [18]: merge(left, right, on='ST_NAME', sort=False)\nOut[18]: \n    ST_NAME  value  value2\n0    Oregon  4.685   6.218\n1  Nebraska  2.491   0.001\n\n Text: DataFrame.join is a bit of legacy method and apparently doesn't do column-on-column joins (originally it did index on column using the on parameter, hence the \"legacy\" designation).\n\nAPI:\npandas.merge\npandas.DataFrame.join\n","label":[[34,39,"Mention"],[407,421,"Mention"],[595,607,"API"],[608,629,"API"]],"Comments":[]}
{"id":60841,"text":"ID:8921377\nPost:\nText: I think you want pylab.bar(x,y,width=1) or equally pyplot's bar method. if not checkout the gallery for the many styles of plots you can do. Each image comes with example code showing you how to make it using matplotlib.\n\nAPI:\npylab.bar\nmatplotlib.pyplot.bar\n","label":[[83,86,"Mention"],[260,281,"API"]],"Comments":[]}
{"id":60842,"text":"ID:9119538\nPost:\nText: You can use the vmin and vmax keyword arguments of imshow as documented here. In particular, if you modify your imshow call to\n Code: plt.imshow(k, vmin=0, vmax=1)\n\n Text: the colours will be normalised as if there was a value 0 and a value 1 present in the data.\n\nAPI:\nmatplotlib.pyplot.imshow.vmin\nmatplotlib.pyplot.imshow.vmax\nmatplotlib.pyplot.imshow\nmatplotlib.pyplot.imshow\n","label":[[39,43,"Mention"],[48,52,"Mention"],[74,80,"Mention"],[135,141,"Mention"],[293,322,"API"],[323,352,"API"],[353,377,"API"],[378,402,"API"]],"Comments":[]}
{"id":60843,"text":"ID:3453527\nPost:\nText: If you use matplotlib on Mac OS X on the remote machine (B), you must first make sure that you use one of the X11-based display back-ends, since the native Mac OS X back-end cannot export its plots to another display.  Selecting a back-end can be achieved with\n Code: import matplotlib\nmatplotlib.use('GTK')  # Or any other X11 back-end\n\n Text: The list of supported back-ends can be obtained by giving use() an incorrect back-end name: matplotlib then prints an error message listing the possible back-ends.\n Text: ssh X11 forwarding can then be used to display matplotlib plots.\n\nAPI:\nmatplotlib.use\n","label":[[426,431,"Mention"],[610,624,"API"]],"Comments":[]}
{"id":60844,"text":"ID:4816815\nPost:\nText: Update:\nI wasn't happy with gradient so I found it more reliable to use numpy.diff.\n Text: Regarding the issue of noise, the mathematical problem is to locate maxima\/minima if we want to look at noise we can use something like convolve which was mentioned earlier.\n Code: import numpy as np\nfrom matplotlib import pyplot\n\na=np.array([10.3,2,0.9,4,5,6,7,34,2,5,25,3,-26,-20,-29],dtype=np.float)\n\ngradients=np.diff(a)\nprint gradients\n\n\nmaxima_num=0\nminima_num=0\nmax_locations=[]\nmin_locations=[]\ncount=0\nfor i in gradients[:-1]:\n        count+=1\n\n    if ((cmp(i,0)>0) & (cmp(gradients[count],0)<0) & (i != gradients[count])):\n        maxima_num+=1\n        max_locations.append(count)     \n\n    if ((cmp(i,0)<0) & (cmp(gradients[count],0)>0) & (i != gradients[count])):\n        minima_num+=1\n        min_locations.append(count)\n\n\nturning_points = {'maxima_number':maxima_num,'minima_number':minima_num,'maxima_locations':max_locations,'minima_locations':min_locations}  \n\nprint turning_points\n\npyplot.plot(a)\npyplot.show()\n\n\nAPI:\nnumpy.diff\nnumpy.convolve\n","label":[[250,258,"Mention"],[1061,1075,"API"]],"Comments":[]}
{"id":60845,"text":"ID:14130395\nPost:\nText: This is kind of fun.  I make no guarantees that this is the real pandas-fu; I'm still at the \"numpy + better indexing\" stage of learning pandas myself.  That said, something like this should get the job done.\n Text: First, we make a toy correlation matrix to play with:\n Code: >>> import pandas as pd\n>>> import numpy as np\n>>> frame = pd.DataFrame(np.random.randn(1000, 5), columns=['a', 'b', 'c', 'd', 'e'])\n>>> corr = frame.corr()\n>>> corr\n          a         b         c         d         e\na  1.000000  0.022246  0.018614  0.022592  0.008520\nb  0.022246  1.000000  0.033029  0.049714 -0.008243\nc  0.018614  0.033029  1.000000 -0.016244  0.049010\nd  0.022592  0.049714 -0.016244  1.000000 -0.015428\ne  0.008520 -0.008243  0.049010 -0.015428  1.000000\n\n Text: Then we make a copy, and use tril_indices_from to get at the lower indices to mask them:\n Code: >>> c2 = corr.copy()\n>>> c2.values[np.tril_indices_from(c2)] = np.nan\n>>> c2\n    a        b         c         d         e\na NaN  0.06952 -0.021632 -0.028412 -0.029729\nb NaN      NaN -0.022343 -0.063658  0.055247\nc NaN      NaN       NaN -0.013272  0.029102\nd NaN      NaN       NaN       NaN -0.046877\ne NaN      NaN       NaN       NaN       NaN\n\n Text: and now we can do stats on the flattened array:\n Code: >>> c2.unstack().mean()\n-0.0072054178481488901\n>>> c2.unstack().std()\n0.043839624201635466\n\n\nAPI:\nnumpy.tril_indices_from\n","label":[[816,833,"Mention"],[1391,1414,"API"]],"Comments":[]}
{"id":60846,"text":"ID:1643457\nPost:\nText: Another way is to use masked arrays:\n Code: import numpy as np\na = np.array([[ np.nan,   2.,   3., np.nan], [  1.,   2.,   3., 9]])\nprint(a)\n# [[ NaN   2.   3.  NaN]\n#  [  1.   2.   3.   9.]]\n\n Text: The np.ma.masked_invalid method returns a masked array with nans and infs masked out:\n Code: print(np.ma.masked_invalid(a))\n[[-- 2.0 3.0 --]\n [1.0 2.0 3.0 9.0]]\n\n Text: The np.ma.compress_cols method returns a 2-D array with any column containing a \nmasked value suppressed:\n Code: a=np.ma.compress_cols(np.ma.masked_invalid(a))\nprint(a)\n# [[ 2.  3.]\n#  [ 2.  3.]]\n\n Text: See \nmanipulating-a-maskedarray\n\nAPI:\nnumpy.ma.masked_invalid\nnumpy.ma.compress_cols\n","label":[[227,247,"Mention"],[396,415,"Mention"],[634,657,"API"],[658,680,"API"]],"Comments":[]}
{"id":60847,"text":"ID:10257695\nPost:\nText: pandas is a python library designed for analysing data sets with different datatypes.\n Text: If your data is in data.txt, you can read it with pandas.read_csv() and than sort the resulting DataFrame.\n Code: >>> import datetime\n>>> import pandas as pd\n\n>>> def date_converter(date_string):\n...     return datetime.datetime.strptime(datestring, '%B %d, %Y').date()\n>>> df = pd.read_csv('data.txt', sep='\\t', header=None,\n...                  converters={2:date_converter})\n>>> print df\n  X.1   X.2         X.3  X.4  X.5  X.6            X.7\n0  AX   123  2010-12-20    1    2  8.0     hello this\n1  AX  2313  2009-04-19    2    3  4.0       hi there\n2  AX  4532  2010-12-19    6    2  8.0       nice tie\n3  AX  1244  2011-01-10    3    4  8.0      king tale\n4  BX   214  2009-09-10    2    3  9.0      this king\n5  BX   114  2003-02-09    4    9  4.0    his brought\n6  BX  3214  2006-09-01    1    3  3.0       is great\n7  MG   980  2007-04-20    2    4  7.1  not available\n8  MG   246  2005-05-08    5    1  2.1      make goat\n\n>>> df = df.set_index(['X.1', 'X.3'])  # using a hierarchical index\n>>> df = df.sort_index()\n>>> print df\n                 X.2  X.4  X.5  X.6            X.7\nX.1 X.3                                           \nAX  2009-04-19  2313    2    3  4.0       hi there\n    2010-12-19  4532    6    2  8.0       nice tie\n    2010-12-20   123    1    2  8.0     hello this\n    2011-01-10  1244    3    4  8.0      king tale\nBX  2003-02-09   114    4    9  4.0    his brought\n    2006-09-01  3214    1    3  3.0       is great\n    2009-09-10   214    2    3  9.0      this king\nMG  2005-05-08   246    5    1  2.1      make goat\n    2007-04-20   980    2    4  7.1  not available\n\n Text: As it is numpy based, it should be the right choice for large data sets.\n\nAPI:\npandas.read_csv\npandas.DataFrame\n","label":[[213,222,"Mention"],[1819,1835,"API"]],"Comments":[]}
{"id":60848,"text":"ID:9226239\nPost:\nText: I would write something like this:\n Code: import matplotlib.pyplot as plt\nclass LineDrawer(object):\n    lines = []\n    def draw_line(self):\n        ax = plt.gca()\n        xy = plt.ginput(2)\n\n        x = [p[0] for p in xy]\n        y = [p[1] for p in xy]\n        line = plt.plot(x,y)\n        ax.figure.canvas.draw()\n\n        self.lines.append(line)\n\n Text: Using ginput() you can avoid more complicated event handling.  The way it 'works' is you plot something:\n Code: plt.plot([1,2,3,4,5])\nld = LineDrawer()\nld.draw_line() # here you click on the plot\n\n Text: For saving\/loading the line data to a file you can easily implement a method using pickle or shelve.  You can also pass the necessary metadata by the method draw_line()\n\nAPI:\nmatplotlib.pyplot.ginput\n","label":[[384,392,"Mention"],[757,781,"API"]],"Comments":[]}
{"id":60849,"text":"ID:13216292\nPost:\nText: Assuming your data source is in a csv file, \n Code: from pandas.io.parsers import read_csv\ndf = read_csv(\"radar_data.csv\")\n\ndf  # shows what is in df\n\n       loc  speed  time\n0    A     63     0\n1    B     61     0\n2    C     63     0\n3    D     65     0\n4    A     73     5\n5    B     73     5\n6    C     75     5\n7    D     75     5\n8    A     67     0\n9    B     68     0\n10   C     68     0\n11   D     70     0\n\n Text: Note that I did not set loc as the index yet so it uses an autoincrement integer index.\n Code: panel = df.set_index(['loc', 'time']).sortlevel(0).to_panel()\n\n Text: However, if your data frame is already using loc as the index, we will need to append the time column into it so that we have a loc-time hierarchal index. This can be done using the new append option in the set_index method.  Like this:-\n Code: panel = df.set_index(['time'], append=True).sortlevel(0).to_panel()\n\n Text: In either case, we should arrive at this scenario:-\n Code: panel  # shows what panel is\n\n<class 'pandas.core.panel.Panel'>\nDimensions: 1 (items) x 4 (major) x 2 (minor)\nItems: speed to speed\nMajor axis: A to D\nMinor axis: 0 to 5\n\npanel[\"speed\"]  # <--- This is what you are looking for.\n\n\ntime   0   5\nloc         \nA     63  67\nB     73  61\nC     68  73\nD     63  68\n\n Text: Hope this helps.\n\nAPI:\npandas.DataFrame.set_index\n","label":[[819,828,"Mention"],[1331,1357,"API"]],"Comments":[]}
{"id":60850,"text":"ID:8920505\nPost:\nText: \nYou can \"lift\" the graph by setting a lower ylim with ax.set_ylim.\nMarker dots can be added to the plot using the marker = 'o'\nparameter setting in the call to plt.plot:\n\n Code: import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\n\nxticks=['Jan','Feb','Mar','April','May']\nx=[1,2,3,4,5]\nyticks = ['Windy', 'Sunny', 'Rainy', 'Cloudy', 'Snowy']\ny=[2,1,3,5,4]\n\nplt.plot(x,y,'b-', marker = 'o') #.2,.1,.7,.8\nplt.subplots_adjust(left =0.2)\n\nplt.xticks(x,xticks)\nplt.yticks(y,yticks)\nax.set_ylim(0.5,max(y))\nplt.show()\n\n\nAPI:\nmatplotlib.axes.Axes.set_ylim\nmatplotlib.pyplot.plot\n","label":[[78,89,"Mention"],[184,192,"Mention"],[572,601,"API"],[602,624,"API"]],"Comments":[]}
{"id":60851,"text":"ID:21423342\nPost:\nText: The fit function itself doesn't support anything like that. However, you can draw the decision tree, including feature labels, with the export_graphviz member function. (Isn't this how you generated the tree above?). Essentially, you'd do something like this:\n Code: iris = load_iris()\nt = tree.DecisionTreeClassifier()\nfitted_tree = t.fit(iris.data, iris.targets)\noutfile = tree.export_graphviz(fitted_tree, out_file='filename.dot', feature_names=iris.feature_names)\noutfile.close()\n\n Text: This will produce a 'dot' file, which graphviz (which must be installed separately) can then \"render\" into a traditional image format (postscript, png, etc.) For example, to make a png file, you'd run:\n Code: dot -Tpng filename.dot > filename.png\n\n Text: The dot file itself is a plain-text format and fairly self-explanatory. If you wanted to tweak the text, a simple find-replace in the text editor of your choice would work. There are also python modules for directly interacting with graphviz and its files. PyDot seems to be pretty popular, but there are others too.\n Text: The shape reference in fit's documentation just refers to the layout of X, the training data matrix. Specifically, it expects the first index to vary over training examples, while the 2nd index refers to features. For example, suppose your data's shape is (150, 4), as is the case for iris.data. The fit function will interpret it as containing 150 training examples, each of which consists of four values. \n\nAPI:\nsklearn.tree.DecisionTreeClassifier.fit\nsklearn.tree.export_graphviz\nsklearn.tree.DecisionTreeClassifier.fit\nsklearn.tree.DecisionTreeClassifier.fit","label":[[28,31,"Mention"],[160,175,"Mention"],[1118,1123,"Mention"],[1395,1398,"Mention"],[1509,1548,"API"],[1549,1577,"API"],[1578,1617,"API"],[1618,1657,"API"]],"Comments":[]}
{"id":60852,"text":"ID:7014366\nPost:\nText: Suppose you want to group the ages into bins defined by age_groups.\nThen you can find which age range each age falls into using np.searchsorted:\n Code: import numpy as np\n\nages=np.array([0,0.05,1,3,5,10,13,19,25,35])\n\nage_groups = np.array([0, .01, .1, 5, 10, 15, 20, 25, 30, 35, 40])\n\nindex=age_groups.searchsorted(ages,side='left')\nfor age,nearest_age in zip(ages,age_groups[index]):\n    print('{a} --> {n}'.format(a=age,n=nearest_age))\n\n Text: yields\n Code: 0.0 --> 0.0\n0.05 --> 0.1\n1.0 --> 5.0\n3.0 --> 5.0\n5.0 --> 5.0\n10.0 --> 10.0\n13.0 --> 15.0\n19.0 --> 20.0\n25.0 --> 25.0\n35.0 --> 35.0\n\n\nAPI:\nnumpy.searchsorted\n","label":[[151,166,"Mention"],[622,640,"API"]],"Comments":[]}
{"id":60853,"text":"ID:13840061\nPost:\nText: You might find the pandas data analysis library useful for this to create a table on which you can easily do these functions. For example:\n Code: import pandas as pd\n\nmonths = [('11\/2010', 196.9876),('11\/2010', 654.9876), ('12\/2010', 234.9876)]\ndf = pd.DataFrame(months, columns=['Month', 'Value'])\n\n Text: df is a DataFrame (i.e. a table) that looks like:\n Code:     Month   Value\n0    11\/2010     196.9876\n1    11\/2010     654.9876\n2    12\/2010     234.9876\n\n Text: You can get the averages and totals by using groupby:\n Code: [7]:  df.groupby('Month').mean()\nOut[7]:         Value    \n        Month   \n        11\/2010  425.9876\n        12\/2010  234.9876\n\nIn [8]: df.groupby('Month').sum()\nOut[8]:          Value\n        Month   \n        11\/2010  851.9752\n        12\/2010  234.9876\n\n\nAPI:\npandas.DataFrame\npandas.DataFrame.groupby\n","label":[[339,348,"Mention"],[537,544,"Mention"],[815,831,"API"],[832,856,"API"]],"Comments":[]}
{"id":60854,"text":"ID:5246271\nPost:\nText: There's a good demo on adjusting image size in matplotlib figures on the SciPy site.\n Text: The effect of show() on the figure size will depend upon which matplotlib backend is used. For example, when I used the TkAgg backend (the default on my system) it added about 12 pixels to the width and height of the figure. But when I switched to the WXAgg backend, the figure size was actually reduced.\n Text: In addition, manually resizing the window displayed by show() will also change the figure size. Moreover, if displaying the figure would require a window too large for the screen, then the window size will be reduced, and the figure size reduced accordingly.\n Text: In any case, your best bet is likely going to be to reset the figure size before rendering the pdf. I.e.:\n Code: fig.set_size_inches(8.27, 11.69)\nfig.savefig('test.pdf')\n\n\nAPI:\nmatplotlib.pyplot.show\nmatplotlib.pyplot.show\n","label":[[129,135,"Mention"],[482,488,"Mention"],[870,892,"API"],[893,915,"API"]],"Comments":[]}
{"id":60855,"text":"ID:14169558\nPost:\nText: There doesn't seem to be a pandas method to compute time differences between entries of an irregular time series, though there is a convenience method to convert a time series index to an array of datetime.datetime objects, which can be converted to datetime.timedelta objects through subtraction.\n Code: In [6]: start_end = pd.DataFrame({'status': [0, 0]},\n                                 index=[pd.datetools.parse('1\/1\/2012'),\n                                        pd.datetools.parse('12\/31\/2012')])\n\nIn [7]: df = df.append(start_end).sort()\n\nIn [8]: df\nOut[8]: \n                     status\n2012-01-01 00:00:00       0\n2012-01-01 12:43:35       1\n2012-03-12 15:46:43       0\n2012-09-26 18:35:11       1\n2012-11-11 02:34:59       0\n2012-12-31 00:00:00       0\n\nIn [9]: pydatetime = pd.Series(df.index.to_pydatetime(), index=df.index)\n\nIn [11]: df['duration'] = pydatetime.diff().shift(-1).\\\n              map(datetime.timedelta.total_seconds, na_action='ignore')\n\nIn [16]: df\nOut[16]: \n                     status  duration\n2012-01-01 00:00:00       0     45815\n2012-01-01 12:43:35       1   6145388\n2012-03-12 15:46:43       0  17117308\n2012-09-26 18:35:11       1   3916788\n2012-11-11 02:34:59       0   4310701\n2012-12-31 00:00:00       0       NaN\n\nIn [17]: (df.status * df.duration).sum() \/ df.duration.sum()\nOut[17]: 0.31906950786402843\n\n Text: Note:\n Text: \nOur answers seem to differ because I set status before the first timestamp to zero, while those entries are NA in your df1 as there's no start value to forward fill and NA values are excluded by pandas mean().\ntimedelta.total_seconds() is new in Python 2.7.\nTiming comparison of this method versus reindexing:\nIn [8]: timeit delta_method(df)\n1000 loops, best of 3: 1.3 ms per loop\n\nIn [9]: timeit redindexing(df)\n1 loops, best of 3: 2.78 s per loop\n\n\n\nAPI:\ndatetime.datetime\ndatetime.timedelta\npandas.DataFrame.mean\ndatetime.timedelta.total_seconds\n","label":[[1595,1601,"Mention"],[1603,1628,"Mention"],[1887,1908,"API"],[1909,1941,"API"]],"Comments":[]}
{"id":60856,"text":"ID:5964666\nPost:\nText: Maybe I'm confused as to what you're doing... If you just want to save the figure, why not use the fig object's savefig method? \n Text: pyplot.savefig saves the active figure, but using a particular figure instance's fig.savefig method saves that particular figure, regardless of which one is active.\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(10)\n\nfigures = [plt.figure() for _ in range(10)]\n\nfor i, fig in enumerate(figures):\n    ax = fig.add_subplot(111)\n    ax.plot(x, i*x)\n    ax.axis('equal')\n\nfor i, fig in enumerate(figures):\n    fig.savefig('temp_slope_%02i.png' % i)\n\n\nAPI:\nmatplotlib.figure.Figure.savefig\nmatplotlib.pyplot.savefig\nmatplotlib.figure.Figure.savefig\n","label":[[135,142,"Mention"],[159,173,"Mention"],[240,251,"Mention"],[637,669,"API"],[670,695,"API"],[696,728,"API"]],"Comments":[]}
{"id":60857,"text":"ID:17217924\nPost:\nText: There is no model dedicated to one shot learning in scikit-learn.\n Text: Furthermore as you should see in the full traceback of your error message, GridSearchCV is using cross validation internally so you cannot use it a on dataset that does not have at least 2 positive samples.\n\nAPI:\nsklearn.model_selection.GridSearchCV\n","label":[[172,184,"Mention"],[310,346,"API"]],"Comments":[]}
{"id":60858,"text":"ID:5183572\nPost:\nText: Use the tolist() method on the matrix object :\n Code: >>> import numpy\n>>> m = numpy.matrix([1, 2, 3])\n>>> type(m)\n<class 'numpy.core.defmatrix.matrix'>\n>>> m.tolist()\n[[1, 2, 3]]\n\n\nAPI:\nnumpy.matrix.tolist\n","label":[[31,39,"Mention"],[210,229,"API"]],"Comments":[]}
{"id":60859,"text":"ID:3580047\nPost:\nText: Use the get_backend() function to obtain a string denoting which backend is in use:\n Code: >>> import matplotlib\n>>> matplotlib.get_backend()\n'TkAgg'\n\n\nAPI:\nmatplotlib.get_backend\n","label":[[31,44,"Mention"],[180,202,"API"]],"Comments":[]}
{"id":60860,"text":"ID:17328858\nPost:\nText: set_index is not inplace (unless you pass inplace=True). otherwise all correct\n Code: In [7]: df = df.set_index(pd.DatetimeIndex(df['b']))\n\nIn [8]: df\nOut[8]: \n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 100 entries, 2013-06-14 09:10:23.523845 to 2013-06-14 10:12:51.650043\nData columns (total 2 columns):\nb    100  non-null values\nc    100  non-null values\ndtypes: datetime64[ns](1), int64(1)\n\n Text: also as a FYI, in forthcoming 0.12 release (next week), \nyou can pass unit=us to specify units of microseconds since epoch\n Code: In [13]: pd.to_datetime(a,unit='us')\nOut[13]: \n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-06-14 13:10:23.523845, ..., 2013-06-14 14:12:51.650043]\nLength: 100, Freq: None, Timezone: None\n\n\nAPI:\npandas.DataFrame.set_index\n","label":[[24,33,"Mention"],[766,792,"API"]],"Comments":[]}
{"id":60861,"text":"ID:14289730\nPost:\nText: Assuming your DataFrame is as follows (with index 'in'), you can use set_index:\n Code: In [1]: df = pd.read_csv('ni.csv', sep='\\s+', index_col=0)\n\nIn [2]: df\nOut[2]: \n    year  ni  d  m   x   y  q\nin                           \n1   2012   1  2  0 NaN NaN  3\n6   2012   2  1  1   9   9  1\n5   2012   3  1  1  17  17  1\n3   2012   4  0  3  37  37  0\n5   2012   5  1  0 NaN NaN  3\n2   2012   6  3  1  15  15  3\n\nIn [3]: df.set_index('ni', drop=False)\nOut[3]: \n    year  ni  d  m   x   y  q\nni                           \n1   2012   1  2  0 NaN NaN  3\n2   2012   2  1  1   9   9  1\n3   2012   3  1  1  17  17  1\n4   2012   4  0  3  37  37  0\n5   2012   5  1  0 NaN NaN  3\n6   2012   6  3  1  15  15  3\n\n Text: Although this is probably ok in many cases, if you are concerned about speed and memory usage you can do this inplace (i.e. change df without creating a copy).\n Code: In [4]: df.set_index('ni', drop=False, inplace=True)\n\n Text: inplace seems to be around 30% faster.\n\nAPI:\npandas.DataFrame\npandas.DataFrame.set_index\npandas.DataFrame\n","label":[[38,47,"Mention"],[93,102,"Mention"],[859,861,"Mention"],[1001,1017,"API"],[1018,1044,"API"],[1045,1061,"API"]],"Comments":[]}
{"id":60862,"text":"ID:13250472\nPost:\nText: I don't think you can specify a column type the way you want (if there haven't been changes reciently and if the 6 digit number is not a date that you can convert to datetime). You could try using np.genfromtxt() and create the DataFrame from there.\n Text: EDIT: Take a look at  Wes Mckinney's blog, there might be something for you. It seems to be that there is a new parser from pandas 0.10 coming in November.\n\nAPI:\nnumpy.genfromtxt\npandas.DataFrame\n","label":[[221,236,"Mention"],[252,261,"Mention"],[443,459,"API"],[460,476,"API"]],"Comments":[]}
{"id":60863,"text":"ID:17134750\nPost:\nText: The easiest way is to use to_datetime:\n Code: df['col'] = pd.to_datetime(df['col'])\n\n Text: It also offers a dayfirst argument for European times (but beware this isn't strict).\n Text: Here it is in action:\n Code: In [11]: pd.to_datetime(pd.Series(['05\/23\/2005']))\nOut[11]:\n0   2005-05-23 00:00:00\ndtype: datetime64[ns]\n\n Text: You can pass a specific format:\n Code: In [12]: pd.to_datetime(pd.Series(['05\/23\/2005']), format=\"%m\/%d\/%Y\")\nOut[12]:\n0   2005-05-23\ndtype: datetime64[ns]\n\n\nAPI:\npandas.to_datetime\n","label":[[50,61,"Mention"],[514,532,"API"]],"Comments":[]}
{"id":60864,"text":"ID:8490990\nPost:\nText: Using matplotlib I was able to get this to work:\n Code: import csv\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nwith open('data.csv') as f:\n    r = csv.reader(f)\n    C, Y, X = zip(*r)\n\n    X = [float(x) for x in X]\n    Y = [float(y) for y in Y]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.hexbin(X, Y, C, gridsize=200)\n    plt.show()\n\n Text: However, the gridsize=200 is just not a good enough resolution, and when using higher values, matplotlib uses ridiculous amounts of memory and is extremely slow, on a data CSV file of ~1000 rows.\n Text: It seems to be a known issue that hexbin() has performance issues.\n\nAPI:\nmatplotlib.axes.Axes.hexbin","label":[[625,633,"Mention"],[664,691,"API"]],"Comments":[]}
{"id":60865,"text":"ID:14345393\nPost:\nText: This looks like a bug in to_excel, for the moment as a workaround I would recommend using to_csv (which seems not to show this issue).\n Text: I added this as an issue on github.\n Text: To answer the second question, if you really need to use to_excel...\n Text: You can use filter to select only those columns which include '-ba':\n Code: In [21]: filter(lambda x: '-ba' in x, tab2.columns)\nOut[21]: ['east-ba', 'north-ba', 'south-ba']\n\nIn [22]: tab2[filter(lambda x: '-ba' in x, tab2.columns)]\nOut[22]: \n        east-ba  north-ba  south-ba\nGender                             \n     f        1         0         1\n     m        1         1         0\n\n\nAPI:\npandas.DataFrame.to_excel\npandas.DataFrame.to_csv\npandas.DataFrame.to_excel\n","label":[[49,57,"Mention"],[114,120,"Mention"],[266,274,"Mention"],[678,703,"API"],[704,727,"API"],[728,753,"API"]],"Comments":[]}
{"id":60866,"text":"ID:4521652\nPost:\nText: Here's a little example using leastsq:\n Code: import numpy as np\nimport scipy.optimize as optimize\nimport collections\n\nx = np.array([821,576,473,377,326,300])\ny = np.array([255,235,208,166,157,140])\n\ndef sigmoid(p,x):\n    x0,y0,c,k=p\n    y = c \/ (1 + np.exp(-k*(x-x0))) + y0\n    return y\n\ndef residuals(p,x,y):\n    return y - sigmoid(p,x)\n\nParam=collections.namedtuple('Param','x0 y0 c k')\np_guess=Param(x0=600,y0=200,c=100,k=0.01)\np,cov,infodict,mesg,ier = optimize.leastsq(\n    residuals,p_guess,args=(x,y),full_output=1,warning=True)\np=Param(*p)\nxp = np.linspace(100, 1600, 1500)\nprint('''\\\nx0 = {p.x0}\ny0 = {p.y0}\nc = {p.c}\nk = {p.k}\n'''.format(p=p))\n\n Text: You could compute the residuals this way:\n Code: resid=residuals(p,x,y)\nprint(resid)\n# [ 0.76205302 -2.010142    2.60265297 -3.02849144  1.6739274 ]\n\n Text: But you don't have to compute resid -- infodict['fvec'] already contains the info.\n Code: print(infodict['fvec'])\n# [ 0.76205302 -2.010142    2.60265297 -3.02849144  1.6739274 ]\n\nchisq=(infodict['fvec']**2).sum()\n# dof is degrees of freedom\ndof=len(x)-len(p)\nrmse=np.sqrt(chisq\/dof)\nprint(rmse)\n# 5.40092057562\n\n\nAPI:\nscipy.optimize.leastsq\n","label":[[53,60,"Mention"],[1161,1183,"API"]],"Comments":[]}
{"id":60867,"text":"ID:2590381\nPost:\nText: You can't directly tell matplotlib to make a histogram from an input file - you'll need to open the file yourself and get the data from it. How you'd do that depends on the format of the file - if it's just a file with a number on each line, you can just go through each line, strip() spaces and newlines, and use float() to convert it to a number.\n\nAPI:\nstr.strip\nfloat\n","label":[[300,307,"Mention"],[378,387,"API"]],"Comments":[]}
{"id":60868,"text":"ID:7125856\nPost:\nText: I assume you can run the code you posted at least once. The problem only manifests itself after running the posted code many times. Correct?\n Text: If so, the following avoids the problem without really identifying the source of the problem. \nMaybe that is a bad thing, but this works in a pinch: Simply use multiprocessing to run the memory-intensive code in a separate process. You don't have to worry about fig.clf() or plt.close() or del a,b or gc.collect(). All memory is freed when the process ends.\n Code: import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport numpy as np      \n\nimport multiprocessing as mp\n\ndef worker():\n    N=1000000\n    a = np.arange(N)\n    b = np.random.randn(N)\n\n    fig = plt.figure(num=1, dpi=100, facecolor='w', edgecolor='w')\n    fig.set_size_inches(10,7)\n    ax = fig.add_subplot(111)\n    ax.plot(a, b)\n\n    fig.savefig('\/tmp\/random.png')   # code gives me an error here\n\nif __name__=='__main__':\n    proc=mp.Process(target=worker)\n    proc.daemon=True\n    proc.start()\n    proc.join()\n\n Text: You don't have to proc.join() either. The join will block the main process until the worker completes. If you omit the join, then the main process simply continues with the worker process working in the background.\n\nAPI:\nmatplotlib.figure.Figure.clf\nmatplotlib.pyplot.close\ngc.collect\nmultiprocessing.Process.join\n","label":[[433,442,"Mention"],[446,457,"Mention"],[1095,1106,"Mention"],[1298,1326,"API"],[1327,1350,"API"],[1362,1390,"API"]],"Comments":[]}
{"id":60869,"text":"ID:15709354\nPost:\nText: First using apply you could add a column with the signed shares (positive for Buy negative for Sell):\n Code: In [11]: df['signed_shares'] = df.apply(lambda row: row['nr_shares']\n                                                    if row['transaction'] == 'Buy'\n                                                    else -row['nr_shares'],\n                                        axis=1)\n\nIn [12]: df\nOut[12]: \n            year  month  day symbol transaction  nr_shares  signed_shares\nindex                                                                    \n2011-01-10  2011      1   10   AAPL         Buy       1500           1500\n2011-01-13  2011      1   13   GOOG        Sell       1000          -1000\n\n Text: Use just those columns of interest to you and unstack them:\n Code: In [13]: df[['symbol', 'signed_shares']].set_index('symbol', append=True)\nOut[13]: \n                   signed_shares\nindex      symbol               \n2011-01-10 AAPL             1500\n2011-01-13 GOOG            -1000\n\nIn [14]: a = df[['symbol', 'signed_shares']].set_index('symbol', append=True).unstack()\n\nIn [15]: a\nOut[15]: \n            signed_shares      \nsymbol               AAPL  GOOG\nindex                          \n2011-01-10           1500   NaN\n2011-01-13            NaN -1000\n\n Text: Reindex over whatever date range you like:\n Code: In [16]: rng = pd.date_range('2011-01-10', periods=4)\n\nIn [17]: a.reindex(rng)\nOut[17]: \n            signed_shares      \nsymbol               AAPL  GOOG\n2011-01-10           1500   NaN\n2011-01-11            NaN   NaN\n2011-01-12            NaN   NaN\n2011-01-13            NaN -1000\n\n Text: Finally fill in the NaNs with 0 using fillna:\n Code: In [18]: a.reindex(rng).fillna(0)\nOut[18]: \n            signed_shares      \nsymbol               AAPL  GOOG\n2011-01-10           1500     0\n2011-01-11              0     0\n2011-01-12              0     0\n2011-01-13              0 -1000\n\n Text: As @DSM points out, you can do [13]-[15] much nicer using pivot_table:\n Code: In [20]: df.reset_index().pivot_table('signed_shares', 'index', 'symbol')\nOut[20]: \nsymbol      AAPL  GOOG\nindex                 \n2011-01-10  1500   NaN\n2011-01-13   NaN -1000\n\n\nAPI:\npandas.DataFrame.apply\npandas.DataFrame.fillna\npandas.DataFrame.pivot_table\n","label":[[36,41,"Mention"],[1675,1681,"Mention"],[1992,2003,"Mention"],[2195,2217,"API"],[2218,2241,"API"],[2242,2270,"API"]],"Comments":[]}
{"id":60870,"text":"ID:17291339\nPost:\nText: Edit: \nIn the newer version of pandas, you can pass the sheet name as a parameter to pandas.read_excel() : \n Code: file_name =  # path to file + file name\nsheet =  # sheet name or sheet number or list of sheet numbers and names\n\nimport pandas as pd\ndf = pd.read_excel(io=file_name, sheet_name=sheet)\nprint(df.head(5))  # print first 5 rows of the dataframe\n\n Text: Check the docs for examples on how to pass sheet_name: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.read_excel.html\n Text: Old version: \nyou can use pandas package as well....\nIf you're working with an Excel file with a single sheet, you can simply use df = pd.read_excel(file_name)\n Text: df.head() will print first 5 rows of your Excel file\n Text: When you are working with an excel file with multiple sheets, you can use pandas.ExcelFile :\n Code: import pandas as pd\nxl = pd.ExcelFile(path + filename)\nxl.sheet_names\n\n>>> [u'Sheet1', u'Sheet2', u'Sheet3']\n\ndf = xl.parse(\"Sheet1\")\ndf.head()\n\n\n Text: If you're working with an Excel file with a single sheet, you can simply use:\n Code: import pandas as pd\ndf = pd.read_excel(path + filename)\nprint df.head()\n\n\nAPI:\npandas.read_excel\npandas.read_excel\npandas.DataFrame.head\npandas.ExcelFile","label":[[664,688,"Mention"],[696,705,"Mention"],[1191,1208,"API"],[1209,1230,"API"]],"Comments":[]}
{"id":60871,"text":"ID:13837723\nPost:\nText: Not possible according to the source code. See lines 819 and 830, format strings are hardcoded to %0.2f. If you really want it, just change it in your local file sklearn\/metrics\/metrics.py. Better yet, add an argument to classification_report with a precision number and use that. And submit your patch to the project!\n\nAPI:\nsklearn.metrics.classification_report\n","label":[[245,266,"Mention"],[349,386,"API"]],"Comments":[]}
{"id":60872,"text":"ID:8998541\nPost:\nText: Have a look at the Transformations tutorial (wow, that took a lot of digging to find -- !)\n Text: In particular, axes.transData.transform(points) returns pixel coordinates where (0,0) is the bottom-left of the viewport.\n Code: import matplotlib.pyplot as plt\n\n# set up a figure\nfig = plt.figure()\nax = fig.add_subplot(111)\nx = np.arange(0, 10, 0.005)\ny = np.exp(-x\/2.) * np.sin(2*np.pi*x)\nax.plot(x,y)\n\n# what's one vertical unit & one horizontal unit in pixels?\nax.transData.transform([(0,1),(1,0)])-ax.transData.transform((0,0))\n# Returns:\n# array([[   0.,  384.],   <-- one y unit is 384 pixels (on my computer)\n#        [ 496.,    0.]])  <-- one x unit is 496 pixels.\n\n Text: There are various other transforms you can do -- coordinates relative to your data, relative to the axes, as a proportion of the figure, or in pixels for the figure (the transformations tutorial is really good).\n Text: TO convert between pixels and points (a point is 1\/72 inches), you may be able to play around with matplotlib.transforms.ScaledTransform and fig.dpi_scale_trans (the tutorial has something on this, I think).\n\nAPI:\nmatplotlib.transforms.CompositeGenericTransform.transform\nmatplotlib.transforms.ScaledTranslation\nmatplotlib.figure.Figure.dpi_scale_trans\n","label":[[136,168,"Mention"],[1021,1058,"Mention"],[1063,1082,"Mention"],[1136,1193,"API"],[1194,1233,"API"],[1234,1274,"API"]],"Comments":[]}
{"id":60873,"text":"ID:5502162\nPost:\nText: I hope this helps.  I've always had a hard time with matplotlib's dates.  Matplotlib requires a float format which is days since epoch.  The helper functions num2date and date2num along with python builtin datetime can be used to convert to\/from.  The formatting business was lifted from this example.  You can change an axis on any plot to a date axis using set_major_formatter.\n Code: import numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import dates\nimport datetime\n\na = np.array([\n    [1293605162197, 0, 0],\n    [1293605477994, 63, 0],\n    [1293605478057, 0, 0],\n    [1293605478072, 2735, 1249],\n    [1293606162213, 0, 0],\n    [1293606162229, 0, 0]])\n\nd = a[:,0]\ny1 = a[:,1]\ny2 = a[:,2]\n\n# convert epoch to matplotlib float format\ns = d\/1000\nms = d-1000*s  # not needed?\ndts = map(datetime.datetime.fromtimestamp, s)\nfds = dates.date2num(dts) # converted\n\n# matplotlib date format object\nhfmt = dates.DateFormatter('%m\/%d %H:%M')\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.vlines(fds, y2, y1)\n\nax.xaxis.set_major_locator(dates.MinuteLocator())\nax.xaxis.set_major_formatter(hfmt)\nax.set_ylim(bottom = 0)\nplt.xticks(rotation='vertical')\nplt.subplots_adjust(bottom=.3)\nplt.show()\n\n\nAPI:\nmatplotlib.dates.num2date\nmatplotlib.dates.date2num\nmatplotlib.axis.Axis.set_major_formatter\n","label":[[181,189,"Mention"],[194,202,"Mention"],[382,401,"Mention"],[1236,1261,"API"],[1262,1287,"API"],[1288,1328,"API"]],"Comments":[]}
{"id":60874,"text":"ID:11698391\nPost:\nText: If you have pandas installed, checkout the read_fwf function that imports a fixed-width file and creates a DataFrame (2-d tabular data structure). It'll save you lines of code on import and also give you a lot of data munging functionality if you want to do any additional data manipulations.\n\nAPI:\npandas.read_fwf\npandas.DataFrame\n","label":[[67,75,"Mention"],[131,140,"Mention"],[323,338,"API"],[339,355,"API"]],"Comments":[]}
{"id":60875,"text":"ID:15144847\nPost:\nText: Even though df.loc[idx] may be a copy of a portion of df, assignment to df.loc[idx] modifies df itself. (This is also true of df.iloc and df.ix.)\n Text: For example,\n Code: import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A':[9,10]*6,\n                   'B':range(23,35),\n                   'C':range(-6,6)})\n\nprint(df)\n#      A   B  C\n# 0    9  23 -6\n# 1   10  24 -5\n# 2    9  25 -4\n# 3   10  26 -3\n# 4    9  27 -2\n# 5   10  28 -1\n# 6    9  29  0\n# 7   10  30  1\n# 8    9  31  2\n# 9   10  32  3\n# 10   9  33  4\n# 11  10  34  5\n\n Text: Here is our boolean index:\n Code: idx = (df['C']!=0) & (df['A']==10) & (df['B']<30)\n\n Text: We can modify those rows of df where idx is True by assigning to df.loc[idx, ...]. For example,\n Code: df.loc[idx, 'A'] += df.loc[idx, 'B'] * df.loc[idx, 'C']\nprint(df)\n\n Text: yields\n Code:       A   B  C\n0     9  23 -6\n1  -110  24 -5\n2     9  25 -4\n3   -68  26 -3\n4     9  27 -2\n5   -18  28 -1\n6     9  29  0\n7    10  30  1\n8     9  31  2\n9    10  32  3\n10    9  33  4\n11   10  34  5\n\n\nAPI:\npandas.DataFrame.loc\npandas.DataFrame\npandas.DataFrame.loc\npandas.DataFrame\npandas.DataFrame.iloc\npandas.DataFrame.ix\npandas.DataFrame\npandas.DataFrame.loc\n","label":[[36,47,"Mention"],[78,80,"Mention"],[96,107,"Mention"],[117,119,"Mention"],[150,157,"Mention"],[162,167,"Mention"],[690,692,"Mention"],[727,738,"Mention"],[1055,1075,"API"],[1076,1092,"API"],[1093,1113,"API"],[1114,1130,"API"],[1131,1152,"API"],[1153,1172,"API"],[1173,1189,"API"],[1190,1210,"API"]],"Comments":[]}
{"id":60876,"text":"ID:3274222\nPost:\nText: I think you can use the xticks function to set string labels:\n Code: nums = arange(2003, 2010)\nxticks(nums, (str(n) for n in nums))\n\n Text: EDIT: This is a better way:\n Code: gca().xaxis.set_major_formatter(FormatStrFormatter('%d'))\n\n Text: or something like that, anyway. (In older versions of Matplotlib the method was called setMajorFormatter.)\n\nAPI:\nmatplotlib.pyplot.xticks\nmatplotlib.axis.Axis.set_major_formatter\n","label":[[47,53,"Mention"],[351,368,"Mention"],[377,401,"API"],[402,442,"API"]],"Comments":[]}
{"id":60877,"text":"ID:6353051\nPost:\nText: Sure! To set the ticks, just, well... Set the ticks (see matplotlib.pyplot.xticks or ax.set_xticks).  (Also, you don't need to manually set the facecolor of the patches. You can just pass in a keyword argument.)\n Text: For the rest, you'll need to do some slightly more fancy things with the labeling, but matplotlib makes it fairly easy.\n Text: As an example:\n Code: import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.ticker import FormatStrFormatter\n\ndata = np.random.randn(82)\nfig, ax = plt.subplots()\ncounts, bins, patches = ax.hist(data, facecolor='yellow', edgecolor='gray')\n\n# Set the ticks to be at the edges of the bins.\nax.set_xticks(bins)\n# Set the xaxis's tick labels to be formatted with 1 decimal place...\nax.xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))\n\n# Change the colors of bars at the edges...\ntwentyfifth, seventyfifth = np.percentile(data, [25, 75])\nfor patch, rightside, leftside in zip(patches, bins[1:], bins[:-1]):\n    if rightside < twentyfifth:\n        patch.set_facecolor('green')\n    elif leftside > seventyfifth:\n        patch.set_facecolor('red')\n\n# Label the raw counts and the percentages below the x-axis...\nbin_centers = 0.5 * np.diff(bins) + bins[:-1]\nfor count, x in zip(counts, bin_centers):\n    # Label the raw counts\n    ax.annotate(str(count), xy=(x, 0), xycoords=('data', 'axes fraction'),\n        xytext=(0, -18), textcoords='offset points', va='top', ha='center')\n\n    # Label the percentages\n    percent = '%0.0f%%' % (100 * float(count) \/ counts.sum())\n    ax.annotate(percent, xy=(x, 0), xycoords=('data', 'axes fraction'),\n        xytext=(0, -32), textcoords='offset points', va='top', ha='center')\n\n\n# Give ourselves some more room at the bottom of the plot\nplt.subplots_adjust(bottom=0.15)\nplt.show()\n\n\nAPI:\nmatplotlib.pyplot.xticks\nmatplotlib.axes.Axes.set_xticks\n","label":[[108,121,"Mention"],[1832,1863,"API"]],"Comments":[]}
{"id":60878,"text":"ID:7769497\nPost:\nText: You're not actually adding a new axes.\n Text: Matplotlib is detecting that there's already a plot in that position and returning it instead of a new axes object.\n Text: (Check it for yourself. ax and newax will be the same object.)\n Text: There's probably not a reason why you'd want to, but here's how you'd do it.\n Text: (Also, don't call newax = plt.axes() and then call fig.add_subplot(newax) You're doing the same thing twice.)\n Text: Edit: With newer (>=1.2, I think?) versions of matplotlib, you can accomplish the same thing as the example below by using the label kwarg to fig.add_subplot.  E.g. newax = fig.add_subplot(111, label='some unique string')\n Code: import matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\n\n# If you just call `plt.axes()` or equivalently `fig.add_subplot()` matplotlib  \n# will just return `ax` again. It _won't_ create a new axis unless we\n# call fig.add_axes() or reset fig._seen\nnewax = fig.add_axes(ax.get_position(), frameon=False)\n\nax.plot(range(10), 'r-')\nnewax.plot(range(50), 'g-')\nnewax.axis('equal')\n\nplt.show()\n\n Text: Of course, this looks awful, but it's what you're asking for...\n Text: I'm guessing from your earlier questions that you just want to add a second x-axis? If so, this is a completely different thing.\n Text: If you want the y-axes linked, then do something like this (somewhat verbose...):\n Code: import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nnewax = ax.twiny()\n\n# Make some room at the bottom\nfig.subplots_adjust(bottom=0.20)\n\n# I'm guessing you want them both on the bottom...\nnewax.set_frame_on(True)\nnewax.patch.set_visible(False)\nnewax.xaxis.set_ticks_position('bottom')\nnewax.xaxis.set_label_position('bottom')\nnewax.spines['bottom'].set_position(('outward', 40))\n\nax.plot(range(10), 'r-')\nnewax.plot(range(21), 'g-')\n\nax.set_xlabel('Red Thing')\nnewax.set_xlabel('Green Thing')\n\nplt.show()\n\n Text: If you want to have a hidden, unlinked y-axis, and an entirely new x-axis, then you'd do something like this:\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots()\nfig.subplots_adjust(bottom=0.2)\n\nnewax = fig.add_axes(ax.get_position())\nnewax.patch.set_visible(False)\nnewax.yaxis.set_visible(False)\n\nfor spinename, spine in newax.spines.iteritems():\n    if spinename != 'bottom':\n        spine.set_visible(False)\n\nnewax.spines['bottom'].set_position(('outward', 25))\n\nax.plot(range(10), 'r-')\n\nx = np.linspace(0, 6*np.pi)\nnewax.plot(x, 0.001 * np.cos(x), 'g-')\n\nplt.show()\n\n Text: Note that the y-axis values for anything plotted on newax are never shown.\n Text: If you wanted, you could even take this one step further, and have independent x and y axes (I'm not quite sure what the point of it would be, but it looks neat...):\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots()\nfig.subplots_adjust(bottom=0.2, right=0.85)\n\nnewax = fig.add_axes(ax.get_position())\nnewax.patch.set_visible(False)\n\nnewax.yaxis.set_label_position('right')\nnewax.yaxis.set_ticks_position('right')\n\nnewax.spines['bottom'].set_position(('outward', 35))\n\nax.plot(range(10), 'r-')\nax.set_xlabel('Red X-axis', color='red')\nax.set_ylabel('Red Y-axis', color='red')\n\nx = np.linspace(0, 6*np.pi)\nnewax.plot(x, 0.001 * np.cos(x), 'g-')\n\nnewax.set_xlabel('Green X-axis', color='green')\nnewax.set_ylabel('Green Y-axis', color='green')\n\n\nplt.show()\n\n Text: You can also just add an extra spine at the bottom of the plot. Sometimes this is easier, especially if you don't want ticks or numerical things along it.   Not to plug one of my own answers too much, but there's an example of that here: How do I plot multiple X or Y axes in matplotlib?\n Text: As one last thing, be sure to look at the parasite axes examples if you want to have the different x and y axes linked through a specific transformation.\n\nAPI:\nmatplotlib.pyplot.axes\nmatplotlib.figure.Figure.add_subplot\nmatplotlib.figure.Figure.add_subplot\nmatplotlib.figure.Figure.add_subplot\n","label":[[372,382,"Mention"],[397,419,"Mention"],[605,620,"Mention"],[636,684,"Mention"],[3872,3894,"API"],[3895,3931,"API"],[3932,3968,"API"],[3969,4005,"API"]],"Comments":[]}
{"id":60879,"text":"ID:15375176\nPost:\nText: apply takes a function to apply to each value, not the series, and accepts kwargs.\nSo, the values do not have the .size() method.\n Text: Perhaps this would work:\n Code: from pandas import *\n\nd = {\"my_label\": Series(['A','B','A','C','D','D','E'])}\ndf = DataFrame(d)\n\n\ndef as_perc(value, total):\n    return value\/float(total)\n\ndef get_count(values):\n    return len(values)\n\ngrouped_count = df.groupby(\"my_label\").my_label.agg(get_count)\ndata = grouped_count.apply(as_perc, total=df.my_label.count())\n\n Text: The .agg() method here takes a function that is applied to all values of the groupby object.\n\nAPI:\npandas.Series.apply\npandas.Series\npandas.core.groupby.SeriesGroupBy.agg\n","label":[[24,29,"Mention"],[79,85,"Mention"],[534,540,"Mention"],[629,648,"API"],[649,662,"API"],[663,700,"API"]],"Comments":[]}
{"id":60880,"text":"ID:1613597\nPost:\nText: Numpy has a set function numpy.setmember1d() that works on sorted and uniqued arrays and returns exactly the boolean array that you want. If the input arrays don't match the criteria you'll need to convert to the set format and invert the transformation on the result.\n Code: import numpy as np\na = np.array([6,1,2,3,4,5,6])\nb = np.array([1,4,5])\n\n# convert to the uniqued form\na_set, a_inv = np.unique1d(a, return_inverse=True)\nb_set = np.unique1d(b)\n# calculate matching elements\nmatches = np.setmea_set, b_set)\n# invert the transformation\nresult = matches[a_inv]\nprint(result)\n# [False  True False False  True  True False]\n\n Text: Edit:\nUnfortunately the setmember1d method in numpy is really inefficient. The search sorted and assign method you proposed works faster, but if you can assign directly you might as well assign directly to the result and avoid lots of unnecessary copying. Also your method will fail if b contains anything not in a. The following corrects those errors:\n Code: result = np.zeros(a.shape, dtype=np.bool)\nidxs = a.searchsorted(b)\nidxs = idxs[np.where(idxs < a.shape[0])] # Filter out out of range values\nidxs = idxs[np.where(a[idxs] == b)] # Filter out where there isn't an actual match\nresult[idxs] = True\nprint(result)\n\n Text: My benchmarks show this at 91us vs. 6.6ms for your approach and 109ms for numpy setmember1d on 1M element a and 100 element b.\n\nAPI:\nnumpy.setmember1d\nnumpy.setmember1d\nnumpy.setmember1d\n","label":[[681,692,"Mention"],[1363,1374,"Mention"],[1434,1451,"API"],[1452,1469,"API"]],"Comments":[]}
{"id":60881,"text":"ID:4315914\nPost:\nText: Pure numpy\n\n Code: numpy.loadtxt(open(\"test.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n\n Text: \nCheck out the loadtxt documentation. \n Text: You can also use python's csv module:\n\n Code: import csv\nimport numpy\nreader = csv.reader(open(\"test.csv\", \"rb\"), delimiter=\",\")\nx = list(reader)\nresult = numpy.array(x).astype(\"float\")\n\n Text: \nYou will have to convert it to your favorite numeric type. I guess you can write the whole thing in one line:\n Text: \nresult = numpy.array(list(csv.reader(open(\"test.csv\", \"rb\"), delimiter=\",\"))).astype(\"float\")\n\n Text: Added Hint:\n Text: You could also use pandas.io.parsers.read_csv and get the associated numpy array which can be faster.\n\nAPI:\nnumpy.loadtxt\npandas.io.parsers.read_csv\n","label":[[130,137,"Mention"],[703,716,"API"]],"Comments":[]}
{"id":60882,"text":"ID:7778713\nPost:\nText: If you want to use only numpy, you can use structured arrays and the lib.recfunctions.join_by function (see http:\/\/pyopengl.sourceforge.net\/pydoc\/numpy.lib.recfunctions.html). A little example:\n Code: In [1]: import numpy as np\n   ...: import numpy.lib.recfunctions as rfn\n   ...: a = np.array([(1, 10.), (2, 20.), (3, 30.)], dtype=[('id', int), ('A', float)])\n   ...: b = np.array([(2, 200.), (3, 300.), (4, 400.)], dtype=[('id', int), ('B', float)])\n\nIn [2]: rfn.join_by('id', a, b, jointype='inner', usemask=False)\nOut[2]: \narray([(2, 20.0, 200.0), (3, 30.0, 300.0)], \n      dtype=[('id', '<i4'), ('A', '<f8'), ('B', '<f8')])\n\n Text: Another option is to use pandas (documentation). I have no experience with it, but it provides more powerful data structures and functionality than standard numpy, \"to make working with relational or labeled data both easy and intuitive\". And it certainly has joining and merging functions (for example see http:\/\/pandas.sourceforge.net\/merging.html#joining-on-a-key).\n\nAPI:\nnumpy.lib.recfunctions.join_by\n","label":[[92,116,"Mention"],[1039,1069,"API"]],"Comments":[]}
{"id":60883,"text":"ID:2054655\nPost:\nText: Python 3.1 has a collections.OrderedDict class that can be used for this purpose. It's very efficient, too: \"Big-O running times for all methods are the same as for regular dictionaries.\"\n Text: The code for OrderedDict itself is compatible with Python 2.x, though some inherited methods (from the _abcoll module) do use Python 3-only features. However, they can be modified to 2.x code with minimal effort.\n\nAPI:\ncollections.OrderedDict\ncollections.OrderedDict\n","label":[[231,242,"Mention"],[461,484,"API"]],"Comments":[]}
{"id":60884,"text":"ID:1828405\nPost:\nText: Can you show us how you are using np.meshgrid? There is a very good chance that you really don't need meshgrid because numpy broadcasting can do the same thing without generating a repetitive array.\n Text: For example,\n Code: import numpy as np\n\nx=np.arange(2)\ny=np.arange(3)\n[X,Y] = np.meshgrid(x,y)\nS=X+Y\n\nprint(S.shape)\n# (3, 2)\n# Note that meshgrid associates y with the 0-axis, and x with the 1-axis.\n\nprint(S)\n# [[0 1]\n#  [1 2]\n#  [2 3]]\n\ns=np.empty((3,2))\nprint(s.shape)\n# (3, 2)\n\n# x.shape is (2,).\n# y.shape is (3,).\n# x's shape is broadcasted to (3,2)\n# y varies along the 0-axis, so to get its shape broadcasted, we first upgrade it to\n# have shape (3,1), using np.newaxis. Arrays of shape (3,1) can be broadcasted to\n# arrays of shape (3,2).\ns=x+y[:,np.newaxis]\nprint(s)\n# [[0 1]\n#  [1 2]\n#  [2 3]]\n\n Text: The point is that S=X+Y can and should be replaced by s=x+y[:,np.newaxis] because \nthe latter does not require (possibly large) repetitive arrays to be formed. It also generalizes to higher dimensions (more axes) easily. You just add np.newaxis where needed to effect broadcasting as necessary.\n Text: See http:\/\/www.scipy.org\/EricsBroadcastingDoc for more on numpy broadcasting.\n\nAPI:\nnumpy.meshgrid\nnumpy.meshgrid\nnumpy.newaxis\n","label":[[57,68,"Mention"],[125,133,"Mention"],[1076,1086,"Mention"],[1228,1242,"API"],[1243,1257,"API"],[1258,1271,"API"]],"Comments":[]}
{"id":60885,"text":"ID:18277991\nPost:\nText: You can create a custom CV iterator, for instance by taking inspiration on  LeaveOneGroupOut or LeaveOneGroupOut to implement the structure you are interested in.\n Text: Alternatively you can prepare your own precomputed folds encoded as an array of integers (representing sample indices between 0 and n_samples - 1) and then pass that CV iterator as the cv argument of the cross_val_score and GridSearchCV utilities:\n Code: >>> X, y = make_classification(n_samples=10)\n>>> import numpy as np\n>>> from sklearn.datasets import make_classification\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.model_selection import cross_val_score\n>>> cv_splits = [\n...     (np.array([0, 1, 2, 3]), np.array([4, 5, 6])),\n...     (np.array([1, 2, 3, 4]), np.array([5, 6, 7])),\n...     (np.array([5, 6, 8, 9]), np.array([1, 2, 3, 4])),\n... ]\n>>> cross_val_score(LogisticRegression(), X, y, cv=cv_splits)\narray([1.        , 0.33333333, 0.75      ])\n\n\nAPI:\nsklearn.model_selection.LeaveOneGroupOut\nsklearn.model_selection.LeaveOneGroupOut\nsklearn.model_selection.cross_val_score\nsklearn.model_selection.GridSearchCV\n","label":[[100,116,"Mention"],[120,136,"Mention"],[398,413,"Mention"],[418,430,"Mention"],[987,1027,"API"],[1028,1068,"API"],[1069,1108,"API"],[1109,1145,"API"]],"Comments":[]}
{"id":60886,"text":"ID:22118042\nPost:\nText: I guess sklearn.preprocessing.StandardScaler would be the first thing you want to try. StandardScaler transforms all of your features into Mean-0-Std-1 features. \n Text: \nThis definitely gets rid of your first problem. AlexaRank will be guaranteed to be spread around 0 and bounded. (Yes, even massive AlexaRank values like 83904803289480 are transformed to small floating point numbers). Of course, the results will not be integers between 1 and 10000 but they will maintain same order as the original ranks. And in this case, keeping the rank bounded and normalized will help solve your second problem like follows.\nIn order to understand why normalization would help in LR, let's revisit the logit formulation of LR. \n\nIn your case, X1, X2, X3 are three TF-IDF features and X4, X5 are Alexa\/Google rank related features. Now, the linear form of equation suggest that the coefficients represent the change in logit of y with one unit change in a variable. Think what happens when your X4 is kept fixed at a massive rank value, say 83904803289480. In that case, the Alexa Rank variable dominates your LR fit and a small change in TF-IDF value has almost no effect on the LR fit. Now one might think that the coefficient should be able to adjust to small\/large values to account for differences between these features. Not in this case --- It's not only the magnitude of variables that matter but also their range. Alexa Rank definitely has a large range and should definitely dominate your LR fit in this case. Therefore, I guess normalizing all variables using StandardScaler to adjust their range will improve the fit. \n\n Text: Here is how you can scale the X matrix. \n Code: sc = proprocessing.StandardScaler().fit(X)\nX = sc.transform(X)\n\n Text: Don't forget to use same scaler to transform X_test.\n Code: X_test = sc.transform(X_test)\n\n Text: Now you can use the fitting procedure etc.\n Code: rd.fit(X, y)\nre.predict_proba(X_test)\n\n Text: Check this out for more on sklearn preprocessing: http:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n Text: Edit: Parsing and column merging part can be easily done using pandas, i.e., there is no need to convert the matrices into list and then append them. Moreover, pandas dataframes can be directly indexed by their column names. \n Code: AlexaAndGoogleTrainData = p.read_table('train.tsv', header=0)[[\"AlexaRank\", \"GooglePageRank\"]]\nAlexaAndGoogleTestData = p.read_table('test.tsv', header=0)[[\"AlexaRank\", \"GooglePageRank\"]]\nAllAlexaAndGoogleInfo = AlexaAndGoogleTestData.append(AlexaAndGoogleTrainData)\n\n Text: Note that we are passing header=0 argument to read_table to maintain original header names from tsv file. And also note how we can index using entire set of columns. Finally, you can stack this new matrix with X using numpy.hstack.\n Code: X = np.hstack((X, AllAlexaAndGoogleInfo))\n\n Text: hstack horizontally combined two multi-dimensional array-like structures provided their lengths are same.\n\nAPI:\nsklearn.preprocessing.StandardScaler\nsklearn.preprocessing.StandardScaler\nsklearn.preprocessing.StandardScaler\npandas.read_table\nnumpy.hstack\nnumpy.hstack\n","label":[[111,125,"Mention"],[1587,1601,"Mention"],[2637,2647,"Mention"],[2880,2886,"Mention"],[3029,3065,"API"],[3066,3102,"API"],[3103,3120,"API"],[3134,3146,"API"]],"Comments":[]}
{"id":60887,"text":"ID:10844760\nPost:\nText: This is not a pandas-specific issue.  In Python, assignment never copies anything:\n Code: >>> a = [1,2,3]\n>>> b = a\n>>> b[0] = 'WHOA!'\n>>> a\n['WHOA!', 2, 3]\n\n Text: If you want a new DataFrame, make a copy with e = d.copy().\n Text: Edit: I should clarify that assignment to a bare name never copies anything.  Assignment to an item or attribute (e.g., a[1] = x or a.foo = bar) is converted into method calls under the hood and may do copying depending on what kind of object a is.\n\nAPI:\npandas.DataFrame\npandas.DataFrame.copy\n","label":[[207,216,"Mention"],[239,247,"Mention"],[511,527,"API"],[528,549,"API"]],"Comments":[]}
{"id":60888,"text":"ID:17070356\nPost:\nText: Take a look at pandas. It has a DataFrame object which can hold your data and allow you manipulate in an intuitive way. It also has a read_csv function which takes out a lot of the hassle when dealing with csv files.\n Text: for example:\n Code: import pandas as pd\n\n#reads your csv file in and returns a DataFrame object as metioned above. \ndf = pd.read_csv(\"your_csv.csv\", sep=' ', names=['co_a','co_b'], header=None, skiprows=2)\n\n#extracts your discordant to separate lists\nlist1 = df.co_a.to_list()\nlist2 = df.co_b.to_list()\n\n Text: you can use df or df.head() to see your dataframe and how your data is managed.  It's also worth mentioning that df.co_a is a Series object, think super list \/ dict, and you can probably do your analysis or manipulation right from there. \n Text: Also if you show me how the comments are in the csv file, I can show you how to ignore them with read_csv.\n Text: I know you were looking for an answer with the csv module but this is a much more advanced tool and might help you out in the long run.\n Text: Hope it helps!\n\nAPI:\npandas.DataFrame\npandas.read_csv\npandas.DataFrame\npandas.DataFrame.head\npandas.Series\npandas.read_csv\n","label":[[56,65,"Mention"],[158,166,"Mention"],[571,573,"Mention"],[577,586,"Mention"],[685,691,"Mention"],[902,910,"Mention"],[1083,1099,"API"],[1100,1115,"API"],[1116,1132,"API"],[1133,1154,"API"],[1155,1168,"API"],[1169,1184,"API"]],"Comments":[]}
{"id":60889,"text":"ID:21911813\nPost:\nText: .set_xticks() on the axes will set the locations and set_xticklabels() will set the displayed text.\n Code: def test(axes):\n    axes.bar(x,y)\n    axes.set_xticks(x)\n    axes.set_xticklabels([i+100 for i in x])\n\n\nAPI:\nmatplotlib.axes.Axes.set_xticks\nmatplotlib.axes.Axes.set_xticklabels\n","label":[[24,37,"Mention"],[77,94,"Mention"],[240,271,"API"],[272,308,"API"]],"Comments":[]}
{"id":60890,"text":"ID:9373130\nPost:\nText: It seems that there are two completely zero columns in the input matrix. This produces a big band of zeros in the internal covariance matrix calculated by gaussian_kde, making it singular and causing the routine to fail.\n Text: If I rewrite your example like this:\n Code: import numpy as np  \nfrom scipy import stats  \nimport matplotlib.pylab as plt\n\nvalid=[0,1,2,3,4,5,6,7,10,11]\nmatrix=np.loadtxt('data.txt',skiprows=1,usecols=valid)\nkdeData = np.array(matrix).T  \nprint kdeData\ngkde = stats.gaussian_kde(kdeData)\nind = np.linspace(-13,13,512)\nkdepdf = gkde.evaluate(kdeData)\nplt.figure()\nplt.plot(ind, stats.norm.pdf(ind), color=\"r\", label='DGP normal')\nplt.plot(ind, kdepdf, label='kde', color=\"g\")\nplt.title('Kernel Density Estimation')\nplt.legend()\nplt.show()\n\n Text: It works:\n\n\nAPI:\nscipy.stats.gaussian_kde\n","label":[[178,190,"Mention"],[814,838,"API"]],"Comments":[]}
{"id":60891,"text":"ID:4073024\nPost:\nText: Ubuntu beat me to it while I was typing this example, but his example just uses linear interpolation, which can be more easily done with numpy.interpolate...  (The difference is only a keyword argument in scipy.interpolate.interp1d, however).\n Text: I figured I'd include my example, as it shows using scipy.interpolate.interp1d with a cubic spline...\n Code: import numpy as np\nimport scipy as sp\nimport scipy.interpolate\nimport matplotlib.pyplot as plt\n\n# Generate some random data\ny = (np.random.random(10) - 0.5).cumsum()\nx = np.arange(y.size)\n\n# Interpolate the data using a cubic spline to \"new_length\" samples\nnew_length = 50\nnew_x = np.linspace(x.min(), x.max(), new_length)\nnew_y = sp.interpolate.interp1d(x, y, kind='cubic')(new_x)\n\n# Plot the results\nplt.figure()\nplt.subplot(2,1,1)\nplt.plot(x, y, 'bo-')\nplt.title('Using 1D Cubic Spline Interpolation')\n\nplt.subplot(2,1,2)\nplt.plot(new_x, new_y, 'ro-')\n\nplt.show()\n\n\nAPI:\nnumpy.interp\nscipy.interpolate.interp1d\nscipy.interpolate.interp1d\n","label":[[160,177,"Mention"],[956,968,"API"]],"Comments":[]}
{"id":60892,"text":"ID:5030884\nPost:\nText: There are 2 ways: \nthe first is to use pythons random number generator from c++. It will probably look something like this:\n Code: boost::python::object randmod = boost::python::import(\"numpy.random\")\nboost::python::object randfunc = randmod.attr(\"RandomState\")\nrandfunc(10)\n\n Text: The second is to wrap and expose the c++ function so that it can be used from python. The code for this is left an an exercise for the student.  \n Text: Edit:\nOnce you have exported the c++ function you would have to make a python object that mimics the interface of numpy.random.RandomState using the c++ function for it's random bits. This is probably more work then you want to do. I have not used numpy, but from the docs it looks like the RandomState object is not-trivial.\n\nAPI:\nnumpy.random.RandomState\nnumpy.random.RandomState\n","label":[[750,761,"Mention"],[816,840,"API"]],"Comments":[]}
{"id":60893,"text":"ID:14086167\nPost:\nText: You may find it faster to extract the index as a column and use apply and bfill.\nSomething like this:\n Code: df['datetime'] = df.index\ndf['stops'] = df.apply(lambda x: x['datetime']\n                                 if x['pct_change'] < -0.015\n                                 else np.nan,\n                        axis=1)\ndf['stops'] = df['stops'].bfill()\n\n\nAPI:\npandas.DataFrame.apply\npandas.DataFrame.bfill","label":[[88,93,"Mention"],[98,103,"Mention"],[386,408,"API"],[409,431,"API"]],"Comments":[]}
{"id":60894,"text":"ID:2332520\nPost:\nText: The problem with the answer provided by doug is that it relies on the fact that the colormap maps zero values to white. This means that colormaps that do not include white color are not useful. The key for solution is cm.set_bad function. You mask the unneeded parts of the matrix with None or with NumPy masked arrays and set_bad to white, instead of the default black. Adopting doug's example we get the following:\n Code: import numpy as NP\nfrom matplotlib import pyplot as PLT\nfrom matplotlib import cm as CM\n\nA = NP.random.randint(10, 100, 100).reshape(10, 10)\nmask =  NP.tri(A.shape[0], k=-1)\nA = NP.ma.array(A, mask=mask) # mask out the lower triangle\nfig = PLT.figure()\nax1 = fig.add_subplot(111)\ncmap = CM.get_cmap('jet', 10) # jet doesn't have white color\ncmap.set_bad('w') # default value is 'k'\nax1.imshow(A, interpolation=\"nearest\", cmap=cmap)\nax1.grid(True)\nPLT.show()\n\n\nAPI:\nmatplotlib.colors.Colormap.set_bad\nmatplotlib.colors.Colormap.set_bad\n","label":[[241,251,"Mention"],[346,353,"Mention"],[912,946,"API"],[947,981,"API"]],"Comments":[]}
{"id":60895,"text":"ID:6117387\nPost:\nText: \"High pass filter\" is a very generic term.  There are an infinite number of different \"highpass filters\" that do very different things (e.g. an edge dectection filter, as mentioned earlier, is technically a highpass (most are actually a bandpass) filter, but has a very different effect from what you probably had in mind.)\n Text: At any rate, based on most of the questions you've been asking, you should probably look into scipy.ndimage instead of scipy.filter, especially if you're going to be working with large images (ndimage can preform operations in-place, conserving memory).  \n Text: As a basic example, showing a few different ways of doing things:\n Code: import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage\nimport Image\n\ndef plot(data, title):\n    plot.i += 1\n    plt.subplot(2,2,plot.i)\n    plt.imshow(data)\n    plt.gray()\n    plt.title(title)\nplot.i = 0\n\n# Load the data...\nim = Image.open('lena.png')\ndata = np.array(im, dtype=float)\nplot(data, 'Original')\n\n# A very simple and very narrow highpass filter\nkernel = np.array([[-1, -1, -1],\n                   [-1,  8, -1],\n                   [-1, -1, -1]])\nhighpass_3x3 = ndimage.convolve(data, kernel)\nplot(highpass_3x3, 'Simple 3x3 Highpass')\n\n# A slightly \"wider\", but sill very simple highpass filter \nkernel = np.array([[-1, -1, -1, -1, -1],\n                   [-1,  1,  2,  1, -1],\n                   [-1,  2,  4,  2, -1],\n                   [-1,  1,  2,  1, -1],\n                   [-1, -1, -1, -1, -1]])\nhighpass_5x5 = ndimage.convolve(data, kernel)\nplot(highpass_5x5, 'Simple 5x5 Highpass')\n\n# Another way of making a highpass filter is to simply subtract a lowpass\n# filtered image from the original. Here, we'll use a simple gaussian filter\n# to \"blur\" (i.e. a lowpass filter) the original.\nlowpass = ndimage.gaussian_filter(data, 3)\ngauss_highpass = data - lowpass\nplot(gauss_highpass, r'Gaussian Highpass, $\\sigma = 3 pixels$')\n\nplt.show()\n\n\nAPI:\nscipy.ndimage\nscipy.ndimage\n","label":[[547,554,"Mention"],[1986,1999,"API"]],"Comments":[]}
{"id":60896,"text":"ID:12228008\nPost:\nText: The RandomForestClassifier is copying the dataset several times in memory, especially when n_jobs is large. We are aware of those issues and it's a priority to fix them:\n Text: \nI am currently working on a subclass of the multiprocessing.Pool class of the standard library that will do no memory copy when numpy.memmap instances are passed to the subprocess workers. This will make it possible to share the memory of the source dataset + some precomputed datastructures between the workers. Once this is fixed I will close this issue on the github tracker.\nThere is an ongoing refactoring that will further decrease the memory usage of RandomForestClassifier by two. However the current state of the refactoring is twice as slow as the master, hence further work is still required.\n\n Text: However none of those fixes will make it to 0.12 release that is scheduled for release next week. Most probably they will be done for 0.13 (planned for release in 3 to 4 months) but offcourse will be available in the master branch a lot sooner.\n\nAPI:\nsklearn.ensemble.RandomForestClassifier\nmultiprocessing.Pool\nnumpy.memmap\nsklearn.ensemble.RandomForestClassifier\n","label":[[28,50,"Mention"],[660,682,"Mention"],[1065,1104,"API"],[1139,1178,"API"]],"Comments":[]}
{"id":60897,"text":"ID:12460988\nPost:\nText: I finally got to solve the problem. Two things had to be done:\n Text: \ntrain_argcands_target is a list and it has to be a numpy array. I'm surprised it worked well before when I just used the estimator directly.\nFor some reason (I don't know why, yet), it doesn't work either if I use the sparse matrix created by the DictVectorizer. I had to, \"manually\", transform each feature dictionary to a feature array with just integers representing each feature value. The transformation process is similar to the one I present in the code for the target values.\n\n Text: Thanks to everyone who tried to help!\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\n","label":[[342,356,"Mention"],[631,672,"API"]],"Comments":[]}
{"id":60898,"text":"ID:9156428\nPost:\nText: It looks like you're after element-wise power-raising?\n Text: Like a*x[i]**(b*x[i]) for each i?\n Text: In that case, you have to use the np.power function:\n Code: def func(x,a,b):\n    return a*np.power(x,b*x)\n\n Text: Then it works.\n Text: (As an aside, it may be worthwhile to convert x and y from lists to numpy arrays: np.array(x)).\n\nAPI:\nnumpy.power\nnumpy.array\n","label":[[160,168,"Mention"],[344,355,"Mention"],[364,375,"API"],[376,387,"API"]],"Comments":[]}
{"id":60899,"text":"ID:2448086\nPost:\nText: To plot interval data, you may use the error bar provided by the errorbar() function and the use axis.xaxis_date() to make matplotlib format the axis like plot_date() function does.\n Text: Here is an example:\n Code: #!\/usr\/bin\/python\n\nimport datetime\nimport numpy as np\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\n\n# dates for xaxis\nevent_date = [datetime.datetime(2008, 12, 3), datetime.datetime(2009, 1, 5), datetime.datetime(2009, 2, 3)]\n\n# base date for yaxis can be anything, since information is in the time\nanydate = datetime.date(2001,1,1)\n\n# event times\nevent_start = [datetime.time(20, 12), datetime.time(12, 15), datetime.time(8, 1,)]\nevent_finish = [datetime.time(23, 56), datetime.time(16, 5), datetime.time(18, 34)]\n\n# translate times and dates lists into matplotlib date format numpy arrays\nstart = np.fromiter((mdates.date2num(datetime.datetime.combine(anydate, event)) for event in event_start), dtype = 'float', count = len(event_start))\nfinish = np.fromiter((mdates.date2num(datetime.datetime.combine(anydate, event)) for event in event_finish), dtype = 'float', count = len(event_finish))\ndate = mdates.date2num(event_date)\n\n# calculate events durations\nduration = finish - start\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\n\n# use errorbar to represent event duration\nax.errorbar(date, start, [np.zeros(len(duration)), duration], linestyle = '')\n# make matplotlib treat both axis as times\nax.xaxis_date()\nax.yaxis_date()\n\nplt.show()\n\n\nAPI:\nmatplotlib.axes.Axes.errorbar\nmatplotlib.axes.Axes.xaxis_date\nmatplotlib.axes.Axes.plot_date\n","label":[[88,98,"Mention"],[120,137,"Mention"],[178,189,"Mention"],[1511,1540,"API"],[1541,1572,"API"],[1573,1603,"API"]],"Comments":[]}
{"id":60900,"text":"ID:12105851\nPost:\nText: ValueError: array is too big. is quite explicit: you cannot allocate a dense array datastructure of (n_samples, n_features) in memory. It's useless (and impossible in your case) to store that many zeros in a contiguous chunk of memory. Use a sparse datastructure as in the DictVectorizer documentation instead.\n Text: Also if you prefer the NLTK API you can use its scikit-learn integration instead of using scikit-learn DictVectorizer:\n Text: http:\/\/nltk.org\/_modules\/nltk\/classify\/scikitlearn.html\n Text: Have a look at the end of the file.\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\nsklearn.feature_extraction.DictVectorizer\n","label":[[297,311,"Mention"],[445,459,"Mention"],[573,614,"API"],[615,656,"API"]],"Comments":[]}
{"id":60901,"text":"ID:4523180\nPost:\nText: The easiest way to save your array including metadata (dtype, dimensions) is to use numpy.save() and numpy.load():\n Code: a = array([[False,  True, False],\n           [ True, False,  True],\n           [False,  True, False],\n           [ True, False,  True],\n           [False,  True, False]], dtype=bool)\nnumpy.save(\"data.npy\", a)\nnumpy.load(\"data.npy\")\n# array([[False,  True, False],\n#        [ True, False,  True],\n#        [False,  True, False],\n#        [ True, False,  True],\n#        [False,  True, False]], dtype=bool)\n\n Text: a.tofile() and numpy.fromfile() would work as well, but don't save any metadata.  You need to pass dtype=bool to fromfile() and will get a one-dimensional array that must be reshape()d to its original shape.\n\nAPI:\nnumpy.save\n\nnumpy.load\n\nnumpy.ndarray.tofile\n\nnumpy.fromfile\n\nnumpy.fromfile\n\nnumpy.ndarray.reshape","label":[[558,568,"Mention"],[671,681,"Mention"],[732,741,"Mention"],[796,816,"API"],[834,848,"API"],[850,871,"API"]],"Comments":[]}
{"id":60902,"text":"ID:1591185\nPost:\nText: accumulate is designed to do what you seem to want; that is, to proprigate an operation along an array.  Here's an example:\n Code: from numpy import *\n\na = array([1,0,0,0])\na[1:] = add.accumulate(a[0:3])\n# a = [1, 1, 1, 1]\n\nb = array([1,1,1,1])\nb[1:] = multiply.accumulate(2*b[0:3])\n# b = [1 2 4 8]\n\n Text: Another way to do this is to explicitly specify the result array as the input array.  Here's an example:\n Code: c = array([2,0,0,0])\nmultiply(c[:3], c[:3], c[1:])\n# c = [  2   4  16 256]\n\n\nAPI:\nnumpy.ufunc.accumulate\n","label":[[23,33,"Mention"],[524,546,"API"]],"Comments":[]}
{"id":60903,"text":"ID:15582359\nPost:\nText: I use the outer method of ufunc to calculate the result, here is the example:\n Text: First, some data:\n Code: import pandas as pd\nimport numpy as np\ndf_a = pd.DataFrame([{\"a\": 1, \"b\": 4}, {\"a\": 2, \"b\": 5}, {\"a\": 3, \"b\": 6}, {\"a\": 4, \"b\": 8}, {\"a\": 1, \"b\": 7}])\ndf_b = pd.DataFrame([{\"c\": 2, \"d\": 7}, {\"c\": 3, \"d\": 8}, {\"c\": 2, \"d\": 10}])\nprint \"df_a\"\nprint df_a\nprint \"df_b\"\nprint df_b\n\n Text: output:\n Code: df_a\n   a  b\n0  1  4\n1  2  5\n2  3  6\n3  4  8\n4  1  7\ndf_b\n   c   d\n0  2   7\n1  3   8\n2  2  10\n\n Text: Inner join, because this only calculate the cartesian product of c & a, memory useage is less than cartesian product of the whole DataFrame:\n Code: ia, ib = np.where(np.less.outer(df_a.a, df_b.c))\nprint pd.concat((df_a.take(ia).reset_index(drop=True), \n                 df_b.take(ib).reset_index(drop=True)), axis=1)\n\n Text: output:\n Code:    a  b  c   d\n0  1  4  2   7\n1  1  4  3   8\n2  1  4  2  10\n3  2  5  3   8\n4  1  7  2   7\n5  1  7  3   8\n6  1  7  2  10\n\n Text: to calculate the left outer join, use numpy.setdiff1d() to find all the rows of df_a that not in the inner join:\n Code: na = np.setdiff1d(np.arange(len(df_a)), ia)\nnb = -1 * np.ones_like(na)\noa = np.concatenate((ia, na))\nob = np.concatenate((ib, nb))\nprint pd.concat([df_a.take(oa).reset_index(drop=True), \n                 df_b.take(ob).reset_index(drop=True)], axis=1)\n\n Text: output:\n Code:    a  b   c   d\n0  1  4   2   7\n1  1  4   3   8\n2  1  4   2  10\n3  2  5   3   8\n4  1  7   2   7\n5  1  7   3   8\n6  1  7   2  10\n7  3  6 NaN NaN\n8  4  8 NaN NaN\n\n\nAPI:\nnumpy.ufunc.outer\nnumpy.ufunc\npandas.DataFrame\nnumpy.setdiff1d\n","label":[[34,39,"Mention"],[50,55,"Mention"],[665,674,"Mention"],[1564,1581,"API"],[1582,1593,"API"],[1594,1610,"API"]],"Comments":[]}
{"id":60904,"text":"ID:4406803\nPost:\nText: Since you're using NumPy, you can use numpy.hypot:\n Code: def func(points):\n    centroid = np.mean(points, axis=0)\n    return np.sum(np.hypot(points[:,0]-centroid[0], points[:,1]-centroid[1]))\n\n Text: I believe, though am not sure, that indexing using the numpy [:,n] syntax is faster than a list comprehension.  Similarly, using np.sum should be faster than using Python's sum.  \n Text: NOTE: hypot calculates the square root, thus moving the square root inside the sum.  This reflects the equation in your question, although your code does something different.\n\nAPI:\nnumpy.hypot\nnumpy.sum\nsum\nnumpy.hypot\n","label":[[353,359,"Mention"],[417,422,"Mention"],[604,613,"API"],[618,629,"API"]],"Comments":[]}
{"id":60905,"text":"ID:7205671\nPost:\nText: I think you would benefit from using the cla() axes method, which clears the axes. You can do this at the top of your script before anything is done; even before the first plot. I would use this instead of close(), because then the second time around the figure will not exist. So I would replace the figure(1) line with\n Code: figure(1)\nfigure(1).gca().cla()\n\n Text: This sets the figure to figure 1, and then gets the current axes (gca()) and then clears it with cla(). Here's a script I ran independantly of django, that worked for me and emulates what I think you are trying to do:\n Code: from pylab import *\n\n#prepare values and labels\nvalues = [34,17,29,6,14]\nlabels = [\"john\",\"jane\",\"jim\",\"jason\",\"judy\"]\n\n# create figure\nfigure(1)\nfigure(1).gca().cla()\n\n# set some parameters\nparams = { 'axes.labelsize': 6, 'text.fontsize': 6, 'font.size': 6, \n           'legend.fontsize': 6, 'xtick.labelsize': 6, \n           'ytick.labelsize': 6,}        \nrcParams.update(params)\n\n# draw, add legend and save\npie(values, labels=labels, shadow=False)\nl = legend(loc='lower center', ncol= 5, bbox_to_anchor=(0.5, -0.25))\nl.get_frame().set_alpha(0.0)\nsavefig('3.png', dpi=100, transparent=True)\n\n\n#close(1)\n\nvalues2 = [24,27,29,16,4]\nlabels2 = [\"dave\",\"donna\",\"derrick\",\"dotty\",\"drew\"]\n\nfigure(1)\nfigure(1).gca().cla()\n\n# draw, add legend and save\npie(values2, labels=labels2, shadow=False)\nl = legend(loc='lower center', ncol= 5, bbox_to_anchor=(0.5, -0.25))\nl.get_frame().set_alpha(0.0)\nsavefig('4.png', dpi=100, transparent=True)\n\n#show()\n\n Text: You probably can get away with only one line; just having this worked for me:\n Code: figure(1).gca().cla()\n\n Text: But it's a bit clearer perhaps the other way. \n\nAPI:\nmatplotlib.axes.Axes.cla\nmatplotlib.pyplot.close\nmatplotlib.figure.Figure.gca\nmatplotlib.axes.Axes.cla\n","label":[[64,69,"Mention"],[229,236,"Mention"],[457,462,"Mention"],[488,493,"Mention"],[1733,1757,"API"],[1758,1781,"API"],[1782,1810,"API"],[1811,1835,"API"]],"Comments":[]}
{"id":60906,"text":"ID:18225477\nPost:\nText: The problem is here:\n Code: X_test = vectorizer.fit_transform(sample_tweets)\n\n Text: fit_transform is intended to be called on the training set, not the test set. On the test set, call transform.\n Text: Also, sample_tweets is a filename. You should open it and read the tweets from it before passing it to a vectorizer. If you do that, then you should finally be able to do something like\n Code: for tweet, sentiment in zip(list_of_sample_tweets, y_pred):\n    print(\"Tweet: %s\" % tweet)\n    print(\"Sentiment: %s\" % sentiment)\n\n\nAPI:\nsklearn.feature_extraction.text.TfidfVectorizer.fit_transform\nsklearn.feature_extraction.text.TfidfVectorizer.transform\n","label":[[109,122,"Mention"],[209,218,"Mention"],[557,618,"API"],[619,676,"API"]],"Comments":[]}
{"id":60907,"text":"ID:9890599\nPost:\nText: When using matplotlib.pyplot.savefig, the file format can be specified by the extension:\n Code: from matplotlib import pyplot as plt\n\nplt.savefig('foo.png')\nplt.savefig('foo.pdf')\n\n Text: That gives a rasterized or vectorized output respectively.\nIn addition, there is sometimes undesirable whitespace around the image, which can be removed with:\n Code: plt.savefig('foo.png', bbox_inches='tight')\n\n Text: Note that if showing the plot, plt.show() should follow plt.savefig(); otherwise, the file image will be blank.\n\nAPI:\nmatplotlib.pyplot.savefig\nmatplotlib.pyplot.show\nmatplotlib.pyplot.savefig\n","label":[[460,470,"Mention"],[485,498,"Mention"],[573,595,"API"],[596,621,"API"]],"Comments":[]}
{"id":60908,"text":"ID:3860687\nPost:\nText: You should be able to do this with the second argument to xticks function:\n Code: xticks(arange(3), ('firmware 1', 'firmware 2', 'firmware 3'))\n\n Text: Update:  I think the following first example named \"tick label like annotations\" on the Matplotlib Transformations page of the SciPy cookbook covers the general method you're after.\n Text: Note that there was an API change, so line 4 should be:\n Code: blend = M.transforms.blended_transform_factory\n\n\nAPI:\nmatplotlib.pyplot.xticks\n","label":[[81,87,"Mention"],[481,505,"API"]],"Comments":[]}
{"id":60909,"text":"ID:9994484\nPost:\nText: As @AbhranilDas mentioned, just use a linear method. There's no need for a non-linear solver like scipy.optimize.lstsq.\n Text: Typically, you'd use numpy.polyfit to fit a line to your data, but in this case you'll need to do use numpy.linalg.lstsq directly, as you want to set the intercept to zero.\n Text: As a quick example:\n Code: import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 4.0, 6.0, 8.0, 10.0, \n              20.0, 40.0, 60.0, 80.0])\n\ny = np.array([0.50505332505407008, 1.1207373784533172, 2.1981844719020001,\n              3.1746209003398689, 4.2905482471260044, 6.2816226678076958,\n              11.073788414382639, 23.248479770546009, 32.120462301367183, \n              44.036117671229206, 54.009003143831116, 102.7077685684846, \n              185.72880217806673, 256.12183145545811, 301.97120103079675])\n\n# Our model is y = a * x, so things are quite simple, in this case...\n# x needs to be a column vector instead of a 1D vector for this, however.\nx = x[:,np.newaxis]\na, _, _, _ = np.linalg.lstsq(x, y)\n\nplt.plot(x, y, 'bo')\nplt.plot(x, a*x, 'r-')\nplt.show()\n\n\nAPI:\nscipy.linalg.lstsq\nnumpy.polyfit\nnumpy.linalg.lstsq\n","label":[[121,141,"Mention"],[1156,1174,"API"]],"Comments":[]}
{"id":60910,"text":"ID:8219171\nPost:\nText: You can find the bbox of the image inside the axis (using get_window_extent), and use the bbox_inches parameter to save only that portion of the image:\n Code: import numpy as np\nimport matplotlib.pyplot as plt\n\ndata=np.arange(9).reshape((3,3))\nfig=plt.figure()\nax=fig.add_subplot(1,1,1)\nplt.axis('off')\nplt.imshow(data)\n\nextent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\nplt.savefig('\/tmp\/test.png', bbox_inches=extent)\n\n Text: I learned this trick from Joe Kington here.\n\nAPI:\nmatplotlib.axes.Axes.get_window_extent\n","label":[[81,98,"Mention"],[527,565,"API"]],"Comments":[]}
{"id":60911,"text":"ID:15781626\nPost:\nText: read_csv's parse_dates argument should be a dictionary with values in list (not a list of lists):\n Code: date_spec = {'transdate': [0, 1, 2]}  # a list\n\ndf2 = pd.read_csv('fruit.csv', header=None, parse_dates=date_spec)\n\nIn [3]: del df2[6]  # Note in 0.11 this may not be needed\n\nIn [4]: df2\nOut[4]: \n                  foo       3      4     5\n0 2011-01-10 00:00:00   Apple    Red  1500\n1 2011-01-13 00:00:00   Apple  Green  1500\n2 2011-01-13 00:00:00  Orange    Red  4000\n3 2011-01-26 00:00:00  Banana    Red  1000\n4 2011-02-02 00:00:00    Pear  Green  4000\n5 2011-02-10 00:00:00    Pear    Red  4000\n6 2011-03-03 00:00:00  Banana  Green  1000\n7 2011-03-03 00:00:00  Orange  Green  2200\n8 2011-06-03 00:00:00  Orange  Green  3300\n\n\nAPI:\npandas.read_csv\n","label":[[24,34,"Mention"],[762,777,"API"]],"Comments":[]}
{"id":60912,"text":"ID:3433503\nPost:\nText: For fitting y = A + B log x, just fit y against (log x).\n Code: >>> x = numpy.array([1, 7, 20, 50, 79])\n>>> y = numpy.array([10, 19, 30, 35, 51])\n>>> numpy.polyfit(numpy.log(x), y, 1)\narray([ 8.46295607,  6.61867463])\n# y  8.46 log(x) + 6.62\n\n Text: For fitting y = AeBx, take the logarithm of both side gives log y = log A + Bx. So fit (log y) against x. \n Text: Note that fitting (log y) as if it is linear will emphasize small values of y, causing large deviation for large y. This is because polyfit (linear regression) works by minimizing i (Y)2 = i (Yi  i)2. When Yi = log yi, the residues Yi = (log yi)  yi \/ |yi|. So even if polyfit makes a very bad decision for large y, the \"divide-by-|y|\" factor will compensate for it, causing polyfit favors small values.\n Text: This could be alleviated by giving each entry a \"weight\" proportional to y. polyfit supports weighted-least-squares via the w keyword argument.\n Code: >>> x = numpy.array([10, 19, 30, 35, 51])\n>>> y = numpy.array([1, 7, 20, 50, 79])\n>>> numpy.polyfit(x, numpy.log(y), 1)\narray([ 0.10502711, -0.40116352])\n#    y  exp(-0.401) * exp(0.105 * x) = 0.670 * exp(0.105 * x)\n# (^ biased towards small values)\n>>> numpy.polyfit(x, numpy.log(y), 1, w=numpy.sqrt(y))\narray([ 0.06009446,  1.41648096])\n#    y  exp(1.42) * exp(0.0601 * x) = 4.12 * exp(0.0601 * x)\n# (^ not so biased)\n\n Text: Note that Excel, LibreOffice and most scientific calculators typically use the unweighted (biased) formula for the exponential regression \/ trend lines. If you want your results to be compatible with these platforms, do not include the weights even if it provides better results.\n Text: Now, if you can use scipy, you could use scipy.optimize.curve_fit to fit any model without transformations.\n Text: For y = A + B log x the result is the same as the transformation method:\n Code: >>> x = numpy.array([1, 7, 20, 50, 79])\n>>> y = numpy.array([10, 19, 30, 35, 51])\n>>> scipy.optimize.curve_fit(lambda t,a,b: a+b*numpy.log(t),  x,  y)\n(array([ 6.61867467,  8.46295606]), \n array([[ 28.15948002,  -7.89609542],\n        [ -7.89609542,   2.9857172 ]]))\n# y  6.62 + 8.46 log(x)\n\n Text: For y = AeBx, however, we can get a better fit since it computes (log y) directly. But we need to provide an initialize guess so curve_fit can reach the desired local minimum.\n Code: >>> x = numpy.array([10, 19, 30, 35, 51])\n>>> y = numpy.array([1, 7, 20, 50, 79])\n>>> scipy.optimize.curve_fit(lambda t,a,b: a*numpy.exp(b*t),  x,  y)\n(array([  5.60728326e-21,   9.99993501e-01]),\n array([[  4.14809412e-27,  -1.45078961e-08],\n        [ -1.45078961e-08,   5.07411462e+10]]))\n# oops, definitely wrong.\n>>> scipy.optimize.curve_fit(lambda t,a,b: a*numpy.exp(b*t),  x,  y,  p0=(4, 0.1))\n(array([ 4.88003249,  0.05531256]),\n array([[  1.01261314e+01,  -4.31940132e-02],\n        [ -4.31940132e-02,   1.91188656e-04]]))\n# y  4.88 exp(0.0553 x). much better.\n\n\nAPI:\nnumpy.polyfit\nnumpy.polyfit\nnumpy.polyfit\nnumpy.polyfit\nscipy.optimize.curve_fit\nscipy.optimize.curve_fit\n","label":[[520,527,"Mention"],[666,673,"Mention"],[772,779,"Mention"],[884,891,"Mention"],[2300,2309,"Mention"],[2930,2943,"API"],[2944,2957,"API"],[2958,2971,"API"],[2972,2985,"API"],[3011,3035,"API"]],"Comments":[]}
{"id":60913,"text":"ID:12769082\nPost:\nText: It seems that you compare a series of scalar values to a string:\n Code: In [73]: node = 'a'\n\nIn [74]: deco = 'b'\n\nIn [75]: data = [(10, 'a', 1), (11, 'b', 2), (12, 'c', 3)]\n\nIn [76]: df = pd.DataFrame(data)\n\nIn [77]: df\nOut[77]: \n    0  1  2\n0  10  a  1\n1  11  b  2\n2  12  c  3\n\nIn [78]: cond = ((df[1] != node) & (df[2] != deco))\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-78-0afad3702859> in <module>()\n----> 1 cond = ((df[1] != node) & (df[2] != deco))\n\n\/home\/...\/python2.7\/site-packages\/pandas\/core\/series.pyc in wrapper(self, other)\n    140             if np.isscalar(res):\n    141                 raise TypeError('Could not compare %s type with Series'\n--> 142                                 % type(other))\n    143             return Series(na_op(values, other),\n    144                           index=self.index, name=self.name)\n\nTypeError: Could not compare <type 'str'> type with Series\n\n Text: Note that pandas can handle strings and numbers in a series, but it not really makes sense to compare strings and numbers, so the error message is useful.\nHowever pandas should perhaps give a more detailed error message. \n Text: If your condition for the column 2 would be a number it would work: \n Code: In [79]: deco = 3\n\nIn [80]: cond = ((df[1] != node) & (df[2] != deco))\n\nIn [81]: df[cond]\nOut[81]: \n    0  1  2\n1  11  b  2\n\n Text: Some comments:\n Text: Maybe some of your confusion is due to a design decision in pandas: \n Text: If you read data from a file with read_csv the default column names of the resulting data frame are set to X.1 to X.N (and to X1 to XN for versions >= 0.9), which are strings.  \n Text: If you create a data frame from exiting arrays or lists or something the column names default to 0 to N and are integers.\n Code: In [23]: df = pd.read_csv(StringIO(data), header=None)\n\nIn [24]: df.columns\nOut[24]: Index([X.1, X.2, X.3], dtype=object)\n\nIn [25]: df.columns[0]\nOut[25]: 'X.1'\n\nIn [26]: type(df.columns[0])\nOut[26]: str\n\nIn [27]: df = pd.DataFrame(randn(2,3))\n\nIn [30]: df.columns\nOut[30]: Int64Index([0, 1, 2])\n\nIn [31]: df.columns[0]\nOut[31]: 0\n\nIn [32]: type(df.columns[0])\nOut[32]: numpy.int64\n\n Text: I opened a ticket to discuss this.\n Text: So your \n Code: In [60]: cond = ((df[1] != node) & (df[2] != deco))\n\n Text: should work for a dataframe created from an array or something, if the type of df[1] and df[2] is the same as the type of node and deco.\n Text: If you have read a file with read_csv than  \n Code: In [60]: cond = ((df['X.2'] != node) & (df['X.3'] != deco))\n\n Text: should work with versions < 0.9, while it should be \n Code: In [60]: cond = ((df['X2'] != node) & (df['X3'] != deco))\n\n Text: with versions >= 0.9.\n\nAPI:\npandas.read_csv\npandas.read_csv\n","label":[[1623,1631,"Mention"],[2584,2592,"Mention"],[2829,2844,"API"],[2845,2860,"API"]],"Comments":[]}
{"id":60914,"text":"ID:9382878\nPost:\nText: http:\/\/matplotlib.sourceforge.net\/api\/pyplot_api.html#matplotlib.pyplot.xticks\n Code: plt.xticks(ticks) # `ticks` is a list here!\n\n Text: Edit: as Yann mentions in a comment, you may also be interested in plt.yticks()\n Text: Result (using plt.xticks(ticks, fontsize=9)):\n\n\nAPI:\nmatplotlib.pyplot.yticks\nmatplotlib.pyplot.xticks\n","label":[[228,240,"Mention"],[262,291,"Mention"],[301,325,"API"],[326,350,"API"]],"Comments":[]}
{"id":60915,"text":"ID:1902378\nPost:\nText: It sounds like you have the interactive mode on, so you should just set it to off using the command ioff()\n\n Text: Note that when interactive mode is off, you'll need to use the command show() to display the plots.\n\nAPI:\nmatplotlib.pyplot.ioff\nmatplotlib.pyplot.show","label":[[123,129,"Mention"],[209,215,"Mention"],[244,266,"API"],[267,289,"API"]],"Comments":[]}
{"id":60916,"text":"ID:16982901\nPost:\nText: You can combine the columns of two DataFrames using concat:\n Code: pd.concat([A, B], axis=1)\n\n Text: To make the index consecutive integers you can use reset_index:\n Code: A.reset_index(inplace=True)\n\n Text: Or, alternatively you can match the index of B to that of A using:\n Code: B.index = A.index\n\n Text: What the \"best\" choice is here I think depends on the context\/the meaning of the index.\n\nAPI:\npandas.DataFrame\npandas.concat\npandas.DataFrame.reset_index\n","label":[[59,69,"Mention"],[76,82,"Mention"],[176,187,"Mention"],[426,442,"API"],[443,456,"API"],[457,485,"API"]],"Comments":[]}
{"id":60917,"text":"ID:741884\nPost:\nText: You can use figure to create a new plot, for example, or use close after the first plot.\n\nAPI:\nmatplotlib.pyplot.figure\nmatplotlib.pyplot.close\n","label":[[34,40,"Mention"],[83,88,"Mention"],[117,141,"API"],[142,165,"API"]],"Comments":[]}
{"id":60918,"text":"ID:4452499\nPost:\nText: When you called fft(wolfer), you told the transform to assume a fundamental period equal to the length of the data.  To reconstruct the data, you have to use basis functions of the same fundamental period = 2*pi\/N.  By the same token, your time index xs has to range over the time samples of the original signal.\n Text: Another mistake was in forgetting to do to the full complex multiplication.  It's easier to think of this as Y[omega]*exp(1j*n*omega\/N).\n Text: Here's the fixed code.  Note I renamed i to ctr to avoid confusion with sqrt(-1), and n to N to follow the usual signal processing convention of using the lower case for a sample, and the upper case for total sample length.  I also imported __future__ division to avoid confusion about integer division.\n Text: forgot to add earlier: Note that SciPy's fft doesn't divide by N after accumulating.  I didn't divide this out before using Y[n]; you should if you want to get back the same numbers, rather than just seeing the same shape.\n Text: And finally, note that I am summing over the full range of frequency coefficients.  When I plotted np.abs(Y), it looked like there were significant values in the upper frequencies, at least until sample 70 or so.  I figured it would be easier to understand the result by summing over the full range, seeing the correct result, then paring back coefficients and seeing what happens.\n Code: from __future__ import division\nimport numpy as np\nfrom scipy import *\nfrom matplotlib import pyplot as gplt\nfrom scipy import fftpack\n\ndef f(Y,x, N):\n    total = 0\n    for ctr in range(len(Y)):\n        total += Y[ctr] * (np.cos(x*ctr*2*np.pi\/N) + 1j*np.sin(x*ctr*2*np.pi\/N))\n    return real(total)\n\ntempdata = np.loadtxt(\"sunspots.dat\")\n\nyear=tempdata[:,0]\nwolfer=tempdata[:,1]\n\nY=fft(wolfer)\nN=len(Y)\nprint(N)\n\nxs = range(N)\ngplt.plot(xs, [f(Y, x, N) for x in xs])\ngplt.show()\n\n\nAPI:\nscipy.fft.fft\nmath.sqrt\nscipy.fft.fft\nnumpy.abs\n","label":[[39,50,"Mention"],[559,567,"Mention"],[839,842,"Mention"],[1127,1136,"Mention"],[1903,1916,"API"],[1917,1926,"API"],[1927,1940,"API"],[1941,1950,"API"]],"Comments":[]}
{"id":60919,"text":"ID:5546990\nPost:\nText: Have you turned interactive mode on using ion()?  The following works for me on OSX, using the Tk backend and running from the shell's command line:\n Code: import matplotlib.pyplot as plt\n\nplt.ion()\nplt.figure()\nfor i in range(10):\n    plt.plot([i], [i], 'o')\n    plt.draw()\nraw_input(\"done >>\")  \n\n Text: That is, as it does each loop, you see the plot change (i.e., it gets redrawn) as each point is added.  Here, btw, if I instead call plt.ioff(), I don't see the figure or any updates.\n\nAPI:\nmatplotlib.pyplot.ion\nmatplotlib.pyplot.ioff\n","label":[[65,70,"Mention"],[462,472,"Mention"],[519,540,"API"],[541,563,"API"]],"Comments":[]}
{"id":60920,"text":"ID:5902371\nPost:\nText: I know about plot_date() but is there a bar_date() out there?\n Text: The general method would be to use set_xticks and set_xticklabels, but I'd like something that can handle time scales from a few hours out to a few years (this means involving the major and minor ticks to make things readable I think).\n Text: Edit: I realized that I am plotting values associated with a specific time interval (that the bar spans). I updated below with the basic solution I used: \n Code: import matplotlib.pyplot as plt  \nimport datetime  \nt=[datetime.datetime(2010, 12, 2, 22, 0),datetime.datetime(2010, 12, 2, 23, 0),         datetime.datetime(2010, 12, 10, 0, 0),datetime.datetime(2010, 12, 10, 6, 0)]  \ny=[4,6,9,3]  \ninterval=1.0\/24.0  #1hr intervals, but maplotlib dates have base of 1 day  \nax = plt.subplot(111)  \nax.bar(t, y, width=interval)  \nax.xaxis_date()   \nplt.show()\n\n\nAPI:\nmatplotlib.pyplot.plot_date\nmatplotlib.axes.Axes.set_xticks\nmatplotlib.axes.Axes.set_xticklabels\n","label":[[36,47,"Mention"],[127,137,"Mention"],[142,157,"Mention"],[898,925,"API"],[926,957,"API"],[958,994,"API"]],"Comments":[]}
{"id":60921,"text":"ID:6339311\nPost:\nText: I'm not entirely clear on what you want, so I'm going to guess, here...\n Text: You want the \"Probability\/Percentile\" values to be a cumulative histogram?\n Text: So for a single plot, you'd have something like this? (Plotting it with markers as you've shown above, instead of the more traditional step plot...)\n Code: import scipy.stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 100 values from a normal distribution with a std of 3 and a mean of 0.5\ndata = 3.0 * np.random.randn(100) + 0.5\n\ncounts, start, dx, _ = scipy.stats.cumfreq(data, numbins=20)\nx = np.arange(counts.size) * dx + start\n\nplt.plot(x, counts, 'ro')\nplt.xlabel('Value')\nplt.ylabel('Cumulative Frequency')\n\nplt.show()\n\n Text: If that's roughly what you want for a single plot, there are multiple ways of making multiple plots on a figure.  The easiest is just to use subplots. \n Text: Here, we'll generate some datasets and plot them on different subplots with different symbols...\n Code: import itertools\nimport scipy.stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some data... (Using a list to hold it so that the datasets don't \n# have to be the same length...)\nnumdatasets = 4\nstds = np.random.randint(1, 10, size=numdatasets)\nmeans = np.random.randint(-5, 5, size=numdatasets)\nvalues = [std * np.random.randn(100) + mean for std, mean in zip(stds, means)]\n\n# Set up several subplots\nfig, axes = plt.subplots(nrows=1, ncols=numdatasets, figsize=(12,6))\n\n# Set up some colors and markers to cycle through...\ncolors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\nmarkers = itertools.cycle(['o', '^', 's', r'$\\Phi$', 'h'])\n\n# Now let's actually plot our data...\nfor ax, data, color, marker in zip(axes, values, colors, markers):\n    counts, start, dx, _ = scipy.stats.cumfreq(data, numbins=20)\n    x = np.arange(counts.size) * dx + start\n    ax.plot(x, counts, color=color, marker=marker, \n            markersize=10, linestyle='none')\n\n# Next we'll set the various labels...\naxes[0].set_ylabel('Cumulative Frequency')\nlabels = ['This', 'That', 'The Other', 'And Another']\nfor ax, label in zip(axes, labels):\n    ax.set_xlabel(label)\n\nplt.show()\n\n Text: If we want this to look like one continuous plot, we can just squeeze the subplots together and turn off some of the boundaries. Just add the following in before calling plt.show()\n Code: # Because we want this to look like a continuous plot, we need to hide the\n# boundaries (a.k.a. \"spines\") and yticks on most of the subplots\nfor ax in axes[1:]:\n    ax.spines['left'].set_color('none')\n    ax.spines['right'].set_color('none')\n    ax.yaxis.set_ticks([])\naxes[0].spines['right'].set_color('none')\n\n# To reduce clutter, let's leave off the first and last x-ticks.\nfor ax in axes:\n    xticks = ax.get_xticks()\n    ax.set_xticks(xticks[1:-1])\n\n# Now, we'll \"scrunch\" all of the subplots together, so that they look like one\nfig.subplots_adjust(wspace=0)\n\n Text: Hopefully that helps a bit, at any rate!\n Text: Edit: If you want percentile values, instead a cumulative histogram (I really shouldn't have used 100 as the sample size!), it's easy to do.\n Text: Just do something like this (using numpy.percentile instead of normalizing things by hand):\n Code: # Replacing the for loop from before...\nplot_percentiles = range(0, 110, 10)\nfor ax, data, color, marker in zip(axes, values, colors, markers):\n    x = np.percentile(data, plot_percentiles)\n    ax.plot(x, plot_percentiles, color=color, marker=marker, \n            markersize=10, linestyle='none')\n\n\nAPI:\nmatplotlib.pyplot.show\nnumpy.percentile\n","label":[[2357,2367,"Mention"],[3547,3569,"API"]],"Comments":[]}
{"id":60922,"text":"ID:9589105\nPost:\nText: Edit: \n Text: I misread your question.  I thought imsave wasn't saving your image properly, but that you were able to import it.  \n Text: scipy.misc.imsave is just a thin wrapper around PIL (the python imaging library). You need to install PIL for it to work.\n Text: \nWhat happens if you just use PIL directly? \n Text: scipy.misc.imsave dates from many years ago when Image didn't support numpy arrays. It's not officially depreciated, but it jumps through a lot of hoops that aren't necessary with recent versions of PIL.\n Text: What happens if you do this:\n Code: import numpy as np\nimport Image\n\n# It's easier to work with a MxNx3 array instead of a 3xMxN array in PIL\na = np.zeros((500, 400, 3))\na[:100, :200, 1] = 255\n\nim = Image.fromarray(a.astype(np.uint8))\nim.save('test.png')\n\n Text: Also, avoid doing from whatever import *.  It's only there for interactive use. It's a very bad idea otherwise.  (In the case of numpy, you're over-riding many python's builtin functions with the numpy versions with don't do the same thing.) Also, don't use the base scipy namespace. It's just numpy with one or two extra functions.  All of scipy is in its submodules. Import what you need directly (e.g. from scipy.misc import imsave)\n\n\nAPI:\nscipy.misc.imsave\nscipy.misc.imsave\nscipy.misc.imsave\n","label":[[73,79,"Mention"],[1259,1276,"API"]],"Comments":[]}
{"id":60923,"text":"ID:2333251\nPost:\nText: If you are trying to predict one value from the other two, then you should use lstsq with the a argument as your independent variables (plus a column of 1's to estimate an intercept) and b as your dependent variable. \n Text: If, on the other hand, you just want to get the best fitting line to the data, i.e. the line which, if you projected the data onto it, would minimize the squared distance between the real point and its projection, then what you want is the first principal component. \n Text: One way to define it is the line whose direction vector is the eigenvector of the covariance matrix corresponding to the largest eigenvalue, that passes through the mean of your data. That said, eig(cov(data)) is a really bad way to calculate it, since it does a lot of needless computation and copying and is potentially less accurate than using svd. See below:\n Code: import numpy as np\n\n# Generate some data that lies along a line\n\nx = np.mgrid[-2:5:120j]\ny = np.mgrid[1:9:120j]\nz = np.mgrid[-5:3:120j]\n\ndata = np.concatenate((x[:, np.newaxis], \n                       y[:, np.newaxis], \n                       z[:, np.newaxis]), \n                      axis=1)\n\n# Perturb with some Gaussian noise\ndata += np.random.normal(size=data.shape) * 0.4\n\n# Calculate the mean of the points, i.e. the 'center' of the cloud\ndatamean = data.mean(axis=0)\n\n# Do an SVD on the mean-centered data.\nuu, dd, vv = np.linalg.svd(data - datamean)\n\n# Now vv[0] contains the first principal component, i.e. the direction\n# vector of the 'best fit' line in the least squares sense.\n\n# Now generate some points along this best fit line, for plotting.\n\n# I use -7, 7 since the spread of the data is roughly 14\n# and we want it to have mean 0 (like the points we did\n# the svd on). Also, it's a straight line, so we only need 2 points.\nlinepts = vv[0] * np.mgrid[-7:7:2j][:, np.newaxis]\n\n# shift by the mean to get the line in the right place\nlinepts += datamean\n\n# Verify that everything looks right.\n\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d as m3d\n\nax = m3d.Axes3D(plt.figure())\nax.scatter3D(*data.T)\nax.plot3D(*linepts.T)\nplt.show()\n\n Text: Here's what it looks like: \n\nAPI:\nnumpy.linalg.lstsq\nnumpy.linalg.eig\nnumpy.linalg.svd\n","label":[[102,107,"Mention"],[718,732,"Mention"],[870,873,"Mention"],[2197,2215,"API"],[2216,2232,"API"],[2233,2249,"API"]],"Comments":[]}
{"id":60924,"text":"ID:5852522\nPost:\nText: Easy:\n Code: N = 100\na = np.eye(N)  # Diagonal Identity 100x100 array\nb = 40*a  # Multiply by a scalar\n\n Text: If you actually want a numpy matrix vs an array, you can do a = np.asmatrix(np.eye(N)) instead. But in general * is element-wise multiplication in numpy.\n\nAPI:\nnumpy.asmatrix\nnumpy.eye\n","label":[[198,209,"Mention"],[210,219,"Mention"],[294,308,"API"],[309,318,"API"]],"Comments":[]}
{"id":60925,"text":"ID:8332885\nPost:\nText: Place fig.autofmt_xdate(rotation = num) after the statement defining ax but before the call to ax.twinx() and :\n Code: import matplotlib.pyplot as plt\nimport matplotlib.dates as md\nimport datetime as dt\nimport numpy as np\n\nnp.random.seed(0)\nt=md.drange(dt.datetime(2009,10,1),\n            dt.datetime(2010,1,15),\n            dt.timedelta(days=1))\nn=len(t)\nx1 = np.cumsum(np.random.random(n) - 0.5) * 40000\nx2 = np.cumsum(np.random.random(n) - 0.5) * 0.002\n\nfig = plt.figure()\n# fig.autofmt_xdate(rotation=25) # does not work\nax1 = fig.add_subplot(1,1,1)\nfig.autofmt_xdate(rotation=25) # works\nax2 = ax1.twinx()\n# fig.autofmt_xdate(rotation=25) # does not work\nax1.plot_date(t, x1, 'r-')\nax2.plot_date(t, x2, 'g-')\nplt.show()\n\n Text: yields\n\nAPI:\nmatplotlib.figure.Figure.autofmt_xdate\nmatplotlib.axes.Axes.twinx\n","label":[[29,62,"Mention"],[118,128,"Mention"],[769,807,"API"],[808,834,"API"]],"Comments":[]}
{"id":60926,"text":"ID:14283678\nPost:\nText: The output of your method:\n Code: In [29]: merged\nOut[29]: \n         bob_id1  gender john_id1 bob_id2 mary_id1 mary_id2\nemployee                                                   \nbob            a    male      NaN       b      NaN      NaN\njohn         NaN    male        x     NaN      NaN      NaN\nmary         NaN  female      NaN     NaN        c        d\n\n Text: A solution with pandas built-in df.combine_first:\n Code: In [28]: reduce(lambda x,y: x.combine_first(y), [df1, df2, df3])\nOut[28]: \n         bob_id1 bob_id2  gender john_id1 mary_id1 mary_id2\nemployee                                                   \nbob            a       b    male      NaN      NaN      NaN\njohn         NaN     NaN    male        x      NaN      NaN\nmary         NaN     NaN  female      NaN        c        d\n\n Text: To add a suffix to the columns of each frame, I'd suggest renaming the columns before calling combine_first.\n Text: On the other hand, you may want to look into an operation like pd.concat([df1, df2, df3], keys=['d1', 'd2', 'd3'], axis=1), which produces a dataframe with MultiIndex columns.  In this case, may want to consider making gender part of the index or live with it's duplication.\n\nAPI:\npandas.DataFrame.combine_first\npandas.DataFrame.combine_first\npandas.concat\npandas.DataFrame\npandas.MultiIndex\n","label":[[424,440,"Mention"],[926,939,"Mention"],[1011,1070,"Mention"],[1089,1098,"Mention"],[1104,1114,"Mention"],[1229,1259,"API"],[1260,1290,"API"],[1291,1304,"API"],[1305,1321,"API"],[1322,1339,"API"]],"Comments":[]}
{"id":60927,"text":"ID:9518691\nPost:\nText: When the parameters fall out of the admissible range, return a wildly huge number (far from the data to be fitted). This will (hopefully) penalize this choice of parameters so much that curve_fit will settle on some other admissible set of parameters as optimal:\n Code: def logistic(x, y0, k, d, a, b):\n    if b > 0 and a > 0:\n        y = (k * pow(1 + np.exp(d - (a * b * x) ), (-1\/b) )) + y0\n    elif b >= -1 or b < 0 or a < 0:\n        y = (k * pow(1 - np.exp(d - (a * b * x) ), (-1\/b) )) + y0\n    else:\n        y = 1e10\n    return y\n\n\nAPI:\nscipy.optimize.curve_fit\n","label":[[209,218,"Mention"],[565,589,"API"]],"Comments":[]}
{"id":60928,"text":"ID:17481681\nPost:\nText: Can do that when you use the read_csv method.   Just add parameter\n Code: na_values=-99.9\n\n Text: as parameter of read_csv method.  Check full documentation\n Text: http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.io.parsers.read_csv.html\n\nAPI:\npandas.read_csv\npandas.read_csv\n","label":[[53,61,"Mention"],[138,146,"Mention"],[280,295,"API"],[296,311,"API"]],"Comments":[]}
{"id":60929,"text":"ID:15433426\nPost:\nText: I'm not the best at date manipulations, but maybe something like this:\n Code: import pandas as pd\nfrom datetime import timedelta\n\ndf = pd.read_csv(\"hourmelt.csv\", sep=r\"\\s+\")\n\ndf = pd.melt(df, id_vars=[\"Date\"])\ndf = df.rename(columns={'variable': 'hour'})\ndf['hour'] = df['hour'].apply(lambda x: int(x.lstrip('h'))-1)\n\ncombined = df.apply(lambda x: \n                    pd.to_datetime(x['Date'], dayfirst=True) + \n                    timedelta(hours=int(x['hour'])), axis=1)\n\ndf['Date'] = combined\ndel df['hour']\n\ndf = df.sort(\"Date\")\n\n Text: Some explanation follows. \n Text: Starting from\n Code: >>> import pandas as pd\n>>> from datetime import datetime, timedelta\n>>> \n>>> df = pd.read_csv(\"hourmelt.csv\", sep=r\"\\s+\")\n>>> df\n         Date  h1  h2  h3  h4  h24\n0  14.03.2013  60  50  52  49   73\n1  14.04.2013   5   6   7   8    9\n\n Text: We can use pd.melt to make the hour columns into one column with that value:\n Code: >>> df = pd.melt(df, id_vars=[\"Date\"])\n>>> df = df.rename(columns={'variable': 'hour'})\n>>> df\n         Date hour  value\n0  14.03.2013   h1     60\n1  14.04.2013   h1      5\n2  14.03.2013   h2     50\n3  14.04.2013   h2      6\n4  14.03.2013   h3     52\n5  14.04.2013   h3      7\n6  14.03.2013   h4     49\n7  14.04.2013   h4      8\n8  14.03.2013  h24     73\n9  14.04.2013  h24      9\n\n Text: Get rid of those hs:\n Code: >>> df['hour'] = df['hour'].apply(lambda x: int(x.lstrip('h'))-1)\n>>> df\n         Date  hour  value\n0  14.03.2013     0     60\n1  14.04.2013     0      5\n2  14.03.2013     1     50\n3  14.04.2013     1      6\n4  14.03.2013     2     52\n5  14.04.2013     2      7\n6  14.03.2013     3     49\n7  14.04.2013     3      8\n8  14.03.2013    23     73\n9  14.04.2013    23      9\n\n Text: Combine the two columns as a date:\n Code: >>> combined = df.apply(lambda x: pd.to_datetime(x['Date'], dayfirst=True) + timedelta(hours=int(x['hour'])), axis=1)\n>>> combined\n0    2013-03-14 00:00:00\n1    2013-04-14 00:00:00\n2    2013-03-14 01:00:00\n3    2013-04-14 01:00:00\n4    2013-03-14 02:00:00\n5    2013-04-14 02:00:00\n6    2013-03-14 03:00:00\n7    2013-04-14 03:00:00\n8    2013-03-14 23:00:00\n9    2013-04-14 23:00:00\n\n Text: Reassemble and clean up:\n Code: >>> df['Date'] = combined\n>>> del df['hour']\n>>> df = df.sort(\"Date\")\n>>> df\n                 Date  value\n0 2013-03-14 00:00:00     60\n2 2013-03-14 01:00:00     50\n4 2013-03-14 02:00:00     52\n6 2013-03-14 03:00:00     49\n8 2013-03-14 23:00:00     73\n1 2013-04-14 00:00:00      5\n3 2013-04-14 01:00:00      6\n5 2013-04-14 02:00:00      7\n7 2013-04-14 03:00:00      8\n9 2013-04-14 23:00:00      9\n\n\nAPI:\npandas.melt\n","label":[[876,883,"Mention"],[2610,2621,"API"]],"Comments":[]}
{"id":60930,"text":"ID:17709453\nPost:\nCode: df['Counts'] = df.groupby(['Color'])['Value'].transform('count')\n\n Text: For example,\n Code: In [102]: df = pd.DataFrame({'Color': 'Red Red Blue'.split(), 'Value': [100, 150, 50]})\n\nIn [103]: df\nOut[103]: \n  Color  Value\n0   Red    100\n1   Red    150\n2  Blue     50\n\nIn [104]: df['Counts'] = df.groupby(['Color'])['Value'].transform('count')\n\nIn [105]: df\nOut[105]: \n  Color  Value  Counts\n0   Red    100       2\n1   Red    150       2\n2  Blue     50       1\n\n Text: Note that transform('count') ignores NaNs. If you want to count NaNs, use transform(len). \n Text: To the anonymous editor: If you are getting an error while using transform('count') it may be due to your version of Pandas being too old. The above works with pandas version 0.15 or newer.\n\nAPI:\npandas.core.groupby.SeriesGroupBy.transform\npandas.core.groupby.SeriesGroupBy.transform\npandas.core.groupby.SeriesGroupBy.transform\n","label":[[501,519,"Mention"],[565,579,"Mention"],[654,672,"Mention"],[785,828,"API"],[829,872,"API"],[873,916,"API"]],"Comments":[]}
{"id":60931,"text":"ID:17131803\nPost:\nText: This looks like something you ought to be able to grab with a (not particularly pretty) regular expression in the sep argument of read_csv:\n Code: In [11]: pd.read_csv(file_name, sep='\\[\\'|\\'\\\"\\]|[ \\'\\\"]*', header=None)\nOut[11]:\n    0     1      2       3        4          5        6   7\n0 NaN  NAME  RA_I1  DEC_I1  Mean_I1  Median_I1  Mode_I1 NaN\n\n Text: You can play about with the regular expression til it truly fits your needs.\n Text: To explain this one:\n Code: sep = ('\\[\\'  # each line startswith ['  (the | means or)\n       '|\\'\\\"\\]'  # endswith '\"] (at least the one I had)\n       '|[ \\'\\\"]+')  # this is the actual delimiter, the + means at least one, so it's a string of \", ' and space in any order.\n\n Text: You can see this hack has left a NaN column at either end. The main reason this is pretty awful is because of the inconsistency of your \"csv\", I would definitely recommend cleaning it up, of course, one way to do that is just to use pandas and then to_csv. If it's generated by someone else... complain (!).\n\nAPI:\npandas.read_csv\npandas.DataFrame.to_csv\n","label":[[154,162,"Mention"],[994,1000,"Mention"],[1059,1074,"API"],[1075,1098,"API"]],"Comments":[]}
{"id":60932,"text":"ID:2314719\nPost:\nText: Here's a variant that might be faster for few indices.  It builds a string of only the desired values so that np.fromstring does less work.\n Code: def get_pos_nextvalues_fewindices(pos_file, indices):\n    result = ''\n    for line in pos_file:\n        if len(line) > 1:\n            s = line.split()\n            for i in indices:\n                result += s[i] + ' '\n        else:\n            return np.array([])\n    result = np.fromstring(result, dtype=float, sep=' ')\n    result = result.reshape(result.size\/len(indeces), len(indeces))\n    return result\n\n Text: This trades off the overhead of split() and an added loop for less parsing.  Or perhaps there's some clever regex trick you can do to extract the desired substrings directly?\n Text: Old Answer\n Text: np.mat('1.23 2.34 3.45 6\\n1.32 2.43 7 3.54') converts the string to a numpy matrix of floating point values.  This might be a faster kernel for you to use.  For instance:\n Code: import numpy as np\ndef ReadFileChunk(pos_file):\n    chunktxt = \"\"\n    for line in pos_file:\n        if len(line) > 1:\n            chunktxt = chunktxt + line\n        else:\n            break\n\n    return np.mat(chunktxt).tolist()\n    # or alternatively\n    #return np.array(np.mat(s))\n\n Text: Then you can move your indexing stuff to another function.  Hopefully having numpy parse the string internally is faster than calling float() repetitively. \n\nAPI:\nnumpy.fromstring\nstr.split\nnumpy.mat\nfloat\n","label":[[133,146,"Mention"],[617,624,"Mention"],[785,791,"Mention"],[1416,1432,"API"],[1433,1442,"API"],[1443,1452,"API"]],"Comments":[]}
{"id":60933,"text":"ID:801457\nPost:\nText: Scipy has a bunch of distributions defined in the scipy.stats package\n Code: import scipy.stats\n\ndef LogNormDist(prob, mean=0, stddev=1):\n    return scipy.stats.lognorm.cdf(prob,stddev,mean)\n\n Text: Update\n Text: Okay, it looks like Scipy's stat definitions are a little nonstandard. Here's the end of the docstring for scipy.stats.lognormal\n Text: \nLognormal distribution\nlognorm.pdf(x,s) = 1\/(sxsqrt(2*pi)) * exp(-1\/2*(log(x)\/s)**2)\n     for x > 0, s > 0.\nIf log x is normally distributed with mean mu and variance sigma**2,\n     then x is log-normally distributed with shape paramter sigma and scale\n     parameter exp(mu).\n\n Text: So maybe try\n Code: return scipy.stats.lognorm.cdf(prob,stddev,scipy.exp(mean))\n\n Text: If that still doesn't work, try getting a few sample points and I'll see if I can find a working relationship.\n Text: Udpate 2\n Text: Oops, I didn't realize that the scale param is a keyword. This one should now work:\n Code: import scipy.stats\n\ndef LogNormDist(prob, mean=0, stddev=1):\n    return scipy.stats.lognorm.cdf(prob,stddev,scale=scipy.exp(mean))\n\n Text: Cheers and good luck with your project!\n\nAPI:\nscipy.stats\nscipy.stats.lognorm\n","label":[[342,363,"Mention"],[1167,1186,"API"]],"Comments":[]}
{"id":60934,"text":"ID:15097561\nPost:\nText: numpy has the functions loadtxt and genfromtxt, but neither is particularly fast.  One of the fastest text readers available in a widely distributed library is the read_csv function in pandas (http:\/\/pandas.pydata.org\/).  On my computer, reading 5 million lines containing two integers per line takes about 46 seconds with numpy.loadtxt, 26 seconds with numpy.genfromtxt, and a little over 1 second with pandas.read_csv.\n Text: Here's the session showing the result.  (This is on Linux, Ubuntu 12.04 64 bit. You can't see it here, but after each reading of the file, the disk cache was cleared by running sync; echo 3 > \/proc\/sys\/vm\/drop_caches in a separate shell.)\n Code: In [1]: import pandas as pd\n\nIn [2]: %timeit -n1 -r1 loadtxt('junk.dat')\n1 loops, best of 1: 46.4 s per loop\n\nIn [3]: %timeit -n1 -r1 genfromtxt('junk.dat')\n1 loops, best of 1: 26 s per loop\n\nIn [4]: %timeit -n1 -r1 pd.read_csv('junk.dat', sep=' ', header=None)\n1 loops, best of 1: 1.12 s per loop\n\n\nAPI:\nnumpy.loadtxt\nnumpy.genfromtxt\npandas.read_csv\nnumpy.loadtxt\nnumpy.genfromtxt\npandas.read_csv\n","label":[[48,55,"Mention"],[60,70,"Mention"],[188,196,"Mention"],[1003,1016,"API"],[1017,1033,"API"],[1034,1049,"API"]],"Comments":[]}
{"id":60935,"text":"ID:5506711\nPost:\nText: matplotlib has animated plots that allow the data to be updated while the plot is displayed:  take a look at this page.\n Text: Your code might look like this:\n Code: import serial\nimport numpy as np\nimport time\nfrom matplotlib import pyplot as plt\n\nplt.ion() # set plot to animated\n\ntagnr=2#Tag number of the sensor that we're pinging  \nsamplingRate=.1#Sampling Rate in seconds  \ngraphbuf=50.#Buffer length in samples of logger graph  \n\n!#Initialize logger graph  \ngdists=np.zeros(graphbuf)  \nax1=plt.axes()  \n\n# make plot\nline, = plt.plot(gdists)\n\n!#Main loop  \nnsr=time.time()#Next sample request  \ntry:  \n    while True:  \n        statreq(tagnr)#Send status request to sensor over serial port  \n        temp,dist=statread(tagnr)#Read reply from sensor over serial port  \n        gdists=np.concatenate((gdists[1:],np.array([dist])))  \n        print gdists\n\n        line.set_ydata(gdists)  # update the data\n        plt.draw() # update the plot\n\n        nsr=nsr+samplingRate  \n        while time.time()<nsr:  \n            pass  \n\nfinally:  \n    ser.close()#Close serial port  \n    print 'Serial port closed.'  \n\n Text: Just some advice (possibly bad):  I would personally use time.sleep in a way that frees up some processor without losing accuracy.  I would also put some error type on your try\/except block.  And I think np.roll is better\/faster than concatenate.\n\nAPI:\ntime.sleep\nnumpy.roll\nnumpy.concatenate\n","label":[[1347,1354,"Mention"],[1377,1388,"Mention"],[1407,1417,"API"],[1418,1435,"API"]],"Comments":[]}
{"id":60936,"text":"ID:3066821\nPost:\nText: Update:  (almost) completely vectorized version below in \"new_function2\"... \n Text: I'll add comments to explain things in a bit.  \n Text: It gives a ~50x speedup, and a larger speedup is possible if you're okay with the output being numpy arrays instead of lists.  As is:\n Code: In [86]: %timeit new_function2(close, volume, INTERVAL_LENGTH)\n1 loops, best of 3: 1.15 s per loop\n\n Text: You can replace your inner loop with a call to np.cumsum()... See my \"new_function\" function below. This gives a considerable speedup...\n Code: In [61]: %timeit new_function(close, volume, INTERVAL_LENGTH)\n1 loops, best of 3: 15.7 s per loop\n\n Text: vs\n Code: In [62]: %timeit old_function(close, volume, INTERVAL_LENGTH)\n1 loops, best of 3: 53.1 s per loop\n\n Text: It should be possible to vectorize the entire thing and avoid for loops entirely, though... Give me an minute, and I'll see what I can do...\n Code: import numpy as np\n\nARRAY_LENGTH = 500000\nINTERVAL_LENGTH = 15\nclose = np.arange(ARRAY_LENGTH, dtype=np.float)\nvolume = np.arange(ARRAY_LENGTH, dtype=np.float)\n\ndef old_function(close, volume, INTERVAL_LENGTH):\n    results = []\n    for i in xrange(len(close) - INTERVAL_LENGTH):\n        for j in xrange(i+1, i+INTERVAL_LENGTH):\n            ret = close[j] \/ close[i]\n            vol = sum( volume[i+1:j+1] )\n            if (ret > 1.0001) and (ret < 1.5) and (vol > 100):\n                results.append( (i, j, ret, vol) )\n    return results\n\n\ndef new_function(close, volume, INTERVAL_LENGTH):\n    results = []\n    for i in xrange(close.size - INTERVAL_LENGTH):\n        vol = volume[i+1:i+INTERVAL_LENGTH].cumsum()\n        ret = close[i+1:i+INTERVAL_LENGTH] \/ close[i]\n\n        filter = (ret > 1.0001) & (ret < 1.5) & (vol > 100)\n        j = np.arange(i+1, i+INTERVAL_LENGTH)[filter]\n\n        tmp_results = zip(j.size * [i], j, ret[filter], vol[filter])\n        results.extend(tmp_results)\n    return results\n\ndef new_function2(close, volume, INTERVAL_LENGTH):\n    vol, ret = [], []\n    I, J = [], []\n    for k in xrange(1, INTERVAL_LENGTH):\n        start = k\n        end = volume.size - INTERVAL_LENGTH + k\n        vol.append(volume[start:end])\n        ret.append(close[start:end])\n        J.append(np.arange(start, end))\n        I.append(np.arange(volume.size - INTERVAL_LENGTH))\n\n    vol = np.vstack(vol)\n    ret = np.vstack(ret)\n    J = np.vstack(J)\n    I = np.vstack(I)\n\n    vol = vol.cumsum(axis=0)\n    ret = ret \/ close[:-INTERVAL_LENGTH]\n\n    filter = (ret > 1.0001) & (ret < 1.5) & (vol > 100)\n\n    vol = vol[filter]\n    ret = ret[filter]\n    I = I[filter]\n    J = J[filter]\n\n    output = zip(I.flat,J.flat,ret.flat,vol.flat)\n    return output\n\nresults = old_function(close, volume, INTERVAL_LENGTH)\nresults2 = new_function(close, volume, INTERVAL_LENGTH)\nresults3 = new_function(close, volume, INTERVAL_LENGTH)\n\n# Using sets to compare, as the output \n# is in a different order than the original function\nprint set(results) == set(results2)\nprint set(results) == set(results3)\n\n\nAPI:\nnumpy.cumsum\n","label":[[457,468,"Mention"],[3016,3028,"API"]],"Comments":[]}
{"id":60937,"text":"ID:1031510\nPost:\nText: The UnivariateSpline class in scipy makes doing splines much more pythonic.\n Code: x = [70, 80, 90, 100, 110]\ny = [49.7, 80.6, 122.5, 153.8, 163.0]\nf = interpolate.UnivariateSpline(x, y, s=0)\nxnew = np.arange(70,111,1)\n\nplt.plot(x,y,'x',xnew,f(xnew))\n\n Text: To find x at y then do:\n Code: yToFind = 140\nyreduced = np.array(y) - yToFind\nfreduced = interpolate.UnivariateSpline(x, yreduced, s=0)\nfreduced.roots()\n\n Text: I thought interpolating x in terms of y might work but it takes a somewhat different route. It might be closer with more points.\n\nAPI:\nscipy.interpolate.UnivariateSpline\n","label":[[27,43,"Mention"],[578,612,"API"]],"Comments":[]}
{"id":60938,"text":"ID:4821690\nPost:\nText: The short, official answer is you're not supposed to do this, though the docs don't really explain why not.\n Text: If you're using a threaded server, I'd be concerned about a race condition.  This should be quite simple to test; just put a call to sleep() in one view, then return an HttpResponse with the name of the current site. While the first view is sleeping, hit a different view on a different domain.\n Text: If you use prefork, I don't imagine that this would cause any problems.  I've used this approach with matplotlib, since it's easiest to set graph properties by changing global configuration with matplotlib.rcParams.update().  I use prefork fcgi, so I can safely assume that each request has the whole process to itself (folks, please correct me if I'm wrong).\n Text: Edit: I think you can do what you want using RequestSite by disabling the sites application. James Bennett's django-registration, for example, instantiates a RequestSite object in that case, which extracts the hostname from the request object.\n\nAPI:\ntime.sleep\nmatplotlib.rcParams.update\n","label":[[271,278,"Mention"],[1057,1067,"API"]],"Comments":[]}
{"id":60939,"text":"ID:8267963\nPost:\nText: Try adding the kw argrument to plt.contourf(...) call: either lw=0 or ls=None. \nhttp:\/\/matplotlib.sourceforge.net\/api\/pyplot_api.html#matplotlib.pyplot.contourf\n\nAPI:\nmatplotlib.pyplot.contourf\n","label":[[54,66,"Mention"],[190,216,"API"]],"Comments":[]}
{"id":60940,"text":"ID:5323902\nPost:\nText: You may find triu_indices useful. Like,\n Code: In []: ti= triu_indices(5, 1)\nIn []: r, c= ti[0][5], ti[1][5]\nIn []: r, c\nOut[]: (1, 3)\n\n Text: Just notice that indices starts from 0. You may adjust it as you like, for example:\n Code: In []: def f(n, c):\n   ..:     n= ceil(sqrt(2* n))\n   ..:     ti= triu_indices(n, 1)\n   ..:     return ti[0][c]+ 1, ti[1][c]+ 1\n   ..:\nIn []: f(len(c), 5)\nOut[]: (2, 4)\n\n\nAPI:\nnumpy.triu_indices\n","label":[[36,48,"Mention"],[433,451,"API"]],"Comments":[]}
{"id":60941,"text":"ID:13786327\nPost:\nText: Here's a couple of suggestions:\n Text: Use date_range for the index:\n Code: import datetime\nimport pandas as pd\nimport numpy as np\n\ntodays_date = datetime.datetime.now().date()\nindex = pd.date_range(todays_date-datetime.timedelta(10), periods=10, freq='D')\n\ncolumns = ['A','B', 'C']\n\n Text: Note: we could create an empty DataFrame (with NaNs) simply by writing:\n Code: df_ = pd.DataFrame(index=index, columns=columns)\ndf_ = df_.fillna(0) # With 0s rather than NaNs\n\n Text: To do these type of calculations for the data, use a NumPy array:\n Code: data = np.array([np.arange(10)]*3).T\n\n Text: Hence we can create the DataFrame:\n Code: In [10]: df = pd.DataFrame(data, index=index, columns=columns)\n\nIn [11]: df\nOut[11]:\n            A  B  C\n2012-11-29  0  0  0\n2012-11-30  1  1  1\n2012-12-01  2  2  2\n2012-12-02  3  3  3\n2012-12-03  4  4  4\n2012-12-04  5  5  5\n2012-12-05  6  6  6\n2012-12-06  7  7  7\n2012-12-07  8  8  8\n2012-12-08  9  9  9\n\n\nAPI:\npandas.date_range\n","label":[[67,77,"Mention"],[970,987,"API"]],"Comments":[]}
{"id":60942,"text":"ID:12219964\nPost:\nText: I'm getting errors running your code. However, to convert a pandas Series to a numpy array, use the pandas.Series.values  method. Wes's documentation is very well done. Spend some time reviewing...\n\nAPI:\npandas.Series\npandas.Series.values\n","label":[[91,97,"Mention"],[228,241,"API"]],"Comments":[]}
{"id":60943,"text":"ID:13261966\nPost:\nCode: concatenated = pd.concat([bb, cc])\n\nconcatenated\n\n                      0         1\nclass  sample                    \n2      22      0.730631  0.656266\n       33      0.871282  0.942768\n3      44      0.081831  0.714360\n       55      0.600095  0.770108\n5      66      0.128320  0.970817\n       66      0.160488  0.969077\n       77      0.919263  0.008597\n6      77      0.811914  0.123960\n       88      0.639887  0.262943\n       88      0.312303  0.660786\n\n Text: Answer To Your Edited Question\n Text: So to answer your edited question, the problem lies with your column names having duplicates.\n Code:  cols = ['d1']*2 + ['d2']*2  # <-- this creates ['d1', 'd1', 'd2', 'd2']\n\n Text: and your dataframes end up having what-is-considered duplicated columns, i.e.\n Code: In [62]: aa\nOut[62]: \n                 d1        d1        d2        d2\nclass idx                                        \n0     00   0.805445  0.442059  0.296162  0.041271\n      11   0.384600  0.723297  0.997918  0.006661\n1     22   0.685997  0.794470  0.541922  0.326008\n      33   0.117422  0.667745  0.662031  0.634429\n\n Text: and \n Code: In [64]: bb\nOut[64]: \n                 d1        d1        d2        d2\nclass idx                                        \n2     44   0.465559  0.496039  0.044766  0.649145\n      55   0.560626  0.684286  0.929473  0.607542\n3     66   0.526605  0.836667  0.608098  0.159471\n      77   0.216756  0.749625  0.096782  0.547273\n4     88   0.619338  0.032676  0.218736  0.684045\n      99   0.987934  0.349520  0.346036  0.926373\n\n Text: pandas.append() (or concat() method) can only append correctly if you have unique column names.\n Text: Try this and you will not get any error:-\n Code: cols2 = ['d1', 'd2', 'd3', 'd4']\n\ncc = pandas.DataFrame(x1, index=y1, columns=cols2)\ncc.index.names = names\n\ndd = pandas.DataFrame(x2, index=y2, columns=cols2)\ncc.index.names = names\n\n Text: Now...\n Code: In [70]: cc.append(dd)\nOut[70]: \n                 d1        d2        d3        d4\nclass idx                                        \n0     00   0.805445  0.442059  0.296162  0.041271\n      11   0.384600  0.723297  0.997918  0.006661\n1     22   0.685997  0.794470  0.541922  0.326008\n      33   0.117422  0.667745  0.662031  0.634429\n2     44   0.465559  0.496039  0.044766  0.649145\n      55   0.560626  0.684286  0.929473  0.607542\n3     66   0.526605  0.836667  0.608098  0.159471\n      77   0.216756  0.749625  0.096782  0.547273\n4     88   0.619338  0.032676  0.218736  0.684045\n      99   0.987934  0.349520  0.346036  0.926373\n\n\nAPI:\npandas.DataFrame.append\npandas.concat\n","label":[[1567,1582,"Mention"],[1587,1595,"Mention"],[2564,2587,"API"],[2588,2601,"API"]],"Comments":[]}
{"id":60944,"text":"ID:4328608\nPost:\nText: While @Eli is quite correct that there usually isn't much of a need to do it, it is possible.  savefig takes a bbox_inches argument that can be used to selectively save only a portion of a figure to an image.\n Text: Here's a quick example:\n Code: import matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\n\n# Make an example plot with two subplots...\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax1.plot(range(10), 'b-')\n\nax2 = fig.add_subplot(2,1,2)\nax2.plot(range(20), 'r^')\n\n# Save the full figure...\nfig.savefig('full_figure.png')\n\n# Save just the portion _inside_ the second axis's boundaries\nextent = ax2.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\nfig.savefig('ax2_figure.png', bbox_inches=extent)\n\n# Pad the saved area by 10% in the x-direction and 20% in the y-direction\nfig.savefig('ax2_figure_expanded.png', bbox_inches=extent.expanded(1.1, 1.2))\n\n Text: The full figure:\n\n Text: Area inside the second subplot:\n\n Text: Area around the second subplot padded by 10% in the x-direction and 20% in the y-direction:\n\n\nAPI:\nmatplotlib.figure.Figure.savefig\nmatplotlib.figure.Figure.savefig.bbox_inches\n","label":[[118,125,"Mention"],[134,145,"Mention"],[1094,1126,"API"],[1127,1171,"API"]],"Comments":[]}
{"id":60945,"text":"ID:13516794\nPost:\nText: The NLTK wrapper for scikit-learn doesn't know about multilabel classification, and it shouldn't because it doesn't implement MultiClassifierI. Implementing that would require a separate class.\n Text: You can either implement the missing functionality, or use scikit-learn without the wrapper. Newer versions of scikit-learn have a DictVectorizer that accepts roughly the same inputs that the NLTK wrapper accepts:\n Code: from sklearn.feature_extraction import DictVectorizer\n\nX_train_raw = [{'a': 1}, {'b': 1}, {'c': 1}]\ny_train = [('first',), ('second',), ('first', 'second')]\n\nv = DictVectorizer()\nX_train = v.fit_transform(X_train_raw)\n\nclf = OneVsRestClassifier(MultinomialNB())\nclf.fit(X_train, y_train)\n\n Text: You can then use X_test = v.transform(X_test_raw) to transform test samples to matrices. A sklearn.pipeline.Pipeline makes this easier by tying a vectorizer and a classifier together in a single object.\n Text: Disclaimer: according to the FAQ, I should disclose my affiliation. I wrote both DictVectorizer and the NLTK wrapper for scikit-learn.\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\nsklearn.feature_extraction.DictVectorizer.transform\nsklearn.pipeline.Pipeline\nsklearn.feature_extraction.DictVectorizer\n","label":[[356,370,"Mention"],[768,791,"Mention"],[1033,1047,"Mention"],[1093,1134,"API"],[1135,1186,"API"],[1213,1254,"API"]],"Comments":[]}
{"id":60946,"text":"ID:4159629\nPost:\nText: Have a look at numpy. It can read the three files into three arrays (using fromfile), calculate the average and export it to a text file (using tofile).\n Code: import numpy as np\n\n\na = np.fromfile('A.csv', dtype=np.int)   \nb = np.fromfile('B.csv', dtype=np.int)   \nc = np.fromfile('C.csv', dtype=np.int)   \n\nd = (a + b + c) \/ 3.0\n\nd.tofile('D.csv')\n\n Text: Size of \"some MB\" should not be a problem.\n\nAPI:\nnumpy.fromfile\nnumpy.ndarray.tofile\n","label":[[98,106,"Mention"],[167,173,"Mention"],[429,443,"API"],[444,464,"API"]],"Comments":[]}
{"id":60947,"text":"ID:445865\nPost:\nText: The matplotlib.cbook module contains a class CallbackRegistry that you might want to have a look at. From the documentation:\n Text: \nHandle registering and disconnecting for a set of signals and\ncallbacks:\n\n   signals = 'eat', 'drink', 'be merry'\n\n   def oneat(x):\n       print 'eat', x\n\n   def ondrink(x):\n       print 'drink', x\n\n   callbacks = CallbackRegistry(signals)\n\n   ideat = callbacks.connect('eat', oneat)\n   iddrink = callbacks.connect('drink', ondrink)\n\n   #tmp = callbacks.connect('drunk', ondrink) # this will raise a ValueError\n\n   callbacks.process('drink', 123)    # will call oneat\n   callbacks.process('eat', 456)      # will call ondrink\n   callbacks.process('be merry', 456) # nothing will be called\n   callbacks.disconnect(ideat)        # disconnect oneat\n   callbacks.process('eat', 456)      # nothing will be called\n\n Text: You probably do not want a dependency to the matplotlib package. I suggest you simply copy-paste the class into your own module from the source code.\n\nAPI:\nmatplotlib.cbook\nmatplotlib.cbook.CallbackRegistry\n","label":[[67,83,"Mention"],[1045,1078,"API"]],"Comments":[]}
{"id":60948,"text":"ID:21362481\nPost:\nText: You should use the same DictVectorizer object which created the train dataset to transform the test_set:\n Code: from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction import DictVectorizer\n\ntraining_set = [\n  {'p1':'A', 'p2':'T', 'p3':'G', 'p4':'C', 'p5':'T', \n   'p6':'A', 'p7':'C', 'p8':'T', 'p9':'G', 'p10':'A', \n   'mass':370.2, 'temp':70.0},\n  {'p1':'A', 'p2':'C', 'p3':'G', 'p4':'T', 'p5':'A', \n   'p6':'C', 'p7':'T', 'p8':'G', 'p9':'A', 'p10':'T', \n   'mass':400.3, 'temp':67.2},\n]\n\ntarget = [1, 0]\n\nvec = DictVectorizer()\ntrain = vec.fit_transform(training_set).toarray()\n\nclf = RandomForestClassifier(n_estimators=1000)\nclf = clf.fit(train, target)\n\n\n# The following part fails.\ntest_set =   {\n  'p1':'A', 'p2':'T', 'p3':'G', 'p4':'C', 'p5':'T', \n  'p6':'A', 'p7':'C', 'p8':'T', 'p9':'G', 'p10':'A', \n  'mass':370.2, 'temp':70.0}\n\ntest = vec.transform(test_set).toarray()\nprint clf.predict_proba(test)\n\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\n","label":[[48,62,"Mention"],[971,1012,"API"]],"Comments":[]}
{"id":60949,"text":"ID:14822400\nPost:\nText: I think you're best bet is to remove these special values when reading in (that way your dtype will be correct, float64).\nTo do this read_csv (and most similar functions) have an na_values argument:\n Text: \nna_values : list-like or dict, default None\n  Additional strings to recognize as NA\/NaN. If dict passed, specific per-column NA values\n\n Text: Note: I've copied ts[55:63] and used read_clipboard (which also takes this argument).\n Code: In [1]: pd.read_clipboard(sep='\\s+', header=None, na_values='<0.1')\nOut[1]: \n            0         1\n0  2006-08-07  0.880910\n1  2006-08-21  1.027876\n2  2007-07-10  0.598284\n3  2007-07-26  0.800000\n4  2007-08-09       NaN\n5  2007-08-23  1.013378\n6  2008-07-14  0.656889\n7  2008-07-29  0.696662\n\n Text: Into a (Time)Series you could use:\n Code: ts = pd.read_clipboard(sep='\\s+', header=None, na_values='<0.1',\n                       index_col=['date'], squeeze=True, names=['date', 'P04'],\n                       parse_dates=['date'])\n\nIn [3]: ts\nOut[3]: \ndate\n2006-08-07    0.880910\n2006-08-21    1.027876\n2007-07-10    0.598284\n2007-07-26    0.800000\n2007-08-09         NaN\n2007-08-23    1.013378\n2008-07-14    0.656889\n2008-07-29    0.696662\nName: P04\n\n Text: This seems a much cleaner way than using:\n Code: ts[ts.str.contains('<0.1')] = np.nan\n\n\nAPI:\npandas.read_csv\npandas.read_clipboard\n","label":[[157,165,"Mention"],[415,429,"Mention"],[1325,1340,"API"],[1341,1362,"API"]],"Comments":[]}
{"id":60950,"text":"ID:6243689\nPost:\nText: The output of loadtxt() is unfortunately a bit inconistent:  If there is only one line in your file, x and y will be scalars, but for more than one line, they will be arrays.  The Python built-in max() only works for iterables, so it only works in the latter case.\n Text: Using the Python built-in max() function instead of numpy.max() is inefficient for NumPy arrays anyway.  So a solution is to use print x.max()\n Text: or print numpy.max(x)\n Text: in the second line.\n\nAPI:\nnumpy.loadtxt\nmax\nmax\nnumpy.max\nnumpy.ndarray.max\nnumpy.max","label":[[37,46,"Mention"],[430,437,"Mention"],[500,513,"API"],[532,549,"API"]],"Comments":[]}
{"id":60951,"text":"ID:20743758\nPost:\nText: This is caused by the default token_pattern for CountVectorizer, which removes tokens of a single character:\n Code: >>> vectorizer_train\nCountVectorizer(analyzer=u'word', binary=False, charset=None,\n        charset_error=None, decode_error=u'strict',\n        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n        tokenizer=None, vocabulary=None)\n>>> pattern = re.compile(vectorizer_train.token_pattern, re.UNICODE)\n>>> print(pattern.match(\"I\"))\nNone\n\n Text: To retain \"I\", use a different pattern, e.g.\n Code: >>> vectorizer_train = CountVectorizer(min_df=0, token_pattern=r\"\\b\\w+\\b\")\n>>> vectorizer_train.fit(x_train)\nCountVectorizer(analyzer=u'word', binary=False, charset=None,\n        charset_error=None, decode_error=u'strict',\n        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n        strip_accents=None, token_pattern='\\\\b\\\\w+\\\\b', tokenizer=None,\n        vocabulary=None)\n>>> vectorizer_train.get_feature_names()\n[u'a', u'am', u'hacker', u'i', u'like', u'nigerian', u'puppies']\n\n Text: Note that the non-informative word \"a\" is now also retained.\n\nAPI:\nsklearn.feature_extraction.text.CountVectorizer\n","label":[[72,87,"Mention"],[1449,1496,"API"]],"Comments":[]}
{"id":60952,"text":"ID:6568248\nPost:\nText: \nUse FixedLocator to statically define explicit tick locations.\nColorbar cbar will have an .ax attribute that will provide access to the usual axis methods including tick formatting. This is not a reference to an axes (e.g. ax1, ax2, etc.).\n\n Code: import numpy as np\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nx = np.arange(10,3000,100)\ny = np.arange(10,3000,100)\nX,Y = np.meshgrid(x,y)\nZ = np.random.random(X.shape)*8000000\nsurf = ax.contourf(X,Y,Z, 8, cmap=plt.cm.jet)\nax.set_ylabel('Log Frequency (Hz)')\nax.set_xlabel('Log Frequency (Hz)')\nax.set_xscale('log')\nax.set_yscale('log')\nax.xaxis.set_minor_formatter(plt.FormatStrFormatter('%d'))\n# defining custom minor tick locations:\nax.xaxis.set_minor_locator(plt.FixedLocator([50,500,2000]))\nax.yaxis.set_ticks_position('left')\nax.xaxis.set_ticks_position('bottom')\nax.tick_params(axis='both',reset=False,which='both',length=8,width=2)\ncbar = fig.colorbar(surf, shrink=0.5, aspect=20, fraction=.12,pad=.02)\ncbar.set_label('Activation',size=18)\n# access to cbar tick labels:\ncbar.ax.tick_params(labelsize=5) \nplt.show()\n\n Text: Edit\n Text: If you want the tick marls, but you want to selectively show the labels, I see nothing wrong with your iteration, except I might use set_visible instead of making the fontsize zero.\n Text: You might enjoy finer control using a FuncFormatter where you can use the value or position of the tick to decide whether it gets shown:\n Code: def show_only_some(x, pos):\n    s = str(int(x))\n    if s[0] in ('2','5'):\n        return s\n    return ''\n\nax.xaxis.set_minor_formatter(plt.FuncFormatter(show_only_some))\n\n\nAPI:\nmatplotlib.ticker.FixedLocator\nmatplotlib.colorbar.Colorbar\nmatplotlib.text.Text.set_visible\nmatplotlib.ticker.FuncFormatter\n","label":[[28,40,"Mention"],[87,95,"Mention"],[1283,1294,"Mention"],[1377,1390,"Mention"],[1660,1690,"API"],[1691,1719,"API"],[1720,1752,"API"],[1753,1784,"API"]],"Comments":[]}
{"id":60953,"text":"ID:20319282\nPost:\nText: Neither of the other answers quite answered the question - 1 was in Cython, one was slower. But both provided very useful hints. Following up on them suggests that scipy.spatial.distance.pdist is the way to go.\n Text: Here's some code:\n Code: import numpy as np\nimport random\nimport sklearn.metrics.pairwise\nimport scipy.spatial.distance\n\nr = np.array([random.randrange(1, 1000) for _ in range(0, 1000)])\nc = r[:, None]\n\ndef option1(r):\n    dists = np.abs(r - r[:, None])\n\ndef option2(r):\n    dists = scipy.spatial.distance.pdist(r, 'cityblock')\n\ndef option3(r):\n    dists = sklearn.metrics.pairwise.manhattan_distances(r)\n\n Text: Timing with IPython:\n Code: In [36]: timeit option1(r)\n100 loops, best of 3: 5.31 ms per loop\n\nIn [37]: timeit option2(c)\n1000 loops, best of 3: 1.84 ms per loop\n\nIn [38]: timeit option3(c)\n100 loops, best of 3: 11.5 ms per loop\n\n Text: I didn't try the Cython implementation (I can't use it for this project), but comparing my results to the other answer that did, it looks like scipy.spatial.distance.pdist is roughly a third slower than the Cython implementation (taking into account the different machines by benchmarking on the np.abs solution).\n\nAPI:\nscipy.spatial.distance.pdist\nscipy.spatial.distance.pdist\nnumpy.abs\n","label":[[1188,1194,"Mention"],[1270,1279,"API"]],"Comments":[]}
{"id":60954,"text":"ID:3584260\nPost:\nText: The argmax() method should help.\n Text: Update\n Text: (After reading comment) I believe the argmax() method would work for multi dimensional arrays as well. The linked documentation gives an example of this:\n Code: >>> a = array([[10,50,30],[60,20,40]])\n>>> maxindex = a.argmax()\n>>> maxindex\n3\n\n Text: Update 2\n Text: (Thanks to KennyTM's comment) You can use unravel_index(a.argmax(), a.shape) to get the index as a tuple:\n Code: >>> from numpy import unravel_index\n>>> unravel_index(a.argmax(), a.shape)\n(1, 0)\n\n\nAPI:\nnumpy.ndarray.argmax\nnumpy.ndarray.argmax\nnumpy.unravel_index\nnumpy.ndarray.argmax\nnumpy.ndarray.shape\n","label":[[27,35,"Mention"],[115,123,"Mention"],[384,397,"Mention"],[398,408,"Mention"],[410,417,"Mention"],[544,564,"API"],[565,585,"API"],[586,605,"API"],[606,626,"API"],[627,646,"API"]],"Comments":[]}
{"id":60955,"text":"ID:17012228\nPost:\nText: You can select the dates that are of interest first, and then use between_time. For example, suppose you have a time series of 72 hours:\n Code: import pandas as pd\nfrom numpy.random import randn\n\nrng = pd.date_range('1\/1\/2013', periods=72, freq='H')\nts = pd.Series(randn(len(rng)), index=rng)\n\n Text: To select the between 20:00 & 22:00 on the 2nd and 3rd of January you can simply do:\n Code: ts['2013-01-02':'2013-01-03'].between_time('20:00', '22:00')\n\n Text: Giving you something like this:\n Code: 2013-01-02 20:00:00    0.144399\n2013-01-02 21:00:00    0.886806\n2013-01-02 22:00:00    0.126844\n2013-01-03 20:00:00   -0.464741\n2013-01-03 21:00:00    1.856746\n2013-01-03 22:00:00   -0.286726\n\n\nAPI:\npandas.Series.between_time\n","label":[[90,102,"Mention"],[724,750,"API"]],"Comments":[]}
{"id":60956,"text":"ID:3669930\nPost:\nText: Since you are using numpy, and wish to refer to arrays by names, it seems perhaps you should be using a recarray:\n Code: import numpy as np\nimport itertools as it\n\na = np.array([1,2,3])\nb = np.array([4,5,6])\nc = np.array([7,8,9])\nnames=('a','b','c')\narr=np.rec.fromarrays([a,b,c],names=names)\n\n Text: Once you define the recarray arr, you can access the individual columns by name:\n Code: In [57]: arr['a']\nOut[57]: array([1, 2, 3])\n\nIn [58]: arr['c']\nOut[58]: array([7, 8, 9])\n\n Text: So, to generate combinations of names, and combine the columns with np.hstack, you could do this:\n Code: for comb in it.combinations(names,2):\n    print(np.hstack(arr[c] for c in comb))\n# [1 2 3 4 5 6]\n# [1 2 3 7 8 9]\n# [4 5 6 7 8 9]\n\n\nAPI:\nnumpy.recarray\nnumpy.hstack\n","label":[[127,135,"Mention"],[577,586,"Mention"],[750,764,"API"],[765,777,"API"]],"Comments":[]}
{"id":60957,"text":"ID:14537767\nPost:\nText: There no convention for storing categorical feature name information. You are free to do as you wish.\n Text: Alternatively you can just store the original data with original format and use DictVectorizer \/ FeatureHasher and LabelBinarizer on the fly when you need to build a model from the data.\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\nsklearn.feature_extraction.FeatureHasher\nsklearn.preprocessing.LabelBinarizer\n","label":[[213,227,"Mention"],[230,243,"Mention"],[248,262,"Mention"],[326,367,"API"],[368,408,"API"],[409,445,"API"]],"Comments":[]}
{"id":60958,"text":"ID:4046233\nPost:\nText: One way to automatically do this is the bbox_inches='tight' kwarg to plt.savefig.\n Text: E.g.\n Code: import matplotlib.pyplot as plt\nimport numpy as np\ndata = np.arange(3000).reshape((100,30))\nplt.imshow(data)\nplt.savefig('test.png', bbox_inches='tight')\n\n Text: Another way is to use fig.tight_layout()\n Code: import matplotlib.pyplot as plt\nimport numpy as np\n\nxs = np.linspace(0, 1, 20); ys = np.sin(xs)\n\nfig = plt.figure()\naxes = fig.add_subplot(1,1,1)\naxes.plot(xs, ys)\n\n# This should be called after all axes have been added\nfig.tight_layout()\nfig.savefig('test.png')\n\n\nAPI:\nmatplotlib.pyplot.savefig\nmatplotlib.figure.Figure.tight_layout\n","label":[[92,103,"Mention"],[308,326,"Mention"],[604,629,"API"],[630,667,"API"]],"Comments":[]}
{"id":60959,"text":"ID:8045984\nPost:\nText: Based on the docs, I'm pretty sure mass doesn't belong there in the hist() call...\n\nAPI:\nmatplotlib.pyplot.hist\n","label":[[91,97,"Mention"],[112,134,"API"]],"Comments":[]}
{"id":60960,"text":"ID:6032083\nPost:\nText: Use p.set_clim([5, 50]) to set the color scaling minimums and maximums in the case of your example. Anything in matplotlib that has a colormap has the get_clim and set_clim methods.\n Text: As a full example:\n Code: import matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Circle\nimport numpy as np\n\n# (modified from one of the matplotlib gallery examples)\nresolution = 50 # the number of vertices\nN = 100\nx       = np.random.random(N)\ny       = np.random.random(N)\nradii   = 0.1*np.random.random(N)\npatches = []\nfor x1, y1, r in zip(x, y, radii):\n    circle = Circle((x1, y1), r)\n    patches.append(circle)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\ncolors = 100*np.random.random(N)\np = PatchCollection(patches, cmap=matplotlib.cm.jet, alpha=0.4)\np.set_array(colors)\nax.add_collection(p)\nfig.colorbar(p)\n\nfig.show()\n\n Text: Now, if we just add p.set_clim([5, 50]) (where p is the patch collection) somewhere before we call fig.show(...), we get this:\n\n\nAPI:\nmatplotlib.collections.PatchCollection.set_clim\nmatplotlib.cm.ScalarMappable.get_clim\nmatplotlib.cm.ScalarMappable.set_clim\nmatplotlib.collections.PatchCollection.set_clim\nmatplotlib.figure.Figure.show","label":[[27,46,"Mention"],[174,182,"Mention"],[187,195,"Mention"],[946,965,"Mention"],[1025,1038,"Mention"],[1060,1107,"API"],[1108,1145,"API"],[1146,1183,"API"],[1184,1231,"API"],[1232,1261,"API"]],"Comments":[]}
{"id":60961,"text":"ID:13086305\nPost:\nText: df.head(n) returns a DataFrame holding the first n rows of df.\nNow to display a DataFrame pandas checks by default the width of the terminal, if this is too small to display the DataFrame a summary view will be shown. Which is what you get in the second case.\n Text: Could you increase the size of your terminal, or disable autodetect on the columns by pd.set_printoptions(max_columns=10)?\n\nAPI:\npandas.DataFrame.head\npandas.DataFrame\npandas.DataFrame\npandas.DataFrame\npandas.set_printoptions\n","label":[[24,34,"Mention"],[45,54,"Mention"],[104,113,"Mention"],[202,211,"Mention"],[377,412,"Mention"],[420,441,"API"],[442,458,"API"],[459,475,"API"],[476,492,"API"],[493,516,"API"]],"Comments":[]}
{"id":60962,"text":"ID:17778560\nPost:\nText: Do it like this\n Code: In [49]: df = DataFrame([['1','2','3','.4',5,6.,'foo']],columns=list('ABCDEFG'))\n\nIn [50]: df\nOut[50]: \n   A  B  C   D  E  F    G\n0  1  2  3  .4  5  6  foo\n\nIn [51]: df.dtypes\nOut[51]: \nA     object\nB     object\nC     object\nD     object\nE      int64\nF    float64\nG     object\ndtype: object\n\n Text: Need to assign columns one-by-one\n Code: In [52]: for k, v in df.iloc[:,0:4].convert_objects(convert_numeric=True).iteritems():\n    df[k] = v\n   ....:     \n\nIn [53]: df.dtypes\nOut[53]: \nA      int64\nB      int64\nC      int64\nD    float64\nE      int64\nF    float64\nG     object\ndtype: object\n\n Text: Convert objects usually does the right thing, so easiest to do this\n Code: In [54]: df = DataFrame([['1','2','3','.4',5,6.,'foo']],columns=list('ABCDEFG'))\n\nIn [55]: df.convert_objects(convert_numeric=True).dtypes\nOut[55]: \nA      int64\nB      int64\nC      int64\nD    float64\nE      int64\nF    float64\nG     object\ndtype: object\n\n Text: assigning via df.iloc[:,4:] with a series on the right-hand side copies the data changing type as needed, so I think this should work in theory, but I suspect that this is hitting a very obscure bug that prevents the object dtype from changing to a real (meaning int\/float) dtype. Should probably raise for now.\n Text: Heres the issue to track this: https:\/\/github.com\/pydata\/pandas\/issues\/4312\n\nAPI:\npandas.DataFrame.iloc\n","label":[[996,1009,"Mention"],[1383,1404,"API"]],"Comments":[]}
{"id":60963,"text":"ID:16949183\nPost:\nText: If it's a csv you can use pandas read_csv like this:\n Code: import pandas\nmy_file = 'example.csv'\ndf = pandas.read_csv(my_file)\n\n Text: if you can't do that for some odd reason, you need to merge single dataframes using pandas concat:\n Text: let us say that strings is a list of the strings similar to the example you gave\n Code: count = 0\ndfs = []\nfor my_str in strings:\n    str_split = my_str.split()\n    data = dict(zip(cols,str_split))\n    panda_frame = pandas.DataFrame(data,index=[count],columns = cols)\n    dfs.append(panda_frame)\n    count += 1\ndf = pandas.concat(dfs)\n\n\nAPI:\npandas.read_csv\npandas.concat\n","label":[[57,65,"Mention"],[251,257,"Mention"],[608,623,"API"],[624,637,"API"]],"Comments":[]}
{"id":60964,"text":"ID:6105481\nPost:\nText: Generally speaking, you can just close the figure.  As a quick example:\n Code: import matplotlib.pyplot as plt\n\nfig1 = plt.figure()\nplt.plot(range(10), 'ro-')\nplt.title('This figure will be saved but not shown')\nfig1.savefig('fig1.png')\nplt.close(fig1)\n\nfig2 = plt.figure()\nplt.plot(range(10), 'bo')\nplt.title('This figure will be shown')\n\nplt.show()\n\n Text: As far as whether or not the first plt.figure() call is superflous, it depends on what you're doing.  Usually, you want to hang on to the figure object it returns and work with that instead of using matplotlib's matlab-ish state machine interface. \n Text: When you're making more complex plots, it often becomes worth the extra line of code to do something like this:\n Code: fig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot(range(10))\n\n Text: The advantage is that you don't have to worry about which figure or axis is \"active\", you just refer to a specific axis or figure object. \n\nAPI:\nmatplotlib.pyplot.figure\n","label":[[417,429,"Mention"],[976,1000,"API"]],"Comments":[]}
{"id":60965,"text":"ID:9604442\nPost:\nText: Just call fig.tight_layout() as you normally would. (pyplot is just a convenience wrapper. In most cases, you only use it to quickly generate figure and axes objects and then call their methods directly.)\n Text: There shouldn't be a difference between the QtAgg backend and the default backend (or if there is, it's a bug).\n Text: E.g.\n Code: import matplotlib.pyplot as plt\n\n#-- In your case, you'd do something more like:\n# from matplotlib.figure import Figure\n# fig = Figure()\n#-- ...but we want to use it interactive for a quick example, so \n#--    we'll do it this way\nfig, axes = plt.subplots(nrows=4, ncols=4)\n\nfor i, ax in enumerate(axes.flat, start=1):\n    ax.set_title('Test Axes {}'.format(i))\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n\nplt.show()\n\n Text: Before Tight Layout\n Text: After Tight Layout\n Code: import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=4, ncols=4)\n\nfor i, ax in enumerate(axes.flat, start=1):\n    ax.set_title('Test Axes {}'.format(i))\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n\nfig.tight_layout()\n\nplt.show()\n\n\nAPI:\nmatplotlib.figure.Figure.tight_layout\nmatplotlib.pyplot\n","label":[[33,51,"Mention"],[76,82,"Mention"],[1116,1153,"API"],[1154,1171,"API"]],"Comments":[]}
{"id":60966,"text":"ID:19034976\nPost:\nText: That`s a bug in scikit project. It is documented here.\n Text: There is a float -> int casting during the fitting process that can crash in some cases (by making the seed points be placed at the corner of the bins instead in the center). There is some code in the link to fix the problem.\n Text: If you don't wanna get your hands into the scikit code (and maintain compatibility between your code with other machines) i suggest you normalize your data before passing it to MeanShift.\n Text: Try this:\n Code: >>>from sklearn import preprocessing\n>>>data2 = preprocessing.scale(dataarray)\n\n Text: And then use data2 into your code.\nIt worked for me.\n Text: If you don't want to do either solution, it is a great opportunity to contribute to the project, making a pull request with the solution :)\n Text: Edit: You probably want to retain information to \"descale\" the results of meanshift. So, use a StandardScaler object, instead using a function to scale.\n Text: Good luck!\n\nAPI:\nsklearn.preprocessing.StandardScaler\n","label":[[920,934,"Mention"],[1002,1038,"API"]],"Comments":[]}
{"id":60967,"text":"ID:17826063\nPost:\nText: As @Rutger points out you can simply use dropna:\n Code: In [11]: df.dropna(subset=['Value1', 'Value2', 'Value3'])\nOut[11]:\n   Num     Date  Value1  Value2  Value3\n0    1  7\/29\/11       1       2       3\n3    4   7\/6\/11      -1       0       2\n\n Text: .\n Text: The initial way I suggested (which is clearly not optimal), once you've read it in as a DataFrame you can remove these rows using notnull (you want to keep only those rows which are all notnull):\n Code: In [21]: df.loc[:, ['Value1', 'Value2', 'Value3']].apply(pd.notnull)\nOut[21]:\n  Value1 Value2 Value3\n0   True   True   True\n1  False   True   True\n2   True  False  False\n3   True   True   True\n\nIn [22]: df.loc[:, ['Value1', 'Value2', 'Value3']].apply(pd.notnull).all(1)\nOut[22]:\n0     True\n1    False\n2    False\n3     True\ndtype: bool\n\n Text: And select only those rows:\n Code: In [23]: df[df.loc[:, ['Value1', 'Value2', 'Value3']].apply(pd.notnull).all(1)]\nOut[23]:\n   Num     Date  Value1  Value2  Value3\n0    1  7\/29\/11       1       2       3\n3    4   7\/6\/11      -1       0       2\n\nIn [24]: df = df[df.loc[:, ['Value1', 'Value2', 'Value3']].apply(pd.notnull).all(1)]\n\n\nAPI:\npandas.DataFrame.dropna\npandas.DataFrame\npandas.notnull\n","label":[[65,71,"Mention"],[372,381,"Mention"],[414,421,"Mention"],[1167,1190,"API"],[1191,1207,"API"],[1208,1222,"API"]],"Comments":[]}
{"id":60968,"text":"ID:17115229\nPost:\nText: What about DataFrame.replace?\n Code: In [9]: mapping = {'set': 1, 'test': 2}\n\nIn [10]: df.replace({'set': mapping, 'tesst': mapping})\nOut[10]: \n   Unnamed: 0 respondent  brand engine  country  aware  aware_2  aware_3  age  \\\n0           0          a  volvo      p      swe      1        0        1   23   \n1           1          b  volvo   None      swe      0        0        1   45   \n2           2          c    bmw      p       us      0        0        1   56   \n3           3          d    bmw      p       us      0        1        1   43   \n4           4          e    bmw      d  germany      1        0        1   34   \n5           5          f   audi      d  germany      1        0        1   59   \n6           6          g  volvo      d      swe      1        0        0   65   \n7           7          h   audi      d      swe      1        0        0   78   \n8           8          i  volvo      d       us      1        1        1   32   \n\n  tesst set  \n0     2   1  \n1     1   2  \n2     2   1  \n3     1   2  \n4     2   1  \n5     1   2  \n6     2   1  \n7     1   2  \n8     2   1  \n\n Text: As @Jeff pointed out in the comments, in pandas versions < 0.11.1, manually tack .convert_objects() onto the end to properly convert tesst and set to int64 columns, in case that matters in subsequent operations.\n\nAPI:\npandas.DataFrame.replace\npandas.DataFrame.convert_objects\n","label":[[35,52,"Mention"],[1208,1226,"Mention"],[1345,1369,"API"],[1370,1402,"API"]],"Comments":[]}
{"id":60969,"text":"ID:10723827\nPost:\nText: countourf will only work with a grid of data. If you're data is scattered, then you'll need to create an interpolated grid matching your data, like this: (note you'll need scipy to perform the interpolation)\n Code: import numpy as np\nfrom scipy.interpolate import griddata\nimport matplotlib.pyplot as plt\nimport numpy.ma as ma\nfrom numpy.random import uniform, seed\n\n# your data\nx = [3,7,9]\ny = [1,4,5]\nz = [20,3,7]\n\n# define grid.\nxi = np.linspace(0,10,300)\nyi = np.linspace(0,6,300)\n# grid the data.\nzi = griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')\n# contour the gridded data, plotting dots at the randomly spaced data points.\nCS = plt.contour(xi,yi,zi,15,linewidths=0.5,colors='k')\nCS = plt.contourf(xi,yi,zi,15,cmap=plt.cm.jet)\nplt.colorbar() # draw colorbar\n# plot data points.\nplt.scatter(x,y,marker='o',c='b',s=5)\nplt.xlim(min(x),max(x))\nplt.ylim(min(y),max(y))\nplt.title('griddata test (%d points)' % len(x))\nplt.show()\n\n Text: See here for the origin of that code.\n\nAPI:\nmatplotlib.pyplot.contourf\n","label":[[24,33,"Mention"],[1022,1048,"API"]],"Comments":[]}
{"id":60970,"text":"ID:6925816\nPost:\nText: If you have numpy, you could read the file into a numpy array. comments='[' tells np.genfromtxt to ignore lines that begin with [. The reshape method places each 16x16 block in its own \"layer\".\n Code: import numpy as np\narr=np.genfromtxt('data.csv',comments='[',delimiter=',',dtype=None)\narr=arr.reshape(-1,16,16)\n\n Text: You can access the nth layer with arr[n].\n\nAPI:\nnumpy.genfromtxt\nnumpy.ndarray.reshape\n","label":[[105,118,"Mention"],[158,165,"Mention"],[393,409,"API"],[410,431,"API"]],"Comments":[]}
{"id":60971,"text":"ID:22264337\nPost:\nText: As X is a sparse array, instead of numpy.hstack, use scipy.sparse.hstack to join the arrays. In my opinion the error message is kind of misleading here.\n Text: This minimal example illustrates the situation:\n Code: import numpy as np\nfrom scipy import sparse\n\nX = sparse.rand(10, 10000)\nxt = np.random.random((10, 1))\nprint 'X shape:', X.shape\nprint 'xt shape:', xt.shape\nprint 'Stacked shape:', np.hstack((X,xt)).shape\n#print 'Stacked shape:', sparse.hstack((X,xt)).shape #This works\n\n Text: Based on the following output\n Code: X shape: (10, 10000)\nxt shape: (10, 1)\n\n Text: one may expect that the hstack in the following line will work, but the fact is that it throws this error:\n Code: ValueError: all the input arrays must have same number of dimensions\n\n Text: So, use scipy.sparse.hstack when you have a sparse array to stack.\n Text: In fact I have answered this as a comment in your another questions, and you mentioned that another error message pops up:\n Code: TypeError: no supported conversion for types: (dtype('float64'), dtype('O'))\n\n Text: First of all, AllAlexaAndGoogleInfo does not have a dtype as it is a DataFrame. To get it's underlying numpy array, simply use AllAlexaAndGoogleInfo.values. Check its dtype. Based on the error message, it has a dtype of object, which means that it might contain non-numerical elements like strings.\n Text: This is a minimal example that reproduces this situation:\n Code: X = sparse.rand(100, 10000)\nxt = np.random.random((100, 1))\nxt = xt.astype('object') # Comment this to fix the error\nprint 'X:', X.shape, X.dtype\nprint 'xt:', xt.shape, xt.dtype\nprint 'Stacked shape:', sparse.hstack((X,xt)).shape\n\n Text: The error message:\n Code: TypeError: no supported conversion for types: (dtype('float64'), dtype('O'))\n\n Text: So, check if there is any non-numerical values in AllAlexaAndGoogleInfo and repair them, before doing the stacking.\n\nAPI:\nnumpy.hstack\nscipy.sparse.hstack\nnumpy.hstack\nscipy.sparse.hstack\npandas.DataFrame\npandas.DataFrame.values\n","label":[[625,631,"Mention"],[1150,1159,"Mention"],[1208,1236,"Mention"],[1956,1968,"API"],[1989,2005,"API"],[2006,2029,"API"]],"Comments":[]}
{"id":60972,"text":"ID:8739526\nPost:\nText: for numerical solution, you can use fsolve:\n Text: http:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.optimize.fsolve.html#scipy.optimize.fsolve\n Code: from scipy.optimize import fsolve\nimport math\n\ndef equations(p):\n    x, y = p\n    return (x+y**2-4, math.exp(x) + x*y - 3)\n\nx, y =  fsolve(equations, (1, 1))\n\nprint equations((x, y))\n\n\nAPI:\nscipy.optimize.fsolve\n","label":[[59,65,"Mention"],[372,393,"API"]],"Comments":[]}
{"id":60973,"text":"ID:10465162\nPost:\nText: You can use DataFrame.interpolate to get a linear interpolation. \n Code: In : df = pandas.DataFrame(numpy.random.randn(5,3), index=['a','c','d','e','g'])\n\nIn : df\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\ng -1.632493  0.938456  0.492695\n\nIn : df2 = df.reindex(['a','b','c','d','e','f','g'])\n\nIn : df2\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb       NaN       NaN       NaN\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf       NaN       NaN       NaN\ng -1.632493  0.938456  0.492695\n\nIn : df2.interpolate()\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb  0.052363 -1.729055  0.114652\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf -1.330113  1.134579  0.000958\ng -1.632493  0.938456  0.492695\n\n Text: For anything more complex, you need to roll-out your own function that will deal with a Series object and fill NaN values as you like and return another Series object.\n\nAPI:\npandas.DataFrame.interpolate\npandas.Series\npandas.Series\n","label":[[36,57,"Mention"],[1090,1096,"Mention"],[1155,1161,"Mention"],[1176,1204,"API"],[1205,1218,"API"],[1219,1232,"API"]],"Comments":[]}
{"id":60974,"text":"ID:4266645\nPost:\nText: scipy.stats.rv_discrete might be what you want.  You can supply your probabilities via the values parameter.  You can then use the rvs() method of the distribution object to generate random numbers.\n Text: As pointed out by Eugene Pakhomov in the comments, you can also pass a p keyword parameter to numpy.random.choice(), e.g.\n Code: numpy.random.choice(numpy.arange(1, 7), p=[0.1, 0.05, 0.05, 0.2, 0.4, 0.2])\n\n Text: If you are using Python 3.6 or above, you can use random.choices() from the standard library  see the answer by Mark Dickinson.\n\nAPI:\nscipy.stats.rv_discrete\nscipy.stats.rv_discrete.rvs\nnumpy.random.choice\n","label":[[154,159,"Mention"],[601,628,"API"]],"Comments":[]}
{"id":60975,"text":"ID:20447929\nPost:\nText: 1) yes. Use DictVectorizer or HashVectorizer from the feature_extraction module.\n2) This is a multilabel problem. Maybe use the OneVsRestClassifier from the multi_class module. It will train a separate classifier for each class.\n3) Using a multilabel classifier \/ one classifier per calss will do that.\n Text: Take a look at http:\/\/scikit-learn.org\/dev\/auto_examples\/grid_search_text_feature_extraction.html\nand http:\/\/scikit-learn.org\/dev\/auto_examples\/plot_multilabel.html\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\nsklearn.feature_extraction.text.HashingVectorizer\nsklearn.feature_extraction\nsklearn.multiclass.OneVsRestClassifier\nsklearn.multiclass\n","label":[[36,50,"Mention"],[54,68,"Mention"],[78,96,"Mention"],[152,171,"Mention"],[181,192,"Mention"],[505,546,"API"],[547,596,"API"],[597,623,"API"],[624,662,"API"],[663,681,"API"]],"Comments":[]}
{"id":60976,"text":"ID:6024648\nPost:\nText: From the documentation, whether or not the drawing elements are destroyed from show() depends on the backend, and the version of matplotlib.  Not having the figures destroyed seems to be available with version 1.1.0.   To figure out which backend is in use, use the get_backend() function.  In my case, I was using the Qt4Agg backend.  By invoking the TkAgg backend, with the call matplotlib.use('TkAgg') the figures were not destroyed before the save.  Now to find out how to change the behavior of the Qt4Agg...  \n\nAPI:\nmatplotlib.pyplot.show\nmatplotlib.get_backend\nmatplotlib.use\n","label":[[102,108,"Mention"],[289,302,"Mention"],[545,567,"API"],[568,590,"API"]],"Comments":[]}
{"id":60977,"text":"ID:17326457\nPost:\nText: It sounds like you have a pandas DataFrame with various strings that you want to convert to indexed values such that each unique string has a unique integer value.\n Text: numpy.unique does what you need. (You already mentioned that you were using numpy, so I'm going to post a numpy solution.)\n Text: For example: \n Code: import numpy as np\nimport pandas\n\ndf = pandas.DataFrame(dict(x=['1', 'a5', 'cde9', '1', 'cde9']))\n\nunique_vals, df['keys'] = np.unique(df.x, return_inverse=True)\n\nprint df\n\n\nAPI:\npandas.DataFrame\nnumpy.unique\n","label":[[57,66,"Mention"],[525,541,"API"]],"Comments":[]}
{"id":60978,"text":"ID:13446268\nPost:\nText: freq='M' is for month-end frequencies (see here). But you can use .shift to shift it by any number of days (or any frequency for that matter):\n Code: pd.date_range(start, end, freq='M').shift(15, freq=pd.datetools.day)\n\n\nAPI:\npandas.DatetimeIndex.shift\n","label":[[90,96,"Mention"],[250,276,"API"]],"Comments":[]}
{"id":60979,"text":"ID:10466763\nPost:\nText: Assuming a pandas.TimeSeries object as the starting point, you can group\nelements by ISO week number and ISO weekday with\ndatetime.date.isocalendar().  The following statement, which ignores ISO year, aggregates the last sample of each day.\n Code: In [95]: daily = ts.groupby(lambda x: x.isocalendar()[1:]).agg(lambda s: s[-1])\n\nIn [96]: daily\nOut[96]: \nkey_0\n(1, 1)     63\n(1, 2)     91\n(1, 3)     73\n...\n(20, 5)    82\n(20, 6)    53\n(20, 7)    63\nLength: 140\n\n Text: There may be cleaner way to perform the next step, but the goal is to change the index from an array of tuples to a MultiIndex object.\n Code: In [97]: daily.index = pandas.MultiIndex.from_tuples(daily.index, names=['W', 'D'])\n\nIn [98]: daily\nOut[98]: \nW   D\n1   1    63\n    2    91\n    3    73\n    4    88\n    5    84\n    6    95\n    7    72\n...\n20  1    81\n    2    53\n    3    78\n    4    64\n    5    82\n    6    53\n    7    63\nLength: 140\n\n Text: The final step is to \"unstack\" weekday from the\nMultiIndex, creating columns for each weekday, and replace the weekday numbers with an abbreviation, to improve readability.\n Code: In [102]: dofw = \"Mon Tue Wed Thu Fri Sat Sun\".split()\n\nIn [103]: grid = daily.unstack('D').rename(columns=lambda x: dofw[x-1])\n\nIn [104]: grid\nOut[104]: \n    Mon  Tue  Wed  Thu  Fri  Sat  Sun\nW                                    \n1    63   91   73   88   84   95   72\n2    66   77   96   72   56   80   66\n...\n19   56   69   89   69   96   73   80\n20   81   53   78   64   82   53   63\n\n Text: To create a line plot for each week, transpose the dataframe, so the columns are week numbers and rows are weekdays (note this step can be avoided by unstacking week number, in place of weekday, in the previous step), and call plot.\n Code: grid.T.plot()\n\n\nAPI:\npandas.TimeSeries\ndatetime.date.isocalendar\npandas.MultiIndex\npandas.DataFrame.plot","label":[[608,618,"Mention"],[1744,1748,"Mention"],[1822,1839,"API"],[1840,1861,"API"]],"Comments":[]}
{"id":60980,"text":"ID:5391258\nPost:\nText: From the documentation:\n Text: \nWithin an axes, the order that the\n  various lines, markers, text,\n  collections, etc appear is determined\n  by the\n  matplotlib.artist.Artist.set_zorder()\n  property. The default order is\n  patches, lines, text, with collections\n  of lines and collections of patches\n  appearing at the same level as regular\n  lines and patches, respectively.\n\n Text: So patches will be drawn below lines by default. You can change the order by specifying the zorder of the rectangle:\n Code: # note alpha is None and visible is True by default\nrect = patches.Rectangle((2, 3), 2, 2, ec=\"gray\", fc=\"CornflowerBlue\", zorder=10)\n\n Text: You can check the zorder of the line on your plot by changing ax.plot(x, y) to lines = ax.plot(x, y) and add a new line of code: print lines[0].zorder. When I did this, the zorder for the line was 2. Therefore, the rectangle will need a zorder > 2 to obscure the line.\n\nAPI:\nmatplotlib.artist.Artist.set_zorder\nmatplotlib.axes.Axes.plot\nmatplotlib.axes.Axes.plot\n","label":[[735,748,"Mention"],[760,773,"Mention"],[984,1009,"API"],[1010,1035,"API"]],"Comments":[]}
{"id":60981,"text":"ID:11708664\nPost:\nText: You can adjust Pandas print options with set_printoptions.\n Code: In [3]: df.describe()\nOut[3]:\n<class 'pandas.core.frame.DataFrame'>\nIndex: 8 entries, count to max\nData columns:\nx1    8  non-null values\nx2    8  non-null values\nx3    8  non-null values\nx4    8  non-null values\nx5    8  non-null values\nx6    8  non-null values\nx7    8  non-null values\ndtypes: float64(7)\n\nIn [4]: pd.set_printoptions(precision=2)\n\nIn [5]: df.describe()\nOut[5]:\n            x1       x2       x3       x4       x5       x6       x7\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\nmean   69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\nstd       17.1     17.1     17.1     17.1     17.1     17.1     17.1\nmin    69000.0  69001.0  69002.0  69003.0  69004.0  69005.0  69006.0\n25%    69012.2  69013.2  69014.2  69015.2  69016.2  69017.2  69018.2\n50%    69024.5  69025.5  69026.5  69027.5  69028.5  69029.5  69030.5\n75%    69036.8  69037.8  69038.8  69039.8  69040.8  69041.8  69042.8\nmax    69049.0  69050.0  69051.0  69052.0  69053.0  69054.0  69055.0\n\n Text: However, this will not work in all cases as Pandas detects your console width, and it will only use to_string if the output fits in the console (see the docstring of set_printoptions).\nIn this case, you can explicitly call to_string as answered by BrenBarn.\n Text: Update\n Text: With version 0.10 the way wide dataframes are printed changed:\n Code: In [3]: df.describe()\nOut[3]:\n                 x1            x2            x3            x4            x5  \\\ncount      8.000000      8.000000      8.000000      8.000000      8.000000\nmean   59832.361578  27356.711336  49317.281222  51214.837838  51254.839690\nstd    22600.723536  26867.192716  28071.737509  21012.422793  33831.515761\nmin    31906.695474   1648.359160     56.378115  16278.322271     43.745574\n25%    45264.625201  12799.540572  41429.628749  40374.273582  29789.643875\n50%    56340.214856  18666.456293  51995.661512  54894.562656  47667.684422\n75%    75587.003417  31375.610322  61069.190523  67811.893435  76014.884048\nmax    98136.474782  84544.484627  91743.983895  75154.587156  99012.695717\n\n                 x6            x7\ncount      8.000000      8.000000\nmean   41863.000717  33950.235126\nstd    38709.468281  29075.745673\nmin     3590.990740   1833.464154\n25%    15145.759625   6879.523949\n50%    22139.243042  33706.029946\n75%    72038.983496  51449.893980\nmax    98601.190488  83309.051963\n\n Text: Furthermore, the API for setting Pandas options changed:\n Code: In [4]: pd.set_option('display.precision', 2)\n\nIn [5]: df.describe()\nOut[5]:\n            x1       x2       x3       x4       x5       x6       x7\ncount      8.0      8.0      8.0      8.0      8.0      8.0      8.0\nmean   59832.4  27356.7  49317.3  51214.8  51254.8  41863.0  33950.2\nstd    22600.7  26867.2  28071.7  21012.4  33831.5  38709.5  29075.7\nmin    31906.7   1648.4     56.4  16278.3     43.7   3591.0   1833.5\n25%    45264.6  12799.5  41429.6  40374.3  29789.6  15145.8   6879.5\n50%    56340.2  18666.5  51995.7  54894.6  47667.7  22139.2  33706.0\n75%    75587.0  31375.6  61069.2  67811.9  76014.9  72039.0  51449.9\nmax    98136.5  84544.5  91744.0  75154.6  99012.7  98601.2  83309.1\n\n\nAPI:\npandas.set_printoptions\npandas.Series.to_string\npandas.set_printoptions\npandas.Series.to_string","label":[[65,81,"Mention"],[1199,1208,"Mention"],[1265,1281,"Mention"],[1322,1331,"Mention"],[3249,3272,"API"],[3273,3296,"API"],[3297,3320,"API"],[3321,3344,"API"]],"Comments":[]}
{"id":60982,"text":"ID:19243527\nPost:\nText: Here's some example code using DictVectorizer. First, let set up some data in the Python shell. I leave reading from a file up to you.\n Code: >>> features = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\",\n...             \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\"]\n>>> input_text = \"\"\"38   Private    215646   HS-grad    9    Divorced    Handlers-cleaners   Not-in-family   White   Male   0   0   40   United-States   <=50K\n... 53   Private    234721   11th   7    Married-civ-spouse  Handlers-cleaners   Husband     Black   Male   0   0   40   United-States   <=50K\n... 30   State-gov  141297   Bachelors  13   Married-civ-spouse  Prof-specialty  Husband     Asian-Pac-Islander  Male   0   0   40   India   >50K\n... \"\"\"\n\n Text: Now, parse these:\n Code: >>> for ln in input_text.splitlines():\n...     values = ln.split()\n...     y.append(values[-1])\n...     d = dict(zip(features, values[:-1]))\n...     samples.append(d)\n\n Text: What have we got now? Let's check:\n Code: >>> from pprint import pprint\n>>> pprint(samples[0])\n{'age': '38',\n 'capital-gain': '0',\n 'capital-loss': '0',\n 'education': 'HS-grad',\n 'education-num': '9',\n 'fnlwgt': '215646',\n 'hours-per-week': '40',\n 'marital-status': 'Divorced',\n 'native-country': 'United-States',\n 'occupation': 'Handlers-cleaners',\n 'race': 'White',\n 'relationship': 'Not-in-family',\n 'sex': 'Male',\n 'workclass': 'Private'}\n>>> print(y)\n['<=50K', '<=50K', '>50K']\n\n Text: These samples are ready for DictVectorizer, so pass them:\n Code: >>> from sklearn.feature_extraction import DictVectorizer\n>>> dv = DictVectorizer()\n>>> X = dv.fit_transform(samples)\n>>> X\n<3x29 sparse matrix of type '<type 'numpy.float64'>'\n        with 42 stored elements in Compressed Sparse Row format>\n\n Text: Et voila, you have X and y that can be passed to an estimator, provided it supports sparse matrices. (Otherwise, pass sparse=False to the DictVectorizer constructor.)\n Text: Test samples can similarly be passed to DictVectorizer.transform; if there are feature\/value combinations in the test set that do not occur in the training set, these will simply be ignored (because the learned model cannot do anything sensible with them anyway).\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\nsklearn.feature_extraction.DictVectorizer\nsklearn.feature_extraction.DictVectorizer\nsklearn.feature_extraction.DictVectorizer.transform\n","label":[[55,69,"Mention"],[1572,1586,"Mention"],[1997,2011,"Mention"],[2073,2097,"Mention"],[2303,2344,"API"],[2345,2386,"API"],[2387,2428,"API"],[2429,2480,"API"]],"Comments":[]}
{"id":60983,"text":"ID:14137002\nPost:\nText: Multi-label classification has a somewhat different API than ordinary classification. Your Y should be a sequence of sequences, e.g. a list of lists, like\n Code: Y = [[\"foo\", \"bar\"],          # the first sample is a foo and a bar\n     [\"foo\"],                 # the second is only a foo\n     [\"bar\", \"baz\"]]          # the third is a bar and a baz\n\n Text: Such a Y can then be fed to an estimator that handles multiple classifications. You can construct such an estimator using the OneVsRestClassifier wrapper:\n Code: from sklearn.multiclass import OneVsRestClassifier\nclf = OneVsRestClassifier(LogisticRegression())\n\n Text: then train with clf.fit(X, Y). clf.predict will now produce sequences of sequences as well.\n Text: UPDATE as of scikit-learn 0.15, this API is deprecated because its input is ambiguous. You should convert the Y I gave above to a matrix with a MultiLabelBinarizer:\n Code: >>> from sklearn.preprocessing import MultiLabelBinarizer\n>>> mlb = MultiLabelBinarizer()\n>>> mlb.fit_transform(Y)\narray([[1, 0, 1],\n       [0, 0, 1],\n       [1, 1, 0]])\n\n Text: Then feed this to an estimator's fit method. Converting back is done with inverse_transform on the same binarizer:\n Code: >>> mlb.inverse_transform(mlb.transform(Y))\n[('bar', 'foo'), ('foo',), ('bar', 'baz')]\n\n\nAPI:\nsklearn.multiclass.OneVsRestClassifier\nsklearn.multiclass.OneVsRestClassifier.fit\nsklearn.multiclass.OneVsRestClassifier.predict\nsklearn.preprocessing.MultiLabelBinarizer\nsklearn.preprocessing.MultiLabelBinarizer.fit\nsklearn.preprocessing.MultiLabelBinarizer.inverse_transform\n","label":[[506,525,"Mention"],[665,678,"Mention"],[680,691,"Mention"],[892,911,"Mention"],[1131,1134,"Mention"],[1172,1189,"Mention"],[1314,1352,"API"],[1353,1395,"API"],[1396,1442,"API"],[1443,1484,"API"],[1485,1530,"API"],[1531,1590,"API"]],"Comments":[]}
{"id":60984,"text":"ID:17333819\nPost:\nText: CountVectorizer will extract trigrams for you (using ngram_range=(3, 3)). The text feature extraction documentation introduces this. Then, just use MultinomialNB exactly like before with the transformed feature matrix.\n Text: Note that this is actually modeling:\n Text: P(document | label) = P(wordX, wordX-1, wordX-2 | label) * P(wordX-1, wordX-2, wordX-3 | label) * ...\n Text: How different is that? Well, that first term can be written as\n Text: P(wordX, wordX-1, wordX-2 | label) = P(wordX | wordX-1, wordX-2, label) * P(wordX-1, wordX-2 | label)\n Text: Of course, all the other terms can be written that way too, so you end up with (dropping the subscripts and the conditioning on the label for brevity):\n Text: P(X | X-1, X-2) P(X-1 | X-2, X-3) ... P(3 | 2, 1) P(X-1, X-2) P(X-2, X-3) ... P(2, 1)\n Text: Now, P(X-1, X-2) can be written as P(X-1 | X-2) P(X-2). So if we do that for all those terms, we have\n Text: P(X | X-1, X-2) P(X-1 | X-2, X-3) ... P(3 | 2, 1) P(X-1 | X-2) P(X-2 | X-3) ... P(2 | 1) P(X-2) P(X-1) ... P(1)\n Text: So this is actually like using trigrams, bigrams, and unigrams (though not estimating the bigram\/unigram terms directly).\n\nAPI:\nsklearn.feature_extraction.text.CountVectorizer\nsklearn.naive_bayes.MultinomialNB\n","label":[[24,39,"Mention"],[172,185,"Mention"],[1190,1237,"API"],[1238,1271,"API"]],"Comments":[]}
{"id":60985,"text":"ID:20499349\nPost:\nText: Looking in the code, TruncatedSVD uses as_float_array() from sklearn.utils. The code for as_float_array() is here.\n Text: As you see, the documentation states that \n Text: \nThe new dtype will be np.float32 or np.float64\n\n Text: I guess you can hack it to be float16 (perhaps remove the use of as_float_array altogether?), but I am not sure what the consequences would be.\n Text: One thing that you should consider is that using larger variables (e.g. float64) in these kinds of numerical algorithms helps with the numerical stability. This is especially important if you are dealing with a very large matrix. If you will use float16 you might be risking getting incorrect results due to numerical issues.\n\nAPI:\nsklearn.decomposition.TruncatedSVD\nsklearn.utils.as_float_array\nsklearn.utils\nsklearn.utils.as_float_array\nsklearn.utils.as_float_array\n","label":[[45,57,"Mention"],[63,79,"Mention"],[113,129,"Mention"],[317,331,"Mention"],[735,769,"API"],[770,798,"API"],[813,841,"API"],[842,870,"API"]],"Comments":[]}
{"id":60986,"text":"ID:9896143\nPost:\nText: I think that the answer is simply that the kmeans in MLPy does not work with sparse inputs. It is non-trivial to code an algorithm to work on sparse inputs.\n Text: The MiniBatchKMeans of scikit-learn works on sparse input (disclaimer: I am a scikit-learn developer).\n\nAPI:\nsklearn.cluster.MiniBatchKMeans\n","label":[[191,206,"Mention"],[296,327,"API"]],"Comments":[]}
{"id":60987,"text":"ID:20491503\nPost:\nText: TruncatedSVD should have no problem processing negative values.\n Text: CountVectorizer\/TfidfVectorizer should not produce negative values. If they do, it's probably a bug.\n Text: I will update my answer if you update your question to:\n Text: \nprovide us with a valid reproduction script, preferably on self-contained data if possible (e.g. define d_train \/ d_test as small list of Python strings that make it possible to reproduce the issue without having access to your real data),\nfix the indentation to make it possible to copy and paste your code snippet and run it,\nprovide the error message you get along with the full traceback.\n\n\nAPI:\nsklearn.decomposition.TruncatedSVD\nsklearn.feature_extraction.text.CountVectorizer\nsklearn.feature_extraction.text.TfidfVectorizer\n","label":[[24,36,"Mention"],[95,110,"Mention"],[111,126,"Mention"],[667,701,"API"],[702,749,"API"],[750,797,"API"]],"Comments":[]}
{"id":60988,"text":"ID:8218887\nPost:\nText: EDIT\n Text: Changed aspect='normal to aspect='auto' since that changed in more recent versions of matplotlib (thanks to @Luke19).\n Text: Assuming : \n Code: import matplotlib.pyplot as plt\n\n Text: To make a figure without the frame :\n Code: fig = plt.figure(frameon=False)\nfig.set_size_inches(w,h)\n\n Text: To make the content fill the whole figure \n Code: ax = plt.Axes(fig, [0., 0., 1., 1.])\nax.set_axis_off()\nfig.add_axes(ax)\n\n Text: Then draw your image on it :\n Code: ax.imshow(your_image, aspect='auto')\nfig.savefig(fname, dpi)\n\n Text: The aspect parameter changes the pixel size to make sure they fill the figure size specified in fig.set_size_inches(). To get a feel of how to play with this sort of things, read through matplotlib's documentation, particularly on the subject of Axes, Axis and Artist.\n\nAPI:\nmatplotlib.figure.Figure.set_size_inches\nmatplotlib.axes.Axes\nmatplotlib.axis.Axis\nmatplotlib.artist.Artist\n","label":[[659,681,"Mention"],[810,814,"Mention"],[816,820,"Mention"],[825,831,"Mention"],[839,879,"API"],[880,900,"API"],[901,921,"API"],[922,946,"API"]],"Comments":[]}
{"id":60989,"text":"ID:10096109\nPost:\nText: The problem is with the variable initguess.  The function optimize.leastsq has the following call signature:\n Text: http:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.optimize.leastsq.html\n Text: It's second argument, x0, has to be an array.  Your list\n Code: initguess = [v0,0.5,0.5]\n\n Text: won't be converted to an array because v0 is a list instead of an int or float.  So you get an error when you try to convert initguess from a list to an array in the leastsq function.\n Text: I would adjust the variable params from\n Code: def errfunc(params,myfun,y0,Tx,y):\n\n Text: so that it is a 1-D array.  Make the first few entries the values of v0 then append beta and gamma to that.\n\nAPI:\nscipy.optimize.leastsq\nscipy.optimize.leastsq\n","label":[[82,98,"Mention"],[490,497,"Mention"],[719,741,"API"],[742,764,"API"]],"Comments":[]}
{"id":60990,"text":"ID:17426255\nPost:\nText: The problem is not with the classifier, it is with the vectorizer. TfidfVectorizer has a parameter token_pattern : string, which is a \"Regular expression denoting what constitutes a token, only used if tokenize == word. The default regexp select tokens of 2 or more letters characters (punctuation is completely ignored and always treated as a token separator).\" (emphasis added). The tokenizer throws out the word i, resulting in an empty document. Naive Bayes then classifies that as class 1, because this is the most frequent class in the training data.\n Text: Depending on the data, you might want to consider using a uniform prior for Naive Bayes.\n Text: Further hints as to why things may not be working:\n Text: There might be some other oddity in the way your pipeline is set up. I find it useful to inspect the inputs and outputs of each stage (tokenizer, vectorizer, classifier, etc). Investing some time into writing a unit test will save you lots of time in the long run. \n Text: Once you are satisfied everything is working fine, try evaluating your classifier on the test data. My suspicion is that there is considerable overlap between your classes, particularly label.i_don't_know and label.i. If this is the case, the classifier will perform poorly.\n\nAPI:\nsklearn.feature_extraction.text.TfidfVectorizer\n","label":[[91,106,"Mention"],[1300,1347,"API"]],"Comments":[]}
{"id":60991,"text":"ID:20299709\nPost:\nText: No, LinearSVC will not compute probabilities because it's not trained to do so. Use sklearn.linear_model.LogisticRegression, which uses the same algorithm as LinearSVC but with the log loss. It uses the standard logistic function for probability estimates:\n Code: 1. \/ (1 + exp(-decision_function(X)))\n\n Text: (For the same reason, SGDClassifier will only output probabilities when loss=\"log\", not using its default loss function which causes it to learn a linear SVM.)\n\nAPI:\nsklearn.svm.LinearSVC\nsklearn.linear_model.LogisticRegression\nsklearn.svm.LinearSVC\nsklearn.linear_model.SGDClassifier\n","label":[[28,37,"Mention"],[182,191,"Mention"],[356,369,"Mention"],[500,521,"API"],[562,583,"API"],[584,618,"API"]],"Comments":[]}
{"id":60992,"text":"ID:18041006\nPost:\nText: Use the predict_proba method to get probabilities. predict gives class labels.\n Code: >>> lr = LogisticRegression()\n>>> X = np.random.randn(3, 4)\n>>> y = [1, 0, 0]\n>>> lr.fit(X, y)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n>>> lr.predict_proba(X[0])\narray([[ 0.49197272,  0.50802728]])\n\n Text: (If you had read the documentation, you would have found this out.)\n\nAPI:\nsklearn.linear_model.LogisticRegression.predict_proba\nsklearn.linear_model.LogisticRegressionCV.predict\n","label":[[32,45,"Mention"],[75,82,"Mention"],[503,556,"API"],[557,606,"API"]],"Comments":[]}
{"id":60993,"text":"ID:8964779\nPost:\nText: As other folks have mentioned, for a really large file, you're better off iterating.\n Text: However, you do commonly want the entire thing in memory for various reasons.\n Text: genfromtxt is much less efficient than loadtxt (though it handles missing data, whereas loadtxt is more \"lean and mean\", which is why the two functions co-exist).\n Text: If your data is very regular (e.g. just simple delimited rows of all the same type), you can also improve on either by using numpy.fromiter.\n Text: If you have enough ram, consider using np.loadtxt('yourfile.txt', delimiter=',') (You may also need to specify skiprows if you have a header on the file.)\n Text: As a quick comparison, loading ~500MB text file with loadtxt uses ~900MB of ram at peak usage, while loading the same file with genfromtxt uses ~2.5GB.\n Text: Loadtxt\n\n Text: Genfromtxt\n\n Text: Alternately, consider something like the following.  It will only work for very simple, regular data, but it's quite fast. (loadtxt and genfromtxt do a lot of guessing and error-checking. If your data is very simple and regular, you can improve on them greatly.)\n Code: import numpy as np\n\ndef generate_text_file(length=1e6, ncols=20):\n    data = np.random.random((length, ncols))\n    np.savetxt('large_text_file.csv', data, delimiter=',')\n\ndef iter_loadtxt(filename, delimiter=',', skiprows=0, dtype=float):\n    def iter_func():\n        with open(filename, 'r') as infile:\n            for _ in range(skiprows):\n                next(infile)\n            for line in infile:\n                line = line.rstrip().split(delimiter)\n                for item in line:\n                    yield dtype(item)\n        iter_loadtxt.rowlength = len(line)\n\n    data = np.fromiter(iter_func(), dtype=dtype)\n    data = data.reshape((-1, iter_loadtxt.rowlength))\n    return data\n\n#generate_text_file()\ndata = iter_loadtxt('large_text_file.csv')\n\n Text: Fromiter\n\nAPI:\nnumpy.genfromtxt\nnumpy.loadtxt\nnumpy.loadtxt\nnumpy.fromiter\nnumpy.loadtxt\nnumpy.loadtxt\nnumpy.genfromtxt\nnumpy.loadtxt\nnumpy.genfromtxt\n","label":[[200,210,"Mention"],[239,246,"Mention"],[288,295,"Mention"],[557,598,"Mention"],[733,740,"Mention"],[808,818,"Mention"],[998,1005,"Mention"],[1010,1020,"Mention"],[1925,1941,"API"],[1942,1955,"API"],[1956,1969,"API"],[1985,1998,"API"],[1999,2012,"API"],[2013,2029,"API"],[2030,2043,"API"],[2044,2060,"API"]],"Comments":[]}
{"id":60994,"text":"ID:10949895\nPost:\nText: You need to transform string-valued features to numeric ones in a NumPy array; DictVectorizer does that for you.\n Code: samples = [['asdf', '1'], ['asdf', '0']]\n# turn the samples into dicts\nsamples = [dict(enumerate(sample)) for sample in samples]\n\n# turn list of dicts into a numpy array\nvect = DictVectorizer(sparse=False)\nX = vect.fit_transform(samples)\n\nclf = DecisionTreeClassifier()\nclf.fit(X, ['2', '3'])\n\n Text: Remember to use vect.transform on the test samples, after converting those to dicts.\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\nsklearn.feature_extraction.DictVectorizer.transform\n","label":[[103,117,"Mention"],[461,475,"Mention"],[536,577,"API"],[578,629,"API"]],"Comments":[]}
{"id":60995,"text":"ID:16054128\nPost:\nText: TfidfVectorizer and others text vectorizers classes in scikit-learn just take a list of Python unicode strings as input. You can thus load the text the way you want depending on the source: database query using SQLAlchemy, json stream from an HTTP API, a CSV file or random text files in folders.\n Text: For the last option, if the class information is stored in the folder names holding the text files you can use the load_files utility function. \n\nAPI:\nsklearn.feature_extraction.text.TfidfVectorizer\nsklearn.datasets.load_files\n","label":[[24,39,"Mention"],[443,453,"Mention"],[479,526,"API"],[527,554,"API"]],"Comments":[]}
{"id":60996,"text":"ID:14232097\nPost:\nText: The return value from a RandomForestRegressor is an array of floats:\n Code: In [3]: rf = RandomForestRegressor(n_estimators=10, min_samples_split=2, n_jobs=-1)\n\nIn [4]: rf.fit([[1,2,3],[4,5,6]],[-1,1])\nOut[4]: \nRandomForestRegressor(bootstrap=True, compute_importances=False,\n           criterion='mse', max_depth=None, max_features='auto',\n           min_density=0.1, min_samples_leaf=1, min_samples_split=2,\n           n_estimators=10, n_jobs=-1, oob_score=False,\n           random_state=<mtrand.RandomState object at 0x7fd894d59528>,\n           verbose=0)\n\nIn [5]: rf.predict([1,2,3])\nOut[5]: array([-0.6])\n\nIn [6]: rf.predict([[1,2,3],[4,5,6]])\nOut[6]: array([-0.6,  0.4])\n\n Text: So you're trying to index a float like (-0.6)[1], which is not possible.\n Text: As a side note, the model does not return probabilities.\n\nAPI:\nsklearn.ensemble.RandomForestRegressor\n","label":[[48,69,"Mention"],[852,890,"API"]],"Comments":[]}
{"id":60997,"text":"ID:9016570\nPost:\nText: The source of your confusion is the use of the numpy dot operator with scipy sparse matrices. For numpy and scipy matrices (note matrices, not arrays), the * operator computes dot products, like this:\n Code: In [47]: import scipy.sparse as sp\n\nIn [48]: import numpy as np\n\nIn [49]: D=sp.csr.csr_matrix(np.diagflat(np.random.random(100)))\n\nIn [50]: A=sp.csr.csr_matrix(np.diagflat(np.random.random(100)))\n\nIn [51]: y=np.random.random(100)\n\nIn [52]: y_next = A*(D*y)\n\nIn [53]: print y_next.shape, type(y_next)\n(100,) <type 'numpy.ndarray'>\n\nIn [54]: print y_next\n[ 0.00478446  0.0234117   0.02234696  0.23123913  0.15545059  0.366065\n  0.05674736  0.00238582  0.08701694  0.00099934  0.01687756  0.08190578\n  0.17570485  0.08015175  0.00301985  0.00491663  0.09450794  0.1141585\n  0.02753342  0.0462671   0.02075956  0.21261696  0.82611774  0.09058998\n  0.33545702  0.31456356  0.00260624  0.0449429   0.2431993   0.06302444\n  0.01901411  0.02553964  0.02442291  0.02169692  0.15085474  0.41331208\n  0.09486585  0.01001604  0.48898697  0.03557272  0.22931588  0.0760863\n  0.37686888  0.02801424  0.3280943   0.1695001   0.02890001  0.11712331\n  0.02996858  0.43608624  0.00905409  0.00655408  0.01618681  0.1417559\n  0.0057121   0.0010656   0.02067559  0.05223334  0.14035328  0.0457123\n  0.1273495   0.17688214  0.39300249  0.00625762  0.05356745  0.26719959\n  0.08349373  0.05969248  0.02332782  0.0218782   0.1716797   0.04823102\n  0.03117486  0.00172426  0.08514879  0.09505655  0.17030885  0.00953221\n  0.00134071  0.03951708  0.00243708  0.04247436  0.32152315  0.02039932\n  0.00436897  0.00097858  0.08876351  0.00824626  0.12004067  0.01060241\n  0.11929884  0.01207807  0.10467955  0.02536641  0.602902    0.04115373\n  0.00472405  0.05108167  0.28946041  0.19071962]\n\n Text: Alternatively, the sparse matrix classes own dot method will also work:\n Code: In [55]: print D.dot(A.dot(y)) - (D*(A*y))\n\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n Text: Using numpy.dot seems to result in what is internally  a sparse matrix with an ndarray type. The repeated entries you are seeing are the individual products which get summed down into final dot products, I think.\n\nAPI:\nnumpy.dot\nscipy.sparse.csr_matrix.dot\nnumpy.dot\nnumpy.ndarray\n","label":[[76,79,"Mention"],[1849,1852,"Mention"],[2421,2428,"Mention"],[2561,2570,"API"],[2571,2598,"API"],[2609,2622,"API"]],"Comments":[]}
{"id":60998,"text":"ID:12950584\nPost:\nText: Most machine learning algorithm process input samples that are vector of floats such that a small (often euclidean) distance between a pair of samples means that the 2 samples are similar in a way that is relevant for the problem at hand.\n Text: It is the responsibility of the machine learning practitioner to find a good set of float features to encode. This encoding is domain specific hence there is not general way to build that representation out of the raw data that would work across all application domains (various NLP tasks, computer vision, transaction log analysis...). This part of the machine learning modeling work is called feature extraction. When it involves a lot of manual work, this is often referred to as feature engineering.\n Text: Now for your specific problem, POS tags of a window of words around a word of interest in a sentence (e.g. for sequence tagging such as named entity detection) can be encoded appropriately by using the DictVectorizer feature extraction helper class of scikit-learn.\n\nAPI:\nsklearn.feature_extraction.DictVectorizer\n","label":[[983,997,"Mention"],[1053,1094,"API"]],"Comments":[]}
{"id":60999,"text":"ID:8943575\nPost:\nText: You are stacking vertically in ext and horizontally in lat. \n Text: Try:\n Code: ext = sc.vstack([lat[-1,:], lat, lat[0,:]])\nlat = sc.hstack([ext[:,-1], ext, ext[:,0]])\n\n Text: EDIT:\n Text: The code above will only work if lat is originally a matrix, rather than an array. If that's not the case, you can convert by lat = sc.matrix(lat). Also note that I eliminated the extra square brackets in the argument to hstack and vstack.\n\nAPI:\nscipy.matrix\nscipy.hstack\nscipy.vstack\n","label":[[344,358,"Mention"],[433,439,"Mention"],[444,450,"Mention"],[458,470,"API"],[471,483,"API"],[484,496,"API"]],"Comments":[]}
{"id":61000,"text":"ID:3718712\nPost:\nText: You shouldn't need any custom axes.  The Timeseries Scikit is great, but you don't need it at all to work with dates in matplotlib...\n Text: You'll probably want to use the various functions in matplotlib.dates, plot_date to plot your values, imshow (and\/or pcolor in some cases) to plot your specgrams of various sorts, and matplotlib.mlab.specgram to compute them.\n Text: For your subplots, you'll want to use the sharex kwarg when creating them, so that they all share the same x-axis.  To disable the x-axis labels on some axes when using sharing an x-axis between the plots, you'll need to use something like matplotlib.pyplot.setp(ax1.get_xticklabels(), visible=False). (It's a slightly crude hack, but it's the only way to only display x-axis labels on the bottom subplot when sharing the same x-axis between all subplots)   To adjust the spacing between subplots, see subplots_adjust. \n Text: Hope all that makes some sense... I'll add a quick example of using all of that when I have time later today...\n Text: Edit:\nSo here's a rough example of the idea.  Some things (e.g. multicolored axis labels) shown in your example are rather difficult to do in matplotlib. (Not impossible, but I've skipped them here...)\n Code: import datetime\n\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom matplotlib import mlab\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef main():\n    #-- Make a series of dates\n    start = datetime.datetime(2010,9,15,8,0)\n    end = datetime.datetime(2010,9,15,18,0)\n    delta = datetime.timedelta(seconds=1)\n\n    # Note: \"time\" is now an array of floats, where 1.0 corresponds\n    # to one day, and 0.0 corresponds to 1900 (I think...)\n    # It's _not_ an array of datetime objects!\n    time = mpl.dates.drange(start, end, delta)\n\n    num = time.size\n\n    #-- Generate some data\n    x = brownian_noise(num) \n    y = brownian_noise(num)\n    z = brownian_noise(num)\n\n    plot(x, y, z, time)\n    plt.show()\n\ndef plot(x, y, z, time):\n    fig = plt.figure()\n\n    #-- Panel 1\n    ax1 = fig.add_subplot(311)\n    im, cbar = specgram(x, time, ax1, fig)\n    ax1.set_ylabel('X Freq. (Hz)')\n    ax1.set_title('Fake Analysis of Something')\n\n    #-- Panel 2\n    ax2 = fig.add_subplot(312, sharex=ax1)\n    im, cbar = specgram(y, time, ax2, fig)\n    ax2.set_ylabel('Y Freq. (Hz)')\n\n    #-- Panel 3\n    ax3 = fig.add_subplot(313, sharex=ax1)\n    # Plot the 3 source datasets\n    xline = ax3.plot_date(time, x, 'r-')\n    yline = ax3.plot_date(time, y, 'b-')\n    zline = ax3.plot_date(time, z, 'g-')\n    ax3.set_ylabel(r'Units $(\\mu \\phi)$')\n\n    # Make an invisible spacer...\n    cax = make_legend_axes(ax3)\n    plt.setp(cax, visible=False)\n\n    # Make a legend\n    ax3.legend((xline, yline, zline), ('X', 'Y', 'Z'), loc='center left', \n            bbox_to_anchor=(1.0, 0.5), frameon=False)\n\n    # Set the labels to be rotated at 20 deg and aligned left to use less space\n    plt.setp(ax3.get_xticklabels(), rotation=-20, horizontalalignment='left')\n\n    # Remove space between subplots\n    plt.subplots_adjust(hspace=0.0)\n\ndef specgram(x, time, ax, fig):\n    \"\"\"Make and plot a log-scaled spectrogram\"\"\"\n    dt = np.diff(time)[0] # In days...\n    fs = dt * (3600 * 24) # Samples per second\n\n    spec_img, freq, _ = mlab.specgram(x, Fs=fs, noverlap=200)\n    t = np.linspace(time.min(), time.max(), spec_img.shape[1])\n\n    # Log scaling for amplitude values\n    spec_img = np.log10(spec_img)\n\n    # Log scaling for frequency values (y-axis)\n    ax.set_yscale('log')\n\n    # Plot amplitudes\n    im = ax.pcolormesh(t, freq, spec_img)\n\n    # Add the colorbar in a seperate axis\n    cax = make_legend_axes(ax)\n    cbar = fig.colorbar(im, cax=cax, format=r'$10^{%0.1f}$')\n    cbar.set_label('Amplitude', rotation=-90)\n\n    ax.set_ylim([freq[1], freq.max()])\n\n    # Hide x-axis tick labels\n    plt.setp(ax.get_xticklabels(), visible=False)\n\n    return im, cbar\n\ndef make_legend_axes(ax):\n    divider = make_axes_locatable(ax)\n    legend_ax = divider.append_axes('right', 0.4, pad=0.2)\n    return legend_ax\n\ndef brownian_noise(num):\n    x = np.random.random(num) - 0.5\n    x = np.cumsum(x)\n    return x\n\n\nif __name__ == '__main__':\n    main()\n\n\nAPI:\nmatplotlib.dates\nmatplotlib.pyplot.plot_date\nmatplotlib.pyplot.imshow\nmatplotlib.pyplot.pcolor\nmatplotlib.mlab.specgram\nmatplotlib.pyplot.setp\nmatplotlib.axes.Axes.get_xticklabels\nmatplotlib.pyplot.subplots_adjust\n","label":[[235,244,"Mention"],[266,272,"Mention"],[281,287,"Mention"],[660,681,"Mention"],[899,914,"Mention"],[4238,4265,"API"],[4266,4290,"API"],[4291,4315,"API"],[4364,4400,"API"],[4401,4434,"API"]],"Comments":[]}
{"id":61001,"text":"ID:17018759\nPost:\nText: HashingVectorizer will work if you iteratively chunk your data into batches of 10k or 100k documents that fit in memory for instance.\n Text: You can then pass the batch of transformed documents to a linear classifier that supports the partial_fit method (e.g. SGDClassifier or PassiveAggressiveClassifier) and then iterate on new batches.\n Text: You can start scoring the model on a held-out validation set (e.g. 10k documents) as you go to monitor the accuracy of the partially trained model without waiting for having seen all the samples.\n Text: You can also do this in parallel on several machines on partitions of the data and then average the resulting coef_ and intercept_ attribute to get a final linear model for the all dataset.\n Text: I discuss this in this talk I gave in March 2013 at PyData: http:\/\/vimeo.com\/63269736\n Text: There is also sample code in this tutorial on paralyzing scikit-learn with IPython.parallel taken from: https:\/\/github.com\/ogrisel\/parallel_ml_tutorial\n\nAPI:\nsklearn.feature_extraction.text.HashingVectorizer\nsklearn.linear_model.SGDClassifier\nsklearn.linear_model.PassiveAggressiveClassifier\n","label":[[24,41,"Mention"],[284,297,"Mention"],[301,328,"Mention"],[1021,1070,"API"],[1071,1105,"API"],[1106,1154,"API"]],"Comments":[]}
{"id":61002,"text":"ID:6307292\nPost:\nText: Have a look at numpy.interp:\n Text: http:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.interp.html\n\n Text: (EDIT: numpy.interp only provides linear interpolation which, evidently, is not what the OP is looking for. Instead use the scipy methods like interp1d using kind='nearest')\n Text: http:\/\/docs.scipy.org\/doc\/scipy\/reference\/interpolate.html\n Text: What it sounds like you want to do is use the altitude points of one data set to interpolate the values of the other. This can be done pretty easily with either the numpy method or one of the scipy interpolation methods.\n\nAPI:\nnumpy.interp\nnumpy.interp\nscipy.interpolate.interp1d\n","label":[[280,288,"Mention"],[637,663,"API"]],"Comments":[]}
{"id":61003,"text":"ID:10170669\nPost:\nText: You just need to have your values from smallest to largest :).  It shouldn't be a problem for you @a different ben, but beware readers from the future, depth[indices] will throw a TypeError if depth is a list instead of a numpy array! \n Code: >>> indices = np.argsort(depth)\n>>> depth = depth[indices]\n>>> temp = temp[indices]\n>>> splrep(depth, temp)\n(array([-7.05, -7.05, -7.05, -7.05, -5.03, -4.05, -3.  , -2.01, -1.01,\n        1.  ,  1.  ,  1.  ,  1.  ]), array([ 16.0147    ,  15.54473241,  16.90606794,  14.55343229,\n        15.12525673,  14.0717599 ,  15.19657895,  14.40437622,\n        14.7036    ,   0.        ,   0.        ,   0.        ,   0.        ]), 3)\n\n Text: Hat tip to @FerdinandBeyer for the suggestion of argsort instead of my ugly \"zip the values, sort the zip, re-assign the values\" method.\n\nAPI:\nnumpy.argsort\n","label":[[748,755,"Mention"],[842,855,"API"]],"Comments":[]}
